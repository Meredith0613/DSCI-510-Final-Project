title,company,description,location,role_query,skills,skills_extracted,salary_raw
"[remote] data scientist, market insights and analytics",sharkninja,"note: the job is a remote job and is open to candidates in usa. sharkninja is a global product design and technology company known for its innovative lifestyle solutions. the data scientist role involves building and deploying advanced models to drive commercial growth, collaborating with various teams to integrate data science into decision-making processes. responsibilities • design, build, and deploy predictive and prescriptive models (forecasting, elasticity, segmentation, attribution, etc.) in the cloud • combine and analyze large-scale data sources — including pos, retailer, amazon vendor central, and syndicated data — to identify key performance drivers • collaborate closely with market insights, category, and sales teams to embed data science into commercial decision-making • automate and scale analytics using python, sql, and snowflake to improve speed and consistency • partner with data foundations and visualization teams to strengthen data pipelines and power bi reporting • translate complex model results into clear business recommendations and present them to senior stakeholders skills • 4+ years of experience in data science or advanced analytics, ideally within consumer goods, retail, or e-commerce • proficiency building a variety of statistical and machine learning models (e.g. time series forecasting, linear/logistic regression, tree based and deep learning models) • proficiency in python and/or r for machine learning (e.g. scikit-learn, prophet, pytorch etc.), sql, and visualization tools (power bi preferred) • experience working with big data in the cloud (snowflake, pyspark etc.) and/or experience deploying and monitoring machine learning models in production • strong communication and problem-solving skills with the ability to translate technical insights into business outcomes • bachelor's or master's degree in data science, statistics, computer science, or other strongly quantitative field benefits • medical insurance • dental insurance • vision insurance • flexible spending accounts • health savings accounts (hsa) with company contribution • 401(k) retirement plan with matching • employee stock purchase program • life insurance • ad&d • short-term disability insurance • long-term disability insurance • generous paid time off • company holidays • parental leave • identity theft protection • pet insurance • pre-paid legal insurance • back-up child and eldercare days • product discounts • referral bonus program • competitive health insurance • retirement plans • wellness programs company overview • sharkninja is a consumer goods company that offers beauty, home care, kitchen appliances, and outdoor electronic products. it was founded in 1995, and is headquartered in needham, massachusetts, usa, with a workforce of 1001-5000 employees. its website is http://www.sharkninja.com. company h1b sponsorship • sharkninja has a track record of offering h1b sponsorships, with 37 in 2025, 33 in 2024, 18 in 2023, 22 in 2022, 29 in 2021, 24 in 2020. please note that this does not guarantee sponsorship for this specific role.",anywhere,Data Scientist,"['cloud', 'data pipeline', 'deep learning', 'machine learning', 'power bi', 'pyspark', 'python', 'pytorch', 'r', 'recommendation', 'regression', 'scikit-learn', 'snowflake', 'spark', 'sql', 'statistics', 'time series']","['cloud', 'data pipeline', 'deep learning', 'machine learning', 'power bi', 'pyspark', 'python', 'pytorch', 'r', 'recommendation', 'regression', 'scikit-learn', 'snowflake', 'spark', 'sql', 'statistics', 'time series']",122K–149K a year
data scientist (remote),outlier ai,"join a global community of talented professionals to shape the future of ai. earn up to $15 usd/hr and additional rewards based on quality of submission. outlier is committed to improving the intelligence & safety of ai models. owned and operated by scale ai , we've recently been featured in forbes for partnering experts with top ai labs to provide the high quality data for llms. we believe ai can only perform as well as the data it's trained on. that's why we work with contributors from all over the world , who help improve ai models by providing expert human feedback . this data has led to ai advancements for the world's leading ai labs and large language model builders. we've built a best-in-class remote work platform for our freelance contributors to provide valuable, specialized skills, and we in turn strive to provide them with a positive experience based on our core pillars of reliability, transparency, and flexibility. what you will be doing we are looking for someone who speaks fluent english to contribute their expertise toward training and refining cutting-edge ai systems. • adopt a ""user mindset"" to produce natural data to meet the realistic needs you have or would use ai for. • use the tool of rubrics to address user needs in a structured way. • evaluate ai outputs by reviewing and ranking reasoning and problem-solving responses from large language models. • contribute across projects depending on your specific skillset and experience. what we're looking for • education : bachelor's degree or higher (or currently enrolled). • analytical and problem-solving skills : ability to develop complex, professional-level prompts and evaluate nuanced ai reasoning. • strong writing : clear, concise, and engaging writing to explain decisions or critique responses. • attention to detail : commitment to accuracy and ability to assess technical aspects of model outputs. nice to haves: • experience in fields like literature, creative writing, history, philosophy, theology, etc. • prior writing or editorial experience (content strategist, technical writer, editor, etc.). • interest or background in ai, machine learning, or creative tech tools. compensation and benefits earn up to $15 usd/hr, paid out weekly rates vary based on quality, accuracy, and time spent. paid via paypal & airtm free access to model playground interact, experiment and engage with leading large language models free of cost flexible schedule and time commitment no contracts, no 9-to-5. you control your schedule. (most experts spend 5-10 hours/week, up to 40 hours working from home join a global community of coding experts join a global network of experts contributing to advanced ai tools disclaimer: for non-core work, such as during initial project onboarding or project overtime phases, lower rates may apply. certain projects offer incentive payments. please review the payment terms for each project. equal opportunity employer: outlier is committed to fostering a diverse and inclusive work environment. we welcome applicants from all backgrounds and celebrate diversity in our workforce.",colorado,Data Scientist,"['machine learning', 'r']","['machine learning', 'r']",
"data scientist, data and machine learning, wwps us federal","amazon web services, inc.","description application deadline: applications will be accepted on an ongoing basis are you excited to help the us intelligence community design, build, and implement ai algorithms, including advanced generative ai solutions, to augment decision making while meeting the highest standards for reliability, transparency, and scalability? the amazon web services (aws) us federal professional services team works directly with us intelligence community agencies and other public sector entities to achieve their mission goals through the adoption of machine learning (ml) and generative ai methods. we build models for text, image, video, audio, and multi-modal use cases, leveraging both traditional ml approaches and state-of-the-art generative models including large language models (llms), text-to-image generation, and other advanced ai capabilities to fit the mission. our team collaborates across the entire aws organization to bring access to product and service teams, to get the right solution delivered and drive feature innovation based on customer needs. at aws, we're hiring experienced data scientists with a background in both traditional and generative ai who can help our customers understand the opportunities their data presents, and build solutions that earn the customer trust needed for deployment to production systems. in this role, you will work closely with customers to deeply understand their data challenges and requirements, and design tailored solutions that best fit their use cases. you should have broad experience building models using all kinds of data sources, and building data-intensive applications at scale. you should possess excellent business acumen and communication skills to collaborate effectively with stakeholders, develop key business questions, and translate requirements into actionable solutions. you will provide guidance and support to other engineers, sharing industry best practices and driving innovation in the field of data science and ai. this position requires that the candidate selected must currently possess and maintain an active ts/sci security clearance. the position further requires the candidate to opt into a commensurate clearance for each government agency for which they perform aws work. key job responsibilities as a data scientist, you will: • collaborate with ai/ml scientists and architects to research, design, develop, and evaluate ai algorithms to address real-world challenges • interact with customers directly to understand the business problem, help and aid them in implementation of ai solutions, deliver briefing and deep dive sessions to customers and guide customer on adoption patterns and paths to production. • create and deliver best practice recommendations, tutorials, blog posts, sample code, and presentations adapted to technical, business, and executive stakeholder • provide customer and market feedback to product and engineering teams to help define product direction • this position may require up to 25% local travel. about the team why aws? amazon web services (aws) is the world’s most comprehensive and broadly adopted cloud platform. we pioneered cloud computing and never stopped innovating — that’s why customers from the most successful startups to global 500 companies trust our robust suite of products and services to power their businesses. diverse experiences aws values diverse experiences. even if you do not meet all of the qualifications and skills listed in the job description, we encourage candidates to apply. if your career is just starting, hasn’t followed a traditional path, or includes alternative experiences, don’t let it stop you from applying. inclusive team culture here at aws, it’s in our nature to learn and be curious. our employee-led affinity groups foster a culture of inclusion that empower us to be proud of our differences. ongoing events and learning experiences, including our conversations on race and ethnicity (core) and amazecon (diversity) conferences, inspire us to never stop embracing our uniqueness. work/life balance we value work-life harmony. achieving success at work should never come at the expense of sacrifices at home, which is why flexible work hours and arrangements are part of our culture. when we feel supported in the workplace and at home, there’s nothing we can’t achieve in the cloud. mentorship & career growth we’re continuously raising our performance bar as we strive to become earth’s best employer. that’s why you’ll find endless knowledge-sharing, mentorship and other career-advancing resources here to help you develop into a better-rounded professional. basic qualifications • bachelor's degree in computer science, engineering, a related technical field or equivalent • 3+ years of experience building models for business applications. • experience with algorithms, data structures, parsing, numerical optimization, data mining, parallel and distributed computing, high-performance computing, neural deep learning methods and/or machine learning • experience using python and hands on experience building models with deep learning frameworks (i.e. tensorflow, keras, pytorch, mxnet). • current, active us government security clearance of ts/sci. preferred qualifications • masters or phd degree in computer science, engineering, mathematics, operations research, or in a highly quantitative field. • aws experience preferred, with proficiency in a range of aws services (e.g., sagemaker, bedrock, ec2, ecs, eks, opensearch). • practical experience in solving complex problems in an applied environment. • hands on experience building models with deep learning frameworks like pytorch, tensorflow, or jax. • experience building applications leveraging genai. amazon is an equal opportunity employer and does not discriminate on the basis of protected veteran status, disability, or other legally protected status. our inclusive culture empowers amazonians to deliver the best results for our customers. if you have a disability and need a workplace accommodation or adjustment during the application and hiring process, including support for the interview or onboarding process, please visit https://amazon.jobs/content/en/how-we-hire/accommodations for more information. if the country/region you’re applying in isn’t listed, please contact your recruiting partner. the base salary range for this position is listed below. for salaried roles, your amazon package will include listed sign-on payments and restricted stock units (rsus). final compensation will be determined based on factors including experience, qualifications, and location. amazon also offers comprehensive benefits including health insurance (medical, dental, vision, prescription, basic life & ad&d insurance and option for supplemental life plans, eap, mental health support, medical advice line, flexible spending accounts, adoption and surrogacy reimbursement coverage), 401(k) matching, paid time off, and parental leave. learn more about our benefits at https://amazon.jobs/en/benefits/us-benefits-and-stock. colorado $136,000 - $184,000 annually national $125,500 - $212,800 annually","denver, co",Data Scientist,"['aws', 'cloud', 'deep learning', 'excel', 'keras', 'machine learning', 'python', 'pytorch', 'r', 'recommendation', 'scala', 'tensorflow']","['aws', 'cloud', 'deep learning', 'excel', 'keras', 'machine learning', 'python', 'pytorch', 'r', 'recommendation', 'scala', 'tensorflow']",136K–184K a year
"principal data scientist i - quantumblack, ai by mckinsey",mckinsey & company,"your growth driving lasting impact and building long-term capabilities with our clients is not easy work. you are the kind of person who thrives in a high performance/high reward culture - doing hard things, picking yourself up when you stumble, and having the resilience to try another way forward. in return for your drive, determination, and curiosity, we'll provide the resources, mentorship, and opportunities you need to become a stronger leader faster than you ever thought possible. your colleagues—at all levels—will invest deeply in your development, just as much as they invest in delivering exceptional results for clients. every day, you'll receive apprenticeship, coaching, and exposure that will accelerate your growth in ways you won’t find anywhere else. when you join us, you will have: • continuous learning: our learning and apprenticeship culture, backed by structured programs, is all about helping you grow while creating an environment where feedback is clear, actionable, and focused on your development. the real magic happens when you take the input from others to heart and embrace the fast-paced learning experience, owning your journey. • a voice that matters: from day one, we value your ideas and contributions. you’ll make a tangible impact by offering innovative ideas and practical solutions, all while upholding our unwavering commitment to ethics and integrity. we not only encourage diverse perspectives, but they are critical in driving us toward the best possible outcomes. • global community: with colleagues across 65+ countries and over 100 different nationalities, our firm’s diversity fuels creativity and helps us come up with the best solutions for our clients. plus, you’ll have the opportunity to learn from exceptional colleagues with diverse backgrounds and experiences. • world-class benefits: on top of a competitive salary (based on your location, experience, and skills), we provide a comprehensive benefits package to enable holistic well-being for you and your family. your impactas a principal data scientist i, you will tackle complex data challenges, develop advanced models, and create impactful solutions to optimize client performance across diverse industries. you’ll mentor others, contribute to cutting-edge r&d, and collaborate with world-class talent in a dynamic environment that fosters innovation and growth. in this role, you will influence client recommendations by developing data science solutions, writing optimized code, and applying advanced methods to improve performance. you will explore data ecosystems, create statistical models to solve operational challenges, and mentor other data scientists. additionally, you will shape quantumblack’s r&d roadmap and contribute to deep learning and machine learning research. your work will create real-world impact. by uncovering insights from complex data and delivering innovative solutions, you will help clients maintain a competitive edge and transform their operations, driving measurable and lasting improvements. you will be based in one of our north america locations and collaborate closely with data scientists, data engineers, machine learning engineers, designers, and product managers around the world. together, you’ll work on interdisciplinary projects to solve complex business challenges across a range of industries. collaborating with quantumblack leadership, client executives, and technical experts, you will lead the development and application of cutting-edge machine learning and ai solutions to drive measurable business impact you’ll thrive in an unparalleled environment for growth. you’ll tackle high-impact projects, connect technology with business value, and collaborate with inspiring multidisciplinary teams, gaining a holistic perspective of ai’s transformative potential while growing as a technologist and leader. your qualifications and skills • bachelors, masters or phd level in a discipline such as computer science, machine learning, applied statistics, mathematics, engineering or artificial intelligence • 8-11+ years of deep technical experience in distributed computing, machine learning and statistics related work • programming experience in languages such as python, r, scala, sql • knowledge of distributed computing or nosql technologies is a plus • proven application of advanced analytical, data science and statistical methods in the commercial world • client-facing skills (e.g. working in close-knit teams on topics such as data warehousing, machine learning) • while we advocate for using the right tech for the right task, we often leverage the following technologies: python, pyspark, the pydata stack, sql, airflow, databricks, our own open-source data pipelining framework called kedro, dask/rapids, container technologies such as docker and kubernetes, cloud solutions such as aws, gcp, and azure, and more • thought leadership or people leadership experience (e.g. managed project teams or direct reports) • exceptional time management to meet your responsibilities in a complex and largely autonomous work environment • good presentation and communication skills with a knack for explaining complex analytical concepts to people from other fields • strong communication skills, both verbal and written, in english and local office language(s), with the ability to adjust your style to suit different perspectives and seniority levels • willingness to travel","seattle, wa (+12 others)",Data Scientist,"['airflow', 'aws', 'azure', 'cloud', 'databricks', 'deep learning', 'gcp', 'machine learning', 'pyspark', 'python', 'r', 'recommendation', 'scala', 'spark', 'sql', 'statistics']","['airflow', 'aws', 'azure', 'cloud', 'databricks', 'deep learning', 'gcp', 'machine learning', 'pyspark', 'python', 'r', 'recommendation', 'scala', 'spark', 'sql', 'statistics']",
"analyst i, data science",liberty mutual insurance,"description at liberty mutual, the capacity modeling and optimization team within claims and service data science builds advanced forecasting and staffing optimization models that enable best in class workforce planning across our claims and service lines of business. we convert operational data into decision grade analytics that improve assignment strategies, benchmark productivity, and align capacity with demand. we are hiring a junior data scientist to support the work effort assessment and modeling. with guidance from senior team members, you will help build pipelines and quality checks, develop statistical and simulation models that explain and predict claim/exposure durations and action frequencies, evaluate segmentation and assignment strategies, and support work effort-based staffing forecasts and optimization. this role may have in office requirements based on candidate location. level of position offered will be based on skills and experience at manager discretion. responsibilities: • support development of scalable data pipelines and automated quality controls (schema, completeness, drift) across multiple sources. • build statistical models for duration and action frequency; build exposure/phase level features and run exploratory/variance analyses. • assist in clustering/segmentation and hypothesis testing to quantify efficiency and service impacts. help build and run simulation models to compare assignment policies; analyze results and create scenario comparisons. • help building work effort-based demand forecasts and staffing models; implement components of optimization models with supervision. • maximize usable data by applying censoring aware methods, imputation, and reconciliation, document assumptions and ensure reproducibility. • communicate findings through dashboards, reports, and presentations; collaborate with claims and service partners to move insights into practice. • follow mlops best practices (git, reproducible workflows, experiment tracking) under mentorship. preferred skills and experience: • solid foundation in statistics and ml: regression/glm, inference, experimental design; familiarity with survival/censoring, time series, and hierarchical models. • exposure to operations research and simulation: queueing concepts, discrete event or agent-based simulation; familiarity with or tools or pyomo and simpy is a plus. • proficiency in python and sql; experience with pandas, numpy, scikit learn, stats models; visualization using plotly/seaborn and dashboarding (e.g., dash) is a plus. • experience writing clean, tested code with version control (git); familiarity with mlflow and workflow orchestration (e.g., airflow) is a plus. • comfort working with large, complex operational datasets; strong problem solving, communication, and collaboration skills. additional skills and experiences that are nice to have: • coursework or experience in claims/service operations or workforce management. • familiarity with cloud platforms (aws/gcp/azure) and distributed processing (spark). • exposure to docker and ci/cd; experience deploying models or dashboards with supervision. qualifications • solid knowledge of predictive analytics techniques and statistical diagnostics of models. • advance knowledge of predictive toolset; expert resource for tool development. • demonstrated ability to exchange ideas and convey complex information clearly and concisely. • has a value-driven perspective with regard to understanding of work context and impact. • competencies typically acquired through a master’s degree (scientific field of study) and 0-1 years of relevant experience or a bachelor’s degree (scientific field of study) and 3+ years of relevant experience. about us pay philosophy: the typical starting salary range for this role is determined by a number of factors including skills, experience, education, certifications and location. the full salary range for this role reflects the competitive labor market value for all employees in these positions across the national market and provides an opportunity to progress as employees grow and develop within the role. some roles at liberty mutual have a corresponding compensation plan which may include commission and/or bonus earnings at rates that vary based on multiple factors set forth in the compensation plan for the role. at liberty mutual, our goal is to create a workplace where everyone feels valued, supported, and can thrive. we build an environment that welcomes a wide range of perspectives and experiences, with inclusion embedded in every aspect of our culture and reflected in everyday interactions. this comes to life through comprehensive benefits, workplace flexibility, professional development opportunities, and a host of opportunities provided through our employee resource groups. each employee plays a role in creating our inclusive culture, which supports every individual to do their best work. together, we cultivate a community where everyone can make a meaningful impact for our business, our customers, and the communities we serve. we value your hard work, integrity and commitment to make things better, and we put people first by offering you benefits that support your life and well-being. to learn more about our benefit offerings please visit: https://lmi.co/benefits liberty mutual is an equal opportunity employer. we will not tolerate discrimination on the basis of race, color, national origin, sex, sexual orientation, gender identity, religion, age, disability, veteran's status, pregnancy, genetic information or on any basis prohibited by federal, state or local law. fair chance notices • california • los angeles incorporated • los angeles unincorporated • philadelphia • san francisco",anywhere,Data Scientist,"['airflow', 'aws', 'azure', 'cloud', 'clustering', 'dashboard', 'data pipeline', 'gcp', 'numpy', 'pandas', 'python', 'r', 'regression', 'scala', 'seaborn', 'spark', 'sql', 'statistics', 'time series']","['airflow', 'aws', 'azure', 'cloud', 'clustering', 'dashboard', 'data pipeline', 'gcp', 'numpy', 'pandas', 'python', 'r', 'regression', 'scala', 'seaborn', 'spark', 'sql', 'statistics', 'time series']",
healthcare enterprise analytics data scientist senior,amerihealth caritas,"role overview we are seeking highly motivated data scientists to join our team and help drive data-driven innovation in healthcare. this role combines advanced analytics, machine learning, and artificial intelligence to build solutions that improve member outcomes, reduce cost of care, and support population health management. you will work closely with cross-functional teams, including clinical, product, and it, to design and implement scalable models and ai applications that directly impact healthcare delivery. work arrangement • this is a 100% remote position but you must live in est or cst time zones responsibilities • design, develop, and deploy machine learning and ai models to support healthcare use cases such as risk prediction, care management, disease progression, and utilization forecasting • perform data exploration, wrangling, and advanced statistical analysis using structured and unstructured healthcare data (e.g., claims, emr, sdoh, labs) • collaborate with data engineers and mlops teams to operate models in a scalable and maintainable environment • partner with clinical and business stakeholders to understand needs and translate them into technical solutions • evaluate model performance using appropriate metrics and ensure model fairness, transparency, and regulatory compliance • contribute to the design and enhancement of ai pipelines and reusable components • develop dashboards and data visualizations to communicate results and support decision-making • stay current on the latest ai/ml technologies, healthcare trends, and research, and apply them to solve emerging challenges education and experience • master’s degree in data science, computer science, statistics, biomedical informatics, or a related field (phd preferred) • at least five (5)+ years of hands-on experience in data science in a healthcare setting • strong knowledge of python, r, sql, and common ml libraries (e.g., scikit-learn, tensorflow, pytorch, xgboost) • experience with cloud platforms (e.g., azure) and tools like databricks • understanding of healthcare data types and standards (e.g., icd, cpt, hl7, fhir) • experience working with large datasets and distributed computing frameworks (e.g., spark) our comprehensive benefits package flexible work solutions including remote options, hybrid work schedules, competitive pay, paid time off including holidays and volunteer events, health insurance coverage for you and your dependents on day 1, 401(k) tuition reimbursement and more. your career starts now. we’re looking for the next generation of healthcare leaders. at amerihealth caritas, we’re passionate about helping people get care, stay well, and build healthy communities. as one of the nation's leaders in healthcare solutions, we offer our associates the opportunity to impact the lives of millions of people through our national footprint of products, services, and award-winning programs. amerihealth caritas is seeking talented, passionate individuals to join our team. together we can build healthier communities. if you want to make a difference, we’d like to hear from you. headquartered in newtown square, amerihealth caritas is a mission-driven organization with more than 30 years of experience. we deliver comprehensive, outcomes-driven care to those who need it most. we offer integrated managed care products, pharmaceutical benefit management and specialty pharmacy services, behavioral health services, and other administrative services. discover more about us at www.amerihealthcaritas.com.",united states,Data Scientist,"['azure', 'cloud', 'dashboard', 'databricks', 'machine learning', 'python', 'pytorch', 'r', 'scala', 'scikit-learn', 'spark', 'sql', 'statistics', 'tensorflow', 'xgboost']","['azure', 'cloud', 'dashboard', 'databricks', 'machine learning', 'python', 'pytorch', 'r', 'scala', 'scikit-learn', 'spark', 'sql', 'statistics', 'tensorflow', 'xgboost']",
data scientist - ai/ml (colorado only),billgo,"billgo is seeking a data scientist with deep ai and machine learning expertise to help shape the future of data-driven innovation in fintech. the role involves developing intelligent systems for risk modeling, fraud prevention, and customer insights, directly impacting financial decisions and operational efficiency. responsibilities • develop and deploy advanced machine learning models to optimize customer targeting, payment monitoring and growth and operational efficiency opportunities • build forecasting models to improve transaction accuracy, detect anomalies, and assess financial risk • clean, transform, and model large, high-velocity financial datasets with attention to data integrity and compliance • collaborate with product to integrate ai solutions into production systems for real-time financial decisioning • lead a/b tests and model performance evaluations to validate model effectiveness and regulatory compliance • translate technical findings into actionable insights for business leaders and compliance teams • stay on top of advancements in generative ai, llms, and financial ai applications to guide innovation strategy skills • bachelor's or master's degree in data science, computer science, statistics, or related field • 3+ years of experience in a data science or ai-focused role within fintech, banking, or payments • expertise in python, machine learning frameworks (scikit-learn, tensorflow, pytorch), and data pipelines • strong background in supervised/unsupervised learning, anomaly detection, nlp, and generative ai • familiarity with financial data structures, regulatory standards (e.g., pci-dss, gdpr), and model governance • experience with cloud platforms such as snowflake for ml deployment • experience in fraud analytics, risk scoring, or payment decision models • understanding of mlops and continuous model monitoring in regulated environments • familiarity with financial transaction data, open banking apis, or real-time payments systems • experience developing llm-powered assistants or ai copilots for financial operations or support • strong data storytelling and visualization skills (tableau preferred) company overview • billgo is changing the status quo with a modern bill managements and payments platform. it was founded in 2015, and is headquartered in fort collins, colorado, usa, with a workforce of 51-200 employees. its website is https://www.billgo.com. company h1b sponsorship • billgo has a track record of offering h1b sponsorships, with 2 in 2025, 2 in 2024, 3 in 2023, 9 in 2022, 8 in 2021, 5 in 2020. please note that this does not guarantee sponsorship for this specific role.","fort collins, co",Data Scientist,"['cloud', 'data pipeline', 'machine learning', 'nlp', 'python', 'pytorch', 'r', 'scikit-learn', 'snowflake', 'statistics', 'tableau', 'tensorflow']","['cloud', 'data pipeline', 'machine learning', 'nlp', 'python', 'pytorch', 'r', 'scikit-learn', 'snowflake', 'statistics', 'tableau', 'tensorflow']",
business intelligence data scientist,bluesky,"bluesky's vision is to make social more like the web itself. we're building a social media app, but beyond that, we've created a protocol that supports a broad ecosystem of applications and clients. imagine a network of applications — from microblogging to photos to video to long-form writing — all interconnected and accessible with a shared identity. as a business intelligence data scientist at bluesky, you’ll work closely with leadership to maintain our business intelligence and produce insights into product behavior. you’ll be a great fit for this role if you have experience with analytics, statistics, and big data, and love gleaning insights from data. responsibilities: • develop and maintain the internal business intelligence dashboard • develop statistical models to help us understand our users and their needs. • support product team by analyzing features and their impacts on user behavior you might be a good fit if you: • have 5+ years experience in data science, with a focus on social media applications • strong skills in experimental design and analysis, both quantitative and qualitative • have experience dealing with high volume data • have strong sql skills and experience with postgres or bigquery • have strong programming skills in python • experience working with pytorch is a plus • experience working with retool is a plus • experience working with dataform or dbt is a plus • have knowledge of data privacy and security best practices • like working on small, fast-moving teams. • have read the at protocol docs and align with our vision for open social we’re a fully remote team, but an overlap of working hours with pst is required, and for this role, proximity and willingness to travel to seattle is a plus. we offer health, dental, and vision insurance. if you want to work with an excellent team and help define the future of social, we encourage you to apply. additional notes: • the anticipated base salary range for this position is $150,000-$175,000, excluding equity. equity will be considered in the total compensation package. final base salary for this role will be based on the individual’s geographic location, as well as experience level, skill set, training, licenses and certifications.",anywhere,Data Scientist,"['bigquery', 'business intelligence', 'dashboard', 'dbt', 'excel', 'python', 'pytorch', 'r', 'sql', 'statistics']","['bigquery', 'business intelligence', 'dashboard', 'dbt', 'excel', 'python', 'pytorch', 'r', 'sql', 'statistics']",
sr. data scientist,y & l consulting inc.,"sr. data scientist about us chorus is an ai startup that recently spun out of alphabet's moonshot factory. our mission is to transform how the world's goods are made, moved, and managed. we stream data from our customers' physical assets into advanced applications that show them what's happening now, what's likely to happen next, and most importantly what to do about it. we don't sell sensors or raw data; we sell solutions to real business problems. our customers rely on us to turn streams of iot data into the intelligence that runs their operations. we're a fun, supportive, and sharp team building the data refinery that makes this possible. if you want to work at the intersection of ai, iot, and real-world problem-solving, this is the place. company details: supply chain ai, startup with strong revenue growth, 40+ employees. compensation: base $180,000-220,000 (based on location and experience) + bonus + equity location: ability to work hybrid in mountain view, ca **or** boulder, co. the role we're looking for a senior data scientist to build data-intensive applications that solve real customer problems. you'll work directly with b2b customers to deeply understand their operational challenges, then design and build visually-engaging applications that put the right insights and actions at their fingertips. this isn't about creating reports or static visualizations, it's about building the brains and hands that become essential to how our customers run their business. in this role you ll embed in a multi-talented and very supportive ai/ml team. at your fingertips you ll have a world-class data platform, public and proprietary apis serving custom model predictions and ai capabilities to make it all sing. we need you to pull it all together. understand the customer deeply, make the data tell a story, and build the thin-pane-of-glass that will help them win. who you are great with people. an empathetic storyteller. a data expert. an opinionated designer. what you'll do • partner with customers to identify their most critical operational problems and design applications that address them • build data-intensive, visually-engaging applications that intelligently combine apis and iot data into powerful solutions to customer problems • design intuitive interfaces that surface complex patterns, predictions, and recommendations without requiring data expertise • define the metrics and statistical approaches that best serve each customer's specific needs • translate customer problems into technical requirements. communicate insights and application capabilities to both technical and business stakeholders • drive implementation in hands-on mode to turn requirements into reality must have • 3-5 years of experience in a data science, analytics, or quantitative role • master's degree or phd in a quantitative field • exceptional communication skills • b2b customer-facing experience • strong statistical foundation with the intuition to choose the right approach and visualization for each problem • proficiency in data tools like sql, python, pandas, matplotlib/seaborn, xgboost, etc • track record of building solutions that customers adopted and relied on • genuine desire to work in a fast-paced startup environment nice to have • frontend development experience (react, flask, streamlit, or similar frameworks) • experience with big data tools (spark, presto, trino, query optimization) • cloud data platform experience (google cloud platform/bigquery, snowflake, or databricks) • data engineering experience (etl, airflow, dbt, streaming) • production ml engineering experience • experience using ai to code and building customer-facing products that integrate ai. • startup experience","boulder, co",Data Scientist,"['airflow', 'bigquery', 'cloud', 'databricks', 'dbt', 'etl', 'google cloud', 'matplotlib', 'pandas', 'python', 'r', 'recommendation', 'seaborn', 'snowflake', 'spark', 'sql', 'xgboost']","['airflow', 'bigquery', 'cloud', 'databricks', 'dbt', 'etl', 'google cloud', 'matplotlib', 'pandas', 'python', 'r', 'recommendation', 'seaborn', 'snowflake', 'spark', 'sql', 'xgboost']",180K–220K a year
sr/staff data scientist (remote - us),bnsf railway,"be part of a team that values safety, inclusion, and excellence we are one of the largest u.s. railroads transporting the nation’s freight across 28 western states and 3 canadian provinces. as a member of our team, you will play a role in supporting the movement of essential products and materials that help feed, clothe, supply, and power communities throughout america and the world. bnsf | tech: innovating and transforming the future of freight rail bnsf | tech is the technology division making bnsf the preeminent freight and mobility company in north america. are you ready to drive change? if you are passionate about making a difference and eager to advance your career in a dynamic and supportive environment, we want you on our team! join us in reshaping the future of freight rail and discover a fulfilling career where your contributions matter. we are committed to a culture where all employees are included, belong, and have equal opportunity to achieve their full potential. come make a difference with us! learn more about bnsf and our benefits job location: remote us anticipated start date: 01/01/2026 the us base salary range for this full-time position is provided below: salary range: $165,000-$300,000 the range represents the amount bnsf | tech reasonably expects to pay for the position based on the level, scope, and responsibilities of the role. individual compensation and level of position offered is determined by the hiring location and additional factors including but not limited to job-related skills, experience, and relevant education or training. in addition to base pay and bonus eligibility, bnsf offers a comprehensive benefits package. this is a full-time remote position. employees may work from anywhere within the contiguous 48 states of the united states travel is up to 20%. employees will be required to occasionally travel to our corporate headquarters in fort worth, tx for in person meetings. travel expenses for business needs will be covered by bnsf this position is open to candidates who are currently authorized to work in the united states. we are also open to sponsoring h-1b transfers, tn nonimmigrant status, and stem opt candidates with at least 2 years of remaining eligibility. apply early as this job may be removed or filled prior to the closing date, which is approximately seven (7) days after the posting date. data & ai: lead our charge into the future as an ai company by transforming our data assets into a real time enterprise. key responsibilities may include: lead cross-functional collaboration to identify and define analytic initiatives, formulate strategies, and develop solutions to achieve business goals through effective use of data machine learning models. apply data science skills to analyze large, complex datasets and identify meaningful patterns that lead to actionable insights and data-driven solutions to business problems. lead the development and deployment of advanced machine learning models to forecast outcomes and optimize workflows. engage closely with stakeholders to grasp their requirements and deliver actionable insights that drive strategic decision-making. design and present compelling visualizations and reports to effectively communicate analytical findings. oversee the maintenance and enhancement of data pipelines to uphold data quality and precision. keep abreast of emerging trends and breakthroughs in data science and machine learning fields. proficiently extract, aggregate, and transform data from sql and nosql databases, leveraging languages like r, python, or other relevant tools for analysis and modeling. build, test, and validate statistical, and machine learning models and analyses using python, r, or other appropriate language as part of overall solution development. lead implementation of analytic solutions into reporting platforms or production systems by leading the solution design, development, testing, and monitoring. the duties and responsibilities in this posting are representative categories to be used in deciding whether to apply for this position. this is not an exhaustive list of the position’s duties. at bnsf railway, we encourage individuals from all backgrounds to apply, showcasing their skills, experiences and development. we provide resources and tools to help you reach your full potential, fostering a supportive and inclusive environment. basic qualifications • minimum 6 years of experience with building optimization algorithms or relevant experience. • advanced proficiency in programming languages such as python, r, sql, and java. • demonstrated expertise in utilizing data visualization tools to communicate insights clearly and effectively. • in-depth experience with data science cloud platforms and their integration into business solutions. • exceptional intellectual curiosity and a proven ability to thrive in a fast-paced, collaborative team environment. • track record of rapidly acquiring new technical skills and adapting to cutting-edge technologies. • strong communication skills to articulate technical concepts to diverse audiences with clarity and professionalism. • deep understanding and application of statistical analysis and advanced machine learning techniques. • must understand and have proficiency in the core architecture of llms (e.g., transformers and attention mechanisms) • experience with prompt engineering techniques, including chain-of-thought prompting, retrieval-augmented generation (rag), fine-tuning of language models, and evaluation methodologies. • experience with vector databases and embeddings. • experience with model fine-tuning. • experience with gpu optimization. preferred qualifications • bachelor's degree or higher in operations research, computer science, industrial engineering or a related field. ph.d is a plus. • knowledge of geospatial analytics, route optimization, and gis concepts. • previous hands-on experience with ai/machine learning frameworks and tools, showcasing innovative solutions. • extensive background in rail, shipping, airline, logistics, warehousing, supply chain, or transportation industries, or in the high-tech sector. • proficiency in leveraging open-source libraries and frameworks to drive data science initiatives. • seasoned in agile methodologies like scrum, kanban, or safe for efficient project management and delivery at a senior level. at bnsf, you will have access to a comprehensive and competitive benefits package including: • an industry-leading 401(k) and renowned railroad retirement program. • a range of robust health care options for you and your dependents (including domestic partners), including medical, dental, vision, telemedicine, mental health, cancer support, and high-quality care network options. • health care spending accounts (hsa) with employer contributions, as well as life and disability insurance, provided at no cost. • family benefits including parental, pediatric and family building support, adoption and surrogacy reimbursement, and dependent care spending account (with employer match). • access to discounts on travel, gym memberships, counseling services and wellness support. • annual bonus (incentive compensation program) • generous leave / time off policies. • for more information, visit benefits. please be aware of potential fraud that can occur when searching for new career opportunities. please review our faq for more information and awareness. all positions require pre-employment background verification. bnsf railway is an equal opportunity employer, all qualified applicants receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or protected veteran status.",united states,Data Scientist,"['cloud', 'data pipeline', 'excel', 'java', 'machine learning', 'python', 'r', 'sql']","['cloud', 'data pipeline', 'excel', 'java', 'machine learning', 'python', 'r', 'sql']",
"senior data scientist, research, ads metrics",google,"about the position responsibilities • collaborate with stakeholders in cross-projects and team settings to identify and clarify business or product questions to answer. provide feedback to translate and refine business questions into tractable analysis, evaluation metrics, or mathematical models. • use custom data infrastructure or existing data models as appropriate, using specialized knowledge. design and evaluate models to mathematically express and solve defined problems with limited precedent. • own the process of gathering, extracting, and compiling data across sources via relevant tools. format, re-structure, or validate data to ensure quality, and review the dataset to ensure it is ready for analysis. • support and lead search ads generative ai effort using data science, research, develop and maintain north star metrics for search ads to make data-driven decisions. drive the applied research to improve search ads experimentation infrastructure and a/b experiments methodology. requirements • master's degree in statistics, data science, mathematics, physics, economics, operations research, engineering, or a related quantitative field. • 5 years of work experience using analytics to solve product or business problems, coding (e.g., python, r, sql), querying databases or statistical analysis, or 3 years of work experience with a phd degree. nice-to-haves • 8 years of work experience using analytics to solve product or business problems, coding (e.g., python, r, sql), querying databases or statistical analysis, or 6 years of work experience with a phd degree.",united states,Data Scientist,"['experimentation', 'python', 'r', 'sql', 'statistics']","['experimentation', 'python', 'r', 'sql', 'statistics']",
data scientist 2 - full stack,sentilink,"sentilink provides innovative identity and risk solutions, empowering institutions and individuals to transaction with confidence. we’re building the future of identity verification in the united states replacing a clunky, ineffective, and expensive status quo with solutions that are 10x faster, smarter, and more accurate. we’ve seen tremendous traction and are growing extremely quickly. our real-time apis have helped verify hundreds of millions of identities, starting with financial services and rapidly expanding into new markets. sentilink is backed by world-class investors including craft ventures, andreessen horowitz, nyca, and max levchin. we’ve earned recognition from techcrunch, cnbc, bloomberg, forbes, business insider, pymnts, american banker, lendit, and have been named to the forbes fintech 50 list every year since 2023. last but not least, we’ve even made history - we were the first company to go live with the ecbsv and testified before the united states house of representatives on the future of identity. sentilink supports a variety of ways to work, ranging from fully remote to in-office. we operate as a digital-first company with strong collaboration across the u.s. and india. we maintain physical offices in austin, san francisco, new york city, seattle, los angeles, and chicago in the u.s., and in gurugram (delhi) and bengaluru in india. if you’re located near one of these offices, we would love for you to spend time in the office regularly. some roles are hybrid or in-office by design. for example, our engineering team in india works primarily from our gurugram office. role: as a data scientist 2 at sentilink, you will build our core products: models that identify fraudsters and also advance our growing suite of products in financial risk. as an experienced researcher you will be relied upon to be technically capable and the definitive owner of your respective domain. you will often work on projects with high visibility and impact that require deep domain understanding, critical thinking and strong technical abilities. you will work with teams across the company to research new types of fraud, develop new products, and provide analysis to drive sales and marketing. this is a full-stack data science role, involving model development, analysis, and writing production code. you should be interested in having end-to-end ownership and a fast-moving environment where deep domain understanding drives development and unusual insights drive our competitive advantage rather than optimization of new machine learning methodologies. technologies: python 3, postgresql, and aws infrastructure (ec2, s3, rds, redshift, etc.) responsibilities: • develop and maintain sentilink’s fraud detection models through the full model development lifespan: from data acquisition decisions through featurization, focusing labeling resources, model training, experimentation, productionalization, and monitoring. • research new types of fraud and develop new sentilink products around identity verification. • build foundational modeling to drive sentilink’s expanding suite of fraud and financial risk products. • achieve success by researching / developing through iteration, integration of new data sources and inventive feature engineering. • write production-ready code that can be relied on for real-time decision making by our partners. • design, perform, and present analyses that will inform data acquisition, product development, risk operations priorities, marketing, and sales efforts. • work with engineering, risk operations, and data acquisitions to access necessary data, maintain data quality, and support data access requirements: • 2+ years relevant work experience & relevant phd or 4+ years & relevant masters • proven track record of solving complex / high profile business problems with ds / ml solutions • experience in communicating outcomes / progress to senior management / stakeholders • very strong in “end to end” ds development: planning, fleshing out success criteria / metrics, getting buy-in, developing the solution, delivering the solution (prod / deck / strategy doc / etc) • strong practical ml / stats knowledge, i.e. can easily employ the suite of standard ml / stats tools to quickly scope out solutions, and double down where needed. experience with sota ml solutions is a plus • interest in developing deep domain expertise for product-focused work: a background in fraud is not required, but willingness to learn is • experience writing production code and tests • detail oriented and thoughtful - someone we can rely on to make business-changing decisions • experience working at a startup • bonus for familiarity with: identity solutions, fintech, or adjacent industries • candidates must be legally authorized to work in the united states and must live in the united states • thrive in a fast paced environment characterized by the need to solve extremely varied, high impact, open ended problems salary range: • $160,000/year - $210,000/year + equity + benefits perks: • employer paid group health insurance for you and your dependents • 401(k) plan with employer match (or equivalent for non us-based roles) • flexible paid time off • regular company-wide in-person events • home office stipend, and more! corporate values: • follow through • deep understanding • whatever it takes • do something smart",anywhere,Data Scientist,"['aws', 'experimentation', 'feature engineering', 'machine learning', 'python', 'r', 'redshift', 'sql']","['aws', 'experimentation', 'feature engineering', 'machine learning', 'python', 'r', 'redshift', 'sql']",
data scientist (multiple levels),"modern technology solutions, inc. (mtsi)","modern technology solutions inc. (mtsi) is seeking a highly experienced data scientist (multiple levels) to provide systems engineering, integration, & test (seit) support to the united states space force (ussf) operational command and control (c2) acquisition delta responsible for developing software and the supporting architecture for the organize, understand, plan, decide, direct, and monitor capabilities within the c2 joint capability areas (jcas). this position is located in colorado springs, co. job responsibilities will include, but may not be limited to: • leading data analytics efforts and ensuring alignment with 2sts objectives and vision. • overseeing the development and integration of data analytics and business intelligence tools to support squadron strategies and goals. • participating in various staff meetings to influence data capabilities and competencies within the squadron. • collaborating with customers to refine analytical requirements dynamically in an iterative, agile process. • conducting research on cutting-edge data science innovations and adapting them for government environments. • developing proposals to test hypotheses, prioritizing research projects, and establishing project goals. • performing data management tasks and activities required for operational success. • access data from different repositories through scripting or apis to integrate data and present the data to leadership through data visualization tools. required experience • minimum of 5 years of relevant dod/ic or relevant industry work experience. • minimum of 5 years of data science experience. • proficiency in data science techniques to conceptualize, adapt, and apply new models for solving complex problems. • knowledge of dod or private sector business systems, big data analytics, data pipelines, machine learning, artificial intelligence, cognitive sciences, and operational strategies. • advanced skill in articulating and defending complex concepts, often with diverse stakeholders. • strong ability to negotiate and secure executive support for challenging program concepts. • ability to lead interdisciplinary teams consisting of industry, academic, and government experts. • expertise in extracting actionable insights using data analytics methods aligned with squadron requirements. • experience in the following application frameworks: • microsoft office products to include: sharepoint, word, excel, ms project. • adobe acrobat. • confluence and jira. • demonstrated leadership and team-building skills. • extremely motivated individual who is a self-starter and works well in a fast-paced team environment. • strong interpersonal skills to enable working, interfacing, and interrelating with diverse personnel and dynamic teams. • excellent analytical and organizational skills, specifically attention to detail, along with effective time and project management skills. • experience with presenting and communicating technical data to various audiences (government and leadership audience experience preferred). • willingness to mentor colleagues. education requirements • b.s. in mathematics, applied mathematics, statistics, applied statistics, machine learning, data science, operations research, or computer science (required). • m.s. in computer science (desired). security clearance level required • an active ts/sci security clearance is required to start. • u.s. citizenship is required for this position. location / travel requirements: • place of work is the garden of the gods, colorado springs, co. this role is hybrid (on-site/telework). • may require travel up to 15% of the time within the continental united states. the pay range for this position in colorado is $150,000/year to $185,000/year; however, base pay offered may vary depending on established government contract ranges, job-related knowledge, skills, and experience, and other factors. compensation is competitive and may be adjusted for candidates with exceptional qualifications or expertise, including those at the sme level, with potential earnings up to $230,000/year. mtsi also offers a full range of medical, financial, and other benefits, dependent on the position offered. base pay information is based on market location. applicants should apply via mtsi's internal or external careers site. #mtsi #mtsijobs","colorado springs, co",Data Scientist,"['business intelligence', 'data analytics', 'data pipeline', 'elt', 'excel', 'machine learning', 'r', 'statistics']","['business intelligence', 'data analytics', 'data pipeline', 'elt', 'excel', 'machine learning', 'r', 'statistics']",
"staff data scientist, ads performance",pinterest,"about pinterest: millions of people around the world come to our platform to find creative ideas, dream about new possibilities and plan for memories that will last a lifetime. at pinterest, we’re on a mission to bring everyone the inspiration to create a life they love, and that starts with the people behind the product. discover a career where you ignite innovation for millions, transform passion into growth opportunities, celebrate each other’s unique experiences and embrace the flexibility to do your best work. creating a career you love? it’s possible. understanding and improving how we optimize ads through the ads delivery funnel to pinners is fundamental to pinterest’s long-term success. we’re looking for a data scientist to join our team in pushing the boundaries in ads optimization. you will be a key member of an organization of talented data scientists innovating on ads delivery. what you’ll do: • strategy: evolve our strategy by working closely with product and engineering leaders to build on past learnings and shape our platform's future. • experimentation: improve our experimentation capabilities and tools for assessing our ad delivery system, advise on best practices, identify flaws, and build tools and frameworks for experiment analysis. • leadership: lead and mentor peers, improving team output through continuous, candid feedback that recognizes strengths and identifies areas for growth. be a technical leader for junior data scientists. • communication: clearly and concisely communicate complex analytical findings and insights to stakeholders with varying technical backgrounds. • continuous learning: stay informed on the latest developments in data science, causal inference, and advertising technology to drive innovation. what we’re looking for: • 8+ years of combined post-graduate academic and industry experience applying scientific methods to solve real-world problems on web-scale data • direct involvement in the evaluation, refinement, and deployment of ads optimization models for ads delivery systems. • proficiency in leveraging large datasets for performance analysis and gap identification. • proven ability to apply scientific methods to solve real-world problems on web-scale data • expertise in sql and python • strong business and product sense: delight in shaping vague questions into well-defined analyses and success metrics that drive business decisions • excellent communication skills: able to lead initiatives across multiple product areas and communicate findings with leadership and product teams • experience leading key technical projects and substantially influencing the scope and output of others • experience in digital ad delivery stacks preferred in-office requirement statement: we let the type of work you do guide the collaboration style. that means we're not always working in an office, but we continue to gather for key moments of collaboration and connection. this role will need to be in the office for in-person collaboration 1 times per week and therefore needs to be in a commutable distance from one of the following offices: san francisco, ca or seattle, wa. relocation statement: • this position is not eligible for relocation assistance. visit our pinflex page to learn more about our working model. #li-nm4 #li-remote at pinterest we believe the workplace should be equitable, inclusive, and inspiring for every employee. in an effort to provide greater transparency, we are sharing the base salary range for this position. the position is also eligible for equity. final salary is based on a number of factors including location, travel, relevant prior experience, or particular skills and expertise. information regarding the culture at pinterest and benefits available for this position can be found here. us based applicants only $163,064—$335,720 usd our commitment to inclusion: pinterest is an equal opportunity employer and makes employment decisions on the basis of merit. we want to have the best qualified people in every job. all qualified applicants will receive consideration for employment without regard to race, color, ancestry, national origin, religion or religious creed, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, age, marital status, status as a protected veteran, physical or mental disability, medical condition, genetic information or characteristics (or those of a family member) or any other consideration made unlawful by applicable federal, state or local laws. we also consider qualified applicants regardless of criminal histories, consistent with legal requirements. if you require a medical or religious accommodation during the job application process, please complete this form for support.",anywhere,Data Scientist,"['aws', 'excel', 'experimentation', 'python', 'r', 'sql']","['aws', 'excel', 'experimentation', 'python', 'r', 'sql']",
"data scientist ii, pricing",root,"root was founded on the belief that car insurance is broken, and we set out to change it. we’re harnessing the power of technology to revolutionize this archaic, complicated industry. using machine learning and mobile telematic platforms, we’ve built one of the most innovative insurtech companies in the world. the opportunity we believe that a disruptive insurance company must have a principled quantitative framework at its foundation. at root, we are committed to the rigorous development and effective deployment of modern statistical machine learning methods to problems in the insurance industry. as a data scientist (ds) ii within root’s pricing data science team, you will be instrumental in refining our pricing models while ensuring compliance with nuanced regulatory requirements. this role offers the opportunity to collaborate closely with teams across product, actuarial, and state product management, addressing regulatory feedback from departments of insurance and fine-tuning our models to meet market-specific needs. your work will involve fitting and adjusting models, while ensuring our filings are data-driven, defensible, and optimized for performance. you'll apply your technical expertise in r (e.g., tidyverse, data.table) and sql to analyze, manipulate, and model data effectively. experience with h2o and advanced modeling techniques is also highly valued. if you’re eager to use your data science skills in a dynamic, high-impact area, we invite you to join our team and help shape the future of pricing in the insurance industry. root is a “work where it works best” company, meaning we will support you working in whatever location that works best for you across the us. we will continue to have our headquarters in columbus to give more flexibility and more choice about how we live and work. salary range: $116,664 - $145,830 (bonus and lti eligible) root is a “work where it works best” company. this means we will support you working in whatever location that works best for you across the us. how you will make an impact • support the refinement of pricing models to ensure compliance with regulatory requirements, responding to objections and inquiries from departments of insurance. • contribute to the continuous improvement of our pricing models, driving both regulatory compliance and performance optimization. • collaborate with product, actuarial, and state product management teams to ensure alignment across departments. • apply your technical skills in r, sql, and h2o to analyze and optimize pricing models. • contribute to large-scale data science modeling projects, employing best practices in coding, version control, and project execution. what you will need to succeed • advanced degree in a quantitative discipline (master’s or phd preferred) and 2+ years of experience applying advanced quantitative techniques, ideally in the insurance or financial services industry. • strong proficiency in r (e.g., tidyverse, data.table) and sql, with the ability to independently query, manipulate, and model data. • familiarity with pricing models and understanding of insurance regulatory environments is a strong plus. • experience with h2o and other advanced modeling techniques is a plus. • technical proficiency in version control systems such as git, with experience contributing to data science projects. • experience with cloud utilities such as aws (e.g., ec2, s3) is a plus for scalable data processing and storage. • strong business intelligence and data visualization skills, with the ability to communicate insights to both technical and non-technical stakeholders. • an ownership mentality, with the ability to take initiative and drive work forward in a collaborative team environment. • strong communication skills, enabling you to explain complex technical concepts clearly to cross-functional stakeholders. as part of root's interview process, we kindly ask that all candidates be on camera for virtual interviews. this helps us create a more personal and engaging experience for both you and our interviewers. being on camera is a standard requirement for our process and part of how we assess fit and communication style, so we do require it to move forward with any applicant's candidacy. if you have any concerns, feel free to let us know once you are contacted. we’re happy to talk it through. please see our privacy notice available here for more information on how we process your personal data.",anywhere,Data Scientist,"['aws', 'business intelligence', 'cloud', 'machine learning', 'r', 'scala', 'sql']","['aws', 'business intelligence', 'cloud', 'machine learning', 'r', 'scala', 'sql']","116,664–145,830 a year"
senior data scientist - revenue intelligence,"github, inc.","about github github is the world’s leading platform for agentic software development — powered by copilot to build, scale, and deliver secure software. over 180 million developers, including more than 90% of the fortune 100 companies, use github to collaborate, and more than 77,000 organisations have adopted github copilot. locations in this role you can work from remote, united states overview github revenue is growing its data science team and we're seeking experienced professionals to elevate our data and analytics efforts. as a senior data scientist in revenue, you will leverage your deep expertise and knowledge of data science, machine learning, and business to lead data acquisition efforts, conduct thorough review of data analysis and data quality, form hypotheses and discover insights in the data to support business stakeholders and their decision making. you will provide feedback to the engineering team to identify potential future business opportunities, and track advances in industry and academia to adapt algorithms and techniques to drive innovation and develop new solutions. the ideal candidate will contribute to the impact of our data science initiatives and gain deep insights into the latest advancements in ai, machine learning and data science. responsibilities • lead data acquisition efforts and ensure data is properly formatted and accurately described, while adhering to github's privacy policies • mentor others in data cleaning and data analysis best practices. identify gaps in current data sets and drive onboarding of new data sets from production systems or third-party vendors. • resolve data integrity problems in collaboration with relevant teams to promote upstream change and long-term quality • leverage broad and deep knowledge of modeling techniques, ai/ml tools, programming languages and query languages to create models, conduct experiments, analyze results, evaluating the methodology and performance of team members' models and recommending improvements. anticipate the risks of data leakage, bias/variance tradeoff, and methodological limitations. • drive best practices relative to model validation, implementation, and application, and partners with teams across the organization to identify and explore new opportunities for driving transformative solutions for our stakeholders and customers. • develop and articulate data-driven strategies in consideration of business priorities and lead conversations with end customers and/or internal stakeholders to understand, define, and solve business problems. • track advances in industry and academia, and adapt algorithms and/or techniques to drive innovation and develop new solutions. serves as a subject matter expert and mentor for team members. • communicate complex statistics, and machine learning topics to diverse audiences (e.g., multidisciplinary teams, customers, technical and non-technical audiences) • independently writes efficient, readable, extensible code that spans multiple features/solutions. contributes to the code/model review process by providing feedback and suggestions for implementation and improvement. • drive operational excellence for model deployment (i.e. performance, scalability, monitoring, maintenance, integration into engineering production system, stability) • produce project plans to define necessary steps required for completion, leading to a measurable improvement in business performance metrics over time. utilize project results to decide on next steps (e.g., deployment, further iterations, new projects). qualifications required qualifications: • bachelor's degree in data science, mathematics, physics, statistics, economics, operations research, computer science, or related field and 5+ years experience in data science (e.g., managing structured and unstructured data, applying statistical techniques) or related field • or master's degree in data science, mathematics, physics, statistics, economics, operations research, computer science, or related field and 3+ years experience in data science (e.g., managing structured and unstructured data, applying statistical techniques) or related field • or doctorate in data science, mathematics, physics, statistics, economics, operations research, computer science, or related field and 1+ year(s) experience in data science (e.g., managing structured and unstructured data, applying statistical techniques) or related field • or equivalent experience • 3 + years of experience in programming languages such as python or r, experience with query languages such as sql and kql, and with data manipulation tools like spark and airflow preferred qualifications: • technical understanding of data science techniques for regression, classification, time-series analysis, experimental design, causal inference • able to clearly communicate findings to non-technical stakeholders through storytelling and visualization with tools like jupyter notebooks or azure data explorer / powerbi dashboards compensation range the base salary range for this job is usd $112,800.00 - usd $299,300.00 /yr. these pay ranges are intended to cover roles based across the united states. an individual's base pay depends on various factors including geographical location and review of experience, knowledge, skills, abilities of the applicant. at github certain roles are eligible for benefits and additional rewards, including annual bonus and stock. these rewards are allocated based on individual impact in role. in addition, certain roles also have the opportunity to earn sales incentives based on revenue or utilization, depending on the terms of the plan and the employee's role. github values • customer-obsessed • ship to learn • growth mindset • own the outcome • better together • diverse and inclusive manager fundamentals • model • coach • care leadership principles • create clarity • generate energy • deliver success who we are github is the world’s leading ai-powered developer platform with 150 million developers and counting. we’re also home to the biggest open-source community on earth (and 99% of the world’s software has open-source code in its dna). many of the apps and programs you use every day are built on github. our teams are dreamers, doers, and pioneers, leading the way in ai, driving humanitarian efforts around the globe, and even sending open source to mars (and beyond!). at github, our goal is to create the space you need to do your best work. we’re remote-first and offer competitive pay, generous learning and growth opportunities, and excellent benefits to support you, wherever you are—because we know that people flourish when they can work on their own terms. join us, and let’s change the world, together. eeo statement github is made up of people from a wide variety of backgrounds and lifestyles. we embrace diversity and invite applications from people of all walks of life. we don't discriminate against employees or applicants based on gender identity or expression, sexual orientation, race, religion, age, national origin, citizenship, disability, pregnancy status, veteran status, or any other differences. also, if you have a disability, please let us know if there's any way we can make the interview process better for you; we're happy to accommodate!",united states,Data Scientist,"['airflow', 'azure', 'classification', 'dashboard', 'data analysis', 'excel', 'machine learning', 'python', 'r', 'regression', 'scala', 'spark', 'sql', 'statistics']","['airflow', 'azure', 'classification', 'dashboard', 'data analysis', 'excel', 'machine learning', 'python', 'r', 'regression', 'scala', 'spark', 'sql', 'statistics']",
[remote] data scientist – entry level (remote),next jobs,"note: the job is a remote job and is open to candidates in usa. next jobs is hiring motivated individuals for a flexible remote role that supports day-to-day operations and contributes to various workflows related to digital, content, research, data, and ai. the role involves assisting with tasks, ensuring accuracy, and improving workflows within the team. responsibilities • assist with everyday tasks based on project needs (content, data, research, analysis, operations, ai review, documentation, or coordination) • review, organise, and update information with accuracy and attention to detail • communicate effectively with the team and provide timely progress updates • help identify process gaps and contribute to smoother workflows skills • motivated individuals for a flexible 100% remote role • support day-to-day tasks, ensuring smooth operations • contribute to digital, content, research, data, and ai-related workflows • assist with everyday tasks based on project needs (content, data, research, analysis, operations, ai review, documentation, or coordination) • review, organise, and update information with accuracy and attention to detail • communicate effectively with the team and provide timely progress updates • help identify process gaps and contribute to smoother workflows • strong command of written english and effective communication skills • basic familiarity with digital tools, spreadsheets, or online productivity platforms • analytical mindset with attention to detail and accuracy • eagerness to learn, adapt, and contribute to team goals benefits • flexible hours • weekly pay • no experience needed • work from anywhere • 100% remote work with complete scheduling flexibility • weekly payouts and a stable income structure • skill-building opportunities in research, content, and ai-related tasks • supportive work culture encouraging growth, feedback, and professional learning company overview • next jobs is dedicated to facilitating seamless career transitions and ensuring professionals are always positioned for their next strategic step. it was founded in undefined, and is headquartered in , with a workforce of 11-50 employees. its website is .",anywhere,Data Scientist,['r'],['r'],5K–8K a month
data science summer intern (remote & paid),experian,"you will join experian's r&d lab, developing analytical solutions by applying new technologies, including quantum computing. responsibilities: • apply ml, data mining, or text mining techniques to extract data insights and solve complex business issues • design data structures and storage schemes for efficient handling • explore and improve the application of quantum computing for fraud detection, digital marketing and finance forecasting • conduct return on investment and benefit analysis • document and present model process and model performance about experian experian is a global data and technology company, powering opportunities for people and businesses around the world. we help redefine lending practices, uncover and prevent fraud, simplify healthcare, create digital marketing solutions, and gain deeper insights into the automotive market, all using our unique combination of data, analytics, and software. we also assist millions of people in realizing their financial goals and saving time and money. we operate across a range of markets, from financial services to healthcare, automotive, agrifinance, insurance, and many more industry segments. we invest in people and new advanced technologies to unlock the power of data and to innovate. we're focused on powering opportunities, which is why the experian summer internship program gives students across the country the chance to apply their education to real-world challenges through meaningful, hands-on projects. rooted in our 'people first' philosophy, our interns experience firsthand our commitment to personal and professional development. proudly named one of the top 100 internship programs three years in a row, we invite you to join us and explore your potential with a team that's invested in your growth. a ftse 100 index company listed on the london stock exchange (expn), we have a team of 25,500 people across 32 countries. our corporate headquarters are in dublin, ireland. learn more at experianplc.com. experience and skills • currently enrolled in a phd degree program in cs, physics, or a related field • return to school in fall 2026 to complete degree • experience in analytics, data mining, or predictive modeling • experience with regression, neural network, or ensemble methods • proficient in python, r, java, c++, or c • experience with generative ai and algorithms/applications of quantum computing additional information benefits/perks: • fully remote • volunteer time off • great compensation • flexible work schedule • eligible for 401(k) participation in 90 days at experian, our people and culture set us apart. we're deeply committed to creating an environment where everyone feels they belong and can excel. from inclusion and authenticity to work/life balance, development, wellness, collaboration, and recognition, we focus on what truly matters. our people-first approach has earned us global recognition: world's best workplaces™ 2024 (fortune top 25), great place to work™ 2025 in 26 countries, and glassdoor best places to work 2024, among others. want to see what life at experian is really like? explore experian life on social or visit our careers site. our compensation reflects the cost of labor across several u.s. geographic markets. the hourly pay range for this position is listed above. within this range, individual pay is determined by work location and additional factors such as job-related skills, experience, and education. experian is proud to be an equal opportunity employer for all groups protected under applicable federal, state and local law, including protected veterans and individuals with disabilities. if you have a disability or special need that requires accommodation, please let us know at the earliest opportunity. #li-remote #earlycareers",united states,Data Scientist,"['c++', 'excel', 'java', 'python', 'r', 'regression']","['c++', 'excel', 'java', 'python', 'r', 'regression']",
data scientist 5 - availability,netflix,"netflix is one of the world's leading entertainment services, with over 300 million paid memberships in over 190 countries enjoying tv series, films and games across a wide variety of genres and languages. members can play, pause and resume watching as much as they want, anytime, anywhere, and can change their plans at any time. everything that we build to delight our members relies on our extensive technical infrastructure, which spans from services we rent from aws to our custom-built content delivery network, open connect. keeping our internal platforms, services and infrastructure stable is crucial to delivering a great member experience. as a senior data scientist focused on infrastructure availability you will join a data org of analytics engineers, data engineers, and data scientists who partner with our infrastructure engineering teams to create metrics, build tools, perform analyses and make recommendations on how to improve the reliability and availability of our systems using data. you will work closely with partners in data science, software engineering, performance engineering and technical product management to develop metrics to assess current performance across systems and product domains, define what adequate performance looks like, understand the root causes of availability issues, and make recommendations for how the business should prioritize system performance improvements. the ideal candidate will excel in metric research & development, statistical modeling, causal inference, and share a passion for continuously improving the way we use data to make the netflix infrastructure better. to learn more about our team, read here. in this role, you will: • develop metrics to measure the availability, reliability and performance of netflix’s infra stack • partner with data engineering, data science and analytics engineers to productionize system level availability metrics and dashboards • partner with software engineers, performance engineers, technical product managers and other product teams to conduct root cause and causal inference analysis of availability issues and make recommendations for how to remediate • connect with the larger analytics community at netflix to bring more visibility to our work. you are: • experienced in metric development and measurement in the domain of system reliability / availability for large, distributed systems, platforms, and/or infrastructure • fluent in at least one analytics and scripting language like python. • experienced in engineering data pipelines and etl in languages like sql and workflow orchestration tools like apache airflow. • passionate about using data to help guide strategic decision making. • an exceptional communicator with both technical and non-technical audiences. experienced at telling stories with data and communicating to stakeholders at all levels of the business. • experienced in building strong relationships with stakeholders and colleagues to tackle big, cross-functional problems. • comfortable with ambiguity, and thrive with minimal oversight and process. • a strong product thinker with good product sense: you know how to get at what our partners need and how to develop solutions to satisfy those needs. generally, our compensation structure consists solely of an annual salary; we do not have bonuses. you choose each year how much of your compensation you want in salary versus stock options. to determine your personal top of market compensation, we rely on market indicators and consider your specific job family, background, skills, and experience to determine your compensation in the market range. the range for this role is $170 - $720. netflix provides comprehensive benefits including health plans, mental health support, a 401(k) retirement plan with employer match, stock option program, disability programs, health savings and flexible spending accounts, family-forming benefits, and life and serious injury benefits. we also offer paid leave of absence programs. full-time hourly employees accrue 35 days annually for paid time off to be used for vacation, holidays, and sick paid time off. full-time salaried employees are immediately entitled to flexible time off. see more detail about our benefits here. netflix is a unique culture and environment. learn more here. inclusion is a netflix value and we strive to host a meaningful interview experience for all candidates. if you want an accommodation/adjustment for a disability or any other reason during the hiring process, please send a request to your recruiting partner. we are an equal-opportunity employer and celebrate diversity, recognizing that diversity builds stronger teams. we approach diversity and inclusion seriously and thoughtfully. we do not discriminate on the basis of race, religion, color, ancestry, national origin, caste, sex, sexual orientation, gender, gender identity or expression, age, disability, medical condition, pregnancy, genetic makeup, marital status, or military service. job is open for no less than 7 days and will be removed when the position is filled.",united states,Data Scientist,"['airflow', 'aws', 'dashboard', 'data pipeline', 'etl', 'excel', 'python', 'r', 'recommendation', 'sql']","['airflow', 'aws', 'dashboard', 'data pipeline', 'etl', 'excel', 'python', 'r', 'recommendation', 'sql']",
advanced degree data scientist - full-time intern conversion,oracle,"• *this fte conversion requisition is only for 2025 oracle pd interns to be rehired for full-time roles** intended for students graduating with their masters/phd degree by, or have graduated within, 12 months of cohort start date in june/july 2026. target start date: early-june or mid-july 2026 designs, develops and programs methods, processes, and systems to consolidate and analyze unstructured, diverse “big data” sources to generate actionable insights and solutions for client services and product enhancement. interacts with product and service teams to identify questions and issues for data analysis and experiments. develops and codes software programs, algorithms and automated processes to cleanse, integrate and evaluate large datasets from multiple disparate sources. identifies meaningful insights from large data and metadata sources; interprets and communicates insights and findings from analysis and experiments to product, service, and business managers. duties and tasks are standard with some variation. completes own role largely independently within defined policies and procedures. objective minimum qualifications: to be considered for a data scientist position, the objective minimum qualifications (omqs) below must be met. please ensure the application clearly indicates that you meet these omqs. • have graduated with a graduate degree in computer science, engineering management, information systems management, business analytics, (or an equivalent science/engineering field) within 12 months of actual start date, no later than august 2026. • are proficient (e.g., can complete coding projects without any assistance) in at least one of the following programming languages: java, c, c++, python, sql, javascript, r, php, swift, go, c#, matlab, julia, kotlin • have academic course work, projects, internships, and/or research experience in one or more of the following computer science areas: • artificial intelligence / machine learning / natural language processing • big data / data structures / algorithms • cloud computing • computer systems / distributed systems /embedded systems / operating systems • database systems/design • object oriented design • web/mobile development • user interface design • attend a university in the us. • authorized to work in the us in 2026. preferred qualifications: • minimum 3.0 gpa",united states,Data Scientist,"['c#', 'c++', 'cloud', 'data analysis', 'java', 'machine learning', 'matlab', 'natural language processing', 'python', 'r', 'sql']","['c#', 'c++', 'cloud', 'data analysis', 'java', 'machine learning', 'matlab', 'natural language processing', 'python', 'r', 'sql']",
"manager, data science",cardinal health,"what data science contributes to cardinal health the data & analytics function oversees the analytics life-cycle in order to identify, analyze and present relevant insights that drive business decisions and anticipate opportunities to achieve a competitive advantage. this function manages analytic data platforms, the access, design and implementation of reporting/business intelligence solutions, and the application of advanced quantitative modeling. data science applies base, scientific methodologies from various disciplines, techniques and tools that extracts knowledge and insight from data to solve complex business problems on large data sets, integrating multiple systems. responsibilities • works closely with vps, directors, managers, business, and technical it personal to solve problems by providing tools to increase quality and compliance. • supervises two data scientist who perform data and analytical responsibilities. • this position is critical in supporting the distribution quality functions with lrcq and the businesses they support at corporate and well as in the field globally. • ability to identity data sources and utilizes effectively qualifications • 8-12 years of experience, preferred • bachelor’s degree in related field, or equivalent work experience, preferred • demonstrated experience with tableau, alteryx, and ai tools. what is expected of you and others at this level • manages department operations and supervises professional employees, front line supervisors and/or business support staff • participates in the development of policies and procedures to achieve specific goals • ensures employees operate within guidelines • decisions have a short term impact on work processes, outcomes and customers • interacts with subordinates, peers, customers, and suppliers at various management levels; may interact with senior management • interactions normally involve resolution of issues related to operations and/or projects • gains consensus from various parties involved anticipated salary range: $123,500 - $167,700 bonus eligible: yes benefits: cardinal health offers a wide variety of benefits and programs to support health and well-being. • medical, dental and vision coverage • paid time off plan • health savings account (hsa) • 401k savings plan • access to wages before pay day with myflexpay • flexible spending accounts (fsas) • short- and long-term disability coverage • work-life resources • paid parental leave • healthy lifestyle programs application window anticipated to close: 02/10/2026 *if interested in opportunity, please submit application as soon as possible. the salary range listed is an estimate. pay at cardinal health is determined by multiple factors including, but not limited to, a candidate’s geographical location, relevant education, experience and skills and an evaluation of internal pay equity. candidates who are back-to-work, people with disabilities, without a college degree, and veterans are encouraged to apply. cardinal health supports an inclusive workplace that values diversity of thought, experience and background. we celebrate the power of our differences to create better solutions for our customers by ensuring employees can be their authentic selves each day. cardinal health is an equal opportunity/affirmative action employer. all qualified applicants will receive consideration for employment without regard to race, religion, color, national origin, ancestry, age, physical or mental disability, sex, sexual orientation, gender identity/expression, pregnancy, veteran status, marital status, creed, status with regard to public assistance, genetic status or any other status protected by federal, state or local law.",united states,Data Scientist,"['business intelligence', 'r', 'sas', 'tableau']","['business intelligence', 'r', 'sas', 'tableau']",
staff data scientist - gusto retirement,"gusto, inc.","about gusto at gusto, we're on a mission to grow the small business economy. we handle the hard stuff-like payroll, health insurance, 401(k)s, and hr-so owners can focus on their craft and customers. with teams in denver, san francisco, and new york, we're proud to support more than 400,000 small businesses across the country, and we're building a workplace that represents and celebrates the customers we serve. learn more about our total rewards philosophy. about the role: as a senior/staff data scientist supporting gusto retirement, you will play a crucial role in leveraging experimentation, statistical inference, and causal analysis to drive strategic decision making that contributes to the overall success of our organization. the ideal candidate is a trusted data storyteller with strong statistical and coding skills, and a passion for applying these skills to support the retirement financial platform product for gusto's small business customers. in this role, you will work closely with our cross-functional teams to become an expert in the data for your domain, define and track metrics that help us understand our business performance, and dive deep into our gusto retirement data to deliver insights and answer questions. you'll also integrate ai-assisted practices to accelerate analysis, enhance rigor, and expand the reach of insights across gusto. here's what you'll do day-to-day: • lead: own ambiguous problems with an ai-native approach, design analysis frameworks, and introduce structure that scales across multiple product domains. • strategic partnership: collaborate with product managers, engineering leads, designers, and operations teams to proactively identify opportunities, align on strategy, and guide data-informed decision-making. • analytical rigor: apply advanced statistical methods, causal inference, and experimentation to surface drivers of product performance, separating signal from noise. • experimentation & measurement: design, analyze, and interpret experiments; leverage ai tools and workflows to ensure insights highlight trade-offs and limitations based on sample size and data quality. • execution: deliver multiple high-impact projects, balancing trade-offs to maximize business value, and maintain clear expectations of deliverables and timelines. • communication: present complex findings in a structured, compelling way to technical and non-technical stakeholders, fostering a data-informed mindset across the company. • operate with agility: drive progress with an adaptive and creative mindset to navigate and succeed in a dynamic environment where technologies and project scopes are constantly evolving. • scaling the craft: mentor other data practitioners, uplevel team best practices in experimentation, statistical modeling, and metric interpretation. drive improvements in data quality, rigor, and adoption of better data capabilities across the org. here's what we're looking for: • 5 - 10 years of experience in data science at a product-focused software company • tactical sql skills and experience with python • must be comfortable learning new tools and pivoting quickly. demonstrated comfort with ambiguity and an adaptive mindset in a constantly changing work environment • tangible experience integrating efficient ai-assisted practices to accelerate analysis, enhance rigor, and expand the reach of insights • proven ability to apply statistical methods, causal inference, and a/b testing experimental design to real business problems • excellent communication skills, with a track record of influencing cross-functional stakeholders and leadership • demonstrated experience leading large, technically complex projects with clear business impact • a proactive, resilient problem-solver who independently structures ambiguous problems into actionable insights • passion for mentoring others and raising the bar for data science craft across the team our cash compensation amount for this role is $135,320 to $189,900 annually in denver & most major metro locations, and $163,830 to $202,050 annually for san francisco & new york. final offer amounts are determined by multiple factors including candidate location, experience and expertise and may vary from the amounts listed above. gusto has physical office spaces in denver, san francisco, and new york city. employees who are based in those locations will be expected to work from the office on designated days approximately 2-3 days per week (or more depending on role). the same office expectations apply to all symmetry roles, gusto's subsidiary, whose physical office is in scottsdale. note: the san francisco office expectations encompass both the san francisco and san jose metro areas. when approved to work from a location other than a gusto office, a secure, reliable, and consistent internet connection is required. this includes non-office days for hybrid employees. our customers come from all walks of life and so do we. we hire great people from a wide variety of backgrounds, not just because it's the right thing to do, but because it makes our company stronger. if you share our values and our enthusiasm for small businesses, you will find a home at gusto. gusto is proud to be an equal opportunity employer. we do not discriminate in hiring or any employment decision based on race, color, religion, national origin, age, sex (including pregnancy, childbirth, or related medical conditions), marital status, ancestry, physical or mental disability, genetic information, veteran status, gender identity or expression, sexual orientation, or other applicable legally protected characteristic. gusto considers qualified applicants with criminal histories, consistent with applicable federal, state and local law. gusto is also committed to providing reasonable accommodations for qualified individuals with disabilities and disabled veterans in our job application procedures. we want to see our candidates perform to the best of their ability. if you require a medical or religious accommodation at any time throughout your candidate journey, please fill out this form and a member of our team will get in touch with you. gusto takes security and protection of your personal information very seriously. please review our fraudulent activity disclaimer. personal information collected and processed as part of your gusto application will be subject to gusto's applicant privacy notice.","denver, co",Data Scientist,"['a/b testing', 'excel', 'experimentation', 'python', 'r', 'sql']","['a/b testing', 'excel', 'experimentation', 'python', 'r', 'sql']","163,830–202,050 a year"
data scientist - strategic analytics,acuity insurance,"acuity is seeking a data scientist - strategic analytics to work with business leaders to inform company strategies and decision-making through an objective viewpoint by leveraging the team’s expertise in deep business knowledge, leveraging data as an asset, and utilizing statistical and advanced analytical techniques as appropriate. the role of data scientist – strategic analytics uses advanced statistical and analytical techniques to inform highly complex and sophisticated business questions. the data scientist will apply and integrate advanced statistical, mathematical, data mining, and business analysis skills and techniques to discover new behavioral insights, predict outcomes, prescribe decision options, and advise on turning new insights into actionable opportunities. internal deadline to apply: december 22nd, 2025 essential functions: • develop solutions, to support analytic insights and visualization using mathematical models, algorithms, machine learning techniques, and robust analytics. • partner with business clients and technical leaders to spearhead the development of standards for appropriate statistical methodology, study design, power and sample size estimation, and data analysis plans for behavioral analytics. • provide data driven decision support for key initiatives of company strategy and measurement related to client, claims, sales, and campaign analyses to understand both growth and economic impact. • determine requisite data elements and partners with data engineers to design integrated datasets for analytical research purposes. • work closely with machine learning engineers and business partners to develop and build behavioral data science and analytics products. • effectively present analytical results and derived recommendations for action to senior leaders, peers, and product team members. • regular and predictable attendance. • performs other duties as assigned. education: bachelor’s degree in data science, statistics, math, computer science, economics, or related field. advanced graduate level degree in a quantitative discipline (statistics, applied mathematics, computer science, econometrics, or a related field) is preferred. experience: a minimum of three years relevant experience to include research and data analysis, experiment design and measurement, or application of statistical research techniques. other qualifications: • expertise in one or more development or statistical analysis tool such as r, python, sas, sql, spss, or other tools. tableau experience is a plus. • proven excellence in research, quantitative analysis, problem solving, and analytical working techniques. • statistical knowledge and intuition • strong aptitude and desire for learning new analytical and visualization tools, modeling, and quantitative techniques. initiative to independently design and develop own deliverables while still being a team player. • demonstrates ability to deliver results and recommendations in written, verbal and presentation form at an appropriate level for a business audience. • acuity does not sponsor applicants for u.s. work authorization.* this job is classified as exempt. we are an equal employment opportunity employer. applicants and employees are considered for positions and are evaluated without regard to mental or physical disability, race, color, religion, gender, national origin, age, genetic information, military or veteran status, sexual orientation, marital status or any other protected federal, state/province or local status unrelated to the performance of the work involved. if you have a disability and require reasonable accommodations to apply or during the interview process, please contact our talent acquisition team at careers@acuity.com. acuity is dedicated to offering reasonable accommodations during our recruitment process for qualified individuals.",united states,Data Scientist,"['data analysis', 'excel', 'machine learning', 'python', 'r', 'recommendation', 'sas', 'sql', 'statistics', 'tableau']","['data analysis', 'excel', 'machine learning', 'python', 'r', 'recommendation', 'sas', 'sql', 'statistics', 'tableau']",
"data scientist ii, pricing",root,"root was founded on the belief that car insurance is broken, and we set out to change it. we’re harnessing the power of technology to revolutionize this archaic, complicated industry. using machine learning and mobile telematic platforms, we’ve built one of the most innovative insurtech companies in the world. the opportunity we believe that a disruptive insurance company must have a principled quantitative framework at its foundation. at root, we are committed to the rigorous development and effective deployment of modern statistical machine learning methods to problems in the insurance industry. as a data scientist (ds) ii within root’s pricing data science team, you will be instrumental in refining our pricing models while ensuring compliance with nuanced regulatory requirements. this role offers the opportunity to collaborate closely with teams across product, actuarial, and state product management, addressing regulatory feedback from departments of insurance and fine-tuning our models to meet market-specific needs. your work will involve fitting and adjusting models, while ensuring our filings are data-driven, defensible, and optimized for performance. you'll apply your technical expertise in r (e.g., tidyverse, data.table) and sql to analyze, manipulate, and model data effectively. experience with h2o and advanced modeling techniques is also highly valued. if you’re eager to use your data science skills in a dynamic, high-impact area, we invite you to join our team and help shape the future of pricing in the insurance industry. root is a “work where it works best” company, meaning we will support you working in whatever location that works best for you across the us. we will continue to have our headquarters in columbus to give more flexibility and more choice about how we live and work. salary range: $116,664 - $145,830 (bonus and lti eligible) root is a “work where it works best” company. this means we will support you working in whatever location that works best for you across the us. how you will make an impact • support the refinement of pricing models to ensure compliance with regulatory requirements, responding to objections and inquiries from departments of insurance. • contribute to the continuous improvement of our pricing models, driving both regulatory compliance and performance optimization. • collaborate with product, actuarial, and state product management teams to ensure alignment across departments. • apply your technical skills in r, sql, and h2o to analyze and optimize pricing models. • contribute to large-scale data science modeling projects, employing best practices in coding, version control, and project execution. what you will need to succeed • advanced degree in a quantitative discipline (master’s or phd preferred) and 2+ years of experience applying advanced quantitative techniques, ideally in the insurance or financial services industry. • strong proficiency in r (e.g., tidyverse, data.table) and sql, with the ability to independently query, manipulate, and model data. • familiarity with pricing models and understanding of insurance regulatory environments is a strong plus. • experience with h2o and other advanced modeling techniques is a plus. • technical proficiency in version control systems such as git, with experience contributing to data science projects. • experience with cloud utilities such as aws (e.g., ec2, s3) is a plus for scalable data processing and storage. • strong business intelligence and data visualization skills, with the ability to communicate insights to both technical and non-technical stakeholders. • an ownership mentality, with the ability to take initiative and drive work forward in a collaborative team environment. • strong communication skills, enabling you to explain complex technical concepts clearly to cross-functional stakeholders. as part of root's interview process, we kindly ask that all candidates be on camera for virtual interviews. this helps us create a more personal and engaging experience for both you and our interviewers. being on camera is a standard requirement for our process and part of how we assess fit and communication style, so we do require it to move forward with any applicant's candidacy. if you have any concerns, feel free to let us know once you are contacted. we’re happy to talk it through. please see our privacy notice available here for more information on how we process your personal data.",anywhere,Data Scientist,"['aws', 'business intelligence', 'cloud', 'machine learning', 'r', 'scala', 'sql']","['aws', 'business intelligence', 'cloud', 'machine learning', 'r', 'scala', 'sql']","116,664–145,830 a year"
data scientist,markon,"overview: eager to join a team where your skills are valued, your growth is nurtured, and your impact is profound? look no further than markon, a premier consulting firm deeply dedicated to advancing our nation's most critical missions. at markon, we don't just offer jobs – we offer opportunities for personal and professional transformation. empowering our employees to lead, innovate, and excel, we foster an environment where new ideas are not just welcomed but celebrated. as a perennial washington post top workplace, we prioritize the well-being and success of our team members, ensuring they can bring their best selves to work. headquartered in falls church, virginia, markon has garnered national recognition for our unwavering dedication to excellence in serving the intelligence community, as well as federal civilian and defense agencies. our growing reach extends across 17 states, 116 countries, and 5 continents, where our team of dynamic professionals collaborates to deliver unparalleled program and project management services. markon values people and the tremendous impact each individual can make – which is why we’re consistently recognized as one of the best places to work in federal government consulting. here, you can help solve the nation’s most important challenges, surrounded by colleagues who help you grow, advance, and succeed. we are deeply dedicated to what matters – bringing out the best in each other to advance our clients’ missions. join us and make a meaningful impact. markon is an equal opportunity employer. all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, national origin, age, protected veteran status, or disability status. this job posting will remain open until the position is filled. benefits offered: medical, dental, vision, life insurance, short-term disability, long-term disability, 401(k) match, flexible spending accounts, eap, training and tuition assistance, paid time off, and holidays description: markon’s partner company is seeking a data scientist to support our intelligence customer out of lakewood, co. we’re looking for professionals proficient in python and experienced in automating workflows, data manipulation, and visualization to deliver mission-driven insights and solutions. responsibilities: • automate data workflows and pipelines using python and related tools • clean, manipulate, and visualize large, complex datasets • apply statistical, computational, and analytical methods to extract insights • develop and assess models and algorithms tailored to mission needs • translate data challenges into technical solutions and vice versa • communicate key findings to both technical and non-technical audiences • support mission teams by aligning analytics with operational objectives qualifications: • ts/sci with active polygraph • bachelor’s degree in a stem field + 10 years of relevant experience strongly preferred: experience in five or more of the following areas: • machine learning design and implementation • data science • advanced analytical algorithms • programming • data mining • advanced mathematical foundations • artificial intelligence • workflow development and reproducibility • data management and curation • data modeling and assessment salary range: usd $140,000.00 - usd $150,000.00 /yr. the markon pay range for this position is a general guideline only and not a guarantee of compensation or salary. additional factors considered in extending an offer include (but are not limited to) responsibilities of the job, education, experience, knowledge, skills, and abilities, as well as internal equity, alignment with market data, applicable bargaining agreement (if any), or other law.","lakewood, co",Data Scientist,"['excel', 'machine learning', 'python', 'r']","['excel', 'machine learning', 'python', 'r']",140K–150K a year
sr data scientist (remote),frontdoor,"overview frontdoor is reimagining how homeowners maintain and repair their most valuable asset – their home. as the parent company of two leading brands, we bring over 50 years of experience in providing our members with comprehensive options to protect their homes from costly and unexpected breakdowns through our extensive network of pre-qualified professional contractors. american home shield, the category leader in home service plans with approximately two million members, gives homeowners budget protection and convenience, covering up to 23 essential home systems and appliances. frontdoor is a cutting edge, one-stop app for home repair and maintenance. enabled by our streem technology, the app empowers homeowners by connecting them in real time through video chat with pre-qualified experts to diagnose and solve their problems. the frontdoor app also offers homeowners a range of other benefits including diy tips, discounts and more. for more information about american home shield and frontdoor, please visit frontdoorhome.com . responsibilities summary: frontdoor is looking for a sr. data scientist to support the growth and profitability of the american home shield and frontdoor on-demand businesses. as a member of the analytics team, you will build predictive models & execute complex analyses to answer key business questions in support of company growth and profitability. you will partner with business stakeholders to generate ideas to improve our business, drive insights and recommendations, and then monitor business results. we are seeking an individual who is deeply analytical, highly collaborative, and a strong communicator. responsibilities: • develop and deploy predictive and forecast models using python • partner with stakeholders in multiple departments to lead business initiatives from an analytics perspective • solves challenging, ambiguous problems using a structured, data-driven process • analyze and organize raw data from multiple sources to produce requested data elements • design and analyze experiements across pricing, crm and product teams • presents findings and recommendations from analysis to key stakeholders including senior management • builds reports as needed to monitor initiatives and key business metrics • acts as a coach and mentor to less experienced data scientists and business stakeholders • works effectively with key business stakeholders to build/maintain relationships and understand business objectivesand strategy • ability to manage multiple projects/initiatives • leads complex analyses with minimal supervision qualifications minimum education, licensure and professional certification requirements: bachelor’s degree in economics, statistics, computer science, business analytics, or similar field preferred. master’s degree desired. required skills: • 7+ years of analytics or ds experience focused on customer, marketing, financial or operational analytics • adaptability and agility in changing circumstances; handling multiple priorities and deadlines • expert level proficiency in python , sql, excel and powerpoint required • top candidate will be a self-starter with a sharp eye for details, an ability to work in a fast-paced and ever-changing environment, and the ability to communicate effectively with stakeholders • ability to work with technical, business and front-line teams other/state specific this role pays between $ 140k to $ 160k , and your actual base pay will depend on your skills, qualifications, responsibilities, experience, and location. at frontdoor certain roles are eligible for additional rewards and incentives. speak directly to your recruiter to learn more. our approach to benefits is holistic, and includes health, wellbeing and financial components including: insurance for medical/pharmacy, dental, vision, life, and disability, weight loss and smoking cessation programs, matching 401(k) and ability to participate in our employee stock purchase plan. need help finding the right job? we can recommend jobs specifically for you! job locations us id 2025-3864 category engineering type full time company ahs american home shield corp company ahs american home shield corp",united states,Data Scientist,"['excel', 'python', 'r', 'recommendation', 'sql', 'statistics']","['excel', 'python', 'r', 'recommendation', 'sql', 'statistics']",
data scientist - global supply chain services,1000 kla corporation,"company overview kla is a global leader in diversified electronics for the semiconductor manufacturing ecosystem. virtually every electronic device in the world is produced using our technologies. no laptop, smartphone, wearable device, voice-controlled gadget, flexible screen, vr device or smart car would have made it into your hands without us. kla invents systems and solutions for the manufacturing of wafers and reticles, integrated circuits, packaging, printed circuit boards and flat panel displays. the innovative ideas and devices that are advancing humanity all begin with inspiration, research and development. kla focuses more than average on innovation and we invest 15% of sales back into r&d. our expert teams of physicists, engineers, data scientists and problem-solvers work together with the world’s leading technology providers to accelerate the delivery of tomorrow’s electronic devices. life here is exciting and our teams thrive on tackling really hard problems. there is never a dull moment with us. job description/preferred qualifications job description/preferred qualifications we are seeking a highly motivated and experienced data scientist to join our multifaceted business operations and digital solutions team, supporting kla supply chain services. this senior level role will be instrumental in driving digital transformation through advanced analytics, statistical modeling, and ai innovation. the ideal candidate will lead the development of scalable data science solutions that optimize logistics and other supply chain operations and enable data-driven decision making. responsibilities: lead the design and deployment of statistical analysis, machine learning models, and optimization algorithms to address logistics challenges. architect and sale ai-driven solutions across our logistics network, ensuring alignment with business goals and operational efficiency. collaborate cross functionally with logistics support groups, business analysts, and data experts to identify opportunities and deliver impactful digital solutions. develop and apply statistical methods and mathematical modeling to uncover insights and support strategic initiatives. guide the team in best practices for data science development, including feature engineering, model validation, and performance monitoring. champion the use of modern data science tools and platforms. contribute to the development of network/graph analytics initiatives by applying graph theory and network modeling to uncover relationship and patterns in logistics data. demonstrate strong leadership, communication, and collaboration skills, with the ability to influence cross-functional teams. foster a culture of innovation and continuous learning by promoting knowledge sharing and mentoring others. stay current with industry trends in ai, machine learning, and logistics analytics while evaluating emerging technology for adoption. minimum qualifications strong proficiency in python and sql experience with data engineering and transformation to structure complex data sets for modeling and analysis solid understanding of statistical inference, time series analysis, and optimization techniques preferred qualifications experience working with logistics operations and supply chain experience with network/graph analytics familiarity with sap, snowflake, and power bi minimum qualifications doctorate (academic) degree and related work experience of 3 years; master's level degree and related work experience of 6 years; bachelor's level degree and related work experience of 8 years base pay range: $133,300.00 - $226,600.00 annually primary location: usa-ca-milpitas-kla kla’s total rewards package for employees may also include participation in performance incentive programs and eligibility for additional benefits including but not limited to: medical, dental, vision, life, and other voluntary benefits, 401(k) including company matching, employee stock purchase program (espp), student debt assistance, tuition reimbursement program, development and career growth opportunities and programs, financial planning benefits, wellness benefits including an employee assistance program (eap), paid time off and paid company holidays, and family care and bonding leave. interns are eligible for some of the benefits listed. our pay ranges are determined by role, level, and location. the range displayed reflects the pay for this position in the primary location identified in this posting. actual pay depends on several factors, including state minimum pay wage rates, location, job-related skills, experience, and relevant education level or training. we are committed to complying with all applicable federal and state minimum wage requirements where applicable. if applicable, your recruiter can share more about the specific pay range for your preferred location during the hiring process. kla is proud to be an equal opportunity employer. we will ensure that qualified individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. please contact us at talent.acquisition@kla.com or at +1-408-352-2808 to request accommodation. be aware of potentially fraudulent job postings or suspicious recruiting activity by persons that are currently posing as kla employees. kla never asks for any financial compensation to be considered for an interview, to become an employee, or for equipment. further, kla does not work with any recruiters or third parties who charge such fees either directly or on behalf of kla. please ensure that you have searched kla’s careers website for legitimate job postings. kla follows a recruiting process that involves multiple interviews in person or on video conferencing with our hiring managers. if you are concerned that a communication, an interview, an offer of employment, or that an employee is not legitimate, please send an email to talent.acquisition@kla.com to confirm the person you are communicating with is an employee. we take your privacy very seriously and confidentially handle your information. now hiring curious minds who want to learn even more. explore kla at www.kla.com/careers kla is proud to be an equal opportunity employer. we will ensure that qualified individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. please contact us at talent.acquisition@kla.com or at +1-408-352-2808 to request accommodation. for additional information, view the us know your rights poster on the u.s. equal employment opportunity commission website. we do not accept resumes from headhunters, placement agencies, or other suppliers that have not signed a formal agreement with us.",united states,Data Scientist,"['feature engineering', 'machine learning', 'power bi', 'python', 'r', 'scala', 'snowflake', 'sql', 'time series']","['feature engineering', 'machine learning', 'power bi', 'python', 'r', 'scala', 'snowflake', 'sql', 'time series']",
data scientist (ai/ml),"billgo, inc.","why this role matters we're looking for a data scientist with deep ai and machine learning expertise to help shape the future of data-driven innovation in fintech. you'll work on developing intelligent systems that power risk modeling, fraud prevention, customer insights and targeting, and payment optimization. your models will have a direct impact on financial decisions, operational efficiency, and customer trust across our products. what you'll do • ai-driven insights: develop and deploy advanced machine learning models to optimize customer targeting, payment monitoring and growth and operational efficiency opportunities. • predictive modeling: build forecasting models to improve transaction accuracy, detect anomalies, and assess financial risk. • data engineering & feature design: clean, transform, and model large, high-velocity financial datasets with attention to data integrity and compliance. • ai product integration: collaborate with product to integrate ai solutions into production systems for real-time financial decisioning. • experimentation: lead a/b tests and model performance evaluations to validate model effectiveness and regulatory compliance. • communication: translate technical findings into actionable insights for business leaders and compliance teams. • research & innovation: stay on top of advancements in generative ai, llms, and financial ai applications to guide innovation strategy. what you bring • bachelor's or master's degree in data science, computer science, statistics, or related field. • 3+ years of experience in a data science or ai-focused role within fintech, banking, or payments. • expertise in python, machine learning frameworks (scikit-learn, tensorflow, pytorch), and data pipelines. • strong background in supervised/unsupervised learning, anomaly detection, nlp, and generative ai. • familiarity with financial data structures, regulatory standards (e.g., pci-dss, gdpr), and model governance. • experience with cloud platforms such as snowflake for ml deployment. preferred qualifications • experience in fraud analytics, risk scoring, or payment decision models. • understanding of mlops and continuous model monitoring in regulated environments. • familiarity with financial transaction data, open banking apis, or real-time payments systems. • experience developing llm-powered assistants or ai copilots for financial operations or support. • strong data storytelling and visualization skills (tableau preferred).","fort collins, co",Data Scientist,"['cloud', 'data pipeline', 'experimentation', 'machine learning', 'nlp', 'python', 'pytorch', 'r', 'scikit-learn', 'snowflake', 'statistics', 'tableau', 'tensorflow']","['cloud', 'data pipeline', 'experimentation', 'machine learning', 'nlp', 'python', 'pytorch', 'r', 'scikit-learn', 'snowflake', 'statistics', 'tableau', 'tensorflow']",102K–147K a year
data scientist-poc,sardine,"who we are: we are a leader in fraud prevention and aml compliance. our platform uses device intelligence, behavior biometrics, machine learning, and ai to stop fraud before it happens. today, over 300 banks, retailers, and fintechs worldwide use sardine to stop identity fraud, payment fraud, account takeovers, and social engineering scams. we have raised $145m from world-class investors, including andreessen horowitz, activant, visa, experian, fis, and google ventures. our culture: • we have hubs in the bay area, nyc, austin, and toronto. however, we maintain a remote-first work culture. #workfromanywhere • we hire talented, self-motivated individuals with extreme ownership and high growth orientation. • we value performance and not hours worked. we believe you shouldn't have to miss your family dinner, your kid's school play, friends get-together, or doctor's appointments for the sake of adhering to an arbitrary work schedule. location • remote - us or canada • from home / beach / mountain / cafe / anywhere! • we are a remote-first company with a globally distributed team. you can find your productive zone and work from there. about the role as a data scientist on our team, you’ll play a key role in helping customers stay ahead of evolving fraud threats by designing and deploying data-driven solutions with real-world impact. you’ll work directly with clients to understand their unique fraud challenges, rapidly prototype proof-of-concept models, and build scalable, production-ready solutions using machine learning and graph analytics. you’ll also help standardize modeling workflows and collaborate with engineering to streamline backend systems. this is a hands-on, high-impact role ideal for someone who thrives at the intersection of data science, client-facing problem solving, and real-time risk. what you’ll be doing • champion a data-first approach across internal teams and client engagements, promoting clarity and impact • build and deploy machine learning models to prevent fraud across diverse fintech use cases • use data and models to support the development of risk mitigation strategies and interventions while preserving and improving the user experience • work directly with clients to understand challenges and deliver high-impact, data-driven solutions • evolve our risk metrics, the supporting datasets, and how we measure the causal impact of initiatives • collaborate with engineering to scale models into production and optimize performance what you’ll need • 7+ years of experience in data science or quantitative modeling, ideally in risk or fraud contexts • advanced degree in a quantitative field (mathematics, statistics, computer science, engineering, economics, etc.) • strong working knowledge of python, r, spark, sql, or equivalent • sharp critical thinking and creative problem-solving skills with a bias toward action • proven ability to explain complex technical findings to non-technical stakeholders and clients benefits we offer: • generous compensation in cash and equity • early exercise for all options, including pre-vested • work from anywhere: remote-first culture • flexible paid time off, year-end break, self care days off • health insurance, dental, and vision coverage for employees and dependents - us and canada specific • 4% matching in 401k / rrsp - us and canada specific • macbook pro delivered to your door • one-time stipend to set up a home office — desk, chair, screen, etc. • monthly meal stipend • monthly social meet-up stipend • annual health and wellness stipend • annual learning stipend • unlimited access to an expert financial advisory join a fast-growing company with world-class professionals from around the world. if you are seeking a meaningful career, you found the right place, and we would love to hear from you. to learn more about how we process your personal information and your rights in regards to your personal information as an applicant and sardine employee, please visit our applicant and worker privacy notice.",anywhere,Data Scientist,"['machine learning', 'python', 'r', 'scala', 'spark', 'sql', 'statistics']","['machine learning', 'python', 'r', 'scala', 'spark', 'sql', 'statistics']",
data scientist,adams county,"job the assessment data scientist serves as the assessor’s office senior data science expert, leading the design, implementation, and oversight of advanced computer-assisted mass appraisal (cama) and automated valuation models (avms). this role applies predictive analytics, statistical modeling, and machine learning to improve the accuracy, transparency, and efficiency of property valuations in alignment with statutory deadlines, division of property taxation guidelines, and iaao standards. the position ensures equitable valuation by identifying market trends, detecting anomalies, and improving data quality through collaboration with appraisal, gis, and it teams. additionally, the role translates complex modeling results into clear, actionable insights for leadership, stakeholders, and the public, while mentoring staff to build organizational capacity in advanced analytics. example of duties serve as a senior-level data science and modeling expert in the assessor’s office, responsible for developing and managing advanced computer-assisted mass appraisal (cama) and automated valuation models (avms).design and implement predictive analytics and machine learning models to enhance accuracy, transparency, and efficiency in property valuation processes.ensure compliance with statutory property valuation deadlines, division of property taxation (dpt) guidelines, and international association of assessing officers (iaao) standards through data-driven modeling and quality control.lead the development of scalable statistical and machine learning solutions to identify market trends, detect anomalies, and support equitable valuation of all property types.collaborate with appraisal, gis, and it teams to improve data quality, leverage automation, and integrate cutting-edge analytics into existing workflows.translate complex modeling outputs into actionable insights for leadership, stakeholders, and the public.develop, implement, and maintain advanced avms and cama models using multiple regression, machine learning algorithms, and predictive analytics techniques.perform ratio studies, time trend analysis, and depreciation modeling using both traditional statistical methods and advanced ai-driven approaches.analyze large-scale datasets from cama systems, gis platforms, and external sources to uncover patterns, drivers of value, and predictive variables.build and deploy interactive dashboards and visual analytics tools (tableau, power bi) to communicate key insights and support decision-making.monitor evolving division of property taxation (dpt) guidelines, iaao standards, and emerging technologies to ensure compliance and maintain innovation in valuation practices.implement continuous model performance testing, error analysis, and recalibration to improve predictive accuracy and reduce bias.support automation strategies and process improvements for mass appraisal through scripting, apis, and integration with enterprise systems.train and mentor staff on data literacy, advanced analytics, and machine learning applications in property valuation.present modeling outcomes and predictive insights to management and public stakeholders in a clear, accessible format.perform other related duties as assigned to enhance data-driven operations within the assessor’s office. supplemental information any combination of experience and training that would likely provide the required knowledge and abilities is qualifying. a typical way to obtain the knowledge and abilities would be: experience: six (6) years of increasingly responsible related data collection, process analysis/mapping, research, and statistical analysis experience, including a minimum of two (2) years’ related work experience in property tax assessment or a similar field. education and training: bachelor’s degree in data science, statistics, mathematics, economics, computer science, or a related field.a master’s degree is preferred and may substitute 2 years’ required experience, excluding work in property tax assessment. license or certificate: none. background check: must pass a criminal background check.other: state residency: at the time of appointment, candidate must be a legal resident of the state of colorado.","brighton, co",Data Scientist,"['dashboard', 'machine learning', 'power bi', 'r', 'regression', 'scala', 'statistics', 'tableau']","['dashboard', 'machine learning', 'power bi', 'r', 'regression', 'scala', 'statistics', 'tableau']","91,110.24–132,109.84 a year"
data scientist l6 - games portfolio,netflix,"netflix is one of the world's leading entertainment services, with over 300 million paid memberships in over 190 countries enjoying tv series, films and games across a wide variety of genres and languages. members can play, pause and resume watching as much as they want, anytime, anywhere, and can change their plans at any time. data science and engineering (‘dse’) at netflix is aimed at using data, analytics, causal inference, machine learning (ml), and sciences to improve various aspects of our business. we are seeking an experienced l6 data scientist for the games portfolio team to define and lead the technical strategy and execution for the game portfolio team. this role is responsible for the technical leadership across the team’s core scope: developing the metrics ecosystem, driving incremental value research, establishing forecasting models, synthesizing audience insights, and defining transmedia impact frameworks. you will be the senior technical anchor, ensuring our data, models, and systems are robust, scalable, and provide the objective, timely insights that enable games leadership to make informed decisions on investment, content scheduling, and overall portfolio strategy. in this role, you will: • lead the overall technical vision for forecasting, metrics ecosystems, audience insights, and transmedia research. • own the development and operation of robust and interpretable models for pre-launch and post-launch forecasting. • empower the team to own metrics ecosystem, audience insights, and transmedia research, enabling the netflix games’ objectives as the business grows and evolves. • serve as the most senior technical resource, mentoring the team and fostering data science excellence. • act as a technical bridge and strategic thought partner to games leadership, finance & strategy, engineering, and sister dse teams, ensuring production-grade data products. • proactively synthesize deep technical insights into clear, actionable findings for executive and senior leadership. • autonomously identify and pursue the high-impact research, making compelling, data-driven cases for resource allocation and prioritization across the portfolio space. who will succeed in this role: • ph.d. in statistics, mathematics, economics, computer science, or a related quantitative field. • 8+ years of relevant experience in a data science or ml role, with significant experience defining technical strategy and leading large, complex data science projects end-to-end. • proven experience serving as a tech lead, guiding other technical contributors and owning the technical quality, architecture, and roadmap for a product or data science area. • deep expertise in programming skills in python, with proven experience building and deploying models and data pipelines at large scale using distributed systems. • expert-level knowledge of statistical modeling, causal inference, time series analysis, and applied machine learning techniques, with a portfolio of deployed models. • proven ability to simplify complex technical architecture and research findings into strategic insights for technical partners, cross-functional stakeholders, and executive leadership. • proven ability to translate ambiguous, high-level business goals into a coherent, scalable technical and analytical system. • proven ability to cultivate strong, dependable partnerships with cross-functional teams. nice to have • experience leading the technical definition of core business metrics in the gaming, streaming, or subscription business models. • experience with building production-grade, large-scale ml systems, including mlops best practices. • experience with solving business challenges with deep neural networks. • experience with genai applications to boost developer productivity. our compensation structure consists solely of an annual salary; we do not have bonuses. you choose each year how much of your compensation you want in salary versus stock options. to determine your personal top of market compensation, we rely on market indicators and consider your specific job family, background, skills, and experience to determine your compensation in the market range. the range for this role is $400,000 - $960,000. netflix provides comprehensive benefits including health plans, mental health support, a 401(k) retirement plan with employer match, stock option program, disability programs, health savings and flexible spending accounts, family-forming benefits, and life and serious injury benefits. we also offer paid leave of absence programs. full-time hourly employees accrue 35 days annually for paid time off to be used for vacation, holidays, and sick paid time off. full-time salaried employees are immediately entitled to flexible time off. see more detail about our benefits here. netflix has a unique culture and environment. learn more here. inclusion is a netflix value and we strive to host a meaningful interview experience for all candidates. if you want an accommodation/adjustment for a disability or any other reason during the hiring process, please send a request to your recruiting partner. we are an equal-opportunity employer and celebrate diversity, recognizing that diversity builds stronger teams. we approach diversity and inclusion seriously and thoughtfully. we do not discriminate on the basis of race, religion, color, ancestry, national origin, caste, sex, sexual orientation, gender, gender identity or expression, age, disability, medical condition, pregnancy, genetic makeup, marital status, or military service. job is open for no less than 7 days and will be removed when the position is filled.",united states,Data Scientist,"['data pipeline', 'excel', 'machine learning', 'python', 'r', 'scala', 'statistics', 'time series']","['data pipeline', 'excel', 'machine learning', 'python', 'r', 'scala', 'statistics', 'time series']",
software engineer / data scientist,parsons corporation,"in a world of possibilities, pursue one with endless opportunities. imagine next! at parsons, you can imagine a career where you thrive, work with exceptional people, and be yourself. guided by our leadership vision of valuing people, embracing agility, and fostering growth, we cultivate an innovative culture that empowers you to achieve your full potential. unleash your talent and redefine what’s possible. job description: parsons has emerged as a leader in the development of cutting-edge solutions for the department of defense and intelligence community. our tremendous success can be attributed to our people and our priorities. we hire the best; we make them a priority and we never lose focus on the mission. it’s why we’re here. we have built this cultural legacy by working closely with analysts and operators to understand their needs and delivering meaningful value through innovative, cost effective and intuitive software solutions. our space operations directorate is passionate about making america the undisputed leader in space because we understand that ensuring our nation’s security for future generations depends on it. parsons creates game changing space solutions by teaming highly respected subject matter experts with brilliant technologists. are you an experienced software engineer looking to grow your skillset and work on a project that has real world impact? do you want to be part of a team that is helping the government solve major national security challenges in the space domain? we need your help. our team is looking for a software engineer with a focus on data science, machine learning, and advanced analytical approaches within the space domain awareness mission area. you will work with software engineers and space analysts to develop state-of-the-art solution in pattern of life, anomaly detection, and predictive analysis. this position is in colorado springs and requires working in the office a minimum of 2 days a week. required skills • bachelor’s degree in data science, machine learning, computer science, mathematics or a related technical field with at least 5 years technical experience. relevant experience may be accepted in place of a degree • experience with the following data science technologies • various machine learning libraries (i.e. apache spark, scikit-learn, xgboost, etc.) • data manipulation and pipeline libraries (i.e. pandas, polars, matplotlib, plotly, numpy, scipy, etc.) • data science environments (e.g. jupyter notebook, data bricks, or amazon sage maker) • experience implementing the data science process, developing experiments, reporting and explaining results • experience with python and/or java programming languages • experience designing, developing, documenting, testing, and debugging enterprise applications • familiar with linux/unix operating systems • great interpersonal and communications skills, while establishing and maintaining effective work relationships with team members • must be comfortable working in a fast-paced, flexible environment, and take the initiative to learn new tools and concepts quickly. • experience with common cm tools and best practices • excellent communication skills, in both spoken and written english • active ts security clearance, with sci eligibility preferred • must be a us citizen due to dod contract desired skills • familiarity with the following technologies: • deep learning libraries (i.e. tensorflow, keras, pytorch, etc.) • python web technologies (i.e. dash, flask, django, etc.) • natural language processing (nlp) • sql commands and queries scripting languages (e.g. groovy, python, powershell, bash) • spring boot • devsecops concepts and using ci/cd pipeline technologies such as jenkins, gitlab, docker • aws/govcloud technologies • experience working in an agile software development environment using the scrum methodology • security+ certification • aws cloud practitioner certification security clearance requirement: an active top secret sci security clearance is required for this position.​ this position is part of our federal solutions team. the federal solutions segment delivers resources to our us government customers that ensure the success of missions around the globe. our intelligent employees drive the state of the art as they provide services and solutions in the areas of defense, security, intelligence, infrastructure, and environmental. we promote a culture of excellence and close-knit teams that take pride in delivering, protecting, and sustaining our nation's most critical assets, from earth to cyberspace. throughout the company, our people are anticipating what’s next to deliver the solutions our customers need now. salary range: $100,900.00 - $176,600.00 we value our employees and want our employees to take care of their overall wellbeing, which is why we offer best-in-class benefits such as medical, dental, vision, paid time off, 401(k), life insurance, flexible work schedules, and holidays to fit your busy lifestyle! this position will be posted for a minimum of 3 days and will continue to be posted for an average of 30 days until a qualified applicant is selected or the position has been cancelled. parsons is an equal opportunity employer, and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, veteran status or any other protected status. we truly invest and care about our employee’s wellbeing and provide endless growth opportunities as the sky is the limit, so aim for the stars! imagine next and join the parsons quest—apply today! parsons is aware of fraudulent recruitment practices. to learn more about recruitment fraud and how to report it, please refer to https://www.parsons.com/fraudulent-recruitment/.","colorado springs, co",Data Scientist,"['aws', 'cloud', 'deep learning', 'excel', 'java', 'keras', 'machine learning', 'matplotlib', 'natural language processing', 'nlp', 'numpy', 'pandas', 'python', 'pytorch', 'r', 'scikit-learn', 'spark', 'sql', 'tensorflow', 'xgboost']","['aws', 'cloud', 'deep learning', 'excel', 'java', 'keras', 'machine learning', 'matplotlib', 'natural language processing', 'nlp', 'numpy', 'pandas', 'python', 'pytorch', 'r', 'scikit-learn', 'spark', 'sql', 'tensorflow', 'xgboost']",
senior data scientist,uplight,"uplight is creating a new category of energy. we make software that manages energy resources in homes and businesses—including things like smart thermostats, electric vehicles, solar panels, storage batteries, heat pumps, and even people’s behavior—to generate, shift, or save energy to balance the grid, making it more efficient and reliable. this creates clean energy capacity that can be used by the power grid instead of burning more fossil fuels. our solutions accelerate the transition to clean energy and save money for energy customers. we are seeking a highly skilled data scientist with a focus on optimization and software engineering to join our predictive controls team. our team is responsible for optimizing and dispatching flexible energy resources to participate in a variety of grid and market services, including virtual power plants (vpps), wholesale energy arbitrage, ancillary services, demand response, and grid support. the ideal candidate brings a deep understanding of electricity markets, strong analytical and modeling capabilities, and robust software engineering skills to deploy and maintain optimization logic in a production environment. you will work at the intersection of energy systems, mathematical optimization, and software development, collaborating closely with both internal teams and external customers. how you will make an impact: domain knowledge & modeling • develop and maintain models of distributed energy resources (ders) and flexible energy assets for operational and financial optimization. • apply knowledge of electricity markets and grid services to build realistic and effective dispatch strategies. • understand and interpret regulatory frameworks and market mechanisms (e.g., frequency regulation, capacity markets, bidding logic). optimization & modeling • build, test, and deploy optimization models using tools like pyomo, ampl, or similar. • use solvers such as gurobi, cplex, or open-source alternatives for solving large-scale scheduling and dispatch problems. • design models that balance performance, scalability, and interpretability. software engineering • develop production-grade software, primarily in python, with strong attention to clean code, modularity, and performance. • work with version control (git), ci/cd tools, and containerization (docker/kubernetes) for robust development workflows. • contribute to codebase architecture, testing frameworks, and deployment pipelines. production support & reliability • monitor, debug, and improve live optimization systems and supporting data pipelines. • troubleshoot complex issues across modeling, data quality, and infrastructure layers. • use observability tools (e.g., logging, metrics, alerting) to maintain system reliability. customer-facing solution engineering • interpret and implement non-standard customer requirements, including unique asset behaviors or contractual obligations. • customize optimization logic and integrate customer-specific configurations. • communicate technical concepts clearly to non-technical stakeholders and customers. testing & quality assurance • design integration tests that span multiple platform components (e.g., configurator + optimizer + forecast + dispatch). • implement regression testing to safeguard against unintended model changes. • simulate edge cases and real-world scenarios to validate robustness and performance. what you bring to uplight: • proven experience in mathematical optimization (e.g., lp, milp), especially in energy systems or similar domains. • proficient in programming language (e.g., python) with production-level software engineering experience. • strong understanding of electricity markets, ders, and vpp operation. • familiarity with optimization frameworks (e.g., pyomo, ampl) and solvers (e.g., gurobi, cplex). • experience with ci/cd, docker, version control, and observability practices. • ability to reason about trade-offs between model fidelity, performance, and business value. bonus points: • experience working with flexible energy assets (e.g., batteries, hvac, evs). • exposure to real-time systems or critical infrastructure software. • familiarity with cloud environments (e.g., aws, gcp, azure) and scalable deployment strategies. • prior work in a customer-facing technical role. why join uplight in leading the fight against climate change? at uplight, we're not just offering a job – we're offering a chance to be part of the solution to one of the world's biggest challenges. as a certified b corporation, we're deeply committed to both social and environmental responsibility. here's why you should join our team of passionate uplighters: • make a meaningful impact: your work directly impacts our mission of decarbonization and building a more sustainable future. • grow your career: we offer ample advancement opportunities, robust learning and development programs, and a supportive team environment that fosters collaboration and innovation. • thrive: we offer comprehensive benefits, including flexible time off, generous parental leave, a wellness stipend, and work flexibility to help you thrive both personally and professionally. • belong to an inclusive community: we celebrate diversity and foster an inclusive workplace where everyone feels respected, empowered, and heard. our employee resource groups offer opportunities to connect with colleagues who share your interests and backgrounds. • be part of a growing movement: join a team of dedicated individuals who are passionate about creating a more sustainable future. we offer a collaborative environment where your ideas are valued and your contributions recognized. together, we can build a brighter tomorrow. to learn more about our comprehensive benefits package and other perks, visit uplight.com/careers salary range: $140,000-165,000 usd + bonus application deadline: october 15, 2025 in accordance with the colorado equal pay for equal work act, the approximate annual base compensation range is listed above. the actual offer, reflecting the total compensation package and benefits, will be determined by a number of factors including the applicant's experience, knowledge, skills, and abilities, as well as internal equity among our team. uplight provides equal employment opportunities (eeo) to all employees and applicants for employment without regard to race (including hair texture and hairstyles), color, religion (including head coverings), age, sex, national origin, disability status (including neurodivergence), genetics, protected veteran status, sexual orientation, gender identity or expression, neurotypicality, or any other characteristic protected by federal, state or local laws",united states,Data Scientist,"['aws', 'azure', 'cloud', 'data pipeline', 'gcp', 'python', 'r', 'regression', 'scala']","['aws', 'azure', 'cloud', 'data pipeline', 'gcp', 'python', 'r', 'regression', 'scala']",
"staff data scientist, forecasting",pinterest,"about pinterest: millions of people around the world come to our platform to find creative ideas, dream about new possibilities and plan for memories that will last a lifetime. at pinterest, we’re on a mission to bring everyone the inspiration to create a life they love, and that starts with the people behind the product. discover a career where you ignite innovation for millions, transform passion into growth opportunities, celebrate each other’s unique experiences and embrace the flexibility to do your best work. creating a career you love? it’s possible. what you’ll do • be the technical lead for the forecasting team. own the strategy and implementation of forecasting models of key company metrics (e.g., monthly active users), delivering accurate, interpretable forecasts at scale. • lead the full modeling lifecycle end to end: problem framing, feature engineering, model development and prototyping, experimentation and backtesting, deployment, monitoring/drift detection, and explainability. • set the forecasting technical vision. define model architectures and standards, and partner with engineering to shape the forecasting platform for efficient training/inference today and the scalability needed for the next generation of models. • translate forecasts into decisions. present outputs, scenario analyses, and recommendation frameworks to senior leadership with clarity and brevity. this is a high‑visibility role with regular vp-level exposure. • drive broader time‑series impact beyond point forecasts—e.g., anomaly detection, automated root‑cause analysis, campaign/channel attribution, and early‑warning signals for business health. • embed forecasting into the business. partner with bizops/finance and product teams to integrate forecasts and insights into operational rhythms, executive decision-making, and strategic planning. • lead and mentor. guide the work of at least two data scientists, raising the bar on technical quality, execution, and impact through candid, continuous feedback and coaching. what we’re looking for • 8+ years of combined post-graduate academic and industry experience building and shipping production time‑series/forecasting models with web‑scale data. • a track record of delivering adjustable, well‑calibrated, and explainable forecasting systems that informing decision-making. • strong background in time‑series modeling and applied statistics/econometrics; advanced degree (ms or phd) preferred. • expertise in at least one scripting language (ideally python). • strong sql skills (hive/presto/spark sql) and experience building reliable data pipelines/workflows (e.g., airflow). • business acumen and ownership mindset—able to simplify complex problems, connect model outputs to business levers, and prioritize for impact. • excellent communication skills—able to distill complex analyses and uncertainty into concise narratives for executive audiences. • proven technical leadership—success leading critical projects and materially influencing the scope and output of other contributors. relocation statement: • this position is not eligible for relocation assistance. visit our pinflex page to learn more about our working model. in-office requirement statement: • we let the type of work you do guide the collaboration style. that means we're not always working in an office, but we continue to gather for key moments of collaboration and connection. • this role will need to be in the office for in-person collaboration 1-2 times/quarter and therefore can be situated anywhere in the country. #li-nm4 #li-remote at pinterest we believe the workplace should be equitable, inclusive, and inspiring for every employee. in an effort to provide greater transparency, we are sharing the base salary range for this position. the position is also eligible for equity. final salary is based on a number of factors including location, travel, relevant prior experience, or particular skills and expertise. information regarding the culture at pinterest and benefits available for this position can be found here. us based applicants only $164,695—$339,078 usd our commitment to inclusion: pinterest is an equal opportunity employer and makes employment decisions on the basis of merit. we want to have the best qualified people in every job. all qualified applicants will receive consideration for employment without regard to race, color, ancestry, national origin, religion or religious creed, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, age, marital status, status as a protected veteran, physical or mental disability, medical condition, genetic information or characteristics (or those of a family member) or any other consideration made unlawful by applicable federal, state or local laws. we also consider qualified applicants regardless of criminal histories, consistent with legal requirements. if you require a medical or religious accommodation during the job application process, please complete this form for support.",anywhere,Data Scientist,"['airflow', 'aws', 'data pipeline', 'excel', 'experimentation', 'feature engineering', 'python', 'r', 'recommendation', 'scala', 'spark', 'sql', 'statistics']","['airflow', 'aws', 'data pipeline', 'excel', 'experimentation', 'feature engineering', 'python', 'r', 'recommendation', 'scala', 'spark', 'sql', 'statistics']",
"senior ai data scientist, consumer lending - remote",experian,"experian is looking for an experienced senior ai data scientist to join our analytic innovations group. this individual contributor role will focus on developing agentic ai solutions and advanced marketing analytics models that move strategic decision-making across the consumer credit lifecycle. you will have deep technical expertise in coding intelligent agents, experience in marketing analytics for consumer lending products with modeling knowledge plus experience applying ai to real-world challenges in financial services. this is a fully remote position. you will report to the vp of analytics consulting. you'll have opportunity to: • design, code and test ai agents that support autonomous decision-making in marketing campaigns, customer acquisition, and portfolio optimization. • apply hands-on coding knowledge to create agentic ai techniques to solve complex business problems in consumer lending. • conduct exploratory data analysis and develop predictive models using credit bureau data and other consumer datasets. • prototype and improve on intelligent agent behaviors using python, spark and modern agentic frameworks. • support the deployment and monitoring of ai models in production environments, ensuring performance, reliability and compliance. • collaborate with company partners to translate our requirements into scalable ai solutions. • contribute to the development of new analytical products and services that enhance marketing effectiveness and customer engagement. about experian experian is a global data and technology company, powering opportunities for people and businesses around the world. we help to redefine lending practices, uncover and prevent fraud, simplify healthcare, create marketing solutions, and gain deeper insights into the automotive market, all using our unique combination of data, analytics and software. we also assist millions of people to work towards their financial goals and help them save time and money. we operate across a range of markets, from financial services to healthcare, automotive, agribusiness, insurance, and many more industry segments. we invest in people and new advanced technologies to unlock the power of data. as a ftse 100 index company listed on the london stock exchange (expn), we have a team of 22,500 people across 32 countries. our corporate headquarters are in dublin, ireland. learn more at experianplc.com. experience and skills • expertise in advanced python programming, including experience with data manipulation, modeling and deployment in cloud environments. • proficient in using big data platforms for scalable analytics and deployment • hands-on experience building agentic ai frameworks or tools, including autonomous agents • experience with cloud-based data platforms and production-grade ml deployment. • 5+ years of experience in consumer lending marketing analytics; including an understanding of consumer credit data and marketing analytics methodologies additional information benefits/perks: • competitive compensation package and bonus plan • core benefits including medical, dental, vision, and matching 401k • flexible work environment, ability to work remote, hybrid or in-office • flexible time off including volunteer time off, vacation, sick and 12-paid holidays • explore all our exciting benefits here: https://yourexperianbenefits.com/cand-index.html at experian, our people and culture set us apart. we're deeply committed to creating an environment where everyone feels they belong and can excel. from inclusion and authenticity to work/life balance, development, wellness, collaboration, and recognition, we focus on what truly matters. our people-first approach has earned us global recognition: world's best workplaces™ 2024 (fortune top 25), great place to work™ 2025 in 26 countries, and glassdoor best places to work 2024, among others. want to see what life at experian is really like? explore experian life on social or visit our careers site. our compensation reflects the cost of labor across several u.s. geographic markets. the base pay range for this position is listed above. within this range, individual pay is determined by work location and additional factors such as job-related skills, experience, and education. this position is also eligible for a variable pay opportunity and a comprehensive benefits package. experian is proud to be an equal opportunity employer for all groups protected under applicable federal, state and local law, including protected veterans and individuals with disabilities. if you have a disability or special need that requires accommodation, please let us know at the earliest opportunity. #li-remote",united states,Data Scientist,"['cloud', 'data analysis', 'excel', 'python', 'r', 'scala', 'spark']","['cloud', 'data analysis', 'excel', 'python', 'r', 'scala', 'spark']",
data scientist i,kforce inc,"responsibilities kforce has a client that is seeking a data scientist i in greenwood village, co. summary: we are seeking a data scientist i to support a telecom client's customer experience team, focusing on customer retention through real-time ai solutions. this role is ideal for candidates with strong skills in sql, python, ai, and nlp, with a passion for advancing towards large language model (llm) development. key tasks: • data scientist i will be responsible for mining complex data and providing systems-related advice for their organization • design new ways to incorporate vast information with a focus on information technology topics • work with teams of other it professionals to manage statistical data and create different models based on the needs of their company • possess advanced analytical skills, in addition to their exceptional oral and written communication abilities • process research information for easier consumption and transform it into actionable plans • develop and implement ai-driven solutions to enhance customer retention strategies • work with nlp models to analyze customer interactions and extract insights • as a data scientist i, you will assist in the transition from current nlp approaches to developing custom llm models • utilize sql for data extraction, transformation, and analysis • build and maintain real-time ai models to improve customer experience • collaborate with cross-functional teams to integrate ai solutions into business processes requirements • 3+ years of experience as a data scientist • experience in ai-driven applications, particularly in nlp • proficiency in sql for data manipulation and querying • strong python skills, including experience with ai and nlp libraries • ability to work with real-time data processing and ai model deployment preferred skills • knowledge of large language models (llms) and their development • background in telecommunications or call center analytics the pay range is the lowest to highest compensation we reasonably in good faith believe we would pay at posting for this role. we may ultimately pay more or less than this range. employee pay is based on factors like relevant education, qualifications, certifications, experience, skills, seniority, location, performance, union contract and business needs. this range may be modified in the future. we offer comprehensive benefits including medical/dental/vision insurance, hsa, fsa, 401(k), and life, disability & add insurance to eligible employees. salaried personnel receive paid time off. hourly employees are not eligible for paid time off unless required by law. hourly employees on a service contract act project are eligible for paid sick leave. note: pay is not considered compensation until it is earned, vested and determinable. the amount and availability of any compensation remains in kforce's sole discretion unless and until paid and may be modified in its discretion consistent with the law. this job is not eligible for bonuses, incentives or commissions. kforce is an equal opportunity/affirmative action employer. all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, sexual orientation, gender identity, national origin, age, protected veteran status, or disability status. by clicking “apply today” you agree to receive calls, ai-generated calls, text messages or emails from kforce and its affiliates, and service providers. note that if you choose to communicate with kforce via text messaging the frequency may vary, and message and data rates may apply. carriers are not liable for delayed or undelivered messages. you will always have the right to cease communicating via text by using key words such as stop.","englewood, co",Data Scientist,"['nlp', 'python', 'r', 'sql']","['nlp', 'python', 'r', 'sql']",40–45 an hour
senior product data scientist,courtyard.io,"about courtyard courtyard.io is one of the fastest-growing collectibles startups. from cards to coins, we’re making it faster, easier, and more exciting than ever to discover, collect, and cash out instantly. we’re not just another marketplace. with thrilling pack rips, instant liquidity, and seamless vaulting, courtyard.io delivers the ultimate collecting experience. whether you’re investing, trading, or curating your dream collection, we’ve built a platform that’s trusted, simple, and built for speed. and we’re just getting started. we’re a remote-first company hiring across all functions to push the boundaries of what’s possible in collectibles and digital ownership. about the role we are actively recruiting a senior product data scientist to help revolutionize the way people own and collect. at courtyard.io, you'll play a key role in leveraging data to improve user retention, optimize marketplace efficiency, and drive product strategy. this role requires expertise in sql, a deep understanding of user behavior analytics, and the ability to think strategically about both peer-to-peer and centralized marketplace models. you will join a lean and experienced remote-first team of engineers that prioritizes technical ingenuity, authentic communication, and a get-it-done attitude. you’ll own core parts of our data infrastructure and work directly with the ceo and heads of engineering, marketing and finance to shape our products, business decisions and roadmap. what you'll do • analyze customer behavior to uncover insights that drive engagement, retention, and lifetime value. • profile users to understand how they interact with courtyard and develop user segmentations • develop guiding metrics and kpis for the product team to evaluate success • partner with product teams to develop features that improve conversion and user retention. • design and analyze a/b tests to validate these features and measure product impact. • evaluate the health of our peer-to-peer and centralized marketplace models, identifying inefficiencies and opportunities for growth. • write complex sql queries and pipelines to understand user behavior and empower experiments • create guiding visualizations and dashboards for monitoring the health of the business what you'll bring • you are passionate about data and thrive in a fast-paced, collaborative environment. • 5+ years of experience in a data science, analytics, or similar role in an e-commerce or marketplace environment. • proficiency in sql and experience working with large datasets. • strong analytical and statistical skills, with a deep understanding of user retention and behavioral analytics. • experience with a/b testing, experimentation, and causal inference. experience with multi-armed bandit approaches a plus. • experience with a/b testing platforms such as statsig • understanding of marketplace mechanics, including supply-demand dynamics, network effects, and pricing models. • experience with python or r for data analysis and modeling is a plus. • comfortable with dashboarding/data visualization (looker, tableau, posthog etc.). • you take ownership of your work with a leadership mindset, demonstrating initiative, accountability, and the ability to drive projects forward independently. • proven ability to work independently on a remote-first team, with experience in asynchronous communication. top-notch communication skills are essential. what you'll get in return • a dynamic and engaging environment focused on fostering real growth and innovation • opportunities to create amazing products that our customers truly love and value • comprehensive health insurance packages with dependent coverage • competitive salary with ample opportunities for career advancement and development • enjoy the flexibility of a fully remote work environment • 401(k) plan with a 4% employer match to help you plan for the future • $100 monthly dogfooding stipend to support trying out our products firsthand • access to employee wellness programs designed to support your overall well-being",anywhere,Data Scientist,"['a/b testing', 'dashboard', 'data analysis', 'experimentation', 'looker', 'python', 'r', 'sql', 'tableau']","['a/b testing', 'dashboard', 'data analysis', 'experimentation', 'looker', 'python', 'r', 'sql', 'tableau']",
data scientist / machine learning engineer,the pennsylvania state university,"application instructions: current penn state employee (faculty, staff, technical service, or student), please login to workday to complete the internal application process. please do not apply here, apply internally through workday. current penn state student (not employed previously at the university) and seeking employment with penn state, please login to workday to complete the student application process. please do not apply here, apply internally through workday. if you are not a current employee or student, please click “apply” and complete the application process for external applicants. approval of remote and hybrid work is not guaranteed regardless of work location. for additional information on remote work at penn state, see notice to out of state applicants. position specifics we are seeking highly motivated data science/machine learning research and development engineers to join our computational intelligence and visualization application department of the applied research laboratory (arl) at penn state. you will assist in providing customers with state-of-the-art visualization and decision support. the primary focus will be researching and developing feasible algorithmic solutions to our sponsor’s complex problems given large sets of multivariate data. located in either state college, pa or reston, va arl is an authorized dod skillbridge partner and welcomes all transitioning military members to apply. you will: design and implement machine learning systems, models, and schemes research and apply state-of-the-art machine learning prototypes to sponsor specific domains search and select appropriate data sets before performing data collection and data modeling visualize data for deeper insights generate and present technical research reports and briefings perform statistical analysis in support of pattern of life and anomaly detection perform data fusion and correlation build high quality software prototypes integrated with our various research and pre-production software environments collaborate with other arl staff and sponsors additional responsibilities of higher levels includes:· supervise the work of lower level staff and undergraduate student perform tasks of a larger scope and lead specific tasks within the project scope required skills/knowledge includes: data science/algorithm development data science / machine learning languages to include python research, development and implementation of machine learning models preferred skills/knowledge includes: active ts/sci clearance strongly preferred a master’s degree training and optimizing ml algorithms on gpu hardware architectures, specifically nvidia based working with geo-spatial data statistics, multivariable calculus, and linear algebra exploratory data analysis and visualization data science / machine learning tools to include pandas, pytorch/tensorflow, numpy, matplotlib, postgresql, fastapi software development tools to include git, docker, kubernetes, ssh active ts/sci clearance your work location will be on-site state college, pa or reston, va in a classified environment minimum education, work experience & required certifications if filled as research and development engineer - intermediate professional, this position requires: bachelor's degree - engineering or science 2+ years of relevant experience required certifications: none if filled as research and development engineer - professional, this position requires: bachelor's degree - engineering or science no prior relevant work experience required required certifications: none arl’s purpose is to research and develop innovative solutions to challenging scientific, engineering, and technology problems in support of the navy, the department of defense (dod), and the intel community (ic). for further information on arl, visit our web site at www.arl.psu.edu. background checks/clearances employment with the university will require successful completion of background check(s) in accordance with university policies. all positions at arl require candidates to possess the ability to obtain a government security clearance; you will be notified during the interview process if this position is subject to a government background investigation. you must be a u.s. citizen to apply. employment with the arl will require successful completion of a pre-employment drug screen. salary & benefits the salary range for this position, including all possible grades, is $76,700.00 - $129,500.00.**the proposed salary range may be impacted by geographic differential** salary structure - information on penn state's salary structure penn state provides a competitive benefits package for full-time employees designed to support both personal and professional well-being. in addition to comprehensive medical, dental, and vision coverage, employees enjoy robust retirement plans and substantial paid time off which includes holidays, vacation and sick time. one of the standout benefits is the generous 75% tuition discount, available to employees as well as eligible spouses and children. for more detailed information, please visit our benefits page. campus security crime statistics pursuant to the jeanne clery disclosure of campus security policy and campus crime statistics act and the pennsylvania act of 1988, penn state publishes a combined annual security and annual fire safety report (asr). the asr includes crime statistics and institutional policies concerning campus security, such as those concerning alcohol and drug use, crime prevention, the reporting of crimes, sexual assault, and other matters. the asr is available for review here. eeo is the law penn state is an equal opportunity employer and is committed to providing employment opportunities to all qualified applicants without regard to race, color, religion, age, sex, sexual orientation, gender identity, national origin, disability or protected veteran status. if you are unable to use our online application process due to an impairment or disability, please contact 814-865-1473. federal contractors labor law poster pa state labor law poster penn state policies copyright information hotlines there’s a reason penn state consistently ranks among the top one percent of the world’s universities. across 24 campuses, our 100,000 students and 40,000 faculty and staff know the real measure of success goes beyond the classroom – it’s the positive impact made on communities across the world. our ideals stem from our origins as pennsylvania’s sole land-grant institution, which allow us to continue to bring positive impact to humanity through our teaching, research, and engagement mission. and across the world is a community of penn staters – more than half a million strong – driven by that same spirit to serve our local and global communities. at penn state, passion is met with collaboration in an environment that is rooted in shared pride for the accomplishments of our fellow students, faculty, staff, and alumni. penn staters cheer each other on to build a culture committed to service and engagement. our university values represent our core ethical aspirations for all our daily activities and actions as students, faculty, staff, and volunteers at penn state. our values are as follows; integrity, respect, responsibility, discovery, excellence, and community. we are penn state. one community. impacting many.",united states,Data Scientist,"['data analysis', 'excel', 'machine learning', 'matplotlib', 'numpy', 'pandas', 'python', 'pytorch', 'r', 'sql', 'statistics', 'tensorflow']","['data analysis', 'excel', 'machine learning', 'matplotlib', 'numpy', 'pandas', 'python', 'pytorch', 'r', 'sql', 'statistics', 'tensorflow']",
senior data scientist (remote),tilt finance,"join the tilt team at tilt (formerly empower), we see a side of people that traditional lenders miss. our mobile-first products and machine learning-powered credit models look beyond outdated credit scores, using over 250 real-time financial signals to recognize real potential. named among the next billion-dollar startups, we're not just changing how people access financial products — we're creating a new credit system that backs the working, whatever they're working toward. the opportunity: sr. data scientist our data scientists are responsible for the entire machine learning model development process from conception with stakeholders, creation of model pipelines, technical development, deployment, and partnership with the credit, product, and finance teams to make business decisions. the ideal candidate is passionate about using data and models to drive business growth and help customers improve their financial situations. for this role, we’re looking for an experienced data scientist to work within our talented ds team for tilt cash advance and thrive line of credit products. this individual will help create and improve credit risk models, build ml based paycheck detection algorithms to help the business decrease loss rates and increase approval rates. the right person for this role is someone who shines while solving complex problems and is comfortable with a wide variety of technical tools. tilt is a remote-first company. we drive connectivity through regular company offsites. travel for company offsites is expected at a minimum 2 times a year. how you'll make an impact • grow user base and increase retention through machine learning and analytics • build machine learning models with large scale data sets to address business priorities • design and influence strategies on underwriting, marketing, fraud and customer experience • work closely with our engineering team as you implement models in production • collaborate effectively with operations and product to ensure the work fits into the broader strategy and business context • develop data standards and analytics pipelines to facilitate current and future analysis why you're a great fit • bs degree in engineering, computer science, finance or mathematics • 4+ years industry experience in data mining, machine learning, statistical analysis, and/or predictive modeling • deep understanding of statistics and machine learning techniques, including regression, classification, clustering and optimization experience building predictive models from scratch • strong programming skills in python with intermediate to advanced knowledge of sql • demonstrable experience with ml packages: scikit-learn, lightgbm, xgboost, sparkml, etc. • knowledge in deep learning and experience with dl toolkits (tensorflow, keras, pytorch) is preferred though not required • comfort working with a variety of cross functional partners in tech, product, credit, and business • exceptionally strong problem solving and communication with the ability to both get in the weeds and communicate to an executive audience don’t meet every qualification? we care about potential over your past. if you're bringing ambition and drive to what we're building, we want to hear from you. what you'll get at tilt • virtual-first teamwork: the tilt team is collaborating across 14 countries, 12 time zones, and counting. you’ll get started with a wfh office reimbursement. • competitive pay: we're big on potential, and it's reflected in our competitive compensation packages and generous equity. • complete support: find flexible health plans at every premium level, and substantial subsidies that stand up to global standards. • visibility is yours: you can count on direct exposure to our leadership team — we’re a team where good ideas travel quickly. • paid global onsites: magic happens irl: we gather twice yearly to reconnect over shared meals or kayaking adventures. (we’ve visited vail, san diego, and mexico city, to name a few.) • impact is recognized: growth opportunities follow your contributions, not rigid promotion timelines. the tilt way we're looking for people who chase excellence and impact. those who stand behind their work, celebrating the wins and learning from the missteps equally. we foster an environment where every voice is valued and mutual respect is non-negotiable — brilliant jerks need not apply. we're in this together, working to expand access to fair credit and prove that people are incredible. when you join us, it's not just another day at the [virtual] office, you're helping millions of hardworking people reach better financial futures. you’re pushing ahead in your career? we can get behind that. join us in building the credit system that people deserve.",anywhere,Data Scientist,"['classification', 'clustering', 'deep learning', 'excel', 'keras', 'lightgbm', 'machine learning', 'python', 'pytorch', 'r', 'regression', 'scikit-learn', 'spark', 'sql', 'statistics', 'tensorflow', 'xgboost']","['classification', 'clustering', 'deep learning', 'excel', 'keras', 'lightgbm', 'machine learning', 'python', 'pytorch', 'r', 'regression', 'scikit-learn', 'spark', 'sql', 'statistics', 'tensorflow', 'xgboost']",155K–185K a year
data scientist,markon,"overview: eager to join a team where your skills are valued, your growth is nurtured, and your impact is profound? look no further than markon, a premier consulting firm deeply dedicated to advancing our nation's most critical missions. at markon, we don't just offer jobs – we offer opportunities for personal and professional transformation. empowering our employees to lead, innovate, and excel, we foster an environment where new ideas are not just welcomed but celebrated. as a perennial washington post top workplace, we prioritize the well-being and success of our team members, ensuring they can bring their best selves to work. headquartered in falls church, virginia, markon has garnered national recognition for our unwavering dedication to excellence in serving the intelligence community, as well as federal civilian and defense agencies. our growing reach extends across 17 states, 116 countries, and 5 continents, where our team of dynamic professionals collaborates to deliver unparalleled program and project management services. markon values people and the tremendous impact each individual can make – which is why we’re consistently recognized as one of the best places to work in federal government consulting. here, you can help solve the nation’s most important challenges, surrounded by colleagues who help you grow, advance, and succeed. we are deeply dedicated to what matters – bringing out the best in each other to advance our clients’ missions. join us and make a meaningful impact. markon is an equal opportunity employer. all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, national origin, age, protected veteran status, or disability status. this job posting will remain open until the position is filled. benefits offered: medical, dental, vision, life insurance, short-term disability, long-term disability, 401(k) match, flexible spending accounts, eap, training and tuition assistance, paid time off, and holidays description: markon’s partner company is seeking a data scientist to support our intelligence customer out of lakewood, co. we’re looking for professionals proficient in python and experienced in automating workflows, data manipulation, and visualization to deliver mission-driven insights and solutions. responsibilities: • automate data workflows and pipelines using python and related tools • clean, manipulate, and visualize large, complex datasets • apply statistical, computational, and analytical methods to extract insights • develop and assess models and algorithms tailored to mission needs • translate data challenges into technical solutions and vice versa • communicate key findings to both technical and non-technical audiences • support mission teams by aligning analytics with operational objectives qualifications: • ts/sci with active polygraph • bachelor’s degree in a stem field + 3 years of relevant experience strongly preferred: experience in five or more of the following areas: • machine learning design and implementation • data science • advanced analytical algorithms • programming • data mining • advanced mathematical foundations • artificial intelligence • workflow development and reproducibility • data management and curation • data modeling and assessment salary range: usd $120,000.00 - usd $130,000.00 /yr. the markon pay range for this position is a general guideline only and not a guarantee of compensation or salary. additional factors considered in extending an offer include (but are not limited to) responsibilities of the job, education, experience, knowledge, skills, and abilities, as well as internal equity, alignment with market data, applicable bargaining agreement (if any), or other law.","lakewood, co",Data Scientist,"['excel', 'machine learning', 'python', 'r']","['excel', 'machine learning', 'python', 'r']",120K–130K a year
model risk data scientist,sentilink,"sentilink provides innovative identity and risk solutions, empowering institutions and individuals to transaction with confidence. we’re building the future of identity verification in the united states replacing a clunky, ineffective, and expensive status quo with solutions that are 10x faster, smarter, and more accurate. we’ve seen tremendous traction and are growing extremely quickly. our real-time apis have helped verify hundreds of millions of identities, starting with financial services and rapidly expanding into new markets. sentilink is backed by world-class investors including craft ventures, andreessen horowitz, nyca, and max levchin. we’ve earned recognition from techcrunch, cnbc, bloomberg, forbes, business insider, pymnts, american banker, lendit, and have been named to the forbes fintech 50 list every year since 2023. last but not least, we’ve even made history - we were the first company to go live with the ecbsv and testified before the united states house of representatives on the future of identity. sentilink supports a variety of ways to work, ranging from fully remote to in-office. we operate as a digital-first company with strong collaboration across the u.s. and india. we maintain physical offices in austin, san francisco, new york city, seattle, los angeles, and chicago in the u.s., and in gurugram (delhi) and bengaluru in india. if you’re located near one of these offices, we would love for you to spend time in the office regularly. some roles are hybrid or in-office by design. for example, our engineering team in india works primarily from our gurugram office. this is a remote, us-based role. responsibilities: • build out our foundational processes for generating and surfacing model performance metrics, including crafting + calculating the metrics and building python rails. • act as point of contact for strategic financial partners and manage their model governance needs. regularly conduct performance and exploratory analyses to establish the quality of model outcomes. • providing guidance on the appropriate use of our products, respond to inquiries around model development procedures and generate stability analyses. • create automation to detect inconsistencies or issues with the production models that power our fraud detection products, such as data driven gap analysis to automation via anomaly detection or training challenger models to identify weaknesses. • primarily work with a python ecosystem, using sql, sagemaker, s3, metabase and git to support. requirements: • bachelor’s, master’s, or phd in data science, statistics, computer science, or a related field • 3 years of work experience in a related technical field, or 5-7 years relevant applied academic experience • proven experience in data analysis, modeling, and performance evaluation • strong proficiency in python, and specifically data analysis libraries (pandas, numpy), data visualization (python matplotlib plots, excel plots / bi tools), and sql • ability to interpret and communicate complex data insights to both technical and business audiences • exceptional problem-solving and analytical skills with a focus on actionable results • interest in developing deep domain expertise for model risk analysis and model governance work • ability to thrive in a fast paced environment characterized by the need to solve extremely varied, high impact, open ended problems • proven experience in assessing the quality, stability, performance and behavior of production grade ml models, ideally from the perspective of model governance, fair lending or economic risk is highly preferred • familiarity with fraud detection preferred • experience with github • candidates must be legally authorized to work in the united states and must live in the united states technologies: • python, pandas, numpy, matplotlib, bi tools, sql salary: • $140,000/year - $180,000/year + equity + benefits note: this salary range may be inclusive of several career levels, and the actual base salary within that range will be determined by several components including but not limited to the individual's experience, skills, and qualifications. perks: • employer paid group health insurance for you and your dependents • 401(k) plan with employer match (or equivalent for non us-based roles) • flexible paid time off • regular company-wide in-person events • home office stipend, and more! corporate values: • follow through • deep understanding • whatever it takes • do something smart",anywhere,Data Scientist,"['bi tools', 'data analysis', 'excel', 'matplotlib', 'numpy', 'pandas', 'python', 'r', 'sql', 'statistics']","['bi tools', 'data analysis', 'excel', 'matplotlib', 'numpy', 'pandas', 'python', 'r', 'sql', 'statistics']",
senior data scientist- cecl modeler,southeast toyota finance,"the senior data scientist- cecl modeler will work in the analytics department of southeast toyota finance (setf), a division of jm family enterprises, and will lead the development, validation, and implementation of account-level cecl models and frameworks. this individual will need to have a strong background in credit risk modeling using panel data and an understanding of cecl accounting standards. this role requires cross-functional collaboration with partner teams such as finance, accounting to ensure accurate estimation of credit losses. responsibilities • lead the development and implementation of account-level cecl models- probability of default, exposure at default, loss given default, prepay curves or models, and other related forward-looking credit risk models. • develop and maintain development documentation, code, and implementation documents in compliance with best practices and standards. • perform back testing and benchmarking exercises to ensure model accuracy and robustness. • conduct assessments for overlays and maintain accurate documentation in accordance with best practices and standards. • analyze large datasets in a panel framework with the ability to perform sampling, exploratory analysis, outlier analysis, standardization, data transformation, segmentation analysis, statistical tests on stationarity,and other methods commonly used in cecl modeling exercises. • effectively communicate complex technical concepts to stakeholders. • work on end-to-end modeling processes, including data procurement, model estimation, implementation and deployment, and monitoring and testing. • provide insights from the models and modeling exercises to support business risk management and strategies. • provide comprehensive portfolio tracking, analysis, reporting, and forecasting. provide detailed explanatory analysis and make recommendations to senior management; proactively identifying risks and opportunities based on current trends and future outlooks. • develop and/or enhance advanced reporting solutions: portfolio reporting, portfolio stability reports, and model monitoring. • collaborate with cross-functional teams, including accounting, finance, treasury, and other company stakeholders. mentor junior team members. qualifications • a master’s or ph.d degree in statistics, data science, operations research, mathematics, and/or other quantitative analysis is required • 5+ years of financial, credit risk, and analytics resource management; indirect automotive lending experience and/or consumer lending preferred. • minimum of 5 years of statistical modeling and/or quantitative analysis experience • experience in using python, sas, r, or any comparable statistical or data extraction software is preferred. • experience managing advanced modeling projects and collaborating well with other team members. • strong interpersonal and communication skills (verbal and written) • ability to diplomatically communicate findings across departments and to upper management. • ability to effectively operate with multiple priorities and changing directives. self-motivated with the ability to anticipate department needs, discern work priorities, and meet deadlines. • forward thinker who actively seeks opportunities and proposes solutions. • must be a highly detailed process-driven individual who demonstrates success at building trust and influence while driving for results with and through people. this job description may not be inclusive of all assigned duties, responsibilities, or aspects of the job described, and may be amended at any time at the sole discretion of jm family. all work arrangements are subject to associate performance, business need and manager discretion, and may be revised as necessary. jm family is proud to be an equal opportunity employer jm family enterprises, inc. is an equal employment opportunity employer. we are committed to recruiting, hiring, retaining, and promoting qualified associates without regard to age, race, religion, color, gender, sex (including pregnancy, childbirth and related medical conditions), sexual orientation, gender identity, gender expression, mental or physical disability, national origin, marital status, citizenship, military status, genetic information, veteran status, or any other characteristic protected by federal, state, provincial, or local law. disability accommodations if you have a disability and require a reasonable accommodation to complete the job application process, please contact jm family’s talent acquisition department at talentacquisition@jmfamily.com for assistance. if you have an accommodation request for one of our recruiting events, please notify us at least 72 hours prior so that we may provide assistance.",united states,Data Scientist,"['python', 'r', 'recommendation', 'sas', 'statistics']","['python', 'r', 'recommendation', 'sas', 'statistics']",
senior data scientist,virta health,"senior data scientist virta health is pioneering a new standard of care for people to reclaim their lives. we are in the midst of a public health crisis: obesity rates are at an all-time high and over half of us adults have type 2 diabetes or prediabetes, and despite billions spent on new treatments, outcomes are largely worse. virta reverses these diseases and delivers life-changing results by pairing individualized nutrition with ongoing care from a clinical support team. we have raised over $350 million from top-tier investors, and partner with the largest health plans, employers, and government organizations to help their employees and members restore their health and take back their lives. role overview the advanced analytics & ai (aaa) team is accountable for both pioneering new classes of data that power our business, and ensuring that data is powerful, clear, correct and easy to use throughout the company. the team collaborates closely with the engineering, product development, commercial, and clinical teams to transform data into actionable insights that propel virta's mission forward. as a senior data scientist on the aaa team, you will play a crucial role in shaping our data-driven decision making processes and in driving impactful insights that advance our mission. this role offers a unique blend of hands-on coding and strategic leadership, working closely in partnership with the product development group that supports adoption of our member-facing products. you will get a chance to work closely with stakeholders to create impactful data-driven reports and analyses that strengthen virta’s value proposition. responsibilities • serve as data lead for the adoption squad: • understand the product – strategy, goals, roadmap, users – deeply enough to identify high-impact, strategic opportunities and risks. • use data to steer conversations: bring data-driven hypotheses, challenge assumptions, frame trade-offs, prioritize strategically. • translate vague product hunches into sharp analytical questions, and design the right approach to answer them. • be the source of truth for kpis and product metrics: help define and monitor them, and identify trends and drivers. • drive decision making with data: prioritize analyses that meaningfully inform product decisions and change outcomes, reframe analysis requests that won’t do that into ones that will, and design and deliver the resulting analyses properly and rapidly. • lead experimentation thoughtfully: guide the team on when and when not to experiment, help enable self-serve experimentation, and focus on frequent, small experiments to speed learning. • identify where product strategy and ideas could be better, more easily, or more quickly implemented with the help of ai (ml or genai), and help enable this integration. • show up as a collaborative leader, not simply a service provider, and help focus work on what matters most. • work end-to-end to translate clinical, member, and claims data from our data warehouse into actionable recommendations: work with stakeholders to design reports and dashboards that answer real business questions, design the data flows to support these, and build the required data transformation pipelines. must-haves • experience working at a digital health or similar startup, preferably during the growth phase, in order to function effectively in a fast-paced, rapidly changing environment. • 7+ years of experience in a healthcare analytics or data science role, working with large datasets and conducting statistical and quantitative modeling, melding analytics with strong programming, data mining, clustering and segmentation. • 5+ years of work experience working data science and visualization tools (sql, r, and/or python) • advanced understanding of data storage, etl frameworks, data transformation, and validation • experience in building bi dashboards using tools such as looker or tableau • ability to explain technology, techniques and approaches to others nice-to-haves • experience leading cross-functional efforts to build scalable systems and data pipelines, working with complex data structures • understanding of generative ai concepts and basic usage of genai tools (chatgpt, claude, etc.) • experience building pipelines using dbt • demonstrated project management experience 90 day plan within your first 90 days at virta, we expect you will do the following: • familiarize yourself with the advanced analytics & ai team roles and responsibilities. get a pulse on team structure, key stakeholders, and areas for opportunity with an eye towards scalability and standardization. • integrate with the adoption squad to understand team dynamics, analyze key product features and data pipelines, and familiarize yourself with the squad’s product roadmap • analyze current member journey to understand virta’s technology ecosystem • meet cross-functional stakeholders to understand how their business needs align with squad and team goals • identify at least 1 area of opportunity to leverage genai to increase team efficiency or improve our processes • teach and inspire your team members through knowledge sharing, pair programming, and giving feedback during code reviews values-driven culture virta’s company values drive our culture, so you’ll do well if: • you put people first and take care of yourself, your peers, and our patients equally • you have a strong sense of ownership and take initiative while empowering others to do the same • you prioritize positive impact over busy work • you have no ego and understand that everyone has something to bring to the table regardless of experience • you appreciate transparency and promote trust and empowerment through open access of information • you are evidence-based and prioritize data and science over seniority or dogma • you take risks and rapidly iterate is this role not quite what you're looking for? join our talent community and follow us on linkedin to stay connected! virta has a location based compensation structure. starting pay will be based on a number of factors and commensurate with qualifications & experience. for this role, the compensation range is $167,249 - $216,000. information about virta’s benefits is on our careers page at: https://www.virtahealth.com/careers. as part of your duties at virta, you may come in contact with sensitive patient information that is governed by hipaa. throughout your career at virta, you will be expected to follow virta's security and privacy procedures to ensure our patients' information remains strictly confidential. security and privacy training will be provided. as a remote-first company, our team is spread across various locations with office hubs in denver and san francisco. clinical roles: we currently do not hire in the following states: ak, hi, ri corporate roles: we currently do not hire in the following states: ak, ar, de, hi, me, ms, nm, ok, sd, vt, wi. #li-remote","hygiene, co",Data Scientist,"['clustering', 'dashboard', 'data pipeline', 'data warehouse', 'dbt', 'etl', 'experimentation', 'looker', 'python', 'r', 'recommendation', 'scala', 'sql', 'tableau']","['clustering', 'dashboard', 'data pipeline', 'data warehouse', 'dbt', 'etl', 'experimentation', 'looker', 'python', 'r', 'recommendation', 'scala', 'sql', 'tableau']",
data scientist,dataannotation,"join the dataannotation team and contribute to developing cutting-edge ai systems, while enjoying the flexibility of remote work and setting your own schedule. we are looking for an expert mathematician (part-time work from home) to help advance ai development. as a member of dataannotation’s math team, you’ll be part of a growing community of over 100,000 experts who are driving real-world impact in ai development. our platform offers an engaging blend of flexibility and challenge: you’ll work closely with state-of the art ai models to take on programming tasks that include solving challenging math problems and synthesizing insights through data analysis and visualization. your work directly contributes to refining intelligent systems that learn, adapt, and evolve. some team members fit this work alongside a full-time role, while others treat it as their primary focus, choosing projects and schedules that align with their availability and goals. to get started, once you sign up for an account, you'll take a short assessment (this serves as our version of an interview). if you pass that assessment, you’ll receive an email confirmation, and paid work will become available to you through our platform. benefits: • this is a full-time or part-time remote position • you’ll be able to choose which projects you want to work on • you can work on your own schedule • projects are paid hourly starting at $40+ usd per hour, with bonuses on high-quality and high-volume work responsibilities: • give ai chatbots diverse and complex mathematics problems and evaluate their outputs • evaluate the quality produced by ai models for correctness and performance qualifications: • fluency in english (native or bilingual level) • detail-oriented • proficient in arithmetic, algebra, geometry, calculus, probability, statistics, and inductive/deductive reasoning • a current, in progress, or completed master's and/or phd is preferred but not required note: payment is made via paypal. we will never ask for any money from you. paypal will handle any currency conversions from usd. this job is only available to those in the us, canada, uk, ireland, australia, and new zealand. those located outside of these countries will not see work or assessments available on our site at this time. #math",anywhere,Data Scientist,"['data analysis', 'r', 'statistics']","['data analysis', 'r', 'statistics']",40–50 an hour
data scientist for enrollment management,furman university,"welcome to furman university's career site! important: load all documents in the my experience area under resume/cv. this may include your resume/c.v., cover letter, unofficial transcript, teaching philosophy, etc. please wait for all documents to finish uploading before clicking next. if the process times out you will lose your progress, so please ensure you have all necessary documents available before starting. the drop-down lists only display so many options: please type into a drop-down list to search for your option. for example: in field of study, you may need to enter your field name. if you have no field, type ""none."" if you have previously applied, make sure your information is current as you can transfer it to another application. prior to submitting your application, verify all information for accuracy and ensure you have uploaded all appropriate documents. once submitted, you will not be able to edit your application other than to change your contact information. if you have any questions or need additional assistance, please contact the office of human resources via email at recruitment@furman.edu or by calling 864-294-3103. job title: data scientist for enrollment management job family: professionals full-time/part-time: full time compensation grade: 10s pay type: salary department: vp for enrollment job summary: reporting to the vice president for enrollment management, the data scientist is a member of the enrollment management senior leadership team and understands admission and financial aid strategies for undergraduate students and how to harness data, influence, and model admissions decisions, financial aid, enrollment, and student success. the data scientist will gather, interpret, monitor and analyze data policies, procedures and outcomes. the data scientist will use software engineering, statistics, and machine learning to automate processes and develop solutions to enrollment challenges. job description: essential job duties: design and conduct complex data analysis using student application data, demographic information, financial aid data, student success data, and other relevant sources. develop and implement sophisticated predictive models to inform recruitment, selection, yield, and financial aid strategies. use data reporting tools available through third parties such as the national student clearinghouse and data sharing consortiums to analyze trends and inform strategies throughout the enrollment cycle. create useful data visualizations and reports to effectively communicate complex findings to both highly technical and non-technical audiences. partner with admissions, financial aid, and marketing teams to translate data insights into actionable strategies. maintain a strong working relationship with institutional technology. acquire a deep understanding of relevant aspects of undergraduate admissions at furman. develop relationships with key stakeholders across campus, become familiar with data relating to admissions priorities, and stay abreast with national context (including current events, court cases, and academic research stay up to date on the latest trends and techniques in data science and enrollment management. maintain data security and integrity and ensure compliance with all ferpa regulations. proactively seek and locate data, including outside benchmarks or comparative data, to support or refute proposed decisions. may need to collaborate with other organizations to gather data. drive the collection of new data resources and refinement of existing resources. research and propose options for the collection and warehousing of new data, particularly data that is new to the admissions ecosystem. in partnership with institutional research partners, determine appropriate methodology for data extraction, transformation, storage, and usage. develop processes for loading and updating data that are efficient and re-usable. document all data and transformations thoroughly. set the research agenda for the department, working with the senior management team to ensure an on-going commitment to understanding and evaluating our work. collaborate with various research teams including institutional research office. contribute positively to other duties and projects as assigned. basic qualifications: master’s degree in data science, math, statistics, computer science, or a related quantitative field. minimum 2 years of experience in a data science role. strong programming skills in data analysis languages. the ability to construct large sql databases and query them. expertise in building useful and complex data visualizations. extensive experience in and strong passion for empirical research and answering hard questions with data. preferred qualifications data science experience in the education sector. previous experience working in higher education admissions or related field with organizational processes preferred. education requirements: certification requirements: job posting end date (if date is blank, posting is open ended): welcome to furman university employment opportunities! furman university is a selective private liberal arts and sciences college committed to helping students develop intellectually, personally, and interpersonally and providing the practical skills necessary to succeed in a rapidly-changing world. furman professors are exceptional teacher-scholars who mentor undergraduate students within a campus community that values and encourages diverse ideas and perspectives. our strategic vision, the furman advantage, promises students an individualized four-year pathway facilitated by team of mentors and infused with a rich and varied set of high impact experiences outside the classroom that include undergraduate research, study away, internships, community-focused learning, and opportunities to engage across differences. furman university is an equal opportunity employer and adheres to all applicable federal and state laws regarding nondiscrimination. the university is dedicated to fostering an inclusive and welcoming environment where all individuals represent a multiplicity of identities including gender, race, religion, spiritual belief, sexual orientation, geographic origin, socioeconomic background, ideology, world view, and varied abilities are treated equitably and with respect. employment decisions are based on qualifications, merit, and institutional needs. in keeping with this commitment domestic partners of employees are eligible for comprehensive benefits and faculty/staff affinity groups exist to offer support for faculty/staff that identify as lgbtqia+ and/or black/african-american. the furman student experience is supported by a rich network of centers and institutes that includes the hill institute for innovation and entrepreneurship, the riley institute, the david e. shi institute for sustainable communities, the institute for the advancement of community health, the rinker center for study away and international education, the cothran center for vocational reflection, the furman humanities center, the shucker center for leadership development, the malone center for career engagement, the center for interpersonal connections, and our newest addition, the tocqueville center for the study of democracy and society. furman is located on traditional cherokee land in greenville, south carolina, a racially and culturally diverse community that is one of the fastest growing cities in the southeast. it has been ranked among “america’s ten best” by forbes magazine. greenville has a thriving downtown, excellent private and public schools and a vibrant international community. a 20-mile bike and running trail connects the university to greenville and travelers rest, which was named “one of america’s coolest small towns.” it is within a short driving distance of asheville, charlotte and atlanta. the blue ridge mountains and atlantic beaches are within easy reach.",united states,Data Scientist,"['aws', 'data analysis', 'excel', 'machine learning', 'r', 'sql', 'statistics']","['aws', 'data analysis', 'excel', 'machine learning', 'r', 'sql', 'statistics']",
senior data scientist (remote from canada),jobgether,"this position is posted by jobgether on behalf of a partner company. we are currently looking for a senior data scientist in canada. in this role, you will shape high-impact data initiatives that fuel strategic decision-making and product innovation. you will work with diverse data sources — from financial transactions to customer behavior and system usage — to uncover insights, optimize performance, and drive measurable business outcomes. you will lead advanced analytics projects, build production-grade models, and contribute to next-generation agentic ai applications. operating within a remote, distributed environment, you will collaborate with cross-functional teams to solve complex challenges and elevate data science capabilities across the organization. this is a role for someone who thrives in fast-paced, data-rich settings and enjoys blending technical rigor with strategic thinking. accountabilities: • lead predictive and descriptive analytics initiatives across finance, marketing, product performance, and operational domains to identify trends, optimization opportunities, and critical insights. • design research workflows and oversee full-lifecycle development of data-intensive and ai-driven applications, including agentic ai systems. • own model development end-to-end, from data exploration and feature engineering to deployment, optimization, and long-term maintenance. • partner with teams in sales, marketing, product, and operations to translate business challenges into robust, data-backed solutions. • champion best practices in statistical modeling, mlops, and advanced ai to continuously evolve internal data science capabilities. requirements: • extensive experience as a senior data scientist in brokerage, trading, or financial services, with hands-on exposure to high-volume, high-integrity data environments. • expert proficiency in python and industry-standard libraries such as pandas and scikit-learn, along with advanced sql skills. • strong background in supervised and unsupervised learning, with proven experience deploying models in production. • practical experience working with cloud-native data warehouses such as snowflake, bigquery, redshift, or similar technologies. • ability to manage full project lifecycles including research, development, deployment, and ongoing optimization. • exceptional communication skills and a track record of effective collaboration with non-technical stakeholders in fast-paced settings. • nice to have: experience building applications with large language models or agentic ai frameworks; familiarity with semantic modeling tools such as dbt semantic layer, cube.js, or lookml. benefits: • competitive salary and stock option program. • comprehensive health benefits. • one-time 500 usd home-office setup allowance for new hires. • monthly 150 usd stipend via brex card. • inclusive environment committed to equitable opportunities and diverse hiring. why apply through jobgether? we use an ai-powered matching process to ensure your application is reviewed quickly, objectively, and fairly against the role's core requirements. our system identifies the top-fitting candidates, and this shortlist is then shared directly with the hiring company. the final decision and next steps (interviews, assessments) are managed by their internal team. we appreciate your interest and wish you the best! data privacy notice: by submitting your application, you acknowledge that jobgether will process your personal data to evaluate your candidacy and share relevant information with the hiring employer. this processing is based on legitimate interest and pre-contractual measures under applicable data protection laws (including gdpr). you may exercise your rights (access, rectification, erasure, objection) at any time. #li-cl1",canada,Data Scientist,"['aws', 'bigquery', 'cloud', 'data warehouse', 'dbt', 'feature engineering', 'pandas', 'python', 'r', 'redshift', 'scikit-learn', 'snowflake', 'sql']","['aws', 'bigquery', 'cloud', 'data warehouse', 'dbt', 'feature engineering', 'pandas', 'python', 'r', 'redshift', 'scikit-learn', 'snowflake', 'sql']",
data scientist – entry level; remote,hire sync,"position: data scientist – entry level (remote) data scientist – entry level (remote) – horizon workforce partners • job title: data scientist – entry level (remote) • employment type: remote (part-time/contract) • location: remote within united states, united kingdom, canada, ireland, australia, and new zealand • compensation: estimated range: usd 5,000–8,000 per month, depending on location, experience, scope of responsibilities, and performance expectations for a full‑time schedule about the role this role supports day‑to‑day operational, content, research, data and ai‑related activities to help ensure smooth delivery across multiple projects in a fully remote environment. you will collaborate closely with the team to keep information organised, tasks on track, and workflows efficient. key responsibilities • assist with project tasks such as content preparation, data entry and maintenance, online research, basic analysis, operations support, ai‑output review, documentation, and coordination. • review, organise, and update information with a high level of accuracy and attention to detail. • communicate clearly with team members through written and verbal channels and provide timely updates on task status and progress. skills & qualifications • strong command of written english and clear, professional communication skills. • comfort using digital tools such as email, spreadsheets, project management or online productivity platforms. • analytical mindset with strong attention to detail and accuracy. • ability to manage time, prioritise tasks, and work independently in a remote environment. • interest in operations, research, content, customer support, or data‑related work is helpful but not required; training and onboarding will be provided. what we offer • 100% remote work within the listed countries, with flexible scheduling aligned to team needs and agreed time zones. • weekly payments via secure, compliant payment methods, with a clear and transparent compensation structure. • opportunities to build skills in research, content operations, data handling, and ai‑related workflows. • a supportive work culture that encourages feedback, learning, and long‑term professional growth. you must be legally authorised to work in the country where you are based. we welcome applicants from all backgrounds and make hiring decisions based solely on qualifications, experience, and business needs, in line with applicable employment and anti‑discrimination laws. apply now apply now to be considered for this opportunity. #j-18808-ljbffr",canada,Data Scientist,"['aws', 'r']","['aws', 'r']",$5K–$8K a month
data scientist with machine learning engineer,tek tron it,"key responsibilities data science & analytics • explore, clean, and analyze structured and unstructured datasets. • apply statistical techniques to generate insights and support business decisions. • develop predictive models using regression, classification, clustering, nlp, or time-series methods. • conduct hypothesis testing, a/b experiments, and exploratory data analysis (eda). machine learning engineering • build, train, and fine-tune ml models using frameworks such as tensorflow, pytorch, scikit-learn. • develop and maintain end-to-end ml pipelines, including feature engineering, model training, evaluation, and versioning. • deploy ml models into production using mlops pipelines and tools like mlflow, kubeflow, sagemaker, or vertex ai. • implement monitoring, retraining strategies, and improve model performance. • optimize models for scalability, latency, and resource efficiency. • requirements must-have • bachelor's or master's degree in data science, computer science, statistics, ai/ml, or related field. • 3–7+ years of experience as a data scientist, ml engineer, or similar role. • strong programming skills in python (numpy, pandas, scikit-learn, matplotlib, etc.). • experience with ml frameworks (tensorflow, pytorch). • strong understanding of machine learning algorithms, statistics, and data modeling. • experience deploying models in production (docker, apis, cloud deployment). • knowledge of sql and experience with relational and nosql databases. • familiarity with mlops tools (mlflow, kubeflow, airflow, dvc)",canada,Data Scientist,"['airflow', 'classification', 'cloud', 'clustering', 'data analysis', 'feature engineering', 'machine learning', 'matplotlib', 'nlp', 'numpy', 'pandas', 'python', 'pytorch', 'r', 'regression', 'scala', 'scikit-learn', 'sql', 'statistics', 'tensorflow']","['airflow', 'classification', 'cloud', 'clustering', 'data analysis', 'feature engineering', 'machine learning', 'matplotlib', 'nlp', 'numpy', 'pandas', 'python', 'pytorch', 'r', 'regression', 'scala', 'scikit-learn', 'sql', 'statistics', 'tensorflow']",
data scientist/ algorithms - lyft ads,lyft,"at lyft, our purpose is to serve and connect. we aim to achieve this by cultivating a work environment where all team members belong and have the opportunity to thrive. lyft ads is one of lyft’s newest and fastest-growing businesses, focused on building the world’s largest transportation media network. our mission is to help brands reach riders during key moments of their journey—before, during, and after a ride—by delivering meaningful, contextually relevant ad experiences. we operate at the intersection of mobility data, real-time decision systems, and ai-powered personalization, enabling advertisers to run high-impact campaigns with measurable outcomes. we are seeking an algorithms scientist to help build the next generation of ads relevance, targeting, optimization, and measurement algorithms that power the lyft ads platform. in this role, you will work across large-scale datasets and complex real-time systems to design, prototype, and deploy production-grade machine learning models. you’ll collaborate closely with engineering, product, data science, and sales to translate ambiguous business and advertiser needs into rigorous algorithmic solutions that improve ad performance, enhance marketplace efficiency, and drive meaningful revenue growth. this is a high-impact, highly technical role within a rapidly scaling business line. the ideal candidate brings strong applied machine learning intuition, hands-on modeling experience, and the ability to write clean, efficient production code. you will play a critical role in shaping how advertisers connect with lyft riders—pushing the boundaries of personalization, measurement, and real-time optimization in a dynamic marketplace. responsibilities: • design, develop, and deploy production-grade machine learning models and algorithms that power core lyft ads capabilities, such as ad relevance, targeting, ranking, bid optimization, pacing, campaign delivery, and measurement. • own the end-to-end lifecycle of modeling projects — including problem definition, data exploration, feature engineering, model development, offline evaluation, deployment, and monitoring. • collaborate closely with ads engineering to integrate models into real-time ad-serving and batch decision systems, ensuring performance across latency, scalability, and reliability constraints. • analyze large-scale mobility, behavioral, and ads performance datasets to identify patterns, surface opportunities, and guide ml and ai driven product improvements. • implement rigorous model evaluation frameworks, including offline metrics, statistical tests, calibration, sensitivity analysis, and a/b experimentation to validate both model impact and system-level outcomes. • build robust training pipelines, feature transformations, and scoring infrastructure, ensuring reproducibility, observability, and long-term maintainability. • partner with product, engineering, and sales to translate ambiguous advertiser goals (e.g., increased conversions, reach efficiency, brand lift) into measurable requirements and success metrics. • investigate and resolve model behavior issues, production regressions, calibration drift, and performance anomalies in close partnership with ads infra teams. • drive innovation by staying current with advances in ml for ranking, recommendation, causal inference, optimization, and ads measurement — and proactively identifying opportunities to apply them. • contribute to lyft ads’ modeling and experimentation infrastructure, through model cards, documentation, reproducibility standards, and code quality improvements. experience: • master’s, or phd in machine learning, computer science, statistics, applied mathematics, engineering, or related quantitative fields; or equivalent applied industry experience. • 3–5 years of hands-on ml/applied science experience, ideally involving production models, large-scale systems, or ads/recommendation/relevance domains. strong proficiency in python and machine learning frameworks such as pytorch, tensorflow, jax, or scikit-learn; ability to write clean, efficient, production-adjacent code. • experience working with large-scale datasets and distributed data tools (spark, snowflake, presto, databricks). • practical experience building and evaluating: • ranking and relevance models • optimization or pacing algorithms • predictive models for ctr, cvr, or user response • causal or experimentation-based measurement methods • understanding of online/offline evaluation techniques, including: • offline metrics (auc, ndcg, mrr, calibration) • a/b testing methodologies • bias correction and counterfactual estimation • ability to solve ambiguous problems by structuring analyses, evaluating trade-offs, and proposing algorithmic solutions grounded in scientific rigor. • strong communication skills, with an ability to clearly explain model behavior, constraints, trade-offs, and recommendations to engineering, product, and sales partners. • demonstrated ownership of modeling work, including debugging, monitoring, documentation, and iteration after deployment. • curiosity, initiative, and a track record of delivering measurable improvements through high-quality modeling. benefits: • extended health and dental coverage options, along with life insurance and disability benefits • mental health benefits • family building benefits • child care and pet benefits • access to a lyft funded health care savings account • rrsp plan to help save for your future • in addition to provincial observed holidays, salaried team members are covered under lyft's flexible paid time off policy. the policy allows team members to take off as much time as they need (with manager approval). hourly team members get 15 days paid time off, with an additional day for each year of service • lyft is proud to support new parents with 18 weeks of paid time off, designed as a top-up plan to complement provincial programs. biological, adoptive, and foster parents are all eligible. • subsidized commuter benefits lyft is committed to creating an inclusive workforce that fosters belonging. lyft believes that every person has a right to equal employment opportunities without discrimination because of race, ancestry, place of origin, colour, ethnic origin, citizenship, creed, sex, sexual orientation, gender identity, gender expression, age, marital status, family status, disability, pardoned record of offences, or any other basis protected by applicable law or by company policy. lyft also strives for a healthy and safe workplace and strictly prohibits harassment of any kind. accommodation for persons with disabilities will be provided upon request in accordance with applicable law during the application and hiring process. please contact your recruiter if you wish to make such a request. lyft highly values having employees working in-office to foster a collaborative work environment and company culture. this role will be in-office on a hybrid schedule — team members will be expected to work in the office at least 3 days per week, including on mondays, wednesdays, and thursdays. lyft considers working in the office at least 3 days per week to be an essential function of this hybrid role. your recruiter can share more information about the various in-office perks lyft offers. additionally, hybrid roles have the flexibility to work from anywhere for up to 4 weeks per year. #hybrid the expected base pay range for this position in the toronto area is cad 108,000 - cad $135,000. salary ranges are dependent on a variety of factors, including qualifications, experience and geographic location. range is not inclusive of potential equity offering, bonus or benefits. your recruiter can share more information about the salary range specific to your working location and other factors during the hiring process. original job data scientist/ algorithms - lyft ads posted on grabjobs ©. to flag any issues with this job please use the report job button on grabjobs.",canada,Data Scientist,"['a/b testing', 'databricks', 'experimentation', 'feature engineering', 'machine learning', 'python', 'pytorch', 'r', 'recommendation', 'regression', 'scala', 'scikit-learn', 'snowflake', 'spark', 'statistics', 'tensorflow']","['a/b testing', 'databricks', 'experimentation', 'feature engineering', 'machine learning', 'python', 'pytorch', 'r', 'recommendation', 'regression', 'scala', 'scikit-learn', 'snowflake', 'spark', 'statistics', 'tensorflow']",
data analyst & scientist (tech startup),rankbreeze,"the opportunity we’re looking for a versatile data analyst / scientist to support both our marketing and product teams. in this hybrid role, you'll be responsible for gathering and analyzing data to create impactful reports, while also building algorithms that drive product innovation and improve booking performance for our users. the role is envisioned as 80% analytical and 20% data science in nature. responsibilities data analyst: • conduct data studies and extract insights relevant to vacation rental managers & airbnb hosts. • collecting both internal & external data, including utilizing web scraping. • create compelling reports, charts, and video summaries to communicate findings. • present data-driven insights in an engaging and accessible format. data science: • contribute to ai-driven product strategies and enhancements. • implement systems to democratize data across all teams. • develop algorithms leveraging internal & external data. • optimize and improve existing models. required skills and experience technical expertise: • demonstrated expertise in sql (3+ years) • experience with elt tools such as dbt (1+ years) • familiarity with workflow orchestration tools such as airflow, hevo, fivetran. • knowledge of python, including data processing packages (pandas, numpy) • familiarity with and interest in ai frameworks like pytorch. • experience working with data lakehouse platforms (e.g. postgres, bigquery) • excellent analytical skills with the ability to interpret complex data and derive actionable insights from them. soft skills & mindset: • highly organized and able to manage multiple priorities. • high integrity and strong sense of accountability. • excellent written & video communication skills. • quick learner with the ability to adapt to new challenges. • creative thinker who can generate innovative solutions. • high standards for work quality and output. • leadership qualities with the ability to work independently and cross-functionally why join rankbreeze? • join an innovative team revolutionizing data insights & automation for vacation rental operators. • work remotely in a fast-paced environment with a global impact on our customers all over the world. • build relationships with workmates globally. • work with a highly motivated, detailed, and talented team. this role is a full-time contractor position with all of the following benefits. if you’re in canada, t4 employment is available after a year of continuous service & we may be able to accommodate other residents in the future. regardless of the employment setup, you’ll be part of the team in every capacity. about our company at rankbreeze, we’re on a mission to help vacation rental managers grow their businesses through smarter data and better decision-making. our platform helps property managers optimize their listings, improve airbnb visibility, and set dynamic prices with confidence — turning curious guests into booked reservations. we provide tools for a/b testing, market analysis, and revenue optimization that help thousands of hosts uncover growth opportunities and make every listing reach its full potential. from pricing strategy to visibility improvements, rankbreeze gives property managers the insights they need to move forward with confidence. we’re a global, remote team of ambitious builders who love solving meaningful challenges and seeing the impact of our work in the real world. every voice here matters, and every idea contributes to helping hosts and property managers succeed online. if you’re excited by the idea of working with passionate people and creating tools that help thousands of hosts thrive, you’ll love being part of rankbreeze. how to apply: • respond to all questions in the job posting, including the word “pineapple” to help signal to us you’ve read everything thoroughly. • submit your resume along with a cover letter detailing your interest and fit for the role. • submit pre-interview answers here: https://www.videoask.com/fih394pss please note that only candidates who have completed all screening questions will be considered for this role. job type: full-time pay: $70,000.00 per year application question(s): • can you share a presentation that you’ve worked on & are the most proud of? please share details on your role & how you put the project together. please provide as much detail as possible. • have you worked on web scraping projects before? please provide detailed information about your experience and the specific project(s) you’ve been involved in. • have you ever helped implement a product feature or tool based on your own data findings? if yes, describe the process from insight to implementation. work location: remote",canada,Data Scientist,"['a/b testing', 'airflow', 'bigquery', 'data lake', 'dbt', 'elt', 'excel', 'numpy', 'pandas', 'python', 'pytorch', 'r', 'sql']","['a/b testing', 'airflow', 'bigquery', 'data lake', 'dbt', 'elt', 'excel', 'numpy', 'pandas', 'python', 'pytorch', 'r', 'sql']",
"entry-level data scientist — remote, flexible hours",talent connect,"a growing tech firm in canada is looking for motivated individuals for a remote data scientist position. this entry-level role offers flexible hours and requires no prior experience. responsibilities include assisting in various tasks related to data, research, and ai workflows. the company promotes a supportive culture encouraging skill development and provides weekly payouts. ideal candidates should have strong english communication skills and an eagerness to learn.#j-18808-ljbffr",canada,Data Scientist,['r'],['r'],
data scientist (multiple open positions),gradient ascent,"about gradient ascent gradient ascent is a leading ai and data consultancy that helps organizations harness data-driven insights to drive impact. our team is deeply committed to delivering ai-powered solutions for healthcare, government, and social innovation. role overview the data scientist will apply machine learning and predictive analytics to support case management, risk assessments, and service optimization in the compassionate intervention program. key responsibilities • develop and deploy machine learning models to improve risk assessment and predictive intervention. • apply natural language processing (nlp) and ai techniques to enhance user engagement and automation. • build classification and recommendation models to support compassionate intervention case management. • ensure ai models comply with foip, hia, and ethical ai standards. • collaborate with data engineers to operationalize ml models into production environments. required qualifications • 5+ years of experience in machine learning, ai, or predictive analytics. • expertise in python, tensorflow, pytorch, or scikit-learn. • strong statistical background in regression, classification, time series forecasting. • hands-on experience in mlops and cloud-based ai deployment. • deep understanding of ai governance and data privacy laws (foip, hia, goa policies). • must be located in canada and have required work permits (citizen, pr, etc.). 📩 apply now with the form below or email your resume & cover letter to careers@gradient-ascent.com.",canada,Data Scientist,"['aws', 'classification', 'cloud', 'machine learning', 'natural language processing', 'nlp', 'python', 'pytorch', 'r', 'recommendation', 'regression', 'scikit-learn', 'tensorflow', 'time series']","['aws', 'classification', 'cloud', 'machine learning', 'natural language processing', 'nlp', 'python', 'pytorch', 'r', 'recommendation', 'regression', 'scikit-learn', 'tensorflow', 'time series']",
data science consultant - sfl scientific,deloitte llp,"our deloitte strategy & transactions team helps guide clients through their most critical moments and transformational initiatives. from strategy to execution, this team delivers integrated, end-to-end support and advisory services covering valuation modeling, cost optimization, restructuring, business design and transformation, infrastructure and real estate, mergers and acquisitions (m&a), and sustainability. work alongside clients every step of the way, helping them navigate new challenges, avoid financial pitfalls, and provide practical solutions at every stage of their journey-before, during, and after any major transformational projects or transactions. sfl scientific is a deloitte business that is part of our strategy offering, within our broader strategy & transactions practice mentioned above. this specialized team brings together several key capabilities to architect integrated programs that transform our clients' businesses. we are hiring a data science consultant to support the technical design, development, and deployment of novel ai solutions across healthcare, life sciences, manufacturing, consumer, energy, and other industries. join us at sfl scientific to expand your technical acumen through the lens of professional services and consulting and help create novel solutions to advance your data science & ai career. work you'll do as a data scientist, you will define data strategy, drive technical development, and help us create the next generation of tools, products, and ai services. you will work closely with clients to understand their data sets, strategy, and operational requirements, to drive exploratory data and use case analysis and design long-term solutions. working with a team of interdisciplinary data scientists, engineers, architects, and consultants, our work includes novel areas such as cancer detection, drug discovery, optimizing population health and clinical trials, autonomous systems and edge ai, agentic solutions and framework design, and consumer product innovation. key responsibilities include: • guide clients with high autonomy in ai strategy and development, including understanding organizational needs, performing exploratory data analysis, building and validating models, and deploying models into production • participate in client initiatives to deliver ai/ml solutions, including providing thought leadership, long-term maintenance, and ai strategy objectives • research and implement novel machine learning approaches, including advancing state of the art training, solution design, network design, and hardware optimization • validate ai models and algorithm via code reviews, unit, and integration tests • support prioritization of project performance and model development and ensure ai solutions are delivered to maximize business impact and new initiatives • collaborate with data engineers, data scientists, project managers, and business teams to make sure delivery and presentations align with business objectives the team our strategy offering architects bold strategies to achieve business and mission goals, enabling growth, competitive advantage, technology modernization, and continuous digital and ai transformation. specifically, sfl scientific, a deloitte business, is a data science professional services practice focused on strategy, technology, and solving business challenges with artificial intelligence (ai). the team has a proven track record serving large, market-leading organizations in the private and public sectors, successfully delivering high-quality, novel and complex projects, and offering deep domain and scientific capabilities. made up of experienced ai strategists, data scientists, and ai engineers, they serve as trusted advisors to executives, helping them understand and evaluate new and essential areas for ai investment and identify unique opportunities to transform their businesses. qualifications: • bachelor's degree in a relevant stem field (data science, computer science, engineering, physics, mathematics, etc.) • 2+ years of experience in ai/ml algorithm development using core data science languages and frameworks (python, pytorch, etc.) and data analysis (nlp, time-series analysis, computer vision) • 2+ years of experience and a proven track record applying traditional ml and deep learning techniques (cnns, rnns, gans) across real-world projects, including model tuning and performance validation in production environments • 2+ years of experience deploying and optimizing ml models using tools like kubernetes, docker, tensorrt/trion, rapids, kubeflow, and mlflow • 2+ years of experience in leveraging cloud environments (aws, azure, or gcp) to deploy ai/ml workloads • live within commuting distance to one of deloitte's consulting offices • ability to travel 10%, on average, based on the work you do and the clients and industries/sectors you serve • limited immigration sponsorship may be available preferred: • master's or phd in a relevant stem field (data science, computer science, engineering, physics, mathematics, etc.) • 2+ years of experience working in a client-facing, consulting environment • 2+ years of experience leading project/client engagement teams in the execution of complex ai data science solutions • 1+ year of experience with llm/genai use cases and developing rag solutions, agent-based tools and services, and genai frameworks (i.e., langchain, langgraph, mcp, etc.) for individuals assigned and/or hired to work in california, deloitte is required by law to include a reasonable estimate of the compensation range for this role. this compensation range is specific to california and takes into account the wide range of factors that are considered in making compensation decisions including but not limited to skill sets; experience and training; licensure and certifications; and other business and organizational needs. at deloitte, it is not typical for an individual to be hired at or near the top of the range for their role and compensation decisions are dependent on the facts and circumstances of each case. a reasonable estimate of the current range is $93,225 to $155,375. you may also be eligible to participate in a discretionary annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance. information for applicants with a need for accommodation: https://www2.deloitte.com/us/en/pages/careers/articles/join-deloitte-assistance-for-disabled-applicants.html #monitordeloitte, #deloittejobs, #strategyconsulting, #deloittestrategy, #strategy26, #sfl26",canada,Data Scientist,"['aws', 'azure', 'cloud', 'computer vision', 'data analysis', 'deep learning', 'gcp', 'machine learning', 'nlp', 'python', 'pytorch', 'r']","['aws', 'azure', 'cloud', 'computer vision', 'data analysis', 'deep learning', 'gcp', 'machine learning', 'nlp', 'python', 'pytorch', 'r']",
staff data scientist-2,procore,"this position has the opportunity to be located in any of our us or canada offices, or based remotely in the us or canada. • *what you’ll do**: - work cross-functionally with a diverse slate of stakeholders to understand the business and technical requirements to define impactful ml solutions - provide technical leadership for efforts around tooling and infrastructure that enable teams to complete and maintain ai projects efficiently - coach and mentor junior data scientists on the team; lead code reviews, provide feedback on technical designs, and define best practices for technical development and implementation - be a thought partner and design thinker in the evolution of data science at procore • *what we’re looking for**: - 8+ years of relevant data science experience (6+ years with a master's or 3+ years with a ph.d.) - hands-on experience developing and managing predictive models - real-world experience in deploying predictive models at scale to production - experience with python and sql, statistics, and ml ops or engineering is a plus - superior communication skills with the ability to communicate complex mathematical and statistical concepts in simple terms - experience in saas is preferred • *additional information** • *perks & benefits** at procore, we invest in our employees and provide a full range of benefits and perks to help you grow and thrive. from generous paid time off and healthcare coverage to career enrichment and development programs, learn more details about what we offer and how we empower you to be your best. • *about us** procore technologies is building the software that builds the world. we provide cloud-based construction management software that helps clients more efficiently build skyscrapers, hospitals, retail centers, airports, housing complexes, and more. at procore, we have worked hard to create and maintain a culture where you can own your work and are encouraged and given resources to try new ideas. check us out on glassdoor to see what others are saying about working at procore. we are an equal-opportunity employer and welcome builders of all backgrounds. we thrive in a diverse, dynamic, and inclusive environment. we do not tolerate discrimination against employees on the basis of age, color, disability, gender, gender identity or expression, marital status, national origin, political affiliation, race, religion, sexual orientation, veteran status, or any other classification protected by law. if you'd like to stay in touch and be the first to hear about new roles at procore, join our talent community.",canada,Data Scientist,"['classification', 'cloud', 'python', 'r', 'sql', 'statistics']","['classification', 'cloud', 'python', 'r', 'sql', 'statistics']",
data scientist - llm,yo it consulting,"hiring a data scientist to help build advanced analytics and data-driven infrastructure for its ai lab partner focused on developing intelligent agent-based systems. this role is ideal for analytical thinkers who excel at turning large-scale data into actionable insights and enjoy working at the intersection of machine learning, experimentation, and real-world applications. you’ll be designing data pipelines, statistical models, and performance metrics that drive the next generation of autonomous systems. you’re a great fit if you: • have a strong background in data science, machine learning, or applied statistics. • are proficient in python, sql, and familiar with libraries such as pandas, numpy, scikit-learn, and pytorch/tensorflow. • understand probabilistic modeling, statistical inference, and experimentation frameworks (a/b testing, causal inference). • can collect, clean, and transform complex datasets into structured formats ready for modeling and analysis. • have experience designing and evaluating predictive models, using metrics like precision, recall, f1-score, and roc-auc. • are comfortable working with large-scale data systems (snowflake, bigquery, or similar). • are curious about ai agents, and how data can shape the reasoning, adaptability, and behavior of intelligent systems. • enjoy collaborating with cross-functional teams — from engineers to research scientists — to define meaningful kpis and experiment setups. primary goal of this role to design and implement robust data models, pipelines, and metrics that support experimentation, benchmarking, and continuous learning for agentic ai systems. the role focuses on building data-driven insights into how agents reason, perform, and improve over time across algorithmic and real-world tasks. what you’ll do • develop data collection and preprocessing pipelines for structured and unstructured data from multiple agent simulations. • build and iterate on machine learning models for performance prediction, behavior clustering, and outcome optimization. • design and maintain dashboards and visualization tools for monitoring agent performance, benchmarks, and trends. • conduct statistical analyses to evaluate the efficacy of ai systems under various environments and constraints. • collaborate with engineers to design evaluation frameworks that measure reasoning quality, adaptability, and efficiency. • prototype data-driven tools and feedback loops to automatically improve model accuracy and agent behavior over time. • work closely with ai research teams to translate experimental results into scalable, production-grade insights. why this role is exciting • work at the forefront of ai agent intelligence and help define how data shapes their evolution. • blend machine learning, experimentation, and data engineering in one role. • collaborate with top-tier ai engineers on new agent benchmarks and feedback mechanisms. • contribute to a mission that merges algorithmic reasoning, real-world performance, and human-like decision-making. pay & work structure • you’ll be classified as an hourly contractor to. • paid weekly via stripe connect, based on hours logged. • part-time (20 hrs - 40 hrs/week) with fully remote, async flexibility — work from anywhere, on your own schedule. • weekly bonus of $500 - $1000 usd per 5 task created.",canada,Data Scientist,"['a/b testing', 'bigquery', 'clustering', 'dashboard', 'data pipeline', 'excel', 'experimentation', 'machine learning', 'numpy', 'pandas', 'python', 'pytorch', 'r', 'scala', 'scikit-learn', 'snowflake', 'sql', 'statistics', 'tensorflow']","['a/b testing', 'bigquery', 'clustering', 'dashboard', 'data pipeline', 'excel', 'experimentation', 'machine learning', 'numpy', 'pandas', 'python', 'pytorch', 'r', 'scala', 'scikit-learn', 'snowflake', 'sql', 'statistics', 'tensorflow']",
"data scientist – center of excellence (coe), personal lines pricing",aviva canada inc,"experience aviva individually we are people, but together we are aviva. individually these are just words, but together they are our values – care, commitment, community, and confidence. at aviva canada, we put people first, our employees, our customers, and our communities. we’re proud of a culture built on care, inclusion, and collaboration, where your voice matters and your growth is supported. we’re not just about insurance; we’re about making a real difference by protecting what matters most. the opportunity join a passionate team of data scientists, actuaries, and engineers. data analysis is essential for decision-making at every level of our organization. the insurance industry is undergoing major changes, and you have the chance to be at the forefront of this data-driven technological revolution. aviva is building strong expertise in data science, and our team, personal lines pricing – center of excellence (coe), is at the heart of it. if you are passionate about using data science to tackle tomorrow’s challenges and want to work in a collaborative, forward-thinking environment, this role is for you. what you’ll do transform complex datasets into actionable insights and recommendations to support pricing strategies. participate in all phases of the mlops pricing model lifecycle: data preparation, model development, implementation, and performance monitoring. collaborate with business and engineering partners to enhance pricing solutions based on models and technical platforms. contribute to the development and maintenance of data exploration tools and systems. communicate model results and findings effectively to both technical and non-technical stakeholders. support innovation and continuous improvement in our data science practices. what you’ll bring university degree in data science, actuarial mathematics, computer science, engineering, mathematics, physics, or a related field (master’s degree is an asset). proficiency in both french and english, including excellent oral and written communication. minimum of 3 years of experience in model development and working with large datasets (industry or academic setting). proficiency in python and best practices in software engineering (modularity, code reuse, documentation). strong analytical skills and a keen interest in insurance pricing. excellent communication skills and ability to explain complex concepts clearly. collaborative mindset and eagerness to learn and contribute within a high-performing team. what makes you stand out experience with mlops and deploying models into production. familiarity with cloud platforms and ci/cd tools. knowledge of advanced machine learning and optimization techniques. what you’ll get compelling rewards package including base compensation, eligibility for annual bonus, retirement savings, share plan, health benefits, personal wellness, and volunteer opportunities. hybrid flexible work model. outstanding career development opportunities. we’ll support your professional development education. competitive vacation package with the option to purchase 5 extra days off per year. employee-driven programs focused on gender, lgbtq+, origins, diversity, and inclusion. this job advertisement is for an existing vacancy which has been posted both internally & externally. aviva canada may use ai (artificial intelligence) tools to assist us throughout the recruitment process to screen, assess or select applicants for a position. aviva canada welcomes applications from all qualified individuals and has a process in place to provide accommodations for persons with disabilities at all stages of the hiring process and during employment. if you require an accommodation during the interview or hiring process, please contact your aviva talent acquisition partner so that an appropriate accommodation can be arranged. #li-sg1 #li-hybrid we help our 19.5 million customers to save for the future and manage the risks of everyday life. to give these customers the best possible products and service we know we must make aviva the most attractive choice for talented, entrepreneurial people with diverse backgrounds and an evolving range of expertise and insight. so, we’re passionate about helping our 23,000 people to do the best work of their lives, to enable them to make a positive difference to the lives of our customers.",canada,Data Scientist,"['cloud', 'data analysis', 'excel', 'machine learning', 'python', 'r', 'recommendation']","['cloud', 'data analysis', 'excel', 'machine learning', 'python', 'r', 'recommendation']",
"senior data scientist, ai products",dropbox,"dropbox is a virtual first company. for this role, we are currently only authorized to hire candidates from the following provinces: alberta, british columbia, ontario, and saskatchewan. role description how many times do you get the opportunity to be on the ground floor of a big and important mission? what if you could be one of the top contributors defining the mission, guiding our teams, and influencing the direction of dropbox’s ai-first journey? as a senior data scientist for this new division, you will get to do exactly that. you will join a team of top-tier data scientists and become an integral part of the product organization, helping to scale this new business. joining on the ground floor of this startup team, you’ll partner directly with data science, product, engineering and design leadership to shape the product roadmap, foster a top-tier, data-informed culture, and drive real ai/ml impact and execution along the way! responsibilities partner with product engineers and data engineers to build the reliable, efficient, and scalable data foundations, tools, and processes to drive our ai/ml capabilities’ long-term growth leverage data-driven insights to proactively identify most impactful opportunities, and directly influence product roadmaps and strategies perform exploratory and deep-dive analysis to understand user workflows and engagement patterns on ai features, propose hypothesis, and design & execute experiments with great rigor and efficient data techniques translate complex data insights into implications and recommendations for the business via excellent communication skills, both verbal and written identify what matters most and prioritize ruthlessly for the area you will own contribute to a culture of strong technical ownership, partner with ds leadership to keep evolving ds working model and elevate ds impact work with cross-functional teams (including product, engineering, design, user research, and senior executives) to rapidly execute and iterate requirements bachelors’ or above in quantitative discipline: statistics, applied mathematics, economics, computer science, engineering, or related field 8+ years experience of leveraging data-driven analysis to influence product roadmap and business decision, preferably in a tech company proven track record of being able to work independently, driver measurable business impact, and proactively engage with business stakeholders with minimal direction proficiency in sql, python or other programming/scripting languages deep understanding of statistical analysis, experimentation design, and common analytical techniques like regression, decision trees ability to provide data insights and recommendations for 0→1 product even when sample size is small strong verbal and written communication skills preferred qualifications experience in startups or building 0→1 products expertise in using data to inform ai/ml product development background in saas product and growth analytics compensation canada pay range $157,300—$212,700 cad the range listed above is the expected annual salary/ote for this role, subject to change. salary/ote is just one component of dropbox’s total rewards package. all regular employees are also eligible for the corporate bonus program or a sales incentive (target included in ote) as well as stock in the form of restricted stock units (rsus). company description dropbox isn’t just a workplace—it’s a living lab for more enlightened ways of working. we're a global community of bold visionaries and resourceful doers who are shaping the future of dropbox—and with it the future of work. our virtual first model combines the autonomy of a distributed workplace with the power of human connection, making space for both meaningful work and meaningful relationships. with our start-up mindset and enterprise-level opportunities, you can be who you are and grow into who you’re meant to be. here, you can own your impact to make work more intuitive, joyful, and human—for you as a dropboxer and for hundreds of millions of people worldwide. if you're ready to push boundaries—and yourself— dropbox is ready for you. team description the dropbox data science team transforms data into powerful insights that inform everything we do at dropbox. combining applied analytics techniques with deep business insights, we investigate user behavior, product performance, and market trends to uncover new opportunities for growth, optimization, and innovation. our work is highly collaborative: we work closely with revenue, product, and marketing teams to enable data-driven development and personalized customer solutions. it’s also highly creative, as we experiment to develop tools and dashboards that democratize insights across dropbox. if you are driven by solving challenging problems and using the power of data to deliver impactful, user-centered products and services, join our data sciences team. areas of work include applied analytics, experimentation, data engineering, machine learning, business intelligence, data visualization, and statistical modeling. benefits dropbox is committed to investing in the holistic health and wellbeing of all dropboxers and their families. our benefits and perks programs include, but are not limited to: competitive medical, dental and vision coverage* retirement savings through a defined contribution pension or savings plan** flexible pto/paid time off, paid holidays, volunteer time off, and more, allowing you time to unplug, unwind, and refresh income protection plans: life and disability insurance* business travel protection: travel medical and accident insurance* perks allowance to be used on what matters most to you, whether that’s wellness, learning and development, food and groceries, and much more parental benefits including: parental leave, fertility benefits, adoptions and surrogacy support, and lactation support mental health and wellness benefits additional benefits details are available upon request. • where group plans are not available, allowances may be provided • *benefit, amount, and type are dependent on geographical location, based upon applicable law or company policy dropbox supports responsible use of ai for preparation, but misrepresentation of skills or experience is not permitted. see our ai philosophy . dropbox is an equal opportunity employer. we are a welcoming place for everyone, and we do our best to make sure all people feel supported and connected at work. a big part of that effort is our support for members and allies of internal groups like asians at dropbox, blackdropboxers, enable, todos (latinx), pridebox (lgbtq), vets at dropbox, and women at dropbox.",canada,Data Scientist,"['business intelligence', 'dashboard', 'excel', 'experimentation', 'machine learning', 'python', 'r', 'recommendation', 'regression', 'sas', 'scala', 'sql', 'statistics']","['business intelligence', 'dashboard', 'excel', 'experimentation', 'machine learning', 'python', 'r', 'recommendation', 'regression', 'sas', 'scala', 'sql', 'statistics']",$157K–$213K a year
sr data scientist,pro talent crafter,"job summary architect and own end-to-end machine learning systems— from data ingestion and feature engineering to scalable training, optimization, deployment, and monitoring on aws sagemaker. lead technical design for ml platforms and pipelines, selecting the right aws, open‑source, and mlops tooling to meet performance, cost, and governance requirements. responsibilities • develop advanced models using deep learning and statistical techniques, and optimize them for distributed training, accelerated compute, and real‑time or batch inference. • build reusable ml components, templates, and ci/cd workflows that improve reproducibility, compliance, and engineering velocity across the organization. • operationalize production ml at scale, including multi‑model endpoints, model registries, feature stores, experiment tracking, drift detection, and automated retraining. • troubleshoot complex model, data, and infrastructure issues, delivering root‑cause analysis and long‑term fixes for reliability, latency, performance, and cost efficiency. • partner with product, data engineering, cloud engineering, and it/security teams to embed ml solutions into mission‑critical business systems with enterprise‑grade resilience. • establish and enforce ml governance best practices including model documentation, lineage, versioning, auditability, and alignment with internal and regulatory standards. • provide technical leadership and mentorship to mid‑level and junior ml/ds engineers, reviewing code, guiding architecture choices, and uplifting overall team maturity. • evaluate emerging tools, frameworks, and aws services, advising leadership on how to modernize the ml stack and accelerate high‑value use cases. • represent ml engineering best practices internally, influencing roadmap decisions, contributing to technical design reviews, and shaping long‑term data/ai strategy. required qualifications & skills • 7+ years of experience in applied data science or machine learning engineering, including ownership of production model development and deployment. • 5+ years hands‑on experience with aws cloud services, with deep expertise in aws sagemaker (training jobs, pipelines, feature store, model registry, multi‑model endpoints, serverless inference). • demonstrated experience architecting large‑scale, production‑grade ml pipelines using tools such as airflow, kubeflow, aws step functions, or sagemaker pipelines. • strong proficiency in python, deep learning frameworks (tensorflow, pytorch), and advanced ml architectures (representation learning, forecasting, generative models, anomaly detection). • advanced knowledge of mlops practices including ci/cd (github actions, codepipeline), containerization (docker), orchestration (kubernetes/eks), and monitoring (prometheus, cloudwatch, datadog). • experience with modern data ecosystems, including data lakes/lakehouses, spark, delta/iceberg, and real‑time streaming pipelines. • strong understanding of model governance, responsible ai practices, and production slas. • excellent communication and stakeholder‑facing skills, with the ability to simplify complex technical topics. • experience with collaboration tools such as jira, confluence, github/gitlab. preferred qualifications & skills • experience designing ml architectures for high‑availability, multi‑tenant, or regulated environments (financial services, healthcare, manufacturing, etc.). • background in predictive maintenance, anomaly detection, customer intelligence, personalization, or nlp, depending on domain. • expertise in event‑driven ml, near‑real‑time inference, and streaming architectures (kafka, kinesis). • familiarity with software engineering best practices, including sdlc, testing patterns, code quality automation, and system design principles.",canada,Data Scientist,"['airflow', 'aws', 'cloud', 'data lake', 'deep learning', 'elt', 'excel', 'feature engineering', 'kafka', 'machine learning', 'nlp', 'python', 'pytorch', 'r', 'scala', 'spark', 'tensorflow']","['airflow', 'aws', 'cloud', 'data lake', 'deep learning', 'elt', 'excel', 'feature engineering', 'kafka', 'machine learning', 'nlp', 'python', 'pytorch', 'r', 'scala', 'spark', 'tensorflow']",$100K–$140K a year
applied data scientist – analytics specialist,mercor,"about the job mercor connects elite creative and technical talent with leading ai research labs. headquartered in san francisco, our investors include benchmark, general catalyst, peter thiel, adam d'angelo, larry summers, and jack dorsey. position: applied data scientist type: contract compensation: $75–$100/hour location: remote duration: ~6 weeks commitment: 30+ hours/week role responsibilities • translate business questions into data science problems and analytical workflows. • conduct data wrangling, exploratory analysis, and hypothesis testing using python and sql. • develop statistical models and predictive tools for decision support. • create compelling data visualizations and dashboards with tools like tableau and power bi. • present findings and recommendations to non-technical stakeholders. qualifications must-have • 5+ years of applied data science or analytics experience in business settings. • proficiency in python or r (pandas, numpy, jupyter) and strong sql skills. • experience with data visualization tools (e.g., tableau, power bi). • solid understanding of statistical modeling, experimentation, and a/b testing. • strong communication skills for translating technical work into strategic insights. compensation & legal • independent contractor • paid weekly via stripe connect application process (takes 20–30 mins to complete) • submit your resume • domain expertise interview • short form resources & support • for details about the interview process and platform information, please check: https://talent.docs.mercor.com/welcome/welcome • for any help or support, reach out to: support@mercor.com ps: our team reviews applications daily. please complete your ai interview and application steps to be considered for this opportunity. ,",canada,Data Scientist,"['a/b testing', 'dashboard', 'experimentation', 'numpy', 'pandas', 'power bi', 'python', 'r', 'recommendation', 'sql', 'tableau']","['a/b testing', 'dashboard', 'experimentation', 'numpy', 'pandas', 'power bi', 'python', 'r', 'recommendation', 'sql', 'tableau']",
senior data scientist: genai & ml for onboarding,servicetitan,"a leading software company for the trades is seeking a senior data scientist to enhance customer onboarding using machine learning and ai. the role requires a strong background in data science, proficiency in python and sql, and experience in deploying impactful solutions. the company offers a flexible work environment, comprehensive benefits, and support for learning and growth. the expected salary range is $124,400 cad - $186,600 cad. #j-18808-ljbffr",canada,Data Scientist,"['machine learning', 'python', 'r', 'sql']","['machine learning', 'python', 'r', 'sql']",
junior data scientist,mthree,"junior data scientist the wiley edge banking team is currently seeking a data scientist to work across the front office trading, risk & regulatory compliance trade analytics group, using machine learning and data analytics techniques to design and develop complex data models for client and trading analytics across their fixed income and fx offerings. you will be supporting the transformation from traditional controls monitoring to “on-screen” data driven monitoring and analytical solutions. tools: • python (pandas, numpy) • machine learning algorithms / natural language processing (nlp) • ai / robotics • predictive analysis • good grasp of ml algorithms starting salary: ca$85,000 per annum junior data scientist",canada,Data Scientist,"['data analytics', 'machine learning', 'natural language processing', 'nlp', 'numpy', 'pandas', 'python', 'r']","['data analytics', 'machine learning', 'natural language processing', 'nlp', 'numpy', 'pandas', 'python', 'r']",
data scientist - remote - canada/us - it1232,simplifyad,"as a remote data scientist, you will analyze complex data sets to help our organization make informed decisions. you will develop predictive models and algorithms to solve business problems and improve our products and services. this role requires a strong analytical mindset and the ability to work independently in a remote environment. responsibilities: • collect, process, and analyze large data sets to extract meaningful insights. • develop predictive models and machine learning algorithms. • collaborate with cross-functional teams to understand business needs and provide data-driven solutions. • communicate findings and recommendations to stakeholders through reports and presentations. • stay up-to-date with the latest data science trends and technologies. • design and implement data-driven experiments to validate hypotheses and improve decision-making. requirements: • bachelor's or master's degree in data science, statistics, computer science, or a related field. • minimum of 2 years of proven experience as a data scientist or similar role. • proficiency in programming languages such as python or r. • experience with data visualization tools like tableau or power bi. • strong understanding of machine learning algorithms and statistical analysis. • excellent problem-solving skills and attention to detail. • strong communication and presentation skills. • ability to work independently and manage time effectively in a remote setting. nice-to-have: • experience with big data technologies such as hadoop, spark, or kafka. • knowledge of cloud platforms like aws, azure, or google cloud for data processing and storage. • familiarity with deep learning frameworks such as tensorflow or pytorch. • experience with a/b testing and experimental design. • understanding of data privacy and security best practices. must-have: • legal authorization to work in canada or the usa. • reliable internet connection and a suitable home office setup. • experience with remote work tools and platforms.",canada,Data Scientist,"['a/b testing', 'aws', 'azure', 'cloud', 'deep learning', 'excel', 'google cloud', 'hadoop', 'kafka', 'machine learning', 'power bi', 'python', 'pytorch', 'r', 'recommendation', 'spark', 'statistics', 'tableau', 'tensorflow']","['a/b testing', 'aws', 'azure', 'cloud', 'deep learning', 'excel', 'google cloud', 'hadoop', 'kafka', 'machine learning', 'power bi', 'python', 'pytorch', 'r', 'recommendation', 'spark', 'statistics', 'tableau', 'tensorflow']",
senior data scientist - genai,company 1 - the manufacturers life insurance company,"we are seeking a highly skilled and motivated senior data scientist to join our group functions advanced analytics team. as a senior data scientist - genai, you will develop and implement robust analytical solutions for various parts of manulife and jh by applying traditional and emerging ml and ai techniques including but not limited to generative ai techniques, such as applying prompt engineering, working with rag applications, fine-tuning llm models, and deploying applications on cloud platforms like azure. your expertise in these areas will play a crucial role in driving data-driven decision-making and enhancing our business processes. position responsibilities: bachelor', master's degree, or ph.d. in computer science, data science, statistics, engineering, or a related field. 5+ years of experience in developing probabilistic models, data mining, and machine learning algorithms, including real world experience of applying analytics models, with a strong focus on machine learning, generative ai, prompt engineering, and rag applications proficiency in programming languages such as python and experience with machine learning libraries/frameworks, e.g., pytorch, scikit-learn, hugging face, sql, graph databases (neo4j/cypher, cosmos db/gremlin). knowledge of professional software engineering practices & standard methodologies for the full software development process, including coding standards, code reviews, source control management, build processes, testing, and operations. strong collaboration and elaboration skills; demonstrates a strong commitment to organizational success; shares resources and demonstrates knowledge across the organization. strong problem-solving skills and the ability to think critically and creatively to develop innovative solutions. excellent communication and collaboration skills, with the ability to work optimally in multi-functional teams. required qualifications: 5+ years of experience in developing probabilistic models, data mining, and machine learning algorithms, including real world experience of applying analytics models, with a strong focus on machine learning, generative ai, prompt engineering, and rag applications proficiency in programming languages such as python and experience with machine learning libraries/frameworks, e.g., pytorch, scikit-learn, hugging face, sql, graph databases (neo4j/cypher, cosmos db/gremlin). bachelor', master's degree, or ph.d. in computer science, data science, statistics, engineering, or a related field. preferred qualifications: knowledge of professional software engineering practices & standard methodologies for the full software development process, including coding standards, code reviews, source control management, build processes, testing, and operations. strong collaboration and elaboration skills; demonstrates a strong commitment to organizational success; shares resources and demonstrates knowledge across the organization. strong problem-solving skills and the ability to think critically and creatively to develop innovative solutions. excellent communication and collaboration skills, with the ability to work optimally in multi-functional teams. when you join our team: we’ll empower you to learn and grow the career you want. we’ll recognize and support you in a flexible environment where well-being and inclusion are more than just words. as part of our global team, we’ll support you in shaping the future you want to see. #li-hybrid about manulife and john hancock manulife financial corporation is a leading international financial services provider, helping people make their decisions easier and lives better. to learn more about us, visit https://www.manulife.com/en/about/our-story.html. manulife is an equal opportunity employer at manulife/john hancock, we embrace our diversity. we strive to attract, develop and retain a workforce that is as diverse as the customers we serve and to foster an inclusive work environment that embraces the strength of cultures and individuals. we are committed to fair recruitment, retention, advancement and compensation, and we administer all of our practices and programs without discrimination on the basis of race, ancestry, place of origin, colour, ethnic origin, citizenship, religion or religious beliefs, creed, sex (including pregnancy and pregnancy-related conditions), sexual orientation, genetic characteristics, veteran status, gender identity, gender expression, age, marital status, family status, disability, or any other ground protected by applicable law. it is our priority to remove barriers to provide equal access to employment. a human resources representative will work with applicants who request a reasonable accommodation during the application process. all information shared during the accommodation request process will be stored and used in a manner that is consistent with applicable laws and manulife/john hancock policies. to request a reasonable accommodation in the application process, contact recruitment@manulife.com. referenced salary location toronto, ontario working arrangement hybrid salary range is expected to be between $124,980.00 cad - $174,980.00 cad. if you are applying for this role outside of the primary location, please contact recruitment@manulife.com for the salary range for your location. the actual salary will vary depending on local market conditions, geography and relevant job-related factors such as knowledge, skills, qualifications, experience, and education/training. employees also have the opportunity to participate in incentive programs and earn incentive compensation tied to business and individual performance. manulife offers eligible employees a wide array of customizable benefits, including health, dental, mental health, vision, short- and long-term disability, life and ad&d insurance coverage, adoption/surrogacy and wellness benefits, and employee/family assistance plans. we also offer eligible employees various retirement savings plans (including pension and a global share ownership plan with employer matching contributions) and financial education and counseling resources. our generous paid time off program in canada includes holidays, vacation, personal, and sick days, and we offer the full range of statutory leaves of absence. if you are applying for this role in the u.s., please contact recruitment@manulife.com for more information about u.s.-specific paid time off provisions. we're manulife. and we’re on a mission to make decisions easier and lives better. better is what drives us. it’s what inspires us to find new ways to support customers and colleagues in living longer and healthier lives. it’s the reason we’re dedicated to investing in digital innovation and accelerating a sustainable and economically inclusive future. joining us means you’ll be empowered to learn and grow your career. we’ll recognize and support you in a flexible environment where well-being and inclusion are more than just words. and as part of our global team, you’ll help shape the future you want to see – and discover that better can take you anywhere you want to go. we’re proud of our accomplishments and recognitions. recent awards include: 2024 gallup exceptional workplace award winner manulife named one of forbes world’s best employers 2023 best companies to work for in asia 2023 we’ve been recognized as one of canada’s top 100 employers (2024) manulife included in bloomberg’s 2023 gender-equality index to receive our latest job opportunities directly to your inbox, create an account or sign in and navigate to the ‘job alerts’ section located in the top right corner of the page. from there, you can sign up to receive job alerts. our ambition is to be the most digital, customer-centric global company in our industry. learn more at https://www.manulife.com/.",canada,Data Scientist,"['aws', 'azure', 'cloud', 'excel', 'machine learning', 'python', 'pytorch', 'r', 'scikit-learn', 'sql', 'statistics']","['aws', 'azure', 'cloud', 'excel', 'machine learning', 'python', 'pytorch', 'r', 'scikit-learn', 'sql', 'statistics']",
senior python software engineer – analytics & optimization | nl office,scanmarqed,"senior python software engineer – analytics & optimization | nl office 6 days ago be among the first 25 applicants about the role: are you passionate about building advanced analytics solutions that drive real business impact? join scanmarqed as a senior python software engineer – analytics & optimization and help shape the analytic core of our pulse product, a marketing analytics platform used by leading companies worldwide. you’ll work with a talented, international team, solve complex modeling and optimization challenges, and have the flexibility to work from the uk, the netherlands, or (where appropriate) remotely. key responsibilities design, develop, and maintain robust, well‑structured, and well‑documented python‑based analytics and optimization modules for our pulse product. implement and optimize algorithms for econometric marketing mix modeling, forecasting, and advanced analytics. tackle large‑scale data processing, model optimization, and real‑world forecasting challenges. ensure high code quality through thorough unit and integration testing, promoting long‑term stability and maintainability. apply solid principles and appropriate design patterns to ensure scalable and maintainable architecture. work with modern technologies including scipy, numpy, pandas, sql, cloud platforms, docker, kubernetes, and ci/cd tools. collaborate with cross‑functional teams (data science, consultants, development) across the netherlands, uk, and us, with a focus on time zone overlap with nl/uk. take ownership of key modules and contribute to the continuous improvement of our analytics platform. review code, share knowledge, and contribute to best practices within the team. qualifications minimum 5 years of hands‑on experience with python, including scientific libraries (especially scipy). master’s degree in mathematics, computer science, or a closely related quantitative field. very strong background in mathematics, analytics, or statistical modeling. proven ability to understand and implement complex mathematical concepts and models. experience in developing analytic and optimization solutions in a product environment. valuable: experience with marketing mix modeling, forecasting, or econometric/statistical modeling. valuable: familiarity with numpy, pandas, sql, cloud platforms, docker, kubernetes, ci/cd tools. excellent communication, teamwork, and problem‑solving skills. what we offer the opportunity to shape the analytics engine that drives marketing decisions for global brands. collaborate with a highly skilled, international team. opportunity to mentor junior engineers or take technical ownership of key modules. a fast‑paced, performance‑driven culture with personal attention and growth opportunities. training and development opportunities, including conferences and workshops. a salary matching your knowledge and skills. why join us be part of a dynamic team that values innovation and client satisfaction. work in a collaborative environment where your ideas and contributions are valued. enjoy opportunities for professional growth and development. make a real impact on our clients' success and our company's growth. about scanmarqed we empower brands and media agencies by providing marketing technology and analytical consultancy to extract actionable insights from their data. our software helps our clients understand the impact that their sales and marketing decisions make on consumer behavior. whether we are operating the solutions on behalf of our clients (in an outsourced capacity) or our clients are using the software themselves (insourced), it’s our technology and industry knowledge that helps to make the difference. seniority level mid‑senior level employment type full‑time job function engineering and information technology industries it services and it consulting #j-18808-ljbffr",canada,Data Scientist,"['cloud', 'excel', 'numpy', 'pandas', 'python', 'r', 'scala', 'sql']","['cloud', 'excel', 'numpy', 'pandas', 'python', 'r', 'scala', 'sql']",
summer intern 2026 - data science (toronto office),mackenzie financial corporation,"job description: grade: s3 division: igm-tech igm financial inc. is a leading wealth and asset management company in canada, managing approximately $271 billion in assets. it offers financial planning and investment services to over two million canadians through ig wealth management and mackenzie investments. mackenzie investments, founded in 1967, is a key part of igm's business model, serving as a comprehensive asset-management partner for canadian financial advisors and their clients. at mackenzie investments you can build your career with confidence. we are proud to be recognized as one of canada’s top 100 employers for the fourth consecutive years and one of canada’s best diversity employers. our vision and strategy aim to innovate the industry and support canadians in achieving their financial goals. join our team to engage in continuous learning and skill development in a supportive environment. experience the best of both worlds with our hybrid work environment, where you spend three days a week in the office, connecting, collaborating, and enjoying quality time with your amazing colleagues! our values: be better; we strive for improvement in everything we do. be accountable; we foster clarity and are empowered to act responsibly. be a team; we are united to drive collective impact to achieve our goals position we are currently hiring an intern looking for a summer term position, may to august 2026, in our winnipeg office. candidates must be enrolled in an undergraduate program and plan to return to school after completing the internship to be eligible to apply. this position is responsible for supporting the data science team in analytics and data engineering tasks. the student will assist in developing datasets, models, and reports which are used to understand our business and meet strategic objectives. this position provides a great opportunity for a university student to gain practical experience in the fields of ai and data science. responsibilities: • designing and developing relevant datasets, reports and analysis for operational and strategic initiatives • work with data from multiple sources and databases (manipulate data, prepare data for machine learning) • analyze structured and unstructured data, organize findings and translate into actionable insights • learn various modelling approaches and understand how to evaluate best performance considering model metrics and appropriateness for business interpretation • collaborate closely with relevant teams to ensure models/solutions can be deployed • participate in the development, testing, implementation, documentation and continuous improvement of datasets and data models as a member of the data science team requirements: • currently enrolled in a post-secondary program in data science, computer science, management analytics, statistics or a similar field • knowledge of sql, python, and git • familiarity with looker, tableau, power bi, and/or other bi and reporting tools is a plus • exposure to cloud technologies (e.g. google cloud, azure) is beneficial • good critical thinking skills – conceptualizing, analyzing, synthesizing, and evaluating • willingness to learn how to develop code as part of a team, including clear documentation and code reviews to apply, please include a resume and transcripts (full unofficial version) by october 5, 2025, at 11:59 pm cst. the expected annual base salary range (prorated for the term) for this role is $49,000 - $51,000, which is determined based on year of study, igm program, geographic location and candidate’s skills, knowledge and experience. in addition to base salary, this role is eligible for paid time off. igm is a diverse workplace committed to doing business inclusively - this starts with having a representative workforce! we encourage applications from all qualified candidates that represent the diversity present across canada – including racialized persons, women, indigenous persons, persons with disabilities, 2slgbtqia+ community, gender diverse and neurodiverse individuals, as well as all who may contribute to the further diversification of ideas. mackenzie investments is an accessible employer committed to providing barrier-free recruitment experience. if you require accommodation or this information in an alternate format at any stage of the recruitment process, please reach out to the talent acquisition team who will work with you to meet your needs. how to apply: interested candidates are invited to submit their resume and a cover letter detailing their qualifications and experience to https://www.mackenzieinvestments.com/en/careers. we thank all applicants for their interest in mackenzie investments; however, only those candidates selected for an interview will be contacted. #li-js2 #li-hybrid",canada,Data Scientist,"['azure', 'cloud', 'google cloud', 'looker', 'machine learning', 'power bi', 'python', 'r', 'sql', 'statistics', 'tableau']","['azure', 'cloud', 'google cloud', 'looker', 'machine learning', 'power bi', 'python', 'r', 'sql', 'statistics', 'tableau']",$49K–$51K a year
founding data scientist / machine learning engineer,palladio ai,"seeking founding data scientists and machine learning engineers imagine multiplying your impact you've unlocked major wins in your career - you've shipped models, moved key metrics, and proved what great data science and machine learning can do. you've bent the curve for products used by millions of people. now, picture the idea of leveling up the entire app ecosystem by scaling your impact across many products and companies so every app in your pocket becomes smarter, more engaging, and more essential to the people who rely on it. you can help product teams iterate faster, delight users, and grow revenue, all because of the intelligence you build once and deploy everywhere. we know this ambition: we've done this again and again at places like uber, apple, meta, google, and chime. we've had tens of billions of dollars of impact for products essential to billions of people, and we're ready to scale our impact to the next level. if this sounds like the next chapter you're looking for, read on. why palladio exists dashboards describe the past; teams need their next move. palladio ai is the intelligence layer between raw data and decisive action, surfacing product opportunities that turn into real growth levers and guiding action so product teams iterate with confidence and velocity instead of sifting through noise. what you'll shape our team is building core systems in behavioral modeling, causal inference, forecasting, agentic platforms, and more. you'll help extend those domains: building ml and ai models to detect and surface product opportunities, shipping learning loops that improve themselves with every release. in short, you'll turn first-principles data science into a product that scales across industries. more than any technical challenge, you'll build a platform that helps real people make better decisions, turning data into clarity and clarity into progress. about you • craft and excellence. you dive into messy data, prototype quickly, and iterate until the insight sings. • impact mindset. 6+ years in production ml/ds; you balance scientific rigor with ""it ships today, iteration on the way"" pragmatism. • owner at heart. zero-to-one energizes you; you'd rather set the standard than inherit one. • product voice. you can walk a pm through trade-offs as confidently as you tune a model. • customer-obsessed. you connect with the user's pain, measure success in user outcomes, and translate findings into product wins. • beginner's mind. you stay hungry to learn, because better ideas are always out there. our team our team has shaped the design, data, and systems behind some of the world's most iconic products. we're builders who founded uber's michelangelo, led data and design teams at apple, meta, google, and chime, and shipped products used by billions. we've had tens of billions of dollars of impact. backed by top-tier investors, we're now channeling that experience into a focused, user-obsessed startup so every product team can tap the same class of intelligence at scale. why join now this is the moment to have an outsized impact. you'll work directly with founders and early customers to define not just features but also the fundamental approach to how product intelligence should work. you'll see your ideas in production quickly, affecting real decisions for products used by millions. we're building a company that values clarity, craft, and impact. if those values resonate with you, we'd love to connect. ready for the next level of impact? apply now",canada (+6 others),Data Scientist,"['dashboard', 'excel', 'machine learning', 'r']","['dashboard', 'excel', 'machine learning', 'r']",
senior data scientist - test and experiment,tiger analytics,"tiger analytics is looking for experienced data scientists to join our fast-growing advanced analytics consulting firm. our consultants bring deep expertise in data science, machine learning and ai. we are the trusted analytics partner for multiple fortune 500 companies, enabling them to generate business value from data. our business value and leadership has been recognized by various market research firms, including forrester and gartner. we are looking for top-notch talent as we continue to build the best global analytics consulting team in the world. as a senior data scientist with strong expertise in graph theory and advanced data processing to join our supply chain analytics team. the ideal candidate will have hands-on experience leveraging python, pyspark, and graph-based approaches (preferably with networkx) to solve complex business problems in supply chain optimization.. you will develop efficient and accurate analytical models which mimic business decisions and incorporate those models into analytical data products and tools. you will have the opportunity to drive current and future strategy by leveraging your analytical skills as you ensure business value and communicate the results. key responsibilities • design and manage a/b tests, multivariate tests, and holdout experiments across digital channels, products, and customer journeys. • partner with engineering to set up data pipelines and instrumentation ensuring test integrity and accurate tracking. • define and validate control and treatment groups, ensuring randomization and statistical soundness. • conduct post-experiment analysis using statistical and causal inference methods (e.g., t-tests, bayesian modeling, regression discontinuity, cuped). • quantify impact on kpis such as conversion rate, retention, engagement, or roi. • communicate experiment results, trade-offs, and business recommendations through clear visualizations and storytelling. • contribute to building scalable experimentation frameworks and dashboards. • automate experiment tracking, reporting, and governance. • advocate for best practices in hypothesis creation, sample sizing, and significance testing. • work closely with product managers, marketers, engineers, and data engineers to design testable initiatives. • enable teams to interpret and act on test results confidently. requirements • 8+ years of professional experience in data science, analytics, or related roles. • advanced degree in statistics, data science, economics, or related quantitative field. • exposure to machine learning-based experimentation, personalization, or recommendation systems. • strong knowledge of experimental design, causal inference, and statistical modeling. • proficiency in python, r, or sql for data manipulation and statistical analysis. • experience with experimentation platforms (e.g., optimizely, google optimize, adobe target, or in-house test platforms). • solid grasp of a/b testing metrics, sample size calculations, and confidence intervals. • ability to translate complex test outcomes into clear business insights. • strong communication and stakeholder management skills. benefits this position offers an excellent opportunity for significant career development in a fast-growing and challenging entrepreneurial environment with a high degree of individual responsibility. tiger analytics provides equal employment opportunities to applicants and employees without regard to race, color, religion, age, sex, sexual orientation, gender identity/expression, pregnancy, national origin, ancestry, marital status, protected veteran status, disability status, or any other basis as protected by federal, state, or local law.",canada,Data Scientist,"['a/b testing', 'bayesian', 'dashboard', 'data pipeline', 'excel', 'experimentation', 'machine learning', 'pyspark', 'python', 'r', 'recommendation', 'regression', 'scala', 'spark', 'sql', 'statistics']","['a/b testing', 'bayesian', 'dashboard', 'data pipeline', 'excel', 'experimentation', 'machine learning', 'pyspark', 'python', 'r', 'recommendation', 'regression', 'scala', 'spark', 'sql', 'statistics']",
"senior data scientist - quantumblack, ai by mckinsey",mckinsey and company,"overviewdriving lasting impact and building long-term capabilities with our clients is not easy work. you are the kind of person who thrives in a high performance / high reward culture - doing hard things, picking yourself up when you stumble, and having the resilience to try another way forward.in return for your drive, determination, and curiosity, we'll provide the resources, mentorship, and opportunities you need to become a stronger leader faster than you ever thought possible. your colleagues-at all levels-will invest deeply in your development, just as much as they invest in delivering exceptional results for clients. every day, you'll receive apprenticeship, coaching, and exposure that will accelerate your growth in ways you won't find anywhere else.when you join us, you will have : continuous learning : our learning and apprenticeship culture, backed by structured programs, is all about helping you grow while creating an environment where feedback is clear, actionable, and focused on your development. the real magic happens when you take the input from others to heart and embrace the fast-paced learning experience, owning your journey.a voice that matters : from day one, we value your ideas and contributions. you'll make a tangible impact by offering innovative ideas and practical solutions, all while upholding our unwavering commitment to ethics and integrity. we not only encourage diverse perspectives, but they are critical in driving us toward the best possible outcomes.global community : with colleagues across 65+ countries and over 100 different nationalities, our firm's diversity fuels creativity and helps us come up with the best solutions for our clients. plus, you'll have the opportunity to learn from exceptional colleagues with diverse backgrounds and experiences.world-class benefits : on top of a competitive salary (based on your location, experience, and skills), we provide a comprehensive benefits package to enable holistic well-being for you and your family.you will work on real-world, high-impact projects across a variety of industries, identify micro patterns in data that our clients can exploit to maintain their competitive advantage and watch your technical solutions transform their day-to-day business.you will experience the best environment to grow as a technologist and a leader, develop a sought-after perspective connecting technology and business value by working on real-life problems across a variety of industries and technical challenges to serve our clients on their changing needs. you will be surrounded by inspiring individuals as part of diverse multidisciplinary teams, develop a holistic perspective of ai by partnering with the best design, technical, and business talent in the world as your team members.as a senior data scientist, you will : solve the hardest business problems with our clients in multiple industries worldwide while leading research and development of state-of-the-art machine learning and statistical methodsplay a leading role in bringing the latest advances in deep learning to the world economy, collaborating with industry executives and quantumblack experts to find and execute opportunities to improve business performance using data and advanced machine learning modelsidentify machine learning r&d initiatives that have high potential of applicability in industrywork with quantumblack leadership and client executives to understand business problems and map them into state-of-the-art analytics and ai solutionsuse the latest advances in deep learning, reinforcement learning and ai to solve business problems and derive key insights across various industry sectorsmentor other data scientists and help them grow their knowledge and skillswork closely with data engineers, machine learning engineers and designers to build end-to-end analytics solutions for our clients that drive real impact in the real worldinfluence and help shape the r&d roadmap for quantumblack, especially on deep learningperhaps most importantly, you will work in one of the most talented and diverse data science teams in the worldyou will be part of our global data science community, and you will work with other data scientists, data engineers, machine learning engineers, designers and project managers on interdisciplinary projects, using math, stats and machine learning to derive structure and knowledge from raw data across various industry sectors.you are a highly collaborative individual who can lay aside your own agenda, listening to and learning from colleagues, challenging thoughtfully and prioritizing impact. you search for ways to improve things and work collaboratively with colleagues. you believe in iterative change, experimenting with new approaches, learning and improving to move forward quickly.our tech stackwhile we advocate for using the right tech for the right task, we often leverage the following technologies : python, pyspark, the pydata stack, sql, airflow, databricks, our own open-source data pipelining framework called kedro, dask / rapids, container technologies such as docker and kubernetes, cloud solutions such as aws, gcp, and azure, and more.qualificationsbachelor's, master's or phd level in a discipline such as computer science, machine learning, applied statistics, mathematics, engineering or artificial intelligence7+ years of deep technical experience in distributed computing, machine learning, and statistics related workprogramming experience in languages such as : python, r, scala, sqlknowledge of distributed computing or nosql technologies is a bonusclient-facing skills e.g. working in close-knit teams on topics such as data warehousing, machine learningproven application of advanced analytical, data science and statistical methods in the commercial worlddemonstrated leadership (thought leadership or people leadership e.g. managed project teams or direct reports)good presentation and communication skills, with a knack for explaining complex analytical concepts to people from other fieldswillingness to travel •",canada,Data Scientist,"['airflow', 'aws', 'azure', 'cloud', 'databricks', 'deep learning', 'gcp', 'machine learning', 'pyspark', 'python', 'r', 'scala', 'spark', 'sql', 'statistics']","['airflow', 'aws', 'azure', 'cloud', 'databricks', 'deep learning', 'gcp', 'machine learning', 'pyspark', 'python', 'r', 'scala', 'spark', 'sql', 'statistics']",
remote data scientist – ml research & pipelines,crossing hurdles,"position: remote data scientist – ml research & pipelines (contract) a data advisory firm is seeking a mid-senior level analyst to analyze complex datasets and build predictive models. this remote contract role requires expertise in data science and machine learning, particularly proficiency in python and its libraries. ideal candidates will have demonstrated success in competitions like kaggle. this position involves collaboration with engineering teams and offers flexible hours ranging from 10 to 40 per week. #j-18808-ljbffr",canada,Data Scientist,"['machine learning', 'python', 'r']","['machine learning', 'python', 'r']",$100K–$125K a year
"data scientist, ecl modeling",l301 pc bank,"referred applicants should not apply directly to this role. all referred applicants must first be submitted through workday by a current loblaw colleague. location: 500 lake shore boulevard west, toronto, ontario, m5v 2v9 when you hire great people, great things can happen. pc financial offers unprecedented value to canadians through payment products. we're a different kind of bank with a different type of team—we’re collaborative and supportive and have the freedom and responsibility to thrive. our purpose is to make the everyday simple and better for our customers, and we strive to make every dollar worth more. proudly serving over 3 million customers, pc financial continues to grow by offering payment solutions and services that reward our customers every day. as a subsidiary of loblaws company inc., we share the core values of care, ownership, respect and excellence. we are dedicated to helping canadians live life well. join us on our journey. why this role is important: join our ifrs 9 team as data scientist, ecl modeling, where you will play a pivotal role in shaping and executing on ecl (expected credit loss) modeling. you will oversee the development, enhancement, monitoring, and validation of ecl models, leveraging your skillset to provide data-driven insights for key provision decisions and effective credit risk management. this role offers a challenging and rewarding experience, providing the opportunity to lead a team and make a significant impact on the bank's risk management framework. what you'll do: lead the ecl enhancement roadmap for the team, in alignment with the overall business strategy by identifying and prioritizing key areas of focus. lead, mentor, and manage a high performing team, fostering a culture of continuous learning and development. apply statistical techniques and quantitative methods to validate, calibrate, and back test credit risk models. oversee and conduct in-depth data analysis and interpret complex financial data to identify patterns, trends, and potential risks. engage with senior executives, and business teams to present ecl results, model performance, and key risk insights to enable decision-making. create insightful reports, presentations, and dashboards to communicate model outputs, findings, and recommendations effectively. stay updated on emerging industry trends, regulatory changes, and advancements in credit risk modeling methodologies to continuously improve existing models. collaborate with cross-functional teams to implement model enhancements and provide ongoing support for model performance monitoring and validation. what you bring: post-secondary education in statistics, mathematics, or related quantitative field with 3+ years of experience in credit risk management or credit risk modeling proven experience in applying credit risk modeling principles and techniques in accordance with accounting standards (ifrs 9 , cecl, etc.) strong proficiency in statistical modeling techniques, data analysis, and quantitative methodologies. proficient in programming languages which may include: python, r, sql, sas. experience with gcp (or other cloud) data services (airflow, bigquery) is an asset. excellent communication skills, with the ability to effectively convey complex concepts and findings to both technical and non-technical partners. ability to distill and communicate complex analytic recommendations to both technical and non-technical stakeholders, both orally and in written presentations to influence decision-making come and join a winning team who demonstrates innovation, energy, creativity and vision. we recognize the importance of a diverse workforce and we therefor encourage applications from aboriginal peoples, women, members of a visible minority and persons with a disability. we thank all applicants for their interest, however, only those selected for an interview will be contacted. number of openings: 1 pc financial recognizes canada's diversity as a source of national pride and strength. we have made it a priority to reflect our nation’s evolving diversity in the products we sell, the people we hire, and the culture we create in our organization. accommodation is available upon request for applicants and colleagues with disabilities. in addition, we believe that compliance with laws is about doing the right thing. upholding the law is part of our code of conduct – it reinforces what our customers and stakeholders expect of us. please note: if you have employee self service (ess) on workday, apply to this job via the workday application. #en #ss #legal #on together, we are an organization committed to transforming the everyday banking experience, and providing our customers with the unprecedented value they deserve. we’re looking for entrepreneurial spirits to drive this vision forward, and because our business model is one-of-a-kind, our talent has to be just as unique. that’s where you come in. we will equip you with the freedom to envision, conceptualize and collaborate in a fast paced environment, and you’ll discover what it’s like to push the limits and draw on the talent of your peers. this experience empowers our colleagues on their journey every day, making our promise and vision simple. come work for a canadian bank that thinks differently.",canada,Data Scientist,"['airflow', 'aws', 'bigquery', 'cloud', 'dashboard', 'data analysis', 'excel', 'gcp', 'python', 'r', 'recommendation', 'sas', 'sql', 'statistics']","['airflow', 'aws', 'bigquery', 'cloud', 'dashboard', 'data analysis', 'excel', 'gcp', 'python', 'r', 'recommendation', 'sas', 'sql', 'statistics']",
"staff data scientist, strategic research & reporting",discord,"discord is used by over 200 million people every month for many different reasons, but there’s one thing that nearly everyone does on our platform: play video games. over 90% of our users play games, spending a combined 1.5 billion hours playing thousands of unique titles on discord each month. discord plays a uniquely important role in the future of gaming. we are focused on making it easier and more fun for people to talk and hang out before, during, and after playing games. discord is seeking an experienced and passionate staff data scientist to join our world-class data science & analytics team! in this role, you will play a key part in helping discord fulfill its mission of creating a sense of belonging in online communities. you’ll work closely with the teams that design, build, and support discord, providing insights that directly influence user growth and enhance the user experience. if you’re driven by data, eager to make a meaningful impact, and excited to collaborate with an amazing team, we want to hear from you! if you’re passionate about data, impact, and working on an amazing team, read on! what you'll be doing • partner with executive and finance teams to define and govern discord's most critical business metrics, ensuring consistency, accuracy, and alignment across the company. • own the data governance framework for core metrics, collaborating with data engineering and platform teams to implement validation checks, monitoring systems, and quality standards that maintain trust in our data • conduct deep-dive analyses on critical business trends and user behaviors, translating complex patterns into clear narratives that help discord understand how the platform is evolving and where opportunities exist • proactively socialize insights, dashboards, and reports with technical and non-technical audiences, soliciting feedback on where to improve what you should have • 7+ years of experience autonomously translating ambiguous business problems into deep informative insights through hands-on analysis • 7+ years of experience building performant dashboards using tableau, looker, or similar software - with proficiency in designing clean crisp visualizations • 6+ years experience writing robust, efficient and performant sql • 4+ years defining and standardizing business-critical metrics, with demonstrated ability to build consensus across high level stakeholders • 2+ years collaborating with data engineering and other technical teams to implement data quality checks, validation frameworks, and governance standards • 2+ years of experience in the design, analysis, and interpretation of sound a/b tests in a fast-paced environment at scale • experience with technical leadership, being the point person for one or more stakeholder groups • excellent communication skills, with the ability to deliver complicated findings and explain technical approaches to a variety of audiences • ability to own complex projects from end-to-end, identifying the need for and coordinating cross-functional partnerships • aptitude to proactively identify gaps in data infrastructure, evaluate potential solutions, and coordinate cross-functionally to address issues • last but not least - a collaborative attitude and a healthy dose of natural curiosity! bonus points • passion for discord and/or online communities • experience directly supporting executive or finance teams as the owner for business-critical metrics • experience with analytics for social media or international subscription-based online services, including familiarity with concepts such as social graphs, ltv analysis, funnel analysis, etc. • experience with notification system/platform and lifecycle management. • experience writing performant code in bigquery sql candidates must reside in or be willing to relocate to the san francisco bay area (alameda, contra costa, marin, napa, san francisco, san mateo, santa clara, solano, and sonoma counties). relocation assistance may be available. the us base salary range for this full-time position is $248,000 to $279,000 + equity + benefits. our salary ranges are determined by role and level. within the range, individual pay is determined by additional factors, including job-related skills, experience, and relevant education or training. please note that the compensation details listed in us role postings reflect the base salary only, and do not include equity, or benefits. why discord? discord plays a uniquely important role in the future of gaming. we're a multiplatform, multigenerational and multiplayer platform that helps people deepen their friendships around games and shared interests. we believe games give us a way to have fun with our favorite people, whether listening to music together or grinding in competitive matches for diamond rank. join us in our mission! your future is just a click away! discord is committed to inclusion and providing reasonable accommodations during the interview process. we want you to feel set up for success, so if you are in need of reasonable accommodations, please let your recruiter know. please see our applicant and candidate privacy policy for details regarding discord’s collection and usage of personal information relating to the application and recruitment process by clicking here.",canada (+6 others),Data Scientist,"['bigquery', 'dashboard', 'excel', 'looker', 'r', 'sql', 'tableau']","['bigquery', 'dashboard', 'excel', 'looker', 'r', 'sql', 'tableau']",
lead data scientist - canada,very,"this a full remote job, the offer is available from: canada about very (remote - canada) very is a fully distributed technology firm led by expert problem-solvers who create efficient, scalable solutions that move commercial, industrial, and consumer products from pilot to production in record time. we believe that real innovation happens in the grind — working shoulder to shoulder with clients who are building the future. our team thrives on that energy. when we’re not helping clients deliver business-critical outcomes, we’re refining our craft and celebrating what it means to do hard things well. we’ve built a collaborative, tight-knit culture that thrives in both remote and in-person settings. we’ve won numerous workplace awards over the years, including great place to work certification and recognition from parity.org as a best company for women to advance. our clients include well-known brands like vizio, peloton, clear, iheart radio, and fellowes — all determined to leverage connected devices and ai to drive meaningful impact. our job is simple: help them win. about this role as a lead data scientist at very, you will work with our software, hardware, and product design teams to build full-service solutions for our clients. we focus on building end-to-end hardware and software solutions that meet our client's business needs, and reliable algorithms and machine learning systems are often a part of our offering. an ideal candidate will display technical expertise in algorithms, machine learning and data science, strong communication skills, and the ability to present ideas and results to audiences ranging in technical depth. candidates should also have experience translating business problems into analytical solutions, working in interdisciplinary teams, and building models for production systems. a lead at very is an individual who operates with the highest degree of knowledge and accountability for the delivery of services to our customers. they provide excellent technical leadership and delivery skills, as it pertains to complex, multi-faceted projects at very. they have a strong executive presence, which gives major client stakeholders the confidence that we will deliver, and gives our team the confidence and accountability to do so. as a client services organization, travel may be required up to 10% of the time. what you’ll be working on at very, there is a never-ending supply of variety to the projects we work on. however, it is critical to note that almost all of these projects are production systems. as such, the only consistent research component of this position will revolve around establishing a pattern of delivery that allows the team to implement full-scale applications leveraging data science in a fast, predictable manner. the exact nature of the projects you will be assigned to is unknown, however there is an immediate need for someone to lead projects related to machine vision on the edge. this includes running and fine-tuning pretrained models such as yolo on hardware ranging from rtos on a nxp i.mx6 to docker containers on a jetson orin nx. a strong understanding of managing the edge environment, the ci/cd pipeline and the related azure or aws infrastructure is also required. lead engineers also regularly serve as a sales solutions engineer and are entrusted by the commercial team to be their main technical partner for closing high value contracts. they will travel onsite with clients, fine tune deliverables/staffing plans, and otherwise do what it takes to close these contracts with terms that are conducive to successful delivery. our current tooling our contracts typically involve building a full greenfield iot data pipeline and mlops lifecycles. this extends from the iot sensors and/or actuators, through any local networks, into the cloud, to the user interface and back again. in the context of data science, we typically leverage the following. • git, github, or github actions (ci/cd), pytest (tdd) • the standard scipy stack (numpy, scipy, pandas, scikit-learn, matplotlib) • tools such as sql/postgres, docker, mlflow, pyspark, [j and langchain • jupyter notebooks for prototyping often the data pipeline extends to the cloud where we leverage the following cloud resources. • aws: lambda, ecr/ecs, rds, dynamodb, iot core, greengrass, sagemaker, bedrock • azure: functions, container registry/instances, sql database, machine learning, iot hub • 3rd party: ultralytics, tigerdata, datadog, peridio • terraform for infrastructure-as-code on our full-service builds, we often reach for the following tools. experience with them is not required, but any familiarity with these tools is a plus. our build teams operate with a very high degree of collaboration, so you will definitely have run-ins with the following stacks throughout your time here. • python web development frameworks (django, flask) • react & react native as an iot technology company our data science pipelines include “things”. this will require you to build pipelines and deploy analytical models to hardware on the edge in addition to the cloud. this requires a deep collaboration with the design, software and hardware teams in the following environments. • yocto, linux and macos development environments • embedded c and other lower level languages such as rust • ci/cd including hardware and end-to-end testing and verification • development single-board computers such as rpi, nvidia jetson, nxp i.mx and arduino portenta we value well-tested, reusable code and expect our engineers to be as good of practitioners as they are leaders and teachers. responsibilities work with stakeholders (clients, sales, engineers & designers) to define statements of work (sow) • communicate how very can deliver value to the client • estimate quantity of work required to unlock this value for complex, multi-disciplinary projects • identify related assumptions, risks and dependencies take ownership of the data science components and related systems to ensure project success • architect, build and deploy reliable end-to-end data and ml pipelines into production • ensure the highest level of testing across the full data pipeline and operating envelope • execute and document all algorithm verification testing for certification • build strong relationships with clients and understand their perspective • guide clients through the data science value chain • facilitate complex conversations to achieve alignment to drive positive outcomes continue to expand and evolve the data solutions (ds) practice at very • provide technical guidance to ds and non-ds team members • pairs with senior team members to develop their skills and deliver on projects • continually learn, share and refine your ds skills and knowledge • establish and enact dataops and mlops best practices take on the responsibility of technical lead on complex multi-disciplinary projects • build reliable product roadmaps with technical implementation strategies • monitor and optimize the technical implementation and coordination of the project • identify and mitigate risks and seek assistance when required required qualifications unfortunately, applicants who do not meet these criteria will not be considered. experience: • master’s degree in data science related field • 5+ years of related experience • deployed statistical, ml or other analytical models to production on aws • performed real-time signal processing • lead teams with hardware and software engineers • worked directly with clients and partnered with sales and client success teams to secure new work • partnered with client success and senior executives ensure the success of current and future projects • strong written and spoken communication skills in english • expert-level python development skills related to data science including scipy stack, scikit-learn, tensorflow or pytorch • automated testing, code coverage, model building & evaluation • github ci/cd best practices including github actions and terraform or cloudformation • experience developing, compiling and deploying c, c++ or embedded c software • proficient developing in linux including and light administration nice-to-have • 7+ years of related experience including with connected devices • proficient in embedded real-time signal processing • machine vision and automated speech recognition • deployment and monitoring of ml workflows to nordic, nxp, nvidia or intel hardware at the edge • aws professional level certification • hands-on experience with single-board computers such as rpi skills: in addition to experience, these are the critical skills we look for in all technical roles, and how they should be demonstrated at the lead level. • communicates effectively: demonstrates expert-level communication skills. communicates to inform, engage and inspire. negotiates for positive outcomes with clients on complex projects. • influences broad audiences and creates compelling narratives around their ideas and why they are important. • demonstrates an expert level of knowledge and experience and as a result instills confidence in technical ability by team and clients. • takes calculated risks and shows a commitment to innovation that improves the business and tech community. • accurately estimates full scale engagements for statements of work, as part of the sales process. • leads people through our toughest program scenarios toward successful outcomes. provides quick redirection when needed. compensation cad $164,000–$173,500 per year, commensurate with experience. variable compensation: up to 15%. • *note: this role is based in canada, you must have a valid work permit. we do not sponsor work visas. perks & benefits: • group rrsp match (employer contributes 4% of your annual income) • extended healthcare, dental, and vision insurance • $150 usd/mo cell/internet stipend • $500 usd/yr home office equipment stipend • continuing education stipend ($2,500 usd/yr after one year) • loaned macbook pro and other necessary equipment why work for very we do not promise an easy ride — we promise meaningful work. we work hard because our clients’ success depends on it, and we take pride in delivering when others can’t. we collaborate closely, move fast, and stay grounded in results. we take joy in the process — in the problem-solving, the iteration, and the shared wins that come from doing the hard things well. if you’re looking for a place where every project matters, where the standards are high, and where you’ll grow by pushing yourself and others — welcome to very. important: 1. we don't currently provide visa sponsorship. don't apply if you require this. 2. this job is remote but if you’re not located in the region or country mentioned in the post’s title, do not continue. your application won’t be reviewed. interviewing for a new company is a serious time commitment for all parties involved. please take the time to read this and thoughtfully consider if we would be a good fit for one another. no contractors or agencies. seriously. #li-remote this offer from ""very"" has been enriched by jobgether.com and got a 79% flex score.",canada,Data Scientist,"['aws', 'azure', 'c++', 'cloud', 'data pipeline', 'excel', 'machine learning', 'matplotlib', 'numpy', 'pandas', 'pyspark', 'python', 'pytorch', 'r', 'sas', 'scala', 'scikit-learn', 'spark', 'sql', 'tensorflow']","['aws', 'azure', 'c++', 'cloud', 'data pipeline', 'excel', 'machine learning', 'matplotlib', 'numpy', 'pandas', 'pyspark', 'python', 'pytorch', 'r', 'sas', 'scala', 'scikit-learn', 'spark', 'sql', 'tensorflow']",US$164K–US$174K a year
"data scientist, real time observability",epic games,"what makes us epic? at the core of epic’s success are talented, passionate people. epic prides itself on creating a collaborative, welcoming, and creative environment. whether it’s building award-winning games or crafting engine technology that enables others to make visually stunning interactive experiences, we’re always innovating. being epic means being a part of a team that continually strives to do right by our community and users. we’re constantly innovating to raise the bar of engine and game development. analytics what we do our data & analytics teams build powerful stories and visuals that inform the games we make, the technology we develop, and business decisions that drive epic. what you'll do epic games is seeking to expand its insights team with a focus on observability. we are looking for a data scientist to assist us in delivering and maintaining an epic-quality live service game. this role wears many hats, juggles dynamic priorities, and works toward constantly moving targets. you'll create data tools, workbooks, and dashboards to support global epic development and publishing efforts. in this role, you will • expand on epic’s real-time observability framework by building, maintaining, and working with stakeholders to produce insightful product health monitoring and alerting. • educate key stakeholders on real-time data and provide clear sources of truth to make high-velocity data-driven decisions. • advocate for the player – coordinate across multiple disciplines, teams, and time zones to maintain a high-quality live service product. • dig into data, utilizing existing tools and rapid ad-hoc querying and visualization to help resolve active live issues. • drive observability evolution – continuously re-evaluate existing dashboards for opportunities to improve our data story. work with data teams to leverage the latest data features and tools. what we're looking for • direct experience building or maintaining observability for live service games, including familiarity with instrumentation, event schemas, real-time alerting, and health dashboards • proficiency with grafana, datadog, kibana, or amazon cloudwatch • proficiency with jpath, promql, sql, or strong knowledge of scripting languages. • prior experience in analytics, project management, release management, production, or analytics qa. • strong in-the-moment problem-solving skills and ability to break large problems into manageable and actionable pieces. • ability to communicate equally effectively with both technical and non-technical stakeholders at all levels. • flexible availability - including on-call - and strong time management skills. epic job + epic benefits = epic life we pay 100% for benefits for both employees and dependents and offer coverage for supplemental medical, dental, vision, critical illness, telemedicine, life and ad&d, long term disability insurance as well as weekly indemnity (short term disability) and a retirement savings plan with a competitive employer match. in addition to the eap (employee assistance program), we also offer a robust mental well-being program through modern health, which provides free therapy and coaching for employees & dependents. about us epic games spans across 25 countries with 46 studios and 4,500+ employees globally. for over 25 years, we've been making award-winning games and engine technology that empowers others to make visually stunning games and 3d content that bring environments to life like never before. epic's award-winning unreal engine technology not only provides game developers the ability to build high-fidelity, interactive experiences for pc, console, mobile, and vr, it is also a tool being embraced by content creators across a variety of industries such as media and entertainment, automotive, and architectural design. as we continue to build our engine technology and develop remarkable games, we strive to build teams of world-class talent. like what you hear? come be a part of something epic! epic games deeply values diverse teams and an inclusive work culture, and we are proud to be an equal opportunity employer. learn more about our equal employment opportunity (eeo) policy here. note to recruitment agencies: epic does not accept any unsolicited resumes or approaches from any unauthorized third party (including recruitment or placement agencies) (i.e., a third party with whom we do not have a negotiated and validly executed agreement). we will not pay any fees to any unauthorized third party. further details on these matters can be found here.",canada,Data Scientist,"['cloud', 'dashboard', 'r', 'sql']","['cloud', 'dashboard', 'r', 'sql']",
data scientist – ai & ml specialist,mercor,"about the job mercor connects elite creative and technical talent with leading ai research labs. headquartered in san francisco, our investors include benchmark, general catalyst, peter thiel, adam d'angelo, larry summers, and jack dorsey. position: data scientist type: hourly contractor compensation: $14/hour location: remote commitment: 20–40 hours/week role responsibilities • develop data collection and preprocessing pipelines for structured and unstructured data from multiple agent simulations. • build and iterate on machine learning models for performance prediction, behavior clustering, and outcome optimization. • design and maintain dashboards and visualization tools for monitoring agent performance, benchmarks, and trends. • conduct statistical analyses to evaluate the efficacy of ai systems under various environments and constraints. • collaborate with engineers to design evaluation frameworks that measure reasoning quality, adaptability, and efficiency. • prototype data-driven tools and feedback loops to automatically improve model accuracy and agent behavior over time. qualifications must-have • strong background in data science, machine learning, or applied statistics. • proficient in python, sql, and familiar with libraries such as pandas, numpy, scikit-learn, and pytorch/tensorflow. • understanding of probabilistic modeling, statistical inference, and experimentation frameworks (a/b testing, causal inference). • experience with large-scale data systems (snowflake, bigquery, or similar). • ability to collect, clean, and transform complex datasets into structured formats ready for modeling and analysis. preferred • interest in ai agents and how data can shape their reasoning, adaptability, and behavior. • enjoy collaborating with cross-functional teams to define meaningful kpis and experiment setups. compensation & legal • hourly contractor to mercor. • paid weekly via stripe connect, based on hours logged. • weekly bonus of $500 - $1000 usd per 5 tasks created. application process (takes 20–30 mins to complete) • upload resume • ai interview based on your resume • submit form resources & support • for details about the interview process and platform information, please check: https://talent.docs.mercor.com/welcome/welcome • for any help or support, reach out to: support@mercor.com ps: our team reviews applications daily. please complete your ai interview and application steps to be considered for this opportunity. ,",canada,Data Scientist,"['a/b testing', 'bigquery', 'clustering', 'dashboard', 'experimentation', 'machine learning', 'numpy', 'pandas', 'python', 'pytorch', 'r', 'scikit-learn', 'snowflake', 'sql', 'statistics', 'tensorflow']","['a/b testing', 'bigquery', 'clustering', 'dashboard', 'experimentation', 'machine learning', 'numpy', 'pandas', 'python', 'pytorch', 'r', 'scikit-learn', 'snowflake', 'sql', 'statistics', 'tensorflow']",
sr data and ml scientists,hirevouch,"position overview as a data scientist, you'll play a pivotal role in developing and enhancing our innovative supply chain solutions. you'll collaborate closely with a talented team of passionate engineers and industry experts, applying data engineering and data science techniques to optimize supply chain processes. your expertise will directly contribute to providing our clients with real-time visibility, predictive analytics, and actionable insights. what you will do • develop and implement machine learning models to improve the accuracy of supply chaineta s. • analyze large, complex datasets to extract meaningful insights and identify trends. • design, build, and maintain scalable data pipelines and data lakes to ensure efficient data flow and storage for analytics purposes. • collaborate with cross-functional teams to integrate analytics solutions into our products. • design and build data visualization tools to communicate findings to stakeholders. • continuously research and apply the latest techniques in ai and machine learning to supply chain problems. • contribute to the development of our analytics product suite, ensuring scalability and efficiency. • communicate complex data science concepts to non-technical stakeholders. key responsibilities 1. advanced predictive models: utilize machine learning algorithms to analyze historical data and identify patterns. models such as regression analysis, neural networks, or ensemble methods can predict etas more accurately by considering various factors like traffic conditions, weather, route efficiency, and historical performance. 2. real-time data integration: incorporate real-time data into predictive models. this includes traffic updates, weather conditions, vehicle speed, and location data. real-time data allows for dynamic adjustments to eta predictions as conditions change. 3. historical data analysis: analyze historical data to understand common delays and their causes. this analysis can help in adjusting the predictive models to account for recurrent issues. 4. route optimization: use data analytics to identify the most efficient routes. optimized routing not only shortens travel time but also makes eta predictions more reliable. 5. machine learning for anomaly detection: implement machine learning algorithms to detect anomalies that could affect delivery times, such as unexpected traffic jams or vehicle breakdowns, and adjust etas accordingly. 6. sensor data utilization: leverage data from iot devices and sensors equipped in transportation vehicles. this data can provide insights into vehicle performance, road conditions, and other factors that influence travel time. 7. advanced analytics techniques: employ advanced analytics techniques like time series forecasting, geospatial analysis, and simulation models to enhance the robustness of eta predictions. skills, knowledge and expertise we value diversity and encourage all interested candidates, regardless of whether they meet all qualifications, to apply. while the following qualifications are relevant to our work, they are not strict requirements: • bs or ms degree in data science, statistics, computer science, or a related field. • proven experience in machine learning, statistical modeling (5+ years) • proficiency in programming languages such as python or r. • experience with sql and working with large datasets. • strong expertise in designing and building robust data pipelines, data lakes, and optimizing data storage and retrieval structures. • knowledge of supply chain processes and logistics is a plus. • strong problem-solving and analytical skills. • excellent communication and teamwork abilities. • speaking spanish is a plus! • we prioritize eastern time zone (nyc) and cet (amsterdam) location base for employees so they can join regular team sessions. comfortably.",canada,Data Scientist,"['data analysis', 'data analytics', 'data lake', 'data pipeline', 'excel', 'machine learning', 'python', 'r', 'regression', 'scala', 'sql', 'statistics', 'time series']","['data analysis', 'data analytics', 'data lake', 'data pipeline', 'excel', 'machine learning', 'python', 'r', 'regression', 'scala', 'sql', 'statistics', 'time series']",
data scientist 1,the toronto-dominion bank (canada),"work location: toronto, ontario, canada hours: 37.5 line of business: analytics, insights, & artificial intelligence pay details: 76,800 - 115,200 cad td is committed to providing fair and equitable compensation opportunities to all colleagues. growth opportunities and skill development are defining features of the colleague experience at td. our compensation policies and practices have been designed to allow colleagues to progress through the salary range over time as they progress in their role. the base pay actually offered may vary based upon the candidate's skills and experience, job-related knowledge, geographic location, and other specific business and organizational needs. as a candidate, you are encouraged to ask compensation related questions and have an open dialogue with your recruiter who can provide you more specific details for this role. job description: the model validation (mv) group in enterprise risk, risk management is responsible for the independent validation and approval of all analytical models used for td. the position reports to senior manager, trading surveillance and insider risk models validation. detailed accountabilities include: validates financial crime risk management (fcrm) models for insider risk management conduct validation testing and analysis for different fcrm models under senior manager or manager supervision to support various business initiatives and regulators requirements. have effective communication with different stakeholders from 1st line to discuss and solve model limitations / issues identified during the validation process. deliver high quality model validation reports with adequate information and justification to support validation conclusions and audit / regulator review / assessment of validation process and validation analysis. maintains professional knowledge of ai / ml techniques and developments in fcrm modeling for insider risk management. develops different validation testing tools / programs to assess advanced ai / ml insider risk management models education & experience: post-secondary degree in one or more of the following areas: computer science, statistics, mathematics, financial engineering, or engineering in-depth knowledge of ai graph / network analysis 2 years + experience in either developing or validating fcrm model with exposure in ai/ ml modelling; and experience in fcrm modeling for insider risk management is an asset. proficient in relevant programming languages & software such as python, h2o.ai, scala, etc. excellent verbal and written communication skills excellent time / project management and multitasking skills with minimal supervision. customer accountabilities: works in a highly interactive, team-oriented environment with big data developers, and analytical experts stays current on developments in trading surveillance modelling shareholder accountabilities: works closely with key stakeholders to conduct different validations in time to support different business initiatives executes on the validation plan to deliver results aligned with enterprise model validation procedures and model risk policy employee/team accountabilities: shares knowledge, information, skills, and subject matter expertise among the team; collaborates with other functions and teams to complete validation efficiently and in-time escalate identified issues. who we are: td is one of the world's leading global financial institutions and is the fifth largest bank in north america by branches/stores. every day, we deliver legendary customer experiences to over 27 million households and businesses in canada, the united states and around the world. more than 95,000 td colleagues bring their skills, talent, and creativity to the bank, those we serve, and the economies we support. we are guided by our vision to be the better bank and our purpose to enrich the lives of our customers, communities and colleagues. td is deeply committed to being a leader in customer experience, that is why we believe that all colleagues, no matter where they work, are customer facing. as we build our business and deliver on our strategy, we are innovating to enhance the customer experience and build capabilities to shape the future of banking. whether you’ve got years of banking experience or are just starting your career in financial services, we can help you realize your potential. through regular leadership and development conversations to mentorship and training programs, we’re here to support you towards your goals. as an organization, we keep growing – and so will you. our total rewards package our total rewards package reflects the investments we make in our colleagues to help them and their families achieve their financial, physical, and mental well-being goals. total rewards at td includes a base salary, variable compensation, and several other key plans such as health and well-being benefits, savings and retirement programs, paid time off, banking benefits and discounts, career development, and reward and recognition programs. learn more additional information: we’re delighted that you’re considering building a career with td. through regular development conversations, training programs, and a competitive benefits plan, we’re committed to providing the support our colleagues need to thrive both at work and at home. please be advised that this job opportunity is subject to provincial regulation for employment purposes. it is imperative to acknowledge that each province or territory within the jurisdiction of canada may have its own set of regulations, requirements. colleague development if you’re interested in a specific career path or are looking to build certain skills, we want to help you succeed. you’ll have regular career, development, and performance conversations with your manager, as well as access to an online learning platform and a variety of mentoring programs to help you unlock future opportunities. whether you have a passion for helping customers and want to expand your experience, or you want to coach and inspire your colleagues, there are many different career paths within our organization at td – and we’re committed to helping you identify opportunities that support your goals. training & onboarding we will provide training and onboarding sessions to ensure that you’ve got everything you need to succeed in your new role. interview process we’ll reach out to candidates of interest to schedule an interview. we do our best to communicate outcomes to all applicants by email or phone call. accommodation your accessibility is important to us. please let us know if you’d like accommodations (including accessible meeting rooms, captioning for virtual interviews, etc.) to help us remove barriers so that you can participate throughout the interview process. we look forward to hearing from you! language requirement (quebec only): sans objet us labor & employment posters | california privacy | accessibility | faq our values at td we’re guided by our purpose to enrich the lives of our customers, communities and colleagues, and share a set of values that shape our culture and guide our behavior. in exchange for how our colleagues show up to help td succeed, we are committed to delivering a colleague experience grounded in impact, growth and a culture of care. no matter where you work across td, we empower you to make an impact at work and in your community, explore and grow your career and be part of our caring and inclusive culture. our commitment to diversity, equity, and inclusion at td, we’re committed to fostering an environment where all colleagues are encouraged to bring their authentic selves to work, experience equitable opportunities, and feel respected and supported. we’re dedicated to building an inclusive workforce that reflects the diversity of the customers and the communities in which we live and serve. helping to make an impact in communities – td ready commitment td has a long-standing commitment to help drive progress towards a more inclusive and sustainable future. that’s why we launched the td ready commitment in 2018, now a multi-year north american initiative. under the td ready commitment, we are targeting a total of c$1 billion by 2030 in community giving across four key, interconnected drivers of change: financial security, vibrant planet, connected communities, and better health. it’s our goal to help support change, nurture progress, and contribute to making the world a better, more inclusive place for our customers, colleagues, and communities. learn more: canada | us | europe & asia pacific",canada,Data Scientist,"['excel', 'python', 'r', 'scala', 'statistics']","['excel', 'python', 'r', 'scala', 'statistics']",
data scientist with java expertise,luxoft,"we are looking for an experienced data engineer with machine learning expertise and good understanding of search engines, to work on the following: design, develop, and optimize semantic and vector-based search solutions leveraging lucene/solr and modern embeddings. apply machine learning, deep learning, and natural language processing techniques to improve search relevance and ranking. develop scalable data pipelines and apis for indexing, retrieval, and model inference. integrate ml models and search capabilities into production systems. evaluate, fine-tune, and monitor search performance metrics. collaborate with software engineers, data engineers, and product teams to translate business needs into technical implementations. stay current with advancements in search technologies, llms, and semantic retrieval frameworks. skills must have 5+ years of experience in data science or machine learning engineering, with a focus on information retrieval or semantic search. strong programming experience in both java and python (production-level code, not just prototyping). deep knowledge of lucene, apache solr, or elasticsearch (indexing, query tuning, analyzers, scoring models). experience with vector databases, embeddings, and semantic search techniques. strong understanding of nlp techniques (tokenization, embeddings, transformers, etc.). experience deploying and maintaining ml/search systems in production. solid understanding of software engineering best practices (ci/cd, testing, version control, code review). nice to have experience of work in distributed teams, with us customers experience with llms, rag pipelines, and vector retrieval frameworks. knowledge of spring boot, fastapi, or similar backend frameworks. familiarity with kubernetes, docker, and cloud platforms (aws/azure/gcp). experience with mlops and model monitoring tools. contributions to open-source search or ml projects. other languages english: b2 upper intermediate seniority senior related jobs view all vacancies senior data engineer data science poland wroclaw data scientist data science egypt remote egypt data scientist data science india remote india remote canada, canada req. vr-118052 data science hls & consumer industry 07/11/2025 req. vr-118052 apply for data scientist with java expertise in remote canada",canada,Data Scientist,"['aws', 'azure', 'cloud', 'data pipeline', 'deep learning', 'gcp', 'java', 'machine learning', 'natural language processing', 'nlp', 'python', 'r', 'scala']","['aws', 'azure', 'cloud', 'data pipeline', 'deep learning', 'gcp', 'java', 'machine learning', 'natural language processing', 'nlp', 'python', 'r', 'scala']",
senior data scientist; remote - production ml & ai,bespoke labs,"position: senior data scientist (remote) - production ml & ai a pioneering ai-driven startup is seeking a senior data scientist to take ownership of machine learning projects. the role offers flexibility with part-time hours and competitive pay. candidates should have over 5 years of experience in data science, strong python skills, and familiarity with machine learning tools. join a high-impact team focused on solving real-world problems and enjoy opportunities for growth in a fast-paced environment. #j-18808-ljbffr",canada,Data Scientist,"['machine learning', 'python', 'r']","['machine learning', 'python', 'r']",$100K–$125K a year
"data science intern, 2026 summer canada",atlassian,"overview: working at atlassian atlassians have the flexibility to work where they can collaborate most effectively with their team, whether at home, one of our global offices, or both. as a distributed-first company, all interviews are conducted virtually. we hire in any country where we have a legal entity. learn how we work at go.atlassian.com/distributed. your future org atlassian’s intern program combines hands-on technical training, professional growth opportunities, dedicated mentorship, and strong social connections. this holistic approach empowers students to hit the ground running and sets them up for a successful and meaningful career at atlassian! this position is based in british columbia. to be considered, candidates must be willing and able to work from this location. • our intern roles are not eligible for canada work visa sponsorship now or in the future. candidates with student work permit or any open work permit are welcome to apply. your future team does the challenge of scaling a fast-growing company by powering it with data, reporting, and analytics excite you? using your business instincts and creativity, you will support a market-leading product with nuanced analysis, relevant insights, and recommendations. responsibilities: what you'll do • collaborate with cross-functional teams to identify and prioritize high-impact data science projects aligned with business goals • improve user journey and customer value by managing and understanding trends in large volumes of data • guide measurement culture and understand the impact of our partner's strategy, including designing and analyzing experiments and feature launches • build top-notch dashboards that provide partners and internal team members with a clear understanding of trends and insights. • work with product managers and engineers to translate insights into product improvements. qualifications: your background • fluency with sql or other programming languages like python, scala, or r • experience using dashboarding tools such as tableau, business objects or similar • experience with supervised learning, optimization, and statistical analysis • able to commit to a 15-week full-time (40hrs/week) program during summer 2026 • currently enrolled in a full-time master's degree program and returning to the program after the completion of the internship, graduating by june 2027 compensation at atlassian, we strive to design equitable, explainable, and competitive compensation programs. to support this goal, the baseline of our range is higher than that of the typical market range, but in turn we expect to hire most candidates near this baseline. base pay within the range is ultimately determined by a candidate's skills, expertise, or experience. in the canada, we have one geographic pay zone. for this role, our current base pay range for new hires in this zone are: zone a: $38.00/hr - $43.00/hr cad this role may also be eligible for benefits, bonuses, commissions, and equity. benefits and perks atlassian offers a wide range of perks and benefits designed to support you, your family and to help you engage with your local community. our offerings include health and wellbeing resources, paid volunteer days, and so much more. to learn more, visit https://www.atlassian.com/company/careers/earlycareers about atlassian at atlassian, we're motivated by a common goal: to unleash the potential of every team. our software products help teams all over the planet and our solutions are designed for all types of work. team collaboration through our tools makes what may be impossible alone, possible together. we believe that the unique contributions of all atlassians create our success. to ensure that our products and culture continue to incorporate everyone's perspectives and experience, we never discriminate based on race, religion, national origin, gender identity or expression, sexual orientation, age, or marital, veteran, or disability status. all your information will be kept confidential according to eeo guidelines. to provide you the best experience, we can support with accommodations or adjustments at any stage of the recruitment process. simply inform our recruitment team during your conversation with them. to learn more about our culture and hiring process, visit go.atlassian.com/crh.",canada,Data Scientist,"['dashboard', 'python', 'r', 'recommendation', 'scala', 'sql', 'tableau']","['dashboard', 'python', 'r', 'recommendation', 'scala', 'sql', 'tableau']",$38–$43 an hour
remote data scientist for ai finance insights (contract),crossing hurdles,"a tech-forward company is seeking a data scientist to conduct statistical analyses in the finance sector. the successful candidate will analyze ai performance, provide insights, and create dashboards. the role is remote with flexible hours, suited for someone with strong statistical and programming skills, particularly in python or r. this contract position requires a mid-senior level of expertise.",canada,Data Scientist,"['dashboard', 'python', 'r']","['dashboard', 'python', 'r']",$100K–$125K a year
principle data scientist,autotrader.ca,"we are trader, a canadian leader in digital automotive solutions. our flagship brands — autotrader.ca, autosync, dealertrack canada and cms — help canadians buy, sell, and finance vehicles with confidence. autotrader.ca is canada’s largest automotive marketplace, with over 25 million monthly visits. through autosync, we provide software solutions to 3,500+ dealers, streamlining their operations, marketing, and sales. dealertrack canada is the country’s top automotive financing portal, processing more than 6.5 million credit applications each year. collateral management (cms) is a national tech solution that boosts lien and registration services, recovery services, and insolvency management solutions for canadian lenders. as part of autoscout24 group, europe’s largest online car marketplace, we’re shaping the future of automotive retail in canada and beyond. learn more at tradercorporation.com. join our global data science team and take a leadership role in shaping the future of the automotive marketplace through cutting-edge ai and machine learning. as a principal data scientist, you’ll spearhead high-impact ai initiatives that influence millions of users worldwide, with a focus on both strategic vision and hands-on innovation. in this role, you’ll collaborate cross-functionally with product, engineering, and business teams to design, build, and scale ai solutions that set us apart in the industry. you’ll bring deep technical expertise, a passion for innovation, and a strong product mindset to develop ml products that solve real-world problems and deliver measurable business value. our ideal candidate is an experienced data science leader who thrives in a dynamic, fast-paced environment that combines the stability of an industry leader with the agility of a startup culture. you are curious, proactive, and committed to continuous learning, especially in emerging areas like generative ai and large language models (llms). you should be comfortable engaging with all levels of the organization, from peers to executives, and possess the ability to distill complex information into clear, actionable strategies that drive business decisions and create value. what you’ll do • shape the strategic direction and vision of data science across the organization by identifying transformative ai/ml opportunities and championing the team’s evolving role in a rapidly advancing genai landscape. • lead the design and deployment of predictive and generative ai models that power personalization, pricing, search, and optimization in marketplace and fintech domains. • collaborate with product leaders to align ml initiatives with strategic business goals and drive product innovation through data-driven experimentation and modeling. • architect scalable ml infrastructure and automated workflows using cloud-native tools (e.g., aws, ec2, kubernetes) to support efficient model training, deployment, and analytics across diverse datasets. • ensure long-term performance and compliance of production ai models though robust governance and monitoring. • provide technical leadership and mentorship to data scientists, shaping an innovative team culture and fostering high-performance and continuous learning. • serve as a cross-functional technical leader, shaping company-wide technical initiatives beyond data science. partner with engineering, product, and platform teams to influence architecture, innovation agendas, and technical standards across the organization. • act as a senior technical advisor, leading resolution of complex modeling issues and acting as the escalation point for critical incidents. • lead the exploration and strategic application of genai, identifying high-impact use cases and guiding their integration across products and platforms. what you’ll need • advanced academic credentials in a quantitative field such as computer science, engineering, mathematics, or related discipline • 10+ years of experience in data science, machine learning, or applied ai, with a strong portfolio of high-impact projects in production • expert-level programming skills in python and sql, and fluency with leading ml/ai frameworks (e.g., scikit-learn, tensorflow, pytorch) • direct experience with genai/llm technologies, including tools like hugging face, langchain, openai apis, vector databases, and fine-tuning methods • deep knowledge of machine learning algorithms (supervised, unsupervised, deep learning), including model evaluation, explainability, and selection for business-critical use cases • strong hands-on experience with cloud infrastructure (aws), containerization (docker), and orchestration (jenkins, airflow) • proven capability in mlops, including ci/cd pipelines, model monitoring, versioning, and automated retraining • experience deploying and serving models through apis (e.g., flask, fastapi) in both real-time and batch-processing environments • excellent communication and stakeholder management skills, able to translate complex concepts into actionable insights for non-technical audiences • demonstrated success mentoring teams, guiding technical strategy, and advocating for best practices in experimentation, reproducibility, and ethical ai • experience working in agile product development environments (scrum/kanban); experience influencing product roadmaps is a strong plus bonus points • experience in e-commerce, marketplaces, or high-scale consumer platforms • familiarity with automotive data and applications in pricing, inventory optimization, or recommendation systems • contributions to open-source ai/ml projects, publications, or presentations at industry conferences experience leveraging ai, generative ai (genai) to enhance engineering productivity, automate repetitive tasks, and optimize workflows. candidates should demonstrate the ability to integrate ai-driven solutions into their daily work — such as code generation, debugging, reviews, documentation, and decision support—to improve efficiency for themselves and their teams. a proactive approach to exploring and implementing ai tools that drive innovation and streamline development processes is highly valued if this opportunity excites you, but you're unsure whether your background checks every box, we value passion, curiosity, and a growth mindset. reach out and tell us what strengths you would bring to the team. we look forward to hearing from you! what's in it for you we understand that there is life at work and life outside of work. here are a few benefits we all benefit from that support us to be our creative best. benefits from day 1 • gym discounts • employee and family assistance program • virtual wellness events • conferences & training budget • regular internal training programs financial planning • let us help you invest in your future with 3% matching towards your pension and multiple forms of income protection. for a career where you can drive our business and shape your future, apply now. use of artificial intelligence in hiring: we use artificial intelligence (“ai”) in our hiring process, including to screen, assess, or select applicants for this position. vacancy status: this job posting is for an existing vacancy.",canada,Data Scientist,"['airflow', 'aws', 'cloud', 'deep learning', 'excel', 'experimentation', 'machine learning', 'python', 'pytorch', 'r', 'recommendation', 'scala', 'scikit-learn', 'sql', 'tensorflow']","['airflow', 'aws', 'cloud', 'deep learning', 'excel', 'experimentation', 'machine learning', 'python', 'pytorch', 'r', 'recommendation', 'scala', 'scikit-learn', 'sql', 'tensorflow']",
data scientist i – information technology,abc benefits corporation,"alberta blue cross® is an alberta based organization dedicated to delivering exceptional customer experience and community leadership. we’re committed to providing the best health coverage to over 1.8 million members and take an active role in promoting wellness. we believe in what we do—and place trust in our employees to deliver our vision. working at alberta blue cross® means having a career where you’ll be recognized for your contributions. we value diversity, encourage our team members to maintain a healthy work-life balance and provide opportunities for career growth. overview: reporting to the team manager of data science, it advanced analytics, alberta blue cross® is seeking a data scientist supporting the development and implementation of the organization’s data analytics strategy. this position will be part of a team whose purpose will be to advance the impact that data and analytics will have on the business as well as support and foster a data driven culture. in return this individual will experience a dynamic, highly engaged, and fast-paced organization that is committed to your success. working in a hybrid work style, you will be required to work from home and in-office. as a data scientist, you will play a critical role in the delivery of our enterprise data analytics strategy and development of machine learning and ai capabilities. you will partner with internal stakeholders to drive the adoption of advanced analytics across the business. the ideal candidate thrives on challenge, is unfazed by complexity and persists in the face of ambiguity. you will be highly curious, results-oriented, self-driven and possess an acute sense of business opportunity. what you will do: participate in problem solving initiatives, collaborating within a cross functional team of specialists, to design innovative solutions for clients and business leaders. participate in the wrangling of data, especially regarding the more practical aspects like quality assurance, cleaning, aggregation and integration. apply data science expertise including descriptive, predictive, machine learning, deep learning, and ai techniques to generate new insights and solve business problems. demonstrate creative problem solving and ability to translate data patterns into business insights, cause/effect relationships and variables to amplify predictive power of underlying models. provide business recommendations by distilling complex data science findings into clear, actionable insights, and effectively presenting them through visual displays to stakeholders at all levels, assisting with data-driven decision-making. what you will have: bachelor’s, master’s or doctoral degree in computing science, statistics, mathematics, engineering, or a related field in which advanced statistical methods are used. 2+ years of quantitative analysis experience, including handling, manipulating, and analyzing data and creating analytical reports. ability to translate business questions to data questions and present results in ways that make complicated relationships clear. comfortable using one or more statistical programming languages (i.e., python/r) for data analysis and data engineering languages such as sql and pyspark to work with large data sets and relational databases. familiarity with azure, databricks, power bi and star schema data modeling is preferred. strong written and verbal communication skills. ability to work effectively as an autonomous data scientist on a small project, and to collaborate with an interdisciplinary group on larger projects. disciplined curiosity, consistent with the best traditions of science. deep knowledge and experience of the algorithms and techniques of traditional statistics and modern data science especially as they pertain to business and experimentation. experience in utilizing generative ai to solve business problems with unstructured data, with the focus on enhancing efficiency, fostering innovation, and driving organizational impact. this position will remain open until a suitable candidate is selected. alberta blue cross® is an inclusive employer committed to a workplace that reflects the diversity of the communities we serve. we empower and are advocates for our team members by welcoming, respecting and valuing their unique perspectives, backgrounds, and experiences. we offer the opportunity to work in an innovative, high-energy team-focused environment. if you have the qualifications we are looking for, apply online at careers.ab.bluecross.ca alberta blue cross® is an alberta based not-for-profit, dedicated to delivering exceptional customer experience and community leadership. we are committed to providing the best health coverage to over 1.8 million members and take an active role in promoting the wellness of all albertans. we believe in what we do—and place trust in our employees to deliver our vision. privacy policy terms of use ®* the blue cross symbol and name are registered marks of the canadian association of blue cross plans, an association of independent blue cross plans. licensed to abc benefits corporation for use in operating the alberta blue cross plan. ®† blue shield is a registered trade-mark of the blue cross blue shield association.",canada,Data Scientist,"['azure', 'data analysis', 'data analytics', 'databricks', 'deep learning', 'experimentation', 'machine learning', 'power bi', 'pyspark', 'python', 'r', 'recommendation', 'spark', 'sql', 'statistics']","['azure', 'data analysis', 'data analytics', 'databricks', 'deep learning', 'experimentation', 'machine learning', 'power bi', 'pyspark', 'python', 'r', 'recommendation', 'spark', 'sql', 'statistics']",
"data scientist (﻿azure, python, and sql) - toronto, on",source code,"this a full remote job, the offer is available from: canada data scientist (﻿azure, python, and sql) toronto, on. fully remote 1-year contract (with the possibility of extension) ﻿﻿the ideal candidate is comfortable working with analyzing large data sets to find opportunities to improve product performance and increase value to customers. in order to do that, it's important that this person knows how to communicate well with all levels of the organization, ranging from peer data scientists to business stakeholders. they must have a proven ability to drive business results and comfortable communicating recommendations and findings to a business audience. the right candidate will have a passion for growing their data science acumen. responsibilities: • use predictive modeling to increase and optimize customer experiences • marketplace focused on ico domain. • work with the product manager to prototype new ideas, models, and ways of leveraging lots of data points to improve business outcomes. • utilize appropriate cloud resources based on project demands and data workloads needs to automate and productionalize data science processes. • develop enrichment processes and procedures to improve the cleanliness, usability and understanding of data. • proactively monitor existing machine learning models, their performance, output, and accuracy. • management of large-scale data processing system for preparing structured and unstructured data for analytic modelling. • work along with support teams to troubleshoot and identify customer-reported issues. ﻿must-haves: • 4+ years experience as a data scientist, data engineer, machine learning engineer or a simular position • extensive experience with azure • expert level in python and sql querying skills • experience with training and maintaining ml models﻿ • university degree in stem field nice-to-haves: • experience with pricing estimations and insights • automotive services experience • e-commerce industry experience this offer from ""source code "" has been enriched by jobgether.com and got a 0% flex score.",canada,Data Scientist,"['azure', 'cloud', 'machine learning', 'python', 'r', 'recommendation', 'sql']","['azure', 'cloud', 'machine learning', 'python', 'r', 'recommendation', 'sql']",
lead data scientist - marketing & product analytics,absorb,"""the marketing team is poised for unprecedented growth and opportunity. we're in a very unique position where we've experienced tremendous success but are far from exhausting the whitespace around us. there is a unique opportunity to join a rising organization and make a significant and immediate impact."" mark lynch - senior director, demand generation are you passionate about transforming data into strategy and ai into action? we are currently seeking a senior manager, data science to join our marketing organization, where you’ll serve as a player-coach driving data-informed decision making across growth, retention, and product strategy. in this role, you’ll turn complex signals into clear, actionable insights that optimize acquisition efficiency, customer value, and product prioritization—while pioneering pragmatic ai and llm workflows that move insights closer to real-time impact. what you’ll do: • own the growth & enablement data-science roadmap tied to company okrs; quantify impact and prioritize via roi-based frameworks. • design, run, and interpret rigorous experiments across website/growth and in-product surfaces; choose the right randomization unit (session/user/account), set guardrails, and deliver credible readouts that drive decisions. • improve the cost and quality mix of inbound demand through better targeting, suppression, routing, and budget allocation; partner with marketing to put models and playbooks into motion. • build interpretable churn/health signals with meaningful lead time; partner with cs to identify and test the highest-impact mitigation levers. • use ai to tap into new qualitative and behavioral sources (e.g., calls, emails, community, support, etc.) to generate relevant insights. • stand up ai enabled tools, automation and workflows to make the marketing and product teams more efficient. • support feature prioritization process with data on customer/ prospect preferences. • provide usage/uptake data & reviews to assess success of feature releases. • partner with pm/enablement to quantify impact and de-risk bets with experiments or quasi-experimental methods. • collaborate closely with the bi team to deliver impactful reporting. what you’ll bring: • 7-10 years in data science across growth / marketing and / or product analytics. • strong statistical toolkit (experimentation design, uplift/causal methods, time-series, retention analysis). • advanced sql and python • machine learning (ml) proficiency: capable of developing, evaluating, and productionizing models (e.g., classification, regression, forecasting/uplift) with solid engineering hygiene (versioning, ci/cd, monitoring, docs). • experience with modern data stacks (snowflake/bigquery/redshift; dbt/airflow or similar). • clear communication and influence across executives, pms, marketing, cs. nice-to-have: • experience delivering ai-enabled workflows • experience developing and implementing ai agents are you ready to become an absorber? what we offer: • fully remote-first work with flexible work arrangements • comprehensive health and wellness benefits, generous time off, comprehensive medical and dental benefits based on your country of location • new hire equipment allowance and monthly flex allowance to support your success • endless opportunity for career growth and internal mobility • employee driven de&i programs who are we? absorb software is a remote-first company that provides online training solutions to leading organizations around the world. absorb is a cloud-based learning management system (lms) engineered to inspire learning and fuel business productivity. our online learning platform combines forward-thinking technology built to scale as our customer’s organizations grow. we empower learners to enrich their lives, workplaces and communities. our values are simple: • we achieve exceptional results by genuinely caring about each other and the work we do • we’re united, and we grow through our commitment to elevating continual learning! absorb is proud to be an equal opportunity employer, we celebrate diversity and are committed to creating a safe and inclusive environment for all our people. all employment decisions are based on business needs, job requirements and individual qualifications. in the event a current absorb employee would like to apply for this role they will inform their supervisor prior to submitting their application. successful candidates for this position will be subject to pre-employment background screening, including a criminal record check and must be able to show proof of legal eligibility to work in the country they have applied to without sponsorship. should you require any accommodation during the recruitment process, please indicate this on your application and we will work with you to meet your accessibility needs. for any questions, please contact us at show email #li-remote",canada,Data Scientist,"['airflow', 'bigquery', 'classification', 'cloud', 'dbt', 'experimentation', 'machine learning', 'python', 'r', 'redshift', 'regression', 'snowflake', 'sql']","['airflow', 'bigquery', 'classification', 'cloud', 'dbt', 'experimentation', 'machine learning', 'python', 'r', 'redshift', 'regression', 'snowflake', 'sql']",
"senior data scientist, investing data science",wealthsimple,"the data science & engineering (dse) team consists of analytics engineers, data scientists, and software engineers with diverse educational backgrounds such as math, operations research, economics, computer science, engineering and business. the team is responsible for enabling data-driven decision making and building data products at wealthsimple. we achieve these goals by: • building a high quality and scalable state-of-the-art data warehouse that powers all decision making • leveraging machine learning and algorithms to help wealthsimple build smarter financial products • using decision science to understand the cause and effect of our business decisions about the role: at wealthsimple, our investing teams are the driving force behind self directed investing and managed invest products. we are hiring for an experienced data scientist to join our investing dse team. this individual will work on some of the highest priority initiatives at wealthsimple, ensuring that our product strategy is well grounded in data analytics. some of your projects may include, but are not limited to: • analyze user behaviour to understand how clients interact with the product • develop and implement experiments to evaluate new features and product changes • apply causal inference methods in non-experimental settings • use machine learning algorithms to segment users and personalize product experiences • develop and maintain core data models and pipelines to ensure data quality and accessibility in this role, you will have the opportunity to: • partner with our product, product marketing, and engineering teams to support work across some of wealthsimple's highest priority efforts • build robust, efficient, and integrated data models in cloud data warehouses that will be used as the source of truth for analytics across the investing product organization • apply statistical techniques and build machine learning models to solve real life product problems and to inform product decisions and strategies • apply software engineering best practices like version control and continuous integration to the analytics code base; ensure data models are well tested, documented and maintained • translate business requirements into data models and analysis that will help stakeholders answer key business questions and inform strategic decisions • continuously look for the simplest, most effective way to approach challenges; occam's razor is your friend • take ownership and ship it • work where you are (while our canadian offices are located in toronto, we are a remote-first workforce and you can join from anywhere in canada) skills we are looking for: • strong business acumen and product sense • excellent python and sql skills, first hand experience working with popular python libraries such as pandas, scikit-learn, numpy and jupyter • strong understanding of statistics: both frequentist and bayesian approaches • strong understanding of fundamental machine-learning algorithms: regression and decision trees we are looking for someone who: • is able to articulate their stance in a manner that effectively collaborates with different stakeholders • adjusts quickly to changing priorities and effectively manages complexity and change • has exceptional communication skills and is able to communicate effectively with both technical and non-technical teams • is eager to teach and learn from your team. we value making each other successful • is a lifelong learner. you're constantly seeking and welcoming feedback to continue improving yourself and your craft why wealthsimple? • competitive salary with top-tier health benefits and life insurance • retirement savings matching plan using wealthsimple work • 20 vacation days per year and unlimited sick and mental health days • up to $1,500 per year towards wellness and professional development budgets respectively • 90 days away program: employees can work internationally in eligible countries for up to 90 days per calendar year • a wide variety of peer and company-led employee resource groups (ie. rainbow, women of wealthsimple, black @ ws) • company-wide wellness days off scheduled throughout the year we’re a remote-first team, with over 1,000 employees coast to coast in north america. be a part of our canadian success story and help shape the financial future of millions — join us! read our culture manual and learn more about how we work. dei statement at wealthsimple, we are building products for a diverse world and we need a diverse team to do that successfully. we strongly encourage applications from everyone regardless of race, religion, colour, national origin, gender, sexual orientation, age, marital status, or disability status. accessibility statement wealthsimple provides an accessible candidate experience. if you need any accommodations or adjustments throughout the interview process and beyond, please let us know, and we will work with you to provide the necessary support and make reasonable accommodations to facilitate your participation. we are continuously working to improve our accessibility practices and welcome any feedback or suggestions on how we can better accommodate candidates with accessibility needs.",canada,Data Scientist,"['bayesian', 'cloud', 'data analytics', 'data warehouse', 'excel', 'machine learning', 'numpy', 'pandas', 'python', 'r', 'regression', 'scala', 'scikit-learn', 'sql', 'statistics']","['bayesian', 'cloud', 'data analytics', 'data warehouse', 'excel', 'machine learning', 'numpy', 'pandas', 'python', 'r', 'regression', 'scala', 'scikit-learn', 'sql', 'statistics']",$91K–$194K a year
"lead data scientist, 2 hour learning",crossover,"lead data scientist, 2 hour learning (remote) - $60,000/year usd join crossover as lead data scientist for the 2‑hour learning framework, a model where children complete a full day of academics in a concentrated two hours, then spend their afternoons on creative projects. located remotely, you will convert large volumes of ai‑generated performance metrics into precise, actionable interventions that propel each learner toward 90% mastery and beyond. when a student encounters an obstacle, you will engage them in a live coaching session, eliminate the barrier, and restore momentum. you will also analyze deep streams of student performance data to identify hidden insights, unlock children’s full capabilities, and engineer next‑generation ai solutions capable of doubling learning velocity. base pay range $30.00/hr - $30.00/hr what you will be doing creating highly precise academic data reports, performing structured analysis to identify issues, detect anomalies, and initiate corrective action within 48 hours conducting rapid‑response intervention calls and facilitating post‑assessment coaching to eliminate blockers, distinguish knowledge deficits from mistakes, and sustain student motivation assessing and deploying new educational applications and content while prototyping ai/llm solutions to advance learning methodologies and address specific obstacles collaborating with leadership to optimize tools, dashboards, and mastery benchmarks as the ai tutor advances what you won’t be doing attending endless meetings or delivering generic presentations with no visibility into actual results grading assignments, drafting lesson plans, or administering traditional educational programs—ai and innovation are the foundation of our approach working exclusively in high‑level strategy without execution or becoming mired in repetitive tasks—this position is hands‑on and fluid lead data scientist key responsibilities enhance student learning outcomes through ai‑powered analysis and real‑time academic intelligence, evidenced by improved metrics such as 2× learning velocity, app engagement, lesson comprehension, response accuracy, and diminished negative learning behaviors across the alpha anywhere network. basic requirements proven experience in data science or a comparable role, education, edtech, or a student‑support position combining analytical and interpersonal dimensions proficiency with google sheets data analysis (lookups, pivot tables, filtering) high emotional intelligence, excellent communication abilities, and a kid‑friendly coaching approach experience designing workflows and automations using ai nice‑to‑have requirements foundation in learning science, educational theory, or proven success enhancing outcomes via data‑driven approaches familiarity with generative ai, prompting methods, and foundational coding skills (python, javascript, etc.) for interfacing with ai apis about 2 hour learning education is broken, but 2 hour learning is proving it doesn’t have to be. they’re tearing down the outdated one‑size‑fits‑all model and replacing it with ai‑driven personalized learning that helps kids master academics in just two hours a day. with students consistently ranking in the top 1‑2% nationally and the top 20% achieving an astonishing 6.5× growth, they’re proving that smarter learning is possible. at 2 hour learning, it’s talent and performance that matter. working with us this is a full‑time (40 hours per week), long‑term position. the role is immediately available and requires entering into an independent contractor agreement with crossover as a contractor of record. the compensation level for this role is $30 usd/hour, which equates to $60,000 usd/year assuming 40 hours per week and 50 weeks per year. the payment period is weekly. #j-18808-ljbffr",canada,Data Scientist,"['dashboard', 'data analysis', 'excel', 'java', 'python', 'r']","['dashboard', 'data analysis', 'excel', 'java', 'python', 'r']",
lead data scientist – telematics company,groom & associés / associates,"lead data scientist – telematics company this company is a global leader in iot and connected transportation. they are advancing security, connecting commercial vehicles to the internet and providing web-based analytics to help customers better manage their fleets. the company’s open platform and marketplace, offering hundreds of third-party solution options, allows both small and large businesses to automate operations by integrating vehicle data with their other data assets. processing billions of data points a day, the company leverages data analytics and machine learning to improve productivity, optimize fleets through the reduction of fuel consumption, enhance driver safety and achieve strong compliance to regulatory changes. backed by a team of industry leading data scientists, engineers and ai experts, we serve over 50,000 customers across 160 countries, processing billions of data points hourly from more than 4 million vehicles. data security and privacy are at the forefront of all we do—trusted by fortune 500 organizations and some of the largest public sector fleets in the world, we meet top cybersecurity standards. the company’s open platform and diverse marketplace offers hundreds of fleet-ready third-party solutions. role: as a data science team lead, your key area of responsibility is to be accountable for the end-to-end implementation and deployment of advanced generative ai solutions, including the design, development, and optimization of complex data models. this position drives cross-functional projects, ensuring the team's work directly aligns with core organizational goals to deliver innovative products and services in the telematics industry. with a strong blend of technical expertise and leadership, this position upholds a high technical standard for the team's output while leveraging strong problem-solving and statistical analysis to ensure all data products meet critical business needs. this position will receive strategic guidance from the senior leadership of the department while owning the technical delivery and mentoring the team to reach their full potential title: lead data scientist location: 3 offices in ontario. hybrid 2 days in office. 155 university avenue, 2nd floor, unit 200 toronto, ontario, m5h 3b7, canada 2440 winston park drive, oakville, ontario, l6h 7v2, canada 137 glasgow street, unit 340, kitchener, ontario, n2g 4x8, canada type of position: full time, permanent salary: 150k-185k + performance bonus of 5k-10k + 3-4 week’s vacation + 4% rrsp matching start date: start of 2026 key responsibilities: • lead generative ai engineering team in developing a clear technical roadmap. translate strategic business goals into actionable and innovative generative ai solutions for the telematics industry. • design and implement robust, scalable, and secure architectures for ai-powered systems. ensure solutions are optimized for performance, efficiency, and safety, while balancing critical considerations like ai safety and latency. • research, develop, and implement novel generative ai powered solutions. champion advanced techniques such as prompt engineering and retrieval augmented generation (rag) to solve complex business challenges. • establish and maintain a high technical bar for the team's output. lead the design of objective evaluation systems to rigorously assess model quality, performance, and user experience. • continuously monitor and integrate cutting-edge advancements in generative ai, deep learning, and prompt engineering into the development lifecycle. actively engage with the broader ai research community to bring in best practices and new methodologies. • serve as the primary technical subject matter expert, guiding the genai engineering team on a day-to-day basis and providing hands-on support for complex technical challenges. • actively mentor and coach team members on best practices in ai product development, providing timely and constructive feedback to support their professional growth. • manage communication with cross-functional stakeholders, translating complex technical insights into clear, actionable reports and presentations. ensure comprehensive documentation of research, methodologies, and model designs is maintained. • drive the recruitment and hiring of top-tier data scientists for the ace team, assessing both technical skills and cultural fit. • encourage and enable the team to experiment with new technologies and methodologies, making pragmatic trade-offs to meet project goals while fostering a culture of continuous learning. required qualifications • bachelor's diploma/degree specialization in computer science, data science, mathematics, or a related field. • minimum of 3+ years of relevant industry experience in a data science or machine learning role. • experience in a team leadership or senior-level role, including mentoring junior data scientists. • hands-on experience with the entire lifecycle of a generative ai project, from development to production deployment. • expertise in modern generative ai and llms, including transformer architectures, prompt engineering, and rag systems. • expert proficiency in python, langgraph, langchain and sql • comprehensive understanding of machine learning principles, statistical analysis best practices. • experience designing scalable ai architectures on cloud platforms (gcp, azure). • prior experience in telematics, iot, or large-scale data systems is a strong plus.",canada,Data Scientist,"['azure', 'cloud', 'data analytics', 'deep learning', 'gcp', 'machine learning', 'python', 'r', 'scala', 'sql']","['azure', 'cloud', 'data analytics', 'deep learning', 'gcp', 'machine learning', 'python', 'r', 'scala', 'sql']",$160K–$185K a year
staff data scientist/ ai products,dropbox,"role description how many times do you get the opportunity to be on the ground floor of a big and important mission? what if you could be one of the top contributors defining the mission, guiding our teams, and influencing the direction of dropbox’s ai-first journey? as a staff data scientist for this new division, you will get to do exactly that. you will join a team of top-tier data scientists and become an integral part of the product organization, helping to scale this new business. joining on the ground floor of this startup team, you’ll partner directly with the head of data science and product, engineering and design leadership to shape the product roadmap, foster a top-tier, data-informed culture, and drive real ai/ml impact and execution along the way! responsibilities • partner with product engineers and data engineers to build the reliable, efficient, and scalable data foundations, tools, and processes to drive our ai/ml capabilities’ long-term growth • leverage data-driven insights to proactively identify most impactful opportunities, and directly influence product roadmaps and strategies • perform exploratory and deep-dive analysis to understand user workflows and engagement patterns on ai features, propose hypothesis, and design & execute experiments with great rigor and efficient data techniques • translate complex data insights into implications and recommendations for the business via excellent communication skills, both verbal and written • identify what matters most and prioritize ruthlessly for the area you will own • contribute to a culture of strong technical ownership, coach junior data scientists in the team, and partner with head of data science to keep evolving ds working model and elevate ds impact • work with cross-functional teams (including product, engineering, design, user research, and senior executives) to rapidly execute and iterate requirements • bachelors’ or above in quantitative discipline: statistics, applied mathematics, economics, computer science, engineering, or related field • 10+ years experience of leveraging data-driven analysis to influence product roadmap and business decision, preferably in a tech company • proven track record of being able to work independently, driver measurable business impact, and proactively engage with business stakeholders with minimal direction • proficiency in sql, python or other programming/scripting languages • deep understanding of statistical analysis, experimentation design, and common analytical techniques like regression, decision trees • ability to provide data insights and recommendations for 0→1 product even when sample size is small • strong verbal and written communication skills preferred qualifications • experience in startups or building 0→1 products • expertise in using data to inform ai/ml product development • background in saas product and growth analytics compensation canada pay range $181,900—$246,100 cad original job staff data scientist/ ai products posted on grabjobs ©. to flag any issues with this job please use the report job button on grabjobs.",canada,Data Scientist,"['excel', 'experimentation', 'python', 'r', 'recommendation', 'regression', 'scala', 'sql', 'statistics']","['excel', 'experimentation', 'python', 'r', 'recommendation', 'regression', 'scala', 'sql', 'statistics']",
senior data scientist (remote),the athletic media company,"about us powered by one of the largest global newsrooms in sports media, the athletic brings sports fans the most comprehensive stories about the teams, sports and athletes they love. the athletic’s newsroom of 450+ full-time staff delivers in-depth coverage of hundreds of professional and college teams across more than 47 north american markets and all 20 football clubs in the english premier league, as well as many championship clubs. about the role we are looking for a senior data scientist who will bring a deep understanding of our sportswriting, our in-game experience, and the way our subscribers use the athletic to help us make our best-in-class product even better. in this role, you will work closely with our data, product, growth, and engineering teams to develop advanced machine learning models. these models will help us understand our business, optimize and automate our workflows, and have our product make smarter recommendations. as a member of our small data science team, your work will have a significant impact at the athletic. responsibilities • design, build, and implement mathematical models to drive key long-term product goals, like training a model that predicts which of our free users are most likely to subscribe, or recommending the right articles to the right users at the right time. • lead projects with hands-on analysis and modeling, drawing from multiple analytical methods to choose the right tool and right level of complexity appropriate for the job. • use quantitative analysis to deeply understand what our users are saying, and to develop recommendations based on that analysis. • design and develop data science pipelines in production. • guide complex experiments. • cultivate an outstanding data science environment. requirements • 4-6 years of experience as a data scientist with a proven ability to apply scientific methods to solve real-world problems on web scale data. • strong working knowledge of a scripting language like r or python and how to use it to train machine learning models and put them into production. • a broad understanding of sql with an ability to efficiently pull and write to our data warehouses. • an understanding of subscription media and recommendation engines, with the ability to turn it into insights and optimizations. • knowledge of statistical principles and how they apply to predictive modeling. • ability to drive projects with minimal guidance and prioritize high-impact work. • strong verbal and written communication skills with ability to build cross-functional relationships. • can explain sophisticated concepts to diverse audiences and craft compelling stories with data. the athletic offers unique perks and benefits to all full-time employees based in canada. for international candidates: our global benefits packages offer similar benefits and perks, competitive to the local market. the athletic media company is an equal opportunity employer and enthusiastically encourages people from all backgrounds and experiences to apply. the athletic will consider all applicants without regard to race, religion, color, national origin, ancestry, physical and/or mental disability, medical condition, genetic information, marital status, sex, gender, gender identity, gender expression, transgender status, age, sexual orientation, military or veteran status, or any other protected characteristic under applicable law. click here to review our applicant privacy notice, which describes how and when the athletic media company collects, uses, and shares certain personal information of job applicants and prospective employees. beware of fraudulent job recruiting schemes! our recruiters use careers@theathletic.com exclusively, and our team members will use an email address with @theathletic.com domain. we do not conduct interviews via text or instant message and we do not ask candidates to download software, to purchase equipment through us, or to provide sensitive personally identifiable information, such as bank accounts or social security numbers. if you have been contacted by someone claiming to be a member of the recruiting/hr team at the athletic but operating from a different email address about a job offer, please report it as potential job fraud to the law enforcement and to people@theathletic.com.",canada,Data Scientist,"['data warehouse', 'machine learning', 'python', 'r', 'recommendation', 'sql']","['data warehouse', 'machine learning', 'python', 'r', 'recommendation', 'sql']",
lead data scientist: edge ml & iot production,very,"a leading technology firm is seeking a lead data scientist to work with software, hardware, and product design teams. this role focuses on building end-to-end solutions involving machine learning and data science. the ideal candidate has strong technical expertise, experience leading teams, and excellent communication skills. they will play a key role in managing complex projects while ensuring high-quality delivery. this position offers competitive compensation and remote work flexibility.",canada,Data Scientist,"['excel', 'machine learning', 'r']","['excel', 'machine learning', 'r']",$164K–$174K a year
lead data scientist,fusemachines,"about fusemachines fusemachines is a leading ai strategy, talent, and education services provider. founded by sameer maskey ph.d., adjunct associate professor at columbia university, fusemachines has a core mission of democratizing ai. with a presence in 4 countries (nepal, the united states, canada, and the dominican republic and more than 450 full-time employees) fusemachines seeks to bring its global expertise in ai to transform companies around the world. type: this is remote, full-time position about the role a lead data scientist is responsible for designing and implementing data-driven solutions to complex business problems. the role requires extensive experience in data analysis, agentic ai, statistical modeling, machine learning, and data visualization, as well as the ability to lead a team of data scientists and collaborate with cross-functional teams. responsibilites: • lead a team of data scientists to develop innovative solutions to complex business problems. • collaborate with cross-functional teams, including business stakeholders, product managers, software engineers, and data engineers to develop and implement data-driven solutions. • assess the business needs of clients and identify areas where ai can be used to improve processes, reduce costs, or increase revenue. • design and implement statistical models, machine learning algorithms, predictive analytics models, and agentic systems to solve business problems. • communicate technical insights and recommendations to non-technical stakeholders in a clear and concise manner. • stay up-to-date with the latest developments in data science, machine learning, and artificial intelligence, and apply new technologies and techniques to solve business problems. • mentor and develop the skills of junior data scientists and provide feedback and guidance to help them improve their work. • responsible for developing, implementing, and managing the end-to-end machine learning pipelines. this will involve building, deploying, and maintaining machine learning models, as well as ensuring data quality and system stability. requirements for the role • applicants should have a masters, phd, or advanced training in applied mathematics, engineering, computer science, or a similar related field. • 8+ years experience in data scientist or machine learning. • strong programming skills in languages such as python, r, c++, and sql. • hands-on experience with ml frameworks, such as pytorch, or tensorflow • experience with cloud compute environments such as aws, gcp, azure • experience leading data science teams. • ability to work independently and manage multiple projects simultaneously. • strong problem-solving skills and attention to detail. • excellent communication skills both written and verbal. • experience in industries such as finance, healthcare, e-commerce, retail, or marketing is a plus. equal opportunity employer: race, color, religion, sex, sexual orientation, gender identity, national origin, age, genetic information, disability, protected veteran status, or any other legally protected group status. important: immigration sponsorship policy fusemachines is unable to proceed with candidates who require any form of work authorization or immigration support from the company. pfjncnabul",canada,Data Scientist,"['aws', 'azure', 'c++', 'cloud', 'data analysis', 'excel', 'gcp', 'machine learning', 'python', 'pytorch', 'r', 'recommendation', 'sql', 'tensorflow']","['aws', 'azure', 'c++', 'cloud', 'data analysis', 'excel', 'gcp', 'machine learning', 'python', 'pytorch', 'r', 'recommendation', 'sql', 'tensorflow']",
associate data scientist - genai,company 1 - the manufacturers life insurance company,"we are seeking a highly skilled and motivated associate data scientist to join our team. you will be responsible for developing and implementing ai and machine learning models, using generative ai techniques, such as applying prompt engineering, working with rag applications, fine-tuning llm models, and deploying applications on cloud platforms like azure. your expertise in these areas will play a crucial role in driving data-driven decision-making and enhancing our business processes. position responsibilities: develop and implement ai models to solve complex business problems, using a variety of algorithms and techniques. clean, preprocess, and analyze large datasets to extract meaningful insights and patterns. apply prompt engineering techniques, build with rag (retrieval-augmented generation) applications, and fine-tune language models and improve their performance in specific tasks. collaborate with multi-functional teams to identify and define business requirements, ensuring alignment with data science objectives. apply generative ai techniques to generate synthetic data, create realistic simulations, and enhance data analysis capabilities. design and complete experiments to validate and optimize machine learning models, ensuring accuracy, efficiency, and scalability. deploy machine learning models and applications on cloud platforms like azure ml or databricks, ensuring seamless integration and scalability. stay up-to-date with the latest advancements in machine learning, generative ai, prompt engineering, rag applications, and cloud technologies, and apply them to enhance our data science capabilities. collaborate with data engineers and ml engineers to integrate data science solutions into existing systems and workflows. communicate complex technical concepts and findings to both technical and non-technical partners, ensuring clear understanding and agreement. required qualifications: bachelor', master's degree, or ph.d. in computer science, data science, statistics, or a related field. proven experience as a data scientist including internships or coops, with a strong focus on machine learning, generative ai, prompt engineering, and rag applications. solid understanding of machine learning algorithms, statistical modeling, and data analysis techniques. proficiency in programming languages such as python and experience with machine learning libraries/frameworks (e.g., pytorch, scikit-learn, hugging face). preferred qualifications: strong problem-solving skills and the ability to think critically and creatively to develop innovative solutions. excellent communication and collaboration skills, with the ability to work effectively in multi-functional teams. when you join our team: we’ll empower you to learn and grow the career you want. we’ll recognize and support you in a flexible environment where well-being and inclusion are more than just words. as part of our global team, we’ll support you in shaping the future you want to see. #li-hybrid about manulife and john hancock manulife financial corporation is a leading international financial services provider, helping people make their decisions easier and lives better. to learn more about us, visit https://www.manulife.com/en/about/our-story.html. manulife is an equal opportunity employer at manulife/john hancock, we embrace our diversity. we strive to attract, develop and retain a workforce that is as diverse as the customers we serve and to foster an inclusive work environment that embraces the strength of cultures and individuals. we are committed to fair recruitment, retention, advancement and compensation, and we administer all of our practices and programs without discrimination on the basis of race, ancestry, place of origin, colour, ethnic origin, citizenship, religion or religious beliefs, creed, sex (including pregnancy and pregnancy-related conditions), sexual orientation, genetic characteristics, veteran status, gender identity, gender expression, age, marital status, family status, disability, or any other ground protected by applicable law. it is our priority to remove barriers to provide equal access to employment. a human resources representative will work with applicants who request a reasonable accommodation during the application process. all information shared during the accommodation request process will be stored and used in a manner that is consistent with applicable laws and manulife/john hancock policies. to request a reasonable accommodation in the application process, contact recruitment@manulife.com. referenced salary location toronto, ontario working arrangement hybrid salary range is expected to be between $63,100.00 cad - $113,100.00 cad. if you are applying for this role outside of the primary location, please contact recruitment@manulife.com for the salary range for your location. the actual salary will vary depending on local market conditions, geography and relevant job-related factors such as knowledge, skills, qualifications, experience, and education/training. employees also have the opportunity to participate in incentive programs and earn incentive compensation tied to business and individual performance. manulife offers eligible employees a wide array of customizable benefits, including health, dental, mental health, vision, short- and long-term disability, life and ad&d insurance coverage, adoption/surrogacy and wellness benefits, and employee/family assistance plans. we also offer eligible employees various retirement savings plans (including pension and a global share ownership plan with employer matching contributions) and financial education and counseling resources. our generous paid time off program in canada includes holidays, vacation, personal, and sick days, and we offer the full range of statutory leaves of absence. if you are applying for this role in the u.s., please contact recruitment@manulife.com for more information about u.s.-specific paid time off provisions. we're manulife. and we’re on a mission to make decisions easier and lives better. better is what drives us. it’s what inspires us to find new ways to support customers and colleagues in living longer and healthier lives. it’s the reason we’re dedicated to investing in digital innovation and accelerating a sustainable and economically inclusive future. joining us means you’ll be empowered to learn and grow your career. we’ll recognize and support you in a flexible environment where well-being and inclusion are more than just words. and as part of our global team, you’ll help shape the future you want to see – and discover that better can take you anywhere you want to go. we’re proud of our accomplishments and recognitions. recent awards include: 2024 gallup exceptional workplace award winner manulife named one of forbes world’s best employers 2023 best companies to work for in asia 2023 we’ve been recognized as one of canada’s top 100 employers (2024) manulife included in bloomberg’s 2023 gender-equality index to receive our latest job opportunities directly to your inbox, create an account or sign in and navigate to the ‘job alerts’ section located in the top right corner of the page. from there, you can sign up to receive job alerts. our ambition is to be the most digital, customer-centric global company in our industry. learn more at https://www.manulife.com/.",canada,Data Scientist,"['aws', 'azure', 'cloud', 'data analysis', 'databricks', 'excel', 'machine learning', 'python', 'pytorch', 'r', 'scala', 'scikit-learn', 'statistics']","['aws', 'azure', 'cloud', 'data analysis', 'databricks', 'excel', 'machine learning', 'python', 'pytorch', 'r', 'scala', 'scikit-learn', 'statistics']",
"data scientist | derby, uk",flint bishop,"data scientist data scientist department: debt recovery employment type: permanent - full time location: derby, uk description flint bishop is one of the leading business-to-business debt recovery law firms in the uk. we are the recovery partner for many leading uk brands and the firm has won all three major national debt recovery awards for our strategy and results. data plays an increasingly important part of our business and as a result of continued growth and further investment within our d&a department, we now have an excellent opportunity for a data scientist to join our team based at derby. the firm uses an sql based case management system to manage the recovery of bulk debts and as the basis of internal and external reporting. this role will cover the full spectrum of internal and external data management, performance analytics and collaborating with both internal and external stakeholders to optimise operational strategies and deliver continuous improvement initiatives to maximise client return of investment. we are looking for a hands-on individual who is a natural problem solver with advanced analytical skills, experience of a/b testing & creating/maintaining ml models and delivering technical problems to non-technical audiences. reporting to the head of data & analytics the desired candidate will be keen to join a fast-paced and friendly team to help shape the departments future within flint bishop key responsibilities • champion data accuracy and quality, utilising external 3rd party relationships to enhance our portfolios • provide insight on client portfolios and identify trends across the business • create and maintain bespoke ml models for a variety of business and client needs • lead and manage champion/challenger strategies, to elevate performance on existing contact strategies • liaise with operational leaders, to help maximise contact centre resource to deliver for our clients, leading various projects end-to-end • support and mentor junior team members within d&a skills, knowledge and expertise you will: • 5+ years' experience in a previous analytical role • experience in identifying performance trends and creating ml models to support operational contact strategies • be proficient in python, sql and microsoft excel • have great interpersonal skills and be able to communicate your ideas to a wider audience • be a good problem solver • have strong time management skills, with the ability to juggle multiple projects at once desirable: • a bachelor's degree in a numeric subject (mathematics, statistics, etc.) or equivalent • experience working within credit risk/debt recovery industry • working knowledge of python and/or r • experience with legal systems such as liberate disclosure will be required in the event that a position is offered. financial checks will be required in the event that a position is offered. benefits what we offer? • competitive salary • bonus potential • life insurance • great working environment at our derby head offices • car parking provided by the firm • career development opportunities • 25 days holiday plus bank holidays • options to buy & sell 1 week's holiday • 1 week's holiday carry over • extra day off for your birthday • staff card discount scheme for local shops • social days and evenings on the firm • staff discounts on legal services",derby,Data Scientist,"['a/b testing', 'excel', 'python', 'r', 'sql', 'statistics']","['a/b testing', 'excel', 'python', 'r', 'sql', 'statistics']",
data scientist...,machine learning jobs,"job description join the dataannotation team and contribute to developing cutting-edge ai systems, while enjoying the flexibility of remote work and setting your own schedule. we are looking for an expert mathematician (part-time work from home) to help advance ai development. as a member of dataannotation’s math team, you’ll be part of a growing community of over 100,000 experts who are driving real-world impact in ai development. our platform offers an engaging blend of flexibility and challenge: you’ll work closely with state-of the art ai models to take on programming tasks that include solving challenging math problems and synthesizing insights through data analysis and visualization. your work directly contributes to refining intelligent systems that learn, adapt, and evolve. some team members fit this work alongside a full-time role, while others treat it as their primary focus, choosing projects and schedules that align with their availability and goals. to get started, once you sign up for an account, you'll take a short assessment (this serves as our version of an interview). if you pass that assessment, you’ll receive an email confirmation, and paid work will become available to you through our platform. benefits: • this is a full-time or part-time remote position • you’ll be able to choose which projects you want to work on • you can work on your own schedule • projects are paid hourly starting at $40+ usd per hour, with bonuses on high-quality and high-volume work responsibilities: • give ai chatbots diverse and complex mathematics problems and evaluate their outputs • evaluate the quality produced by ai models for correctness and performance qualifications: • fluency in english (native or bilingual level) • detail-oriented • proficient in arithmetic, algebra, geometry, calculus, probability, statistics, and inductive/deductive reasoning • a current, in progress, or completed master's and/or phd is preferred but not required note: payment is made via paypal. we will never ask for any money from you. paypal will handle any currency conversions from usd. this job is only available to those in the us, canada, uk, ireland, australia, and new zealand. those located outside of these countries will not see work or assessments available on our site at this time. #math",worcester,Data Scientist,"['data analysis', 'r', 'statistics']","['data analysis', 'r', 'statistics']",
data scientist,cgg,"experteer overview as data scientist at viridien, you will transform raw data into structured, scalable datasets within our data hub, designing and maintaining data-processing pipelines. you will collaborate with domain experts and labeling teams, supporting annotation workflows and ml initiatives. you’ll work at the intersection of data engineering, domain knowledge, and machine learning to advance our data ecosystem and drive value for sustainability-focused challenges. pay / benefits • develop data-transformation modules to gather, clean, validate, and structure raw datasets • collaborate with smes and labeling team to understand domain requirements • support annotation workflows with tools and technical feedback • build scalable, reusable data-processing solutions and manage version control with gitlab • troubleshoot data issues and flag inconsistencies or risks • maintain knowledge of the data hub tech stack and data schemas; stay current with tech trends • contribute to data integration, feature design, and machine learning initiatives • design and run experiments; document learnings and share results • use pre-trained ml models and train/fine-tune on internal datasets with evaluation and validation • communicate with team members and cross-functional partners to ensure alignment and transparency tasks • background in data science or related field; master’s degree preferred • proficient in at least one programming language, ideally python, with experience in ml libraries • experience with hybrid ml workflows (traditional ml, llms, embeddings, ontologies, knowledge graphs) • comfortable with relational, nosql, and graph databases • strong data-processing skills including cleaning, filtering, and feature extraction • clear communicator with strong collaboration and presentation skills key requirements • competitive salary • bonus scheme • hybrid model and flexible working with up to 2 days at home • initial 22 days annual leave with future increases • company pension with generous employer contribution • wellbeing unmind app – mental health support",nan,Data Scientist,"['machine learning', 'python', 'r', 'scala', 'sql']","['machine learning', 'python', 'r', 'scala', 'sql']","£55,000–£60,499 a year"
lead data scientist,ge vernova,"we are seeking a lead data scientist with solid experience typically gained over a minimum of 5 years in large multinational companies within the energy sector or related industrial domains such as smart infrastructure or industrial automation. the ideal candidate has hands-on experience in ai/ml model testing, verification, and validation in complex, data-rich environments. in this position, you will be responsible for testing, verifying, and validating advanced ai/ml models for grid innovation, and for contributing to the development of robust validation frameworks across edge and cloud environments. you will collaborate closely with r&d, product teams, and other business units to support the development of effective, reliable, and high-impact ai/ml solutions. job description essential responsibilities: • design and conduct experiments to test and validate ai/ml models in the context of energy systems and grid automation applications. • establish clear validation frameworks to ensure models meet required performance standards and business objectives. • establish test procedures to validate models with real and simulated grid data. • analyze model performance against real-world data to ensure accuracy, reliability, and scalability. • identify and address discrepancies between expected and actual model behavior, providing actionable insights to improve model performance. • implement automated testing strategies and pipeline to streamline model validation processes. • collaborate with data engineers and ml engineers to improve data quality, enhance model performance, and ensure efficient deployment of validated models. • ensure that validation processes adhere to data governance policies and industry standards. • communicate validation results, insights, and recommendations clearly to stakeholders, including product managers and leadership teams. must-have requirements: • experience typically gained over +5 years in large multinational companies within the energy sector or related industrial domains such as smart infrastructure or industrial automation. • master’s, or bachelor’s degree in data science, computer science, electrical engineering, or a related field, with hands-on experience in model validation. • solid experience in validating ai/ml models, ensuring they meet business and technical requirements. • strong knowledge of statistical techniques, model performance metrics, and ai/ml validation methodologies. • proficiency in programming languages such as python, r, or matlab. • experience with data wrangling, feature engineering, and dataset preparation for model validation. • familiarity with machine learning frameworks (e.g., tensorflow, pytorch, scikit-learn) and model evaluation techniques. • experience with cloud platforms (e.g., aws, azure, gcp) and deploying models in cloud environments. • experience with data visualization tools (e.g., tableau, power bi) to effectively present validation results and insights. nice-to-have requirements: • familiarity with big data tools and technologies such as hadoop, kafka, and spark. • knowledge of data governance frameworks and validation standards in the energy sector. • understanding of distributed computing environments and large-scale model deployment. • strong communication skills, with the ability to clearly explain complex validation results to non-technical stakeholders. at ge vernova - grid automation, you will have the opportunity to work on cutting-edge projects that shape the future of energy. we offer a collaborative environment where your expertise will be valued, and your contributions will make a tangible impact. join us and be part of a team that is driving innovation and excellence in control systems. about gev grid solutions: at gev grid solutions we are electrifying the world with advanced grid technologies. as leaders in the energy space our goal is to accelerate the transition for a more energy efficient grid to full fill the needs of tomorrow. with a focus on growth and sustainability ge grid solutions plays a pivotable role in integrating renewables onto the grid to drive to carbon neutral. in grid solutions we help enable the transition for a greener more reliable grid. ge grid solutions has the most advanced and comprehensive product and solutions portfolio within the energy sector. why we come to work: at gev, our engineers are always up for the challenge - and we’re always driven to find the best solution. our projects are unique and interesting, and you’ll need to bring a solution-focused, positive approach to each one to do your best. surrounded by committed, loyal colleagues, if you can dare to bring your ingenuity and desire to make an impact, you’ll be exposed to game-changing, diverse projects that truly allow you to play your part in the energy transition. what we offer: a key role in a dynamic, international working environment with a large degree of flexibility of work agreements competitive benefits, and great development opportunities - including private health insurance. additional information relocation assistance provided: no",west midlands,Data Scientist,"['aws', 'azure', 'cloud', 'excel', 'feature engineering', 'gcp', 'hadoop', 'kafka', 'machine learning', 'matlab', 'power bi', 'python', 'pytorch', 'r', 'recommendation', 'scala', 'scikit-learn', 'spark', 'tableau', 'tensorflow']","['aws', 'azure', 'cloud', 'excel', 'feature engineering', 'gcp', 'hadoop', 'kafka', 'machine learning', 'matlab', 'power bi', 'python', 'pytorch', 'r', 'recommendation', 'scala', 'scikit-learn', 'spark', 'tableau', 'tensorflow']",
data scientist,coforge u.k. ltd,"• proficiency in python and ml frameworks (tensorflow, pytorch). • strong statistical and analytical skills. • experience with a wide range of data science techniques (eg ml, optimisation, simulation, genai, etc.). • demonstrated ability to take models from design through to production deployment, including performance optimization, monitoring, and integration into business workflows beyond proof-of-concept or prototype stages. • familiarity with airline operations or supply chain analytics is desirable. • significant experience in similar roles, with a proven ability to integrate quickly into new teams and deliver immediate value. • ability to frame complex problems and deliver actionable solutions. • strong presentation and storytelling skills for executive audiences. • experience in high-impact consulting or transformation projects. • track record of creating high-impact outcomes and driving stakeholder satisfaction from day one. • experience in building reusable ai components and frameworks for enterprise-scale deployments.",largs,Data Scientist,"['python', 'pytorch', 'r', 'tensorflow']","['python', 'pytorch', 'r', 'tensorflow']",
consumer lending data scientist,apply4u,"data scientist - ml & consumer lending south west, uk hybrid working, strong salary dependent on experience the client south west based, modern office hub, and a major consumer lending portfolio. this is a chance to join a well known financial services group that is investing heavily in data, applied data science, and machine learning to stay ahead of the market and improve how it serves its customers. the business is moving towards cloud native, production grade ml, backed by senior leaders who see data science as central to the next chapter of growth. you will sit close to real decision making, working with product, risk, and engineering teams to turn data and ml into tangible customer outcomes. the role you will join a growing data science team focused on credit cards and wider consumer lending. your work will span the full ml lifecycle, from exploratory analysis and model build, through to working with ml engineers to get models into production and keep them performing as the data strategy matures. the team is open to data scientists at different stages, from those looking to build on a first industry role, through to more experienced practitioners who want broader ownership and influence. what you will be doing building and enhancing python based ml models across the credit card portfolio using sql on large, complex datasets to engineer robust, reusable features partnering with ml engineers to deploy containerised models and support production ml workflows working with product and risk stakeholders to identify and shape high value ml and ai use cases operating within a clear model risk and governance framework, with room to experiment and learn presenting findings in plain language and helping non specialists act on the insights what we are looking for strong grounding in statistics and or machine learning, plus solid python confident sql skills and experience with real world, messy datasets experience of the ml lifecycle, ideally including production or near production environments exposure to modern cloud and analytics tooling, for example bigquery or vertex, and bi tools interest in credit cards or consumer lending, and how ml and ai can improve fairness and outcomes you will join a friendly team that values curiosity, knowledge sharing, and personal growth. there is genuine scope to shape how ml and ai are used in the business, whether you see your future in deeper technical expertise or in a more leadership focused path. if this sounds like the kind of environment you want to be part of, we would love to hear from you",greater manchester,Data Scientist,"['bi tools', 'bigquery', 'cloud', 'machine learning', 'python', 'r', 'sql', 'statistics']","['bi tools', 'bigquery', 'cloud', 'machine learning', 'python', 'r', 'sql', 'statistics']",
data scientist,kellanova,"at kellanova, data isn’t just numbers—it’s the fuel that powers smarter decisions and bold growth. we’re looking for a data scientist to join our dynamic team in manchester and help shape the future of how we leverage insights across e-commerce, sales, marketing, and revenue growth management (rgm). this is a 12-month fixed-term contract role that offers hybrid role, that offers flexibility and collaboration, giving you the chance to work on cutting-edge analytics projects that make a real impact. if you love turning complex data into actionable strategies, thrive in a fast-paced environment, and want to influence decisions that touch millions of consumers, this is your opportunity to shine. you’ll partner with cross-functional teams, apply advanced analytics, and bring fresh thinking to every challenge. a taste of what you’ll be doing e-commerce analytics: develop predictive models to optimise online sales performance, pricing strategies, and digital shelf visibility—helping our brands stand out in the digital marketplace. sales forecasting: build robust demand forecasting models to support trade planning, inventory optimisation, and flawless sales execution. marketing effectiveness: use advanced statistical and machine learning techniques to measure campaign roi, refine customer segmentation, and personalise experiences that drive engagement. revenue growth management (rgm): apply scenario modelling and elasticity analysis to optimise pricing, promotions, and assortment strategies for maximum impact. data integration: consolidate and harmonise data from multiple sources—pos, crm, digital platforms, syndicated data—into unified insights that empower smarter decisions. we’re looking for someone with master’s degree in a stem or related field (data science, mathematics, computer science, engineering). proficiency in python, r, sql, and data visualisation tools such as power bi. experience with machine learning frameworks (scikit-learn, tensorflow, pytorch). ability to present technical findings to non-technical stakeholders in a compelling and actionable way. what’s next after you apply, your application will be reviewed by a real recruiter, so it may take us a few weeks to get back to you by email or phone. visit our how we hire page to get insights into our hiring process and more about what we offer. need assistance throughout the application or hiring process? email european.recruitment@kellanova.com if you join our team, you’ll be rewarded for the difference you make. our comprehensive benefits offer you the support you need through your life events, big or small. visit our benefits page & be sure to ask your recruiter for more specific information. get to know us at kellanova, we are driven by our vision to be the world’s best-performing snacks-led powerhouse, unleashing the full potential of our differentiated brands and our passionate people. our portfolio of iconic, world-class brands include pringles, cheez-it, pop-tarts, morningstar farms, special k, krave, zucaritas, tresor, crunchy nut, among others. kellanova’s culture of best means we bring our best to all that we do in pursuit of our vision to be the world’s best performing snacks-led powerhouse. our culture celebrates boldness and empowers our people to challenge the status quo, achieve results, and win together. our focus on equity, diversity, and inclusion (ed&i) enables us to build a culture of belonging where all employees have a place at the table and are inspired to share their passion, talents and ideas to work. mars has agreed to acquire kellanova in a combination that will shape the future of snacking! the transaction is anticipated to close towards the end of 2025 (subject to customary closing conditions, including regulatory approvals). the companies remain separate until closing. you can learn more at www.futureofsnacking.com and our hiring teams will be happy to discuss further questions if your application advances in the hiring process. what does it take to be the best? someone like you. kellanova is an equal opportunity employer that strives to provide an inclusive work environment, a seat for everyone at the table, and embraces the diverse talent of its people. all qualified applicants will receive consideration for employment without regard to race, color, ethnicity, disability, religion, national origin, gender, gender identity, gender expression, marital status, sexual orientation, age, protected veteran status, or any other characteristic protected by law. for more information regarding our efforts to advance equity, diversity & inclusion, please visit our website here .",stretford,Data Scientist,"['machine learning', 'power bi', 'python', 'pytorch', 'r', 'scikit-learn', 'sql', 'tensorflow']","['machine learning', 'power bi', 'python', 'pytorch', 'r', 'scikit-learn', 'sql', 'tensorflow']",
senior data scientist - pricing strategy,hastings direct,"welcome to hastings direct are you ready to take your expertise in data science to the next level? at hastings direct, we're on a mission to transform the insurance landscape in the uk with cutting-edge technology and a data-driven approach. join us as we build a market-leading pricing technology platform while investing in the growth and development of our people. the fact you're now reading this job advert means we've piqued your interest. if you’re looking for a role that offers the opportunity to innovate within the insurance industry, we want to hear from you! role overview we're seeking a talented and experienced senior data scientist who is eager to drive innovation in our pricing activities. working within a dynamic product team of data scientists, modellers, and analysts, you will be at the forefront of developing advanced data assets and predictive models. we are looking for individuals who are excited to leverage a new tech stack that enhances our model deployment capabilities. if you're ready to invest in your career and contribute to an innovative environment, this could be the perfect opportunity for you. key responsibilities • create and maintain analytical tools for effective risk portfolio management. • develop top-tier models to predict claims outcomes, fraud, and other key risk indicators. • engineer new rating factors to enhance our pricing algorithms. • identify, analyze, and capitalize on new data sources. essential skills/experience • proven track record of delivering machine learning projects from exploratory data analysis through to deployment, including post-deployment evaluation and model updating. • experience collaborating with diverse stakeholders to define problems and develop solutions that drive commercial success. • strong interest in emerging machine learning techniques and their application in business. • proficiency in python, sql, azure ml, git, and azure cloud services. • excellent communication skills. • ability to work collaboratively with data engineers, data scientists, actuaries, and pricing analysts. personal attributes • a natural problem solver who enjoys crafting high-quality solutions for complex real-world challenges. • strategic thinker who also dives into details when necessary. • a flexible and results-oriented mindset that thrives in a fast-paced environment. • proactive ownership and accountability for key projects and workstreams. the interview process our interview process is structured as follows: • initial recruiter screening call. • first stage interview with the hiring leader. • second interview (technical) with a panel. as a disability confident employer, we prioritize inclusivity in our recruitment processes. we encourage candidates requiring adjustments during recruitment to communicate their needs. benefits alongside a competitive salary and £5,000 car allowance, you will enjoy: • flexible working: we champion a flexible hybrid working approach - discuss with your recruiter. • competitive bonus scheme: all colleagues are eligible for our annual performance bonus. • physical wellbeing: private medical insurance for convenience in seeing specialists. • financial wellbeing: life assurance, income protection, and matched pension contributions, plus various financial support options. • mental wellbeing programme: access to mental health resources, including an app and 24/7 support. • additional perks: 27 days annual leave (+ bank holidays), a chance to buy/sell leave, and many more benefits aimed at enhancing your work-life balance. join hastings direct and help us innovate in the insurance industry while benefiting from our commitment to your career growth through training and development. our 4cs principles guide us, focusing on fostering a positive culture for colleagues to drive sustainable growth, benefiting customers and communities alike. we are an equal opportunities employer, ensuring fair treatment for all applicants regardless of background.",leeds,Data Scientist,"['azure', 'cloud', 'data analysis', 'excel', 'machine learning', 'python', 'r', 'sql']","['azure', 'cloud', 'data analysis', 'excel', 'machine learning', 'python', 'r', 'sql']",
senior data scientist,embs engineering,"you ll be joining a lean, high-performing data science team of three in a fintech that s making serious moves in financial services. this role is about end-to-end ownership. from spotting opportunities to deploying models that stick, you ll need to roll up your sleeves, partner with business leaders, and deliver solutions that make a measurable difference. we require hands-on experience building and maintaining ml/ai predictive models. you'll need to evidence previous advanced predictive modelling or end-to-end ownership of said models. we re specifically looking for someone experienced with the full lifecycle of data science projects and advanced modelling (ml/ai) - not just analysis, dashboards, or oversight. this is about leading your own projects, driving outcomes, and being accountable for real commercial impact. location: nottingham (4 days per week in office) salary: c£65,000 - £75,000 (doe) why this role matters your work will shape how the business operates. to give you an example, one of your future teammates has already transformed the collections function by building models that determine who to call, when to call, and when to send comms - driving a step change in efficiency and results. now it s your turn. you ll work with senior stakeholders, dig into business pain points, pitch smart solutions, and deliver predictive models that directly influence decisions across the company. what we re looking for • proven impact - you ve taken models into production and seen them deliver real results. • autonomous leadership - confident in owning projects, engaging stakeholders, and holding yourself accountable. • technical credibility - strong hands-on data science capability (r, python, or similar). what matters is outcomes, not syntax. • commercial mindset - able to translate technical solutions into business impact, spotting opportunities others might miss. • energy & curiosity - proactive, problem-seeking, and solutions-focused. the tech (flexible) • current stack: r, databricks, sql • open to python and other modern tools - what matters is results. what you ll get • high visibility and autonomy - your work won t be buried in layers of hierarchy • a direct line to senior leadership and real influence over business decisions • the chance to work with sharp, passionate people solving real-world problems with data this role is four days a week in the nottingham office. no hiding behind zoom - you ll be embedded in the business, collaborating face-to-face, and influencing directly. if that s a fit for you, this could be a career-defining move! how to apply if you re a data scientist who wants to own projects, deliver real outcomes, and be recognised for your impact, we d love to hear from you.",nottinghamshire,Data Scientist,"['dashboard', 'databricks', 'python', 'r', 'sql']","['dashboard', 'databricks', 'python', 'r', 'sql']",
part-time data scientist,horizon workforce partners,"employment type: remote (part-time/contract) compensation: estimated range: usd 5,000-8,000 per month, based on experience, scope of responsibilities, and performance expectations. about the role as a part-time data scientist, you will play a vital role in supporting day-to-day operations, content management, research, and data activities within a dynamic remote team. this position offers the opportunity to enhance your analytical and operational skills while significantly contributing to multiple ongoing projects. key responsibilities • engage in project tasks including content preparation, data entry and maintenance, online research, basic analysis, operations support, ai-output assessment, and documentation. • ensure high levels of accuracy and attention to detail while reviewing, organizing, and updating information. • communicate effectively with team members through clear written and verbal updates, providing timely information on task statuses and progress. skills & qualifications • excellent command of written english along with strong, professional communication abilities. • proficient in using digital tools such as email, spreadsheets, and project management platforms. • analytical mindset paired with a strong attention to detail and precision. • ability to prioritize tasks, manage time effectively, and work independently in a remote setting. • interest in operations, research, content management, and data-related work is beneficial; comprehensive training and onboarding will be provided. what we offer • a fully remote work environment with flexible scheduling to accommodate team needs. • weekly payments made through secure, compliant methods, offering a transparent compensation structure. • opportunities to develop skills in research, content operations, data management, and ai workflows. • a supportive workplace culture that promotes feedback, learning, and professional growth. (you must be legally authorized to work in your location) we invite applicants from diverse backgrounds to apply, and we make hiring decisions based solely on qualifications, experience, and business requirements, in accordance with employment and anti-discrimination laws. apply now to seize this opportunity!",leeds,Data Scientist,"['aws', 'excel', 'r']","['aws', 'excel', 'r']",
gis data scientist,morgan hunt,"gis data scientist location: glasgow employment type: contract, 2 months, strong chance of extension about the role morgan hunt are working with a leading government organisation to recruit a gis data scientist who can blend spatial analysis, advanced analytics, and problem-solving to turn geospatial data into actionable insights. you'll work with large, complex datasets, build predictive models, and support data-driven decisions across the organisation. if you love maps, patterns, and answering real-world questions with data, this role has your name all over it. key responsibilities • acquire, clean, and manage geospatial datasets from diverse sources • perform spatial analysis, spatial statistics, and geoprocessing to support strategic and operational projects. • develop predictive models and machine-learning workflows using spatial and non-spatial data. • build and maintain spatial databases, data pipelines, and automated etl processes. • create high-quality maps, dashboards, and visualisations for both technical and non-technical stakeholders. • collaborate with cross-functional teams to define requirements and deliver geospatial insights. • implement qa/qc best practices to ensure accuracy, reproducibility, and data governance. • stay current with emerging geospatial technologies, standards, and research. skills & experience essential • strong experience with gis platforms (arcgis, qgis) and geospatial libraries (e.g., geopandas, gdal/ogr, shapely, rasterio). • proficiency in python and/or r for data science and automation. • solid grounding in statistics, spatial analysis, and machine-learning methodologies. • experience with spatial databases (postgis, bigquery gis, sql server spatial). • ability to communicate complex spatial insights clearly to diverse audiences. • experience working with remote sensing and raster datasets. details • £650- £750 per day • inside of ir35 • 2 months, strong chance of extension • glasgow based morgan hunt is a multi-award-winning recruitment business for interim, contract and temporary recruitment and acts as an employment agency in relation to permanent vacancies. morgan hunt is an equal opportunities employer. job suitability is assessed on merit in accordance with the individual's skills, qualifications and abilities to perform the relevant duties required in a particular role.",glasgow,Data Scientist,"['bigquery', 'dashboard', 'data pipeline', 'etl', 'pandas', 'python', 'r', 'sql', 'sql server', 'statistics']","['bigquery', 'dashboard', 'data pipeline', 'etl', 'pandas', 'python', 'r', 'sql', 'sql server', 'statistics']",
data science trainee,it career switch,"trainee data scientist - no experience required are you looking to kick-start a new career as a data scientist we are recruiting for companies who are looking to employ our data science traineeship graduates to keep up with their growth. the best part is you will not need any previous experience as full training will be provided. you will also have the reassurance of a job guarantee (£25k-£45k) within 20 miles of your location upon completion. whether you are working full time, part-time or unemployed, this package has the flexibility to be completed at a pace that suits you.the traineeship is completed in 4 easy steps, you can be placed into your first role in as little as 6-12 months: step 1 - full data science career training you will begin your data science journey by studying a selection of industry-recognized courses that will take you from beginner level all the way through to being qualified to work in a junior data scientist role. through the interactive courses, you will gain knowledge in python, r, machine learning, ai, and much more. you will also complete mini projects to gain practical experience and test your skills while you study. step 2 - comptia data+ comptia data+ is an early-career data analytics certification for professionals tasked with developing and promoting data-driven business decision-making. it teaches data mining, visualization, data governance & data analytics. in any industry, gaining official certifications is very important in the recruitment process. therefore, this globally recognized certification will enhance your cv and make you stand out from the crowd. step 3 - official exam the comptia data+ exam will certify that you have knowledge and skills required to transform business requirements in support of data-driven decisions through mining and manipulating data, applying basic statistical methods, and analysing complex datasets while adhering to governance and quality standards. the exam is 90 minutes long and can be sat either in your local testing centre or online. step 4 - practical projects now that you have completed your theory training and official exams, you will be assigned 2 practical projects by your tutor. the projects are the most important part of the traineeship as it will showcase to employers that you have skills required to work in a data science role. the projects will use real world scenarios where you be utilising all of the skill that you have learned. whilst you are progressing through the projects, you will have the ongoing support from your personal tutor. once both projects have been completed and given the final sign off, you will have completed the traineeship and will be ready to move onto the recruitment stage. your data science role once you have completed all of the mandatory training, which includes the online courses, practical projects and building your own portfolio, we will place you into a data scientist role, where you will be guaranteed a starting salary of £25k-£45k. we have partnered with a number of large organisations strategically located throughout the uk, providing a nationwide reach of jobs for our candidates. we guarantee you will be offered a job upon completion, or we will refund you 100% of your course fees back. we have a proven track record of placing 1000+ candidates into new roles each year. check out our website for our latest success stories.read through the information? passionate about starting a career in data science? apply now and one of our friendly advisors will be in touch.",midlothian,Data Scientist,"['data analytics', 'machine learning', 'python', 'r']","['data analytics', 'machine learning', 'python', 'r']",
data scientist no experience necessary - job guarantee,newto training,"are you ready to start a new career in data analysis?the demand for data analysts has grown by 20% annually, with experienced professionals earning salaries upwards of £58,000.in today’s digital world, data is critical to business decision-making, making the role of a data analyst indispensable. as skills shortages continue to grow, the demand for qualified entry-level professionals is on the rise.with our data analytics career programme we will provide you with:8 training modules: excel, sql, python, r, tableau, power bi, comptia data+ & azure ai fundamentals3 official examinations: microsoft power bi data analyst, comptia data+, & microsoft azure ai fundamentals100+ hours of live instructor-led online classroom trainingreal-world data analyst project work & live labs to boost your cvexam & interview preparationjob guarantee with a salary up to £35,000course cost - £2495, or, £207.91 per monthwe guarantee you will be offered a job upon completion, or we will refund you 100% of your course fees.no prior industry experience is required - no matter your background, previous studies or work history - if you think you have the soft skills (communication skills, passion) needed then we can help you launch the career you want.click 'apply now’ to begin your new data career",united kingdom,Data Scientist,"['azure', 'data analysis', 'data analytics', 'excel', 'power bi', 'python', 'r', 'sql', 'tableau']","['azure', 'data analysis', 'data analytics', 'excel', 'power bi', 'python', 'r', 'sql', 'tableau']",£20k–£50k a year
lead data scientist,kainos,"join kainos and shape the future at kainos, we’re problem solvers, innovators, and collaborators - driven by a shared mission to create real impact. whether we’re transforming digital services for millions, delivering cutting-edge workday solutions, or pushing the boundaries of technology, we do it together. we believe in a people-first culture, where your ideas are valued, your growth is supported, and your contributions truly make a difference. here, you’ll be part of a diverse, ambitious team that celebrates creativity and collaboration. ready to make your mark? join us and be part of something bigger. job profile description kainos is recognised as one of the uk’s leading ai and data businesses, with a decade-long track record of delivering impactful, production-grade ai solutions for clients across government, healthcare, defence, and commercial sectors. kainos is at the forefront of ai innovation, trusted by microsoft, aws, and others to deliver advanced ai and data solutions at citizen scale. our 150-strong ai and data practice brings together deep expertise in machine learning, generative ai, agentic ai and data. we are pioneers in responsible ai, having authored the uk government’s ai cyber security code of practice implementation guide and we partner with leading organisations to ensure ai is deployed ethically, securely and with measurable business value. our teams are at the cutting edge of ai research, and delivery, it is truly an exciting team to join kainos as we further grow our ai capability. main purpose of the role & responsibilities in the business: as a lead data scientist at kainos, you will architect, design, and deliver advanced ai solutions leveraging state-of-the-art machine learning, generative and agentic ai technologies. you will drive the adoption of modern ai frameworks, aiops best practices and scalable cloud-native architectures. your role will involve hands-on technical leadership, collaborating with customers to translate business challenges into trustworthy ai solutions and ensuring responsible ai practices throughout. as a technical mentor, you will foster a culture of innovation, continuous learning, and engineering excellence. it is a fast-paced environment, so it is important for you to make sound, reasoned decisions. you will do this whilst learning about new technologies and approaches, with talented colleagues that will help you to develop and grow. you will manage, coach, and develop a small number of staff, with a focus on managing employee performance and assisting in their career development. you will also provide direction and leadership for your team as you solve challenging problems together. minimum (essential) requirements: • a minimum of a 2.1 degree in computer science, ai, data science, statistics or in a similar quantitative field. • have a deep understanding and developing of ai/ml models, including time series, supervised/unsupervised learning, reinforcement learning and llms. • experience with the latest ai engineering approaches such as prompt engineering, retrieval-augmented generation (rag), and agentic ai. • strong python skills with a grounding in software engineering best practices (ci/cd, testing, code reviews etc). • expertise in data engineering for ai: handling large-scale, unstructured, and multimodal data. • understanding of responsible ai principles, model interpretability, and ethical considerations. • strong interpersonal skills with the ability to lead client projects and establish requirements in non-technical language. • we are passionate about developing people, you will bring experience in managing, coaching, and developing junior members of a team and wider community. desirable: • demonstrable experience with modern deep learning frameworks (e.g. pytorch, tensorflow), fine-tuning or distillation of llms (e.g., gpt, llama, claude, gemini), machine learning libraries (e.g. scikit-learn, xgboost). • experience with data storage for ai, vector databases, semantic search, and knowledge graphs. • contributions to open-source ai projects or research publications. • familiarity with ai security, privacy, and compliance standards e.g. iso42001. embracing our differences at kainos, we believe in the power of diversity, equity and inclusion. we are committed to building a team that is as diverse as the world we live in, where everyone is valued, respected, and given an equal chance to thrive. we actively seek out talented people from all backgrounds, regardless of age, race, ethnicity, gender, sexual orientation, religion, disability, or any other characteristic that makes them who they are. we also believe every candidate deserves a level playing field. our friendly talent acquisition team is here to support you every step of the way, so if you require any accommodations or adjustments, we encourage you to reach out. we understand that everyone's journey is different, and by having a private conversation we can ensure that our recruitment process is tailored to your needs.",belfast (+3 others),Data Scientist,"['aws', 'cloud', 'deep learning', 'excel', 'machine learning', 'python', 'pytorch', 'r', 'scala', 'scikit-learn', 'statistics', 'tensorflow', 'time series', 'xgboost']","['aws', 'cloud', 'deep learning', 'excel', 'machine learning', 'python', 'pytorch', 'r', 'scala', 'scikit-learn', 'statistics', 'tensorflow', 'time series', 'xgboost']",
data scientist - deep learning practitioner,posting date:14/ 11/2025,"white collar factory (95009), united kingdom, london, london data scientist - deep learning practitioner about this role our data science team focuses on the development of machine learning and deep learning solutions, to solve business problems and deliver actionable insights. we are a talented, collaborative and enthusiastic group, who use our expertise to derive insights from complex data, working in close collaboration with our business partners. this role will primarily focus on developing proprietary deep learning models to address critical business challenges in underwriting. the role will also involve supporting our business partners as they develop advanced servicing products using large language models. what you’ll do • develop new deep learning approaches to advance our current underwriting models, which form the heart of our lending business. • apply these to new types of (multi-modal) data in order to stay at the forefront of innovation. • provide consultancy to our tech and product partners, to help design, develop and launch products powered by large language models (llms). this collaboration will help provide seamless experiences for our customers and associates. • use a combination of business acumen, coding and statistical skills to navigate large amounts of data and extract actionable solutions. • work cross-functionally on projects that support key business initiatives and drive sustainable growth. what we’re looking for • experience developing and deploying deep learning models, particularly for sequential data (e.g. time series, language) using techniques such as lstms or transformers. • hands-on experience with modern machine/deep learning frameworks such as pytorch, tensorflow, or hugging face transformers. • familiarity with both pre-training and fine-tuning of large-scale models. • experience working with structured and unstructured data, such as text, logs, or time series and tokenisation techniques. • a strong understanding of probability, statistics, machine learning and familiarity with large data set manipulation. • experience in producing reliable and maintainable code in python, with an ability to adapt to new languages and technologies. • ability to communicate findings to a diverse business focused audience, influencing others in both verbal and written form. • a drive for continued learning through an internal and external focus, in order to develop enterprise and industry leading solutions. we are committed to creating a level playing field and seek to create teams that are representative of our customers and the communities we serve. we’d love to hear from you if you identify with a typically under-represented group in our industry and are particularly keen to hear from women, the lgbtq+ community and ethnic minority candidates. where and how you'll work this is a permanent position based in our nottingham or london office. we have a hybrid working model, so you’ll be based in our office 3 days a week on tuesdays, wednesdays and thursdays, and can work from home on monday and friday. many of our associates have flexible working arrangements, and we're open to talking about an arrangement that works for you. what’s in it for you • bring us all this - and you’ll be well rewarded with a role contributing to the roadmap of an organisation committed to transformation • we offer high performers strong and diverse career progression, investing heavily in developing great people through our capital one university training programmes (and appropriate external providers) • immediate access to our core benefits including pension scheme, bonus, generous holiday entitlement and private medical insurance – with flexible benefits available including season-ticket loans, cycle to work scheme and enhanced parental leave • open-plan workspaces and accessible facilities designed to inspire and support you. our nottingham head-office has a fully-serviced gym, subsidised restaurant, mindfulness and music rooms. in london, you can heighten your mood with a run on our rooftop running track or an espresso at the workshop coffee café what you should know about how we recruit we pride ourselves on hiring the best people, not the same people. building diverse and inclusive teams is the right thing to do and the smart thing to do. we want to work with top talent: whoever you are, whatever you look like, wherever you come from. we know it’s about what you do, not just what you say. that’s why we make our recruitment process fair and accessible. and we offer benefits that attract people at all ages and stages. we also partner with organisations including the women in finance and race at work charters, stonewall and upreach to find people from every walk of life and help them thrive with us. we have a whole host of internal networks and support groups you could be involved in, to name a few: • reach – race equality and culture heritage group focuses on representation, retention and engagement for associates from minority ethnic groups and allies • outfront – to provide lgbtq+ support for all associates • mind your mind – signposting support and promoting positive mental wellbeing for all • women in tech – promoting an inclusive environment in tech • empowher - network of female associates and allies focusing on developing future leaders, particularly for female talent in our industry capital one is committed to diversity in the workplace. if you require a reasonable adjustment, please contact ukrecruitment@capitalone.com all information will be kept confidential and will only be used for the purpose of applying a reasonable adjustment. for technical support or questions about capital one's recruiting process, please send an email to careers@capitalone.com capital one does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site. capital one financial is made up of several different entities. please note that any position posted in canada is for capital one canada, any position posted in the united kingdom is for capital one europe and any position posted in the philippines is for capital one philippines service corp. (copssc). who we are at capital one, we're building a leading information-based technology company. still founder-led by chairman and chief executive officer richard fairbank, capital one is on a mission to help our customers succeed by bringing ingenuity, simplicity, and humanity to banking. we measure our efforts by the success our customers enjoy and the advocacy they exhibit. we are succeeding because they are succeeding. guided by our shared values, we thrive in an environment where collaboration and openness are valued. we believe that innovation is powered by perspective and that teamwork and respect for each other lead to superior results. we elevate each other and obsess about doing the right thing. our associates serve with humility and a deep respect for their responsibility in helping our customers achieve their goals and realize their dreams. together, we are on a quest to change banking for good.",nottingham (+1 other),Data Scientist,"['deep learning', 'machine learning', 'python', 'pytorch', 'r', 'statistics', 'tensorflow', 'time series']","['deep learning', 'machine learning', 'python', 'pytorch', 'r', 'statistics', 'tensorflow', 'time series']",
lead ml / data scientist,blenheim chalcot,"solving the borrowing needs of consumers, and connecting investors and funders seeking to finance consumer assets within their targeted risk-reward parameters. we have a longstanding history of predictable performance for warehouse investors for our on-balance sheet lending, and forward flow lending options for investors and funders seeking to finance consumer assets without specialist consumer credit capabilities. o6k, our business-to-business technology and analytics company, has built our proprietary technology platform from the ground up to support our own lending businesses, but is designed to offer white labelling and platform lending solutions for business partners. this role offers an excellent opportunity for a data science professional to develop within a rapidly growing organisation. the data science team is responsible for delivering insight across our marketing, fraud, underwriting, and customer management strategies. the aim of the position is to advance oakbrook’s data science and innovation capabilities across the full customer lifecycle including developments within machine learning using innovative data sources not traditionally used by consumer finance companies. owning the development and maintenance of predictive models to improve customer outcomes across fraud, marketing, credit and collections advance our autonomous decisioning capability including model deployment, setting policy, and testing across all areas of the business developing test and learn strategies across different lines of business using data driven analysis, monitoring performance and providing clear recommendations to drive the company forward to help support the company’s ambitious growth aspirations providing leadership and identifying opportunities to improve process and procedures through inquisitive analysis analyse, build and integrate machine learning models that have a tangible benefit to the business quality & communication: experience with the full lifecycle data science projects experienced and knowledgeable in predictive modelling, machine learning and ai good programming skills in r, sql, python data visualisation and dashboarding e.g. power bi ability to understand it infrastructure commercial acumen usage of gen ai oakbrook is regulated by the financial conduct authority. you will be required to undertake regular compliance training, and we expect this to be completed within a timely manner. tackle meaningful challenges and accelerate your career with tailored learning and development opportunities designed to help you thrive. ~ comprehensive benefits package enjoy flexible benefits including private medical and dental insurance, perkbox discounts, electric car scheme, life assurance, a competitive pension scheme, and more. ~ generous holiday allowance start with 25 days of annual leave, plus your birthday off! you'll also earn an extra day each year you're with us, alongside long-service rewards such as complimentary bupa health checks and enhanced pension contributions. ~ vibrant office culture enjoy free snacks and refreshments in the office, and connect with colleagues at our regular monthly socials. by submitting your cv you understand that we have a legitimate interest to use your personal data for the purposes of assessing your eligibility for this role. this means that we may use your personal data to contact you to discuss your cv or arrange an interview, or transfer your cv to the hiring manager(s) of the role you have applied for. you can ask us at any time to remove your cv from our database by emailing view email address on alertsclk.com can review our privacy policy here.",nottingham,Data Scientist,"['dashboard', 'excel', 'machine learning', 'power bi', 'python', 'r', 'recommendation', 'sql']","['dashboard', 'excel', 'machine learning', 'power bi', 'python', 'r', 'recommendation', 'sql']",
data scientist and ai engineer on ktp project with carpenters group - grade 7,university of liverpool,"we are seeking to appoint a highly qualified and motivated individual to undertake a role as a data scientist and ai engineer, working on an innovate uk funded, three-year collaborative project between carpenters group and the school of computer science and informatics. carpenters are a market leading insurance legal services provider. working within carpenters, and a team from the school, you will realise a programme of work to automate a number of legal processes and build tools involving ai technologies related to machine learning, natural language processing, neurosymbolic computing and explainable ai. you will work within a research based commercial environment to build a commercial product based on the state-of-the-art ai and data science technology. the salary will be enhanced by a substantial annual training budget to support the post holder’s professional development. applicants should have at least an msc, and a good first degree, in computer science or closely related subject; applicants with phds will be preferred. applicants should also have experience of conducting high quality computer science research and be able to demonstrate sound software engineering and programming skills. the post is available for 3 years and is based at carpenters’ office in liverpool. commitment to diversity the university of liverpool is committed to enhancing workforce diversity. we actively seek to attract, develop, and retain colleagues with diverse backgrounds and perspectives. we welcome applications from all genders/gender identities, black, asian, or minority ethnic backgrounds, individuals living with a disability, and members of the lgbtqia+ community. for full details and to apply online, please visit: recruit.liverpool.ac.uk £39,906 to £46,049 per annum (depending on qualifications and abilities, plus company top-up of £7,400 pa)",liverpool,Data Scientist,"['machine learning', 'natural language processing', 'r']","['machine learning', 'natural language processing', 'r']","£39,906–£46,049 a year"
data science and ai specialist,university of glasgow,"applied data scientist – health and ai (trusted research environment) research track job purpose to provide advanced analytical, epidemiological, and data-science support for research projects using nhs data hosted within the trusted research environment (tre). the postholder will work closely with investigators from nhs greater glasgow and clyde (nhsggc), the university of glasgow (uofg), and industry partners to translate research ideas into robust analytical plans, ensure data are appropriately specified and prepared for analysis, and deliver high-quality, reproducible outputs. the role focuses on real-world health data analysis — including study design, data wrangling, phenotype development, data integration, and statistical and machine-learning methods — to accelerate project delivery, strengthen grant applications, and advance the overall research capability of the tre. main duties and responsibilities 1. support principal investigators by designing and implementing robust analytical and statistical workflows for complex clinical and population health datasets hosted in the tre — including data wrangling, quality assessment, phenotype development, and exploratory analyses. 2. develop reproducible and transparent analytical pipelines, ensuring data provenance, version control, and adherence to ethical and governance standards. 3. working closely with clinicians, researchers, and data engineers across nhs and uofg to define project data requirements, optimise analytical design, and translate research questions into executable analyses. 4. lead on technical aspects of data integration, statistical and machine-learning model development, validation, interpretability, and deployment within the secure tre environment. 5. ensure all research activities comply with nhs data governance, iso standards, and the tre’s ethical frameworks. 6. contribute to demonstration and exemplar projects (e.g., multimodal data integration, digital phenotyping, predictive analytics) that highlight the tre’s analytical and ai capabilities. 7. act as liaison between nhs safe haven, academic researchers, and university services (e.g., information services, centre for data science and ai) advising on data specifications, study design, and appropriate analytical methodologies. 8. support the training and mentoring of researchers and students in applied health data science, statistical methods, and tre workflows. 9. perform administrative and governance-related tasks relevant to tre operations, including documentation, data access tracking, and project coordination. 10. keep up to date with current knowledge and recent advances in the field / discipline. 11. contribute to research outputs, grant applications, and dissemination activities that strengthen tre capabilities and support collaborative funding bids. 12. participate and engage with national and cross-institutional ai/tre initiatives and networks as appropriate. 13. undertake any other reasonable duties as required by the head of school / director of clinical tre 14. contribute to the enhancement of the university’s international profile in line with the university strategy. knowledge, qualifications, skills and experience knowledge/qualifications essential: a1 scottish credit and qualification framework level 12 (phd) in a relevant discipline such as epidemiology, biostatistics, health data science, or health informatics.. a2 strong knowledge of epidemiological and biostatistical principles applied to healthcare data, with experience integrating these with data-science or ai/ml methods. a3 demonstrable understanding of data governance and regulatory requirements for clinical data, including anonymisation, secure data handling protocols and workflows underpinning trusted research environments (tres). a4 understanding of study design, phenotype development, and data quality assessment in real-world healthcare research. desirable: b1 additional formal training or certification in epidemiology, biostatistics, health informatics, or applied ai in healthcare b2 knowledge of data standards and interoperability frameworks (e.g., omop, fhir, snomed ct, icd-10) relevant to real-world data integration. b3 understanding of computable phenotypes, data harmonisation, or ontology development for clinical research. b4 awareness of federated analytics, privacy-preserving computation, or distributed learning within trusted research environments. skills essential: c1 proficiency in r and/or python, with strong skills in health data wrangling, cleaning, integration, and visualisation; experience with analytical andmachine learning frameworks (e.g., tensorflow, pytorch, scikit-learn). c2 ability to manipulate, analyse, and interpret large or complex healthcare datasets within secure computing environments, ensuring reproducibility and integrity. c3 excellent communication and interpersonal skills to work across interdisciplinary teams in both academic and clinical environments. c4 proven ability to explain analytical findings and complex technical concepts to non-specialist stakeholders, including clinicians, policymakers, and industry partner. c5 problem-solving mindset with the ability to work independently and manage multiple priorities. desirable: d1 experience in developing reproducible analysis pipelines using tools such as git, docker, or workflow managers d2 strong skills in data visualisation and dashboarding (e.g., r shiny, plotly, dash, power bi) for communicating insights to clinical and policy audiences. d3 familiarity with advanced analytical techniques, such as causal inference, predictive modelling, or survival analysis in health data contexts. experience essential: e1 significant experience in applied health data analysis — including study design, data specification, data wrangling, statistical analysis, and (where appropriate) machine-learning model development or evaluation. e2 experience working with sensitive health or clinical datasets within secure research environments or safe havens. e3 experience contributing to research publications, technical reports, or grant-funded projects through provision of analytical and methodological expertise. e4 experience working within data governance and ethical frameworks, ideally in healthcare or public sector research. e5 proven commitment to supporting the career development of colleagues and to is iother forms of collegiality appropriate to the career stage desirable: f1 prior experience supporting safe haven/tre governance committees, data access processes, or technical advisory groups. f2 contribution to open-source tools, data models, or methods for healthcare analytics or ai reproducibility. f3 experience in preparing grant applications or preliminary data analyses that directly supported successful research funding. f4 evidence of continuous professional development in health data science, ai ethics, or digital health innovation. informal enquiries should be directed to professor sandosh padmanabhan, sandosh.padmanabhan@glasgow.ac.uk previous applicants should not re-apply for this position. terms and conditions salary will be grade 7, £41,064 - £46,049 per annum. this post is full time (35 hours p/w) and has funding for up to 3 years initially. relocation assistance will be provided where appropriate. as a valued member of our team, you can expect: 1 a warm welcoming and engaging organisational culture, where your talents are developed and nurtured, and success is celebrated and shared. 2 an excellent employment package with generous terms and conditions including 41 days of leave for full time staff, pension - pensions handbook https://www.gla.ac.uk/myglasgow/payandpensions/pensions/, benefits and discount packages. 3 a flexible approach to working. 4 a commitment to support your health and wellbeing, including a free 6-month uofg sport membership for all new staff joining the university https://www.gla.ac.uk/myglasgow/staff/healthwellbeing/. we believe that we can only reach our full potential through the talents of all. equality, diversity and inclusion are at the heart of our values. applications are particularly welcome from across our communities and in particular people from the black, asian and minority ethnic (bame) community, and other protected characteristics who are under-represented within the university. read more on how the university promotes and embeds all aspects of equality and diversity within our community https://www.gla.ac.uk/myglasgow/humanresources/equalitydiversity/. we endorse the principles of athena swan https://www.gla.ac.uk/myglasgow/humanresources/equalitydiversity/athenaswan/ and hold bronze, silver and gold awards across the university. we are investing in our organisation, and we will invest in you too. please visit our website https://www.gla.ac.uk/explore/jobs/ for more information. closing date 8 january 2026 at 23:45",glasgow,Data Scientist,"['dashboard', 'data analysis', 'excel', 'machine learning', 'power bi', 'python', 'pytorch', 'r', 'scikit-learn', 'statistics', 'tensorflow']","['dashboard', 'data analysis', 'excel', 'machine learning', 'power bi', 'python', 'pytorch', 'r', 'scikit-learn', 'statistics', 'tensorflow']",
graduate data scientist 2026,innovative technology,"we are looking for a graduate data scientist, who is looking to work in a fast paced, global, market leading company? here at innovative technology, we have an excellent opportunity for graduate data scientist to join our talented team at our global head office in oldham, greater manchester. overview this role is to maintain and proactively develop machine learning algorithms for current and future products, with a core focus on biometric technologies. your responsibilities as a graduate data scientist • contribute directly to the development, implementation, and validation of machine learning algorithms for our industry-leading biometric and face analysis technologies. • optimize and fine-tune existing models, including tuning and retraining existing convolutional neural networks (cnns). • investigate and resolve underlying system and algorithm issues identified through testing and customer feedback. • drive continuous improvement through research and development of novel techniques in the field. • ensure all code added to the pipeline and shared git repositories is of the required standard, well-documented, and easy to maintain. • package all code with appropriate unit-testing to ensure future conformity and stability. • actively contribute to developing data science activities and proposing new processes for quality development (e.g., standard reporting, source control, integration). your skills and experience • a degree in mathematics, computer science or computational science • basic knowledge of data science/machine learning algorithms and full data processing pipelines. • proficiency in a data science prototyping language such as python or matlab. • understanding of convolutional neural networks (cnns) and feature extraction techniques. • basic knowledge of programming languages including python, c++, and c, along and libraries such as scikit-learn, numpy, and/or scipy. your package perks • a competitive salary • flexible working hours • 32 days holiday, (including public holidays) plus the opportunity to earn up to an extra 13 days holiday each year • enhanced maternity/paternity/adoption leave & pay • enhanced pension contribution • healthcare insurance (including dental) • wellbeing support • life insurance • income protection insurance • educational sponsorship • electric car scheme • free secure parking • onsite electric car charging points • cycle to work scheme • informal dress code • paid breaks, with free hot premium drinks we’re innovative… trading for over 30 years here at innovative technology, where we now have offices on five continents and employ around 400 people, with over 150 based from our state-of-the-art r&d hub. from self-service checkouts to arcade machines, we provide our retail, banking, kiosk, vending, gaming and amusement customers with products and services that help them securely accept automated payments, with our industry-leading technology keeping us at the forefront of our sector. we also provide facial analysis technology for age estimation and control access for some of the world’s leading companies. by being true to our values of innovation, collaboration, respect and drive we’ve seen significant growth and won numerous domestic and international awards, whilst offering outstanding career opportunities and great benefits. you’ll find us on the edge of the pennines and less than half an hour from central manchester, with modern offices, free parking and excellent transport links. we are a disability-confident employer, as such we will shortlist all candidates meeting our minimum criteria (as specified in the job description) who state they have a disability within their application. what’s next? if you are looking to join our award-winning team working on the latest cutting-edge technology, we want to hear from you. a better way... through our people, drive and commitment we push boundaries to deliver innovative products and services.",oldham,Data Scientist,"['c++', 'excel', 'machine learning', 'matlab', 'numpy', 'python', 'r', 'scikit-learn']","['c++', 'excel', 'machine learning', 'matlab', 'numpy', 'python', 'r', 'scikit-learn']",
applied data scientist (research engineer – digital technologies)*,centre for process innovation,"about cpi cpi helps make great ideas and inventions a reality. we’re a team of intelligent people using advances in science and technology to solve the biggest global challenges in healthcare and sustainability. through our incredible people and innovation infrastructure, we collaborate with our partners in industry, academia, government, and the investment community to accelerate the development and commercialisation of innovative products. from health technologies and pharmaceuticals to sustainable food and materials innovations, we turn the entrepreneurial spirit and radical thinking of our people and partners into incredible impact that makes our world a better place. why this role is important for cpi’s work cpi has an exciting opportunity for an applied data scientist (research engineer – digital technologies) to join its established and growing automation and digital team within the formulation technology team at our state of the art: national formulation centre, based at netpark in sedgefield. the ideal candidate will be a cross-disciplinary thinker, combining expertise in chemistry, physics, biology, or mathematics with strong data science skills. in this role, you will leverage modern computational, statistical, and cloud-based technologies to generate insights into complex materials, driving innovations across energy storage, sustainable materials, nanotherapeutics, and consumer goods. a strong foundation in materials at the molecular, atomic, or structural level is highly valued, along with a keen interest in the markets we serve — particularly energy storage, pharmaceuticals, and sustainable materials. experience in applying machine learning, high-dimensional modeling, or data-driven simulation on cloud platforms to address real-world materials challenges will allow you to make an immediate impact. reporting to a team leader, you will leverage your existing skills in data science and be further developed in strategically relevant areas such as machine learning (ml), artificial intelligence (ai), predictive modeling, automation of data capture/processing, and ml-driven design of experiments (doe) methods. using these skills across a combination of low-code platforms, python scripts, and cloud-based technologies for scalable computation and data management, you will contribute to the delivery of innovation projects with our clients and support technical leads in providing advice and solutions to both internal and external stakeholders. in addition, this role offers a unique opportunity to contribute to the development and operation of our 24/7 robotic formulation laboratory, integrating advanced automation with data-driven experimentation to serve our markets. the automation and digital team’s mission is to deploy digital and physical technologies that improve efficiency and quality when innovating in the formulation sector, and enable the generation, curation and analysis of large data sets to produce insights. we combine our digital skills with industrially relevant knowledge, laboratory automation and high throughput experimentation, and state-of-the-art process characterisation, to provide a complete solution for companies seeking to accelerate innovation for new and existing product lines. the team works with companies and organisations of all sizes from global multinationals to 2‑person start-ups, with project durations ranging anywhere from 1 month to 4 years. you will be exposed to an immense variety of projects: in scale (anything from 1 month to multi-year projects); in area of application (batteries, sustainability, nanotherapeutics, fmcg, food and feed sectors) and technical scope of the techniques used. you will be encouraged to explore new techniques (techniques both new to our organisation, and novel techniques emerging from academia). training in new techniques — whether self-taught, peer training or through formal courses — is encouraged. some exciting projects the team have recently worked on are: batcat: batcat is the project that realizes the manufacturability programme from the battery 2030+ roadmap, creating a digital twin for battery manufacturing that integrates data-driven and physics-based methods. it develops a cross-chemistry data space for two technologies, (1) li-ion and na-ion coin cells and (2) redox flow batteries. healingbat: horizon europe eu project is developing advanced sensing, monitoring and self-healing mechanisms to self-repair batteries, leading to the eu batteries of the future.. narrator (nanopharma): the application of state-of-the-art digital techniques and automation to explore lipid nanoparticle (lnp) manufacture for supporting rna vaccines and therapeutics. our digital scientists are helping to chart our way intelligently through libraries of chemical data and make strides in the science of lnp manufacture. full_map: this project will help to develop our capabilities in our a fully automated r&d facility for battery manufacture and analysis from the formulation through to the cell manufacturing served by collaborative mobile robots. other types of projects that we run include: • working with global leaders to apply model predictive control to their formulations. • using clustering and correlation analysis to determine cause-effects or comparisons within datasets (e.g., developing models to predict soap stability, predicting best candidates for new ink materials) • supporting development of our automated platforms in service of our markets • using machine learning techniques to on data to produce actionable insights. • developing soft sensors based on pls and neural net models to predict slurry particle size and viscosity • implementing bayesian methods and/or state-of-the-art methods for optimising the design of experiments approach. • developing a federated learning system to produce global models that enhance production in manufacturing. the role key tasks in the role will include (but are not limited to the below), please download the job description for full details available on the cpi careers page: • supporting the planning and scoping of technical work programmes within digital strategy (e.g. model predictive control, process modelling, data analytics, machine learning, and the application of digital technologies). • undertaking the technical delivery of programmes of work in digital strategy on existing data or data collected from own experiments and researches, and then analysing, interpreting and reporting the results. • keeping up to date with research and techniques relevant to the digital space (e.g. develop in statistical modelling techniques, the mathematical foundations of applied machine learning, skills in process modelling and control, skills in relevant coding languages) and to develop, implement and improve existing methods/technologies in the platform. • growing as an internal expert in data science using knowledge of principles and practices in the field to support non-data science colleagues. • developing and utilising own expertise to build data science capability within the technology team and acting as internal consultant to coach others at cpi. the person we are seeking the successful candidate will be educated to hnc / foundation degree / degree level (or equivalent) in a scientific, engineering or mathematical discipline, plus relevant industrial experience in the application of data science in the prerequisite fields (see job descriptions for further information) and; • will be able to solve and contextualise scientific problems using data science. • will possess willingness to learn new methods of datra science and coding languages. • can demonstrate the ability to apply theoretical and practical scientific methods to contribute to business activities. • will have confidence to use own judgement and initiative within standard engineering / scientific practices, as well as an understanding of when to seek advice from colleagues. • will possess knowledge of / have an awareness of one or more of the following data science and digital skills application methods; • the application of advanced statistical methods (e.g. pca) and modelling to technical problem-solving. • the mathematical foundations of applied machine learning • the application of machine learning, process modelling or implementation of model predictive control. • the use of coding languages (python, r matlab) to create digital solutions for efficient or novel problem solving. • the demonstration of technical and theoretical knowledge in mathematics related to data science • can demonstrate a working knowledge of the principles and practices in data science techniques gained through academia / career to date . • will have a background in / knowledge relevant to the batteries, energy storage and/or materials spaces • can demonstrate evidence of building knowledge sharing and communicating with non-specialist colleagues and stakeholders. applications are particularly welcome from candidates who are educated to masters/phd level (or equivalent) with relevant industrial experience and; • have some experience in using data science on cloud architectures • have chartered status with a relevant professional institution • are a member of a relevant professional body what does cpi offer you? at cpi, we offer a wide range of benefits to our employees, this includes: • up to 36 days holiday, including bank holidays – plus a holiday purchasing scheme • generous pension scheme • life assurance and accident insurance schemes • flexible working • learning and development opportunities • free parking find out more about our culture and benefits. additional information • internal job title will be research engineer 1 – digital technologies / research engineer 2 – digital technologies (appropriate to experience level determined at interview stage) • *research engineer 1 — digital technologies (£26,000-£34,000) / research engineer 2 — digital technologies (up to £40,500) cpi is an organisation based in the uk. commencement of employment is conditional to demonstrating right to work in the uk, sponsorship may be available.",sedgefield,Data Scientist,"['bayesian', 'cloud', 'clustering', 'data analytics', 'experimentation', 'machine learning', 'matlab', 'python', 'r', 'scala']","['bayesian', 'cloud', 'clustering', 'data analytics', 'experimentation', 'machine learning', 'matlab', 'python', 'r', 'scala']",£26k–£40.5k a year
data scientist | health & fitness,nicholson glover,"we’re currently working with one of the uk’s most recognised names in the health & fitness industry. they're looking to hire a data scientist to join their ambitious and growing team. the company with over a decade of industry presence, this brand has expanded to more than 200 locations across the uk and proudly supports a community of over 500,000 members. they’ve built a reputation not just for their size and scale, but for a people-first culture and strong core values that are truly embedded across the business. the role this is an exciting opportunity for a commercially minded data scientist to take ownership of end-to-end modelling and ml pipeline development within a fast-paced, consumer-focused organisation. you’ll lead impactful work across the full lifecycle — from exploration and predictive modelling to deployment and automation — using modern cloud tooling to shape smarter pricing, retention, and customer insight. alongside hands-on modelling, you’ll help strengthen mlops practices and contribute to emerging ai initiatives, working closely with engineering and analytics teams to deliver scalable, high-value solutions. key attributes of the suitable data scientist include: • strong technical foundation in python, sql, and cloud-based ml workflows, with experience building and deploying end-to-end predictive models. • exposure to mlops or ml engineering practices, including pipelines, ci/cd, model monitoring, and working with tools like azure ml or fabric. • commercially minded communicator who can translate data into clear, actionable insight and work collaboratively with technical and non-technical stakeholders.",united kingdom,Data Scientist,"['azure', 'cloud', 'python', 'r', 'scala', 'sql']","['azure', 'cloud', 'python', 'r', 'scala', 'sql']",£60k–£65k a year
"manager, data science & ai - data science, belfast, derry/londonderry",ey,"at ey, we’re all in to shape your future with confidence. we’ll help you succeed in a globally connected powerhouse of diverse teams and take your career wherever you want it to go. join ey and help to build a better working world. data analytics & ai – data science – manager at ey, you’ll have the chance to build a career as unique as you are, with the global scale, support, inclusive culture and technology to become the best version of you. and we’re counting on your unique voice and perspective to help ey become even better, too. join us and build an exceptional experience for yourself, and a better working world for all. the opportunity as part of our data analytics and ai team, the data scientist role is pivotal in enabling our clients to derive significant value from their information assets. through collaboration and domain expertise, our team embeds innovative data analytics solutions into existing business areas, transforming data into strategic assets. we are looking for a data scientist who is passionate about leveraging data to solve complex problems and drive business outcomes. your key responsibilities • * collaboration: the ability to work closely with cross-functional teams to develop, test, and deploy advanced machine learning models, ensuring alignment with business objectives and seamless integration into client operations. • * data analysis: the capability to analyse large and complex datasets to extract actionable insights. your analytical skills will be key in identifying trends, patterns, and anomalies that can inform business strategies. • * data pipeline management: be able to design, implement, and maintain data pipelines that are critical for model training and deployment. your expertise will ensure the reliability and efficiency of our data infrastructure. • * communication: the skill to effectively communicate complex analytical findings and model results to stakeholders, translating data-driven insights into business language that informs decision-making. • * continuous learning: the ability to stay informed of the latest advancements in data science, including techniques and tools, to ensure our team remains at the cutting edge of the field. • * understand key business drivers within an organization and help to articulate and quantify the value that data and ai can deliver. skills and attributes for success to qualify for the role; • * experience in building and deploying machine learning or deep learning models in real-world applications. • * a completed degree (bachelors, masters, or phd). • * excellent leadership and organisational skills, with a track record of developing and leading high-performing teams. • * proficiency in python, sql, and deep learning frameworks such as tensorflow and pytorch. experience with r is a nice-to-have. • * a strong foundation in statistics, mathematics, and programming is essential for success in this role. the following will be an advantage; • * problem-solving: ability to translate business assumptions and rules into feature engineering and model explainability, addressing business problems with data-driven solutions. • * collaborative development: work under the guidance of senior data scientists and solution architects to build models that align with strategic visions and client needs. what we look for we’re interested in candidates with a genuine creative vision and the confidence to make it happen. you can expect plenty of autonomy in this role, so you’ll also need the ability to take initiative and seek out opportunities to improve our current relationships and processes. if you’re serious about auditing and ready to take on some of our clients’ most complex issues, this role is for you. what we offer • * continuous learning: you’ll develop the mindset and skills to navigate whatever comes next. • success as defined by you: we’ll provide the tools and flexibility, so you can make a meaningful impact, your way. • transformative leadership: we’ll give you the insights, coaching and confidence to be the leader the world needs. • diverse and inclusive culture: you’ll be embraced for who you are and empowered to use your voice to help others find theirs. if you can demonstrate that you meet the criteria above, please contact us as soon as possible. the exceptional ey experience. it’s yours to build. apply now. please note; prior to finalizing your application, you will be asked to provide personal information across several dimensions of diversity and inclusiveness. the information you provide is kept entirely confidential and will not be used to evaluate your candidacy. we collect this data to help us analyse our recruitment process holistically and implement actions that promote diversity and inclusiveness. while optional, we encourage you to provide this information to hold us accountable towards our goal of building a better working world. read more about our commitment to diversity& inclusiveness here . we ask because it matters! ey exists to build a better working world, helping to create long-term value for clients, people and society and build trust in the capital markets. enabled by data and technology, diverse ey teams in over 150 countries provide trust through assurance and help clients grow, transform and operate. working across assurance, consulting, law, strategy, tax and transactions, ey teams ask better questions to find new answers for the complex issues facing our world today. ey | building a better working world ey is building a better working world by creating new value for clients, people, society and the planet, while building trust in capital markets. enabled by data, ai and advanced technology, ey teams help clients shape the future with confidence and develop answers for the most pressing issues of today and tomorrow. ey teams work across a full spectrum of services in assurance, consulting, tax, strategy and transactions. fueled by sector insights, a globally connected, multi-disciplinary network and diverse ecosystem partners, ey teams can provide services in more than 150 countries and territories.",belfast,Data Scientist,"['data analysis', 'data analytics', 'data pipeline', 'deep learning', 'excel', 'feature engineering', 'machine learning', 'python', 'pytorch', 'r', 'sql', 'statistics', 'tensorflow']","['data analysis', 'data analytics', 'data pipeline', 'deep learning', 'excel', 'feature engineering', 'machine learning', 'python', 'pytorch', 'r', 'sql', 'statistics', 'tensorflow']",
data scientist,jacobs,"data scientist as an experienced data scientist you will be an inegral part of our team responsible for building,delivering and developing advanced data analytics products. utilising your data analysis, statistical modelling, and machine learning techniques to apply, deploy and visualise solutions to complex business problems. collaborating with key stakeholders you will leverage your analytical skills to indetify, build and understand the scope and impact of requirements and challanges, to develop a foundation for implementing technical solutions. your remit and responsiblities will include; • develop and implement of data-driven solutions to support business objectives. • design, build, and deploy advanced data analytics models and algorithms. • collaborate with cross-functional teams to understand scope of requirements to formulate data science projects. • analyse large datasets to extract actionable insights and inform business decisions. • develop and implement findings and recommendations.. • collaborate with colleagues and peers fostering a support culture that facilitates bot career and personal development. ideally with utilities industry experience, applications are eagerly sough from candidates with the following, skills, experience and attributes: • demonstrable experience of palantir foundry, azure data explorer (kusto). • gis experience (open layers, qgis, geopandas, geoserver) • bachelor's degree or equivalent work experience related to data science, computer science, statistics etc. • extensive track record in applied data science. • demonstrable experience in building and delivering data-driven solutions to clients and users. • proficient in python, sql. • ability to ideate, build and deploy scalable solutions. • nderstanding of key development principles like agile, source control, ci/cd, environment segregation, testing etc #-linm1 https://careers.jacobs.com/en_us/careers/jobdetail/31993 edinburgh|east lothian|united kingdom glasgow|lanarkshire|united kingdom digital and data enterprise functions joining jacobs not only connects you locally but globally. our values stand on a foundation of safety, integrity, inclusion and belonging. we put people at the heart of our business, and we truly believe that by supporting one another through our culture of caring, we all succeed. we value positive mental health and a sense of belonging for all employees. with safety and flexibility always top of mind, we’ve gone beyond traditional ways of working so you have the support, means and space to maximize your potential. you’ll uncover flexible working arrangements, benefits, and opportunities, from well-being benefits to our global giving and volunteering program, to exploring new and inventive ways to help our clients make the world a better place. no matter what drives you, you’ll discover how you can cultivate, nurture, and achieve your goals – all at a single global company. find out more about life at jacobs. we aim to embed inclusion and belonging in everything we do. we know that if we are inclusive, we’re more connected and creative. we accept people for who they are, and we champion the richness of different perspectives, lived experiences and backgrounds in the workplace, as a source of learning and innovation. we are committed to building vibrant communities within jacobs, including through our jacobs employee networks, communities of practice and our find your community initiatives, allowing every employee to find connection, purpose, and belonging. find out more about our jacobs employee networks here. jacobs partners with vercida to help us attract and retain talent from a wide range of backgrounds. for greater online accessibility please visit www.vercida.com/uk/employers/jacobs to view and access our roles. as a disability confident employer, we will interview all disabled applicants who meet the minimum criteria for a vacancy. we welcome applications from candidates who are seeking flexible working and from those who may not meet all the listed requirements for a role. we value collaboration and believe that in-person interactions are crucial for both our culture and client delivery. we empower employees with our hybrid working policy, allowing them to split their work week between jacobs offices/projects and remote locations enabling them to deliver their best work. your application experience is important to us, and we’re keen to adapt to make every interaction even better. if you require further support or reasonable adjustments with regards to the recruitment process (for example, you require the application form in a different format), please contact the team via careers support.",haddington,Data Scientist,"['azure', 'data analysis', 'data analytics', 'machine learning', 'pandas', 'python', 'r', 'recommendation', 'scala', 'sql', 'statistics']","['azure', 'data analysis', 'data analytics', 'machine learning', 'pandas', 'python', 'r', 'recommendation', 'scala', 'sql', 'statistics']",
data scientist - statistician,posting date:22/ 10/2025,"nottingham trent house (95002), united kingdom, nottingham, nottinghamshire data scientist - statistician about this role our data science team focuses on the development of machine learning and deep learning solutions, to solve business problems and deliver actionable insights. we are a talented, collaborative and enthusiastic group, who use our expertise to derive insights from complex data, working in close collaboration with our business partners. this role will primarily focus on feature engineering and insight generation from new types of data and the development of machine learning models to address critical business challenges in underwriting. what you’ll do • develop and maintain the machine learning models which define our competitive advantage in the financial services market. • explore and evaluate data, using advanced feature generation and categorisation techniques, in order to stay at the forefront of innovation. • analyse tabular and non-tabular data, such as text, logs, or time series, to produce powerful new insights. • consult on complex statistical test design, to efficiently learn our way into new areas of the market. • use a combination of business acumen, coding and statistical skills to navigate large amounts of data and extract actionable solutions, working cross-functionally to support key business initiatives and drive sustainable growth. what we’re looking for • a strong understanding of probability, statistics, machine learning, feature extraction and familiarity with large data set manipulation. • experience using deep learning models, particularly for sequential data. • experience working with open banking or credit bureau data. • experience working with multi-modal data; in multiple formats from a variety of different sources. • experience in producing reliable and maintainable code in python, with an ability to adapt to new languages and technologies. • experience of model risk management; technical documentation, coding best practices, the importance of validation and ongoing monitoring. • natural curiosity and proactive engagement with all areas of the business, with a desire to ask questions, challenge the status-quo and identify where data science can add value. • ability to communicate findings to a diverse business focused audience, influencing others in both verbal and written form. • a drive for continued learning through an internal and external focus, in order to develop enterprise and industry leading solutions. we are committed to creating a level playing field and seek to create teams that are representative of our customers and the communities we serve. we’d love to hear from you if you identify with a typically under-represented group in our industry and are particularly keen to hear from women, the lgbtq+ community and ethnic minority candidates. where and how you'll work this is a permanent position based in our nottingham office. we have a hybrid working model, so you’ll be based in our office 3 days a week on tuesdays, wednesdays and thursdays, and can work from home on monday and friday. many of our associates have flexible working arrangements, and we're open to talking about an arrangement that works for you. what’s in it for you • bring us all this - and you’ll be well rewarded with a role contributing to the roadmap of an organisation committed to transformation • we offer high performers strong and diverse career progression, investing heavily in developing great people through our capital one university training programmes (and appropriate external providers) • immediate access to our core benefits including pension scheme, bonus, generous holiday entitlement and private medical insurance – with flexible benefits available including season-ticket loans, cycle to work scheme and enhanced parental leave • open-plan workspaces and accessible facilities designed to inspire and support you. our nottingham head-office has a fully-serviced gym, subsidised restaurant, mindfulness and music rooms what you should know about how we recruit we pride ourselves on hiring the best people, not the same people. building diverse and inclusive teams is the right thing to do and the smart thing to do. we want to work with top talent: whoever you are, whatever you look like, wherever you come from. we know it’s about what you do, not just what you say. that’s why we make our recruitment process fair and accessible. and we offer benefits that attract people at all ages and stages. we also partner with organisations including the women in finance and race at work charters, stonewall and upreach to find people from every walk of life and help them thrive with us. we have a whole host of internal networks and support groups you could be involved in, to name a few: • reach – race equality and culture heritage group focuses on representation, retention and engagement for associates from minority ethnic groups and allies • outfront – to provide lgbtq+ support for all associates • mind your mind – signposting support and promoting positive mental wellbeing for all • women in tech – promoting an inclusive environment in tech • empowher - network of female associates and allies focusing on developing future leaders, particularly for female talent in our industry capital one is committed to diversity in the workplace. if you require a reasonable adjustment, please contact ukrecruitment@capitalone.com all information will be kept confidential and will only be used for the purpose of applying a reasonable adjustment. for technical support or questions about capital one's recruiting process, please send an email to careers@capitalone.com capital one does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site. capital one financial is made up of several different entities. please note that any position posted in canada is for capital one canada, any position posted in the united kingdom is for capital one europe and any position posted in the philippines is for capital one philippines service corp. (copssc). who we are at capital one, we're building a leading information-based technology company. still founder-led by chairman and chief executive officer richard fairbank, capital one is on a mission to help our customers succeed by bringing ingenuity, simplicity, and humanity to banking. we measure our efforts by the success our customers enjoy and the advocacy they exhibit. we are succeeding because they are succeeding. guided by our shared values, we thrive in an environment where collaboration and openness are valued. we believe that innovation is powered by perspective and that teamwork and respect for each other lead to superior results. we elevate each other and obsess about doing the right thing. our associates serve with humility and a deep respect for their responsibility in helping our customers achieve their goals and realize their dreams. together, we are on a quest to change banking for good.",nottingham,Data Scientist,"['deep learning', 'feature engineering', 'machine learning', 'python', 'r', 'statistics', 'time series']","['deep learning', 'feature engineering', 'machine learning', 'python', 'r', 'statistics', 'time series']",
"data scientist, givingtuesday",daro,"about givingtuesday givingtuesday is a global generosity movement unleashing the power of people and organizations to transform their communities and the world. the organization works with partners across sectors and borders to understand the drivers and impacts of generosity, explore giving behaviors and patterns, and use data to inspire more giving around the world. givingtuesday offers the largest philanthropic data collaborative effort in the social sector — with unique, granular datasets from a wide range of organizations featuring key sector information as we scale up, we are expanding our team of data scientists, researchers and engineers, who will continue to grow and improve our unique data assets, methodologies, and technical infrastructure. in pursuit of the goals and expansion of the data commons, givingtuesday partners with key organizations to leverage their expertise to manage and lead different aspects of the work. these data & technology partners (daro, with intent) manage staff, projects, and ongoing functions for the data commons with dedicated staff embedded in givingtuesday in those capacities in cross-functional roles. this role is one of these positions - managed by our partner organizations but embedded in givingtuesday’s data team. data scientist our global data science team works on a diverse set of problems and projects related to learning, insights, and impact measurement in the nonprofit sector. we are looking for a data scientist to join our growing team, where they will work with data engineers, analysts, and other team members to develop compelling and useful knowledge products for givingtuesday stakeholders, including academics, data partners, the social/nonprofit sector, and the general public. in this role you will: • work with a wide range of data types including donation data, transaction records, government and census data, nonprofit tax filings, survey data on perceptions and activity, and philanthropic investment account data, gathered from collaborators and institutional partners in the nonprofit ecosystem • develop quarterly reports on sector-wide trends in monetary giving using transaction records • enhance core data and analytical pipelines by improving data quality validation, automating recurring processes, and implementing methodological updates in workflows to support evolving analytical needs • deliver and write analyses with actionable insights and communicate these findings to cross functional stakeholders of varying technical levels • manage key datasets and improve their usability by creating database dictionaries and user documentation • create impactful data visualisations and interactive data dashboards for stakeholders we are looking for someone with: • demonstrated interest in the nonprofit and philanthropic sector and use of data to promote better social outcomes • advanced analytical skills in a research context, conducting exploratory analysis and mapping data flows, integration of datasets, and reviewing data sources and tools • experience with statistical methods including hypothesis testing, regression analysis, and sampling techniques for the purposes of social science research (such as economics, mixed methods) and/ or business analytics • experience working with scripting languages (python required) and data querying languages (sql preferred) • solid data visualisation skills and an aptitude for translating technical outputs into compelling stories • experience with software development tools and practices (e.g. version control, testing outputs, and applying qa processes) • understanding of legislation around privacy and best practices for securing data • solid relationship management skills, with the ability to collaborate with a variety of internal and external stakeholders on complex research initiatives • outstanding written and oral communication skills in english and an ability to communicate clearly and directly • attention to detail and ability to synthesise diverse datasets givingtuesday is actively seeking candidates with unique and diverse work backgrounds to grow our team. we are especially excited to talk to you if have: • programming skills: python, pyspark, sql, databricks, git, pandas • experience developing and maintaining analytical pipelines, including closely collaborating with data engineering teams • advanced modelling: regressions, clustering, dimensionality reduction, classification, bayesian, time-series analysis, prompt engineering • experience working with data platforms such as databricks (or other forms of cloud data lakes/warehouses/lakehouses) • experience building data exploration tools using code-based frameworks (such as r shiny or streamlit, for example) • an advanced degree in a quantitative research-field (definitely not required!). non-degreed candidates must possess an extensive public record of competent, curiosity-driven data exploration on github, huggingface, kaggle, stackoverflow or similar. location & work hours remote. we are happy to consider applicants based in countries outside of where this is posted. this is a full-time position. we are looking for candidates who can overlap with a 9:00 to 5:00 est work-day, with some flexibility. compensation our compensation is competitive and tailored to align with cost-of-living differences across various regions. we look forward to meeting candidates from diverse backgrounds who can bring unique perspectives to our team! for applicants in the uk, the total annual compensation range for this position is £40,000 - £50,000 per year. application guidelines givingtuesday is committed to a work environment where our employees feel included, valued, and heard. if you require any accessibility accommodation in the interviewing process please let us know. we know that applying for a job takes a lot of time and energy and we treat every application with care and attention. only those applicants who are selected will be contacted. to apply, please provide your resume and a short cover letter describing your interest in the position. we want to hear from you, in your own words. submissions that reflect your personal perspective will stand out more than those written by ai tools. yceajq9sqk",united kingdom,Data Scientist,"['bayesian', 'classification', 'cloud', 'clustering', 'dashboard', 'data lake', 'databricks', 'pandas', 'pyspark', 'python', 'r', 'regression', 'spark', 'sql']","['bayesian', 'classification', 'cloud', 'clustering', 'dashboard', 'data lake', 'databricks', 'pandas', 'pyspark', 'python', 'r', 'regression', 'spark', 'sql']",
data scientist,apply4u,"job description hackajob is collaborating with bet365 to connect them with exceptional tech professionals for this role. company description at bet365, we're one of the world's leading online gambling companies, revolutionising the industry since 2000. founded by denise coates cbe, we now employ over 9,000 people and serve over 100 million customers in 27 languages. our focus on in-play betting has solidified our market-leading position, offering an unmatched experience across 96 sports and 700,000 streaming events. with over 750 concurrent sporting fixtures at peak and more live sports streamed than anyone else in europe, we handle over 6 billion http requests daily and process more than 2 million bets per hour at peak. we empower our employees to push boundaries and explore new ideas, cultivating a culture that celebrates and rewards creativity. this offers employees a wealth of opportunities for growth, giving them the opportunity to make a real impact in the world of online gambling. as a forward-thinking company, we’re breaking new ground in software innovation too, redefining what’s possible for our customers worldwide. job description as a data scientist, you will be responsible for developing machine learning solutions and performing statistical analysis to inform strategic, data-driven business decisions and initiatives. we are seeking a talented and motivated data scientist to join our data analytics team. the department is responsible for monitoring, analysing, and optimising key performance indicators across our range of sports and gaming products. in this role, you will be instrumental in extracting valuable insights from vast datasets, developing predictive models, and contributing to data-driven decision-making across various business functions. you will work collaboratively with stakeholders from areas such as fraud, responsible gambling, trading and branding to identify opportunities, solve complex problems, and build robust data solutions. this is an exciting opportunity to apply cutting-edge data science techniques in a fast-paced, high-volume, and globally recognised industry, utilising a modern and powerful tech stack. this role is eligible for inclusion in the company’s hybrid working from home policy. qualifications • excellent analytical, problem-solving, and critical thinking skills. • phd degree in computer science, statistics, mathematics, or a related quantitative field. • experience using core machine learning techniques, such as regressions, classification, clustering and deep learning. • strong programming skills in languages such as python, r, sql. • familiar with data science libraries and frameworks. • detailed understanding of data mining, data warehousing, and data visualisation techniques. • knowledge of artificial intelligence and its use within data science. • strong communication skills with both technical and non-technical audiences. • knowledge of cloud computing, distributed systems, and big data technologies would be advantageous. additional information • sourcing, cleaning, and validating diverse datasets from various internal and external sources. • conducting in-depth exploratory data analysis to uncover hidden patterns, identify trends, and generate actionable insights that inform strategic business decisions. • developing and deploying robust statistical and machine learning models to address complex business challenges and drive innovative solutions. • designing, implementing, and analysing a/b tests and other controlled experiments to measure the impact of new features, strategies, or models. • contributing to the development and maintenance of scalable data science infrastructure • partnering closely with stakeholders to understand key business goals, and translate them into effective, data-driven solutions. • communicating complex findings and insights to technical and non-technical audiences through visualisations, reports, and presentations. • researching and championing innovative data science techniques, tools, and methodologies. • fostering a culture of continuous learning and innovation within the wider data analytics team. by applying to us you are agreeing to share your personal data in accordance with our recruitment privacy notice - -policy at bet365, we're committed to creating an environment where everyone feels welcome, respected and valued. where all individuals can grow and develop, regardless of their background. we're never ordinary, and we're always striving to be better. if you need any adjustments or accommodations to the recruitment process, at either application or interview, please don’t hesitate to reach out.",staffordshire,Data Scientist,"['classification', 'cloud', 'clustering', 'data analysis', 'data analytics', 'deep learning', 'excel', 'machine learning', 'python', 'r', 'regression', 'scala', 'sql', 'statistics']","['classification', 'cloud', 'clustering', 'data analysis', 'data analytics', 'deep learning', 'excel', 'machine learning', 'python', 'r', 'regression', 'scala', 'sql', 'statistics']",
senior consultant data scientist,ameygroupi,"amey are a leading provider of full life-cycle engineering, operations, and decarbonisation solutions, for transport infrastructure and complex facilities. working for us, you’ll be delivering sustainable infrastructure solutions that enhance life and protect our shared future. our people are driven by a set of strong values, based on safety, insight, and collaboration. the opportunity we have a fantastic opportunity for a senior consultant data scientist to join our digital, data and technology team within our consulting business unit. amey consulting is a leading consultancy that drives innovation through data and technology, enhancing our clients' business operations and processes. we provide solutions that drive insights, improve decision-making, and define a digital strategy that supports our client’s vision, transforming and enabling them to become leaders in a rapidly changing technology and data landscape. as a senior consultant, you will be aspiring leader within our ddat team and will work alongside an experienced pool of other data scientists, solution architects, product owners, project managers, business analysts, software developers and ai experts, among others. you will be responsible for: • working closely with clients to understand the challenges they face, shape these into well-defined requirements and provide analytical solutions • developing approaches and analyses that can be transferred across different sectors, helping clients to make better use of their data and drive intelligent decision-making • exploring new data sets to investigate how they can be used to realise value for clients • developing unique solutions to new and unsolved problems in infrastructure by delivering themselves and/or managing small technical teams • creating and developing relationships with high-profile clients up to director level • opportunity to help with business development to gain new work and expand commissions • collaborating with people from the wider team within the business unit, including software developers and domain experts, to form and support multi-disciplinary teams • continually developing your skills and experience through a combination of formal and on-the-job learning • developing personal expertise and support the development of fellow team members • taking responsibility for the delivery of particular parts of projects to meet or exceed the expectations of the client • ensure project work is delivered in compliance with the project control framework and technical assurance board to agreed standards and targets • ensure compliance with health, safety, environmental and assurance regulations • deputise for principal consultant, when required • understanding, and support where requested, annual business objectives, plans and budgets for business area. • operating according to one amey principles and have a clear understanding of systems and processes • working with others on the team to develop innovative solutions to challenging problems within the infrastructure sector either on existing commissions or by attracting research and development funding what you will bring to us: technical expertise • proven ability to contribute to fast-paced technical projects and quickly understand what needs to be done, even if it’s out of your area of expertise • exposure to managing technical work and directing teams in data & analytics projects • skilled in designing data driver solutions and implementing supervised and unsupervised learning techniques. • experience in model selection, tuning and validation • strong grasp of statistical inference, hypothesis testing and experimental design. • familiarity with ml frameworks such as scikit-learn, pytorch. • ability to interpret and explain complex analysis in simple terms to stakeholders and team members (expert and non-expert alike) • self-sufficient in leveraging python for end-to-end data science tasks from exploration and modelling to automation. • experience/familiarity with agile/scrum delivery. • knowledge of git or other version control systems, and experience using these tools in collaborative software projects • good code hygiene, including modular design, refactoring and testing • you have a bachelor’s degree or higher in a stem subject (or equivalent portfolio/work experience) • you’ve obtained a good knowledge of cloud services e.g. aws, azure (desirable) • you have experience developing web applications for analytics solution (desirable) project controls • demonstrated expertise in balancing financial and technical requirements to ensure successful project management outcomes. • proven experience in leading and managing small to medium teams to deliver projects effectively and efficiently. • skilled in building strong client/stakeholder relationships and providing customer-focused solutions that address key client needs and deliver maximum value. business development • strong communication skills enabling you to build rapport within teams and across diverse clients. • adaptable and flexible, with the ability to thrive in a fast-paced, dynamic environment. • bid writing capabilities and previous experience in tenders is advantageous but not necessary. • a network within your industry, with the aim of developing this further is advantageous but not necessary what we can offer you: at amey, we recognise that our biggest asset is our people. that’s why when you join us, we offer flexibility, career development, a choice of benefits and support that help you through all life’s ups and downs. it’s the reason why investors in people put us among the top 1% of employers. career development – our dedicated progression framework identifies your career pathway and progress within the organisation, and with dedicated mentoring and support on hand to ensure you hit your career milestones. we’re huge advocates for professional development and offer a recognition bonus for those achieving professional qualifications. work-life balance – work-life balance and flexibility are key for our success. we empower our people to make choices that are right for them, with hybrid, part-time and flexible work patterns. and with a network of offices across the uk, we’re open to discussing working options that suit you. wellbeing – health cash plan, 24 gp, support and assistance programmes, wellbeing ambassadors and wellbeing wednesday, dental vouchers edi- at amey we celebrate our people and all that they are. this is reflected in our affinity group networks, providing a community of support and connection, a safe space to share experiences, learn from one another and generate ideas – women @ amey, neurodiversity, armed forces, multicultural network, pride, diversability and parents & carers. social value – you’ll get 2 community involvement days each year to volunteer for a charity of your choice and further opportunities to support fundraising initiatives. plus, a range of other great perks and benefits including: • pension – generous pension scheme which we will contribute to • holidays - minimum 24 days holiday + bank holidays • choices - our flexible benefits scheme is tailored by you, including buying additional annual leave, cycle2work scheme, charity giving and gym membership. • save with amey - our online voucher portal gives you access to thousands of discounts from leading retailers to help you save on shopping, days out, or nights in. your career at amey consulting at amey consulting, we value collective expertise, but also the spark one person can bring. as one of us, you can really be yourself, because your individuality is an asset. you’ll be stretched, but always supported. we’ll recognise your hard work and look after your well-being too. you’ll be empowered to play your part and achieve, encouraged to fulfil your own ambition as well as the shared one. application guidance amey is committed to inclusion and diversity. we welcome applications from all suitably qualified candidates, regardless of their race, gender, disability, religion/belief, sexual orientation, or age. we are also committed to offering applicants with a disability an interview if they meet the minimum requirements for the role. please contact our recruitment team at peopleservices@amey.co.uk to discuss any access needs, reasonable adjustments or additional application support that may be required at any point during the recruitment process. #li-cr1",birmingham,Data Scientist,"['aws', 'azure', 'cloud', 'python', 'pytorch', 'r', 'scikit-learn', 'spark']","['aws', 'azure', 'cloud', 'python', 'pytorch', 'r', 'scikit-learn', 'spark']",
data scientist - summer internship,ncr atleos uk,"join ncr atleos for a 12-week summer internship working on data science projects involving machine learning, data analysis, and model deployment. your role here’s what you will be doing: • analyse large datasets to uncover trends and insights • build and test predictive models using machine learning techniques • support the development of data pipelines and automation tools • create visualizations and dashboards to communicate findings • collaborate with software engineers to integrate models into production systems about you the company is looking for: • a current undergraduate student (penultimate year preferred) studying data science, computer science, mathematics, statistics, or a related field • proficient in python, r, or similar programming languages • familiar with machine learning libraries (e.g., scikit-learn, tensorflow) • comfortable working with sql and cloud-based data platforms • eligible to work in the uk and available full-time from may to august 2026 benefits • competitive internship salary • structured onboarding and mentorship • exposure to global data science practices • networking opportunities with senior engineers and analysts training & development • structured onboarding and mentorship • exposure to global data science practices career progression potential pathway to graduate roles how to apply • applications open in early 2026. • contact the talent acquisition team for more information. • offers of employment are conditional upon the passage of screening criteria applicable to the job. this job may close before the stated closing date, you are encouraged to apply as soon as possible report this job",dundee,Data Scientist,"['cloud', 'dashboard', 'data analysis', 'data pipeline', 'machine learning', 'python', 'r', 'scikit-learn', 'sql', 'statistics', 'tensorflow']","['cloud', 'dashboard', 'data analysis', 'data pipeline', 'machine learning', 'python', 'r', 'scikit-learn', 'sql', 'statistics', 'tensorflow']",
naimuri - senior data scientist,qinetiq,"job title: senior data scientist job location: salford quays, manchester job type: permanent, full-time job id: sf18973 naimuri is offering the chance to help make the uk a safer place through innovation. we partner with government and law enforcement on some of the most challenging data and technology problems out there, and we're looking for a senior data scientist to join our mission. we strongly encourage candidates of all different backgrounds and identities to apply. we are committed to building an inclusive, safe and supportive environment that allows everyone to do their best work. we are happy to support any accessibility or neurodiversity requirements that you may need during the recruitment process. about us: we’ve been around for about ten years and grown from being a little-known tech start-up to creating our own community at the heart of manchester’s thriving tech ecosystem. the name naimuri is japanese and simply means… ‘nai’ meaning ‘not’ ‘muri’ meaning ‘overburden’ this principle guides everything we do, from our technology and processes to our people and culture. we empower our teams to do what they think is right, giving them the confidence to explore new ways of working and deliver the finest solutions in an agile, bias-free environment. our business is focused on 4 cornerstones: wellbeing, empowerment, perpetual edge and delivery. people and culture are at the heart of naimuri, so that collectively, we can realise our mission of ‘making the uk a safer place to be’. about the team: the data capability team at naimuri offers a unique opportunity to apply your skills to impactful projects. it's a rapidly growing, collaborative, and supportive environment where we analyse and investigate data, design solutions to exciting data-driven challenges, and make a real difference for our customers. we are passionate about continuous learning and fostering shared expertise within the team. data scientists within our data capability team are often working on: • analysing customer requirements in long-term projects and new bid work to uncover opportunities for customers to leverage their data. • analysing and modelling customer data, performing statistical analyses, designing cleansing, transformation and normalisation processes, performing feature extraction/reduction, and designing solutions and opportunities. • performing, visualising, and presenting data analyses and analytics to customers and project leads, including on-site. • engineering platforms, databases, and data pipelines as part of broader delivery solutions. • training (inc. transfer learning and feature extraction) and deploying ml/ai models for prediction, detection, classification, etc. • writing or supporting software solutions that implement data science models, tools, and techniques. about the role: as a senior data scientist, you will help maintain our strong reputation for delivering robust solutions by taking a conscientious and scientific approach to customer data. you will use your strong problem-solving skills to design and develop innovative techniques and tools in an agile manner. working collaboratively with other data scientists, engineers, and developers, you will research, experiment, analyse and visualise complex data, presenting your findings to customers and colleagues. a key part of this role is mentoring and supporting earlier-career colleagues, helping to foster a culture of continuous learning and shared expertise across the team. you will work closely with customers, other data scientists, data architects, data engineers, software developers, and testers to: • investigate, transform (with provenance), and model customer data, and potentially create synthetic data in lieu. • apply statistical methods to analyse customer data and be able to report that analysis to co-workers, customers, and project leads. • identify opportunities to apply, design and build algorithms to transform and interrogate data. • visualise and communicate data and model and algorithm outputs for audiences of different levels of understanding. • use data science techniques, including ml/ai, to design and build solutions to customer problems, and work with software developers, data engineers and testers to implement and assure them. • work with data engineers and platform engineers to design, implement and test data ingest pipelines. • work with other data scientists and ml and platform engineers to design, train, test and deploy ml/ai models. • test and compare the effectiveness of different mathematical and computational techniques for working with data. • conduct research into the application or development of new data science techniques, potentially collaborating with our expansive academic network, and co-supervising masters and phd candidates. • experiment design and execution/running, and communication of the experiment plan. about you we're looking for someone who: • has significant industry experience as a data scientist and is passionate about data, with opinions on the best ways of working, techniques, and tooling. • has experience leading a team or project and wants to help others develop and learn. • takes a conscientious, curious, and scientific approach to their work. • continually learning about state-of-the-art techniques in technology, academic, and industry articles. • possesses strong analytical problem-solving abilities to design and develop innovative data science solutions. • can communicate and present complex ideas and findings to diverse audiences, including customers, executives, and non-specialists. • has performed deep dives into data and presented the results of analysis and modelling using tools like jupyter notebooks. • has experience designing and developing data ingestion and transformation pipelines in languages like python, potentially using cloud solutions in aws, azure, or gcp. • is familiar with the full lifecycle of ml/ai models, including collating training data, design, training, evaluation, and deploying automated pipelines. • has experience helping to transform or implement an organisation's data science strategy. • is comfortable designing and executing experiment plans and communicating them to stakeholders. nice to haves: • experience with any of the following specialisms: data synthesis, test and evaluation, ai assurance, knowledge graphs and ontologies, data governance and compliance, or deepfake detection. • creating python-based applications and/or apis. • a degree in a field like data science, physics, computational science, mathematics, or statistics (though we value demonstrable experience just as much!). location: our head office is based in salford quays, manchester, with satellite teams currently in london and gloucestershire. we offer hybrid working where you can work from home for part of your working week with time on site being based on the needs of your assigned delivery and agreed ways of working for your team. this would normally be a maximum of one or two days per week but you would be welcome to spend more days in the office if you preferred. pay and benefits: naimuri pays competitively within the industry based on your role's base location rates. the salary for this position is dependent upon your experience. we assess seniority relative to the team at naimuri during the interviewing process. a full time working week is 37.5 hours and you have flexibility over when you give that time. we also offer part-time working which can be discussed during the recruitment process. our core hours are 10:00am - 3:00pm and our office hours are between 7:30 and 18:00 monday to friday. benefits include: ● flexible/hybrid working options ● a company performance related bonus ● pension matched 1.5x up to 10.5% ● axa group 1 medical cover ● personal training budget ● holiday buy-back scheme ● a flexible benefits scheme recruitment process: we want to ensure that you feel comfortable and confident when interviewing with us. to help you prepare, our recruitment team will discuss the process in more detail with you when you apply. we are happy to support any accessibility or neurodiversity requirements.",manchester,Data Scientist,"['aws', 'azure', 'classification', 'cloud', 'data pipeline', 'gcp', 'python', 'r', 'statistics']","['aws', 'azure', 'classification', 'cloud', 'data pipeline', 'gcp', 'python', 'r', 'statistics']",
data scientist manager in belfast,energy jobline zr,"energy jobline is the largest and fastest growing global energy job board and energy hub. we have an audience reach of over 7 million energy professionals, 400,000+ monthly advertised global energy and engineering jobs, and work with the leading energy companies worldwide. we focus on the oil & gas, renewables, engineering, power, and nuclear markets as well as emerging technologies in ev, battery, and fusion. we are committed to ensuring that we offer the most exciting career opportunities from around the world for our jobseekers. job description join kainos and shape the future at kainos, we’re problem solvers, innovators, and collaborators - driven by a shared mission to create real impact. whether we’re transforming digital services for millions, delivering cutting-edge workday solutions, or pushing the boundaries of technology, we do it together. we believe in a people-first culture, where your ideas are valued, your growth is supported, and your contributions truly make a difference. here, you’ll be part of a diverse, ambitious team that celebrates creativity and collaboration. ready to make your mark? join us and be part of something bigger. job profile description kainos is recognised as one of the uk’s leading ai and data businesses, with a decade-long track record of delivering impactful, production-grade ai solutions for clients across government, healthcare, defence and commercial sectors. kainos is at the forefront of ai innovation, trusted by microsoft, aws, and others to deliver advanced ai and data solutions at scale. our 150-strong ai and data practice brings together deep expertise in machine learning, generative ai, agentic ai and data. we are pioneers in responsible ai, having authored the uk government’s ai cyber security code of practice implementation guide and we partner with leading organisations to ensure ai is deployed ethically, securely and with measurable business value. our teams are at the cutting edge of ai research, and delivery, it is truly an exciting team to join kainos as we further grow our ai capability. main purpose of the role & responsibilities in the business: as a data scientist manager at kainos, you’ll be responsible for successful delivery of advanced ai solutions leveraging state-of-the-art machine learning, generative and agentic ai technologies. you will drive the adoption of modern ai development and scalable cloud- architectures. your role will involve technical leadership, engaging with senior stakeholders to agree architectural principles, strategic direction and system architecture. as a technical leader within kainos and wider industry, you will foster a culture of innovation, continuous learning, and engineering excellence. you will manage, coach and develop a team, with a focus on development of standards and policies, enduring customer relationships and embedding commercial acumen. you will also provide direction and leadership for your team as you solve challenging problems together. minimum (essential) requirements • a minimum of a 2.1 degree in computer science, ai, data science, statistics or in a similar quantitative field. • proven experience of leading multi-disciplinary teams to deliver high quality ai/ml solutions. • demonstrable experience of technical leadership for ai delivery including architecture, product design principles and engineering excellence. • have a deep understanding and developing of ai/ml models, including time series, supervised/unsupervised learning, reinforcement learning and llms. • experience with the latest ai engineering approaches such as prompt engineering, retrieval-augmented (rag) and agentic ai. • strong python skills with a grounding in software engineering best practices (ci/cd, testing, code reviews etc). • expertise in data engineering for ai: handling large-scale, unstructured, and multimodal data. • understanding of responsible ai principles, model interpretability and ethical considerations. • strong interpersonal skills with the ability to lead client projects, manage c-level stakeholders and establish requirements/architecture concepts. • we are passionate about developing people, you will bring experience in managing, coaching and developing junior members of a team and wider community. desirable • demonstrable experience with modern deep learning frameworks (e.g. pytorch, tensorflow), fine-tuning or distillation of llms (e.g. gpt, llama, claude, gemini), machine learning libraries (e.g. scikit-learn, xgboost). • experience with data storage for ai, vector databases, semantic search and knowledge graphs. • actively contributes to open-source ai projects, research publications and industry events/websites. • familiarity with ai security, privacy, and compliance standards e.g. iso42001. embracing our at kainos, we believe in the power of , equity and . we are committed to building a team that is as diverse as the world we live in, where everyone is valued, respected, and given an equal chance to thrive. we actively seek out talented people from all backgrounds, regardless of , , , , , , , or any other characteristic that makes them who they are. we also believe every candidate deserves a level playing field. our friendly talent acquisition team is here to support you every step of the way, so if you require any accommodations or adjustments, we encourage you to reach out. we understand that everyone's journey is different, and by having a private conversation we can ensure that our recruitment process is tailored to your needs. if you are interested in applying for this job please press the apply button and follow the application process. energy jobline wishes you the very best of luck in your next career move.",belfast,Data Scientist,"['aws', 'cloud', 'deep learning', 'excel', 'machine learning', 'python', 'pytorch', 'r', 'scala', 'scikit-learn', 'statistics', 'tensorflow', 'time series', 'xgboost']","['aws', 'cloud', 'deep learning', 'excel', 'machine learning', 'python', 'pytorch', 'r', 'scala', 'scikit-learn', 'statistics', 'tensorflow', 'time series', 'xgboost']",
(senior) forecasting data scientist (m/f/d),sefe securing energy for europe gmbh,"in short are you an experienced data scientist looking for an opportunity to use your expertise in taking our forecasting models to the next level? our portfolio modelling & forecasting team in manchester develop models that support our gas and power energy portfolios and are looking for an experienced data scientist to work with key stakeholders to develop tailored insight into demand and portfolio forecasts. what will you do working with key stakeholders in risk, sales portfolio optimisation, quant execution and risk you will • build, develop, and deploy forecasting models to support business requirements • collaborate with key stakeholders on portfolio insights and sensitivity modelling • advance the monitoring of model accuracy and performance • maintain a continuous learning approach to ensure solutions remain up to date what will you bring you will bring proven experience in a data science role demonstrating the ability to translate business challenges into analytical problems and develop data driven solutions. self-motivated with strong organisational skills you will have effective communication skills (both written and verbal) with the ability to position your message appropriately for the audience through effective presentation skills. you will also demonstrate the following • high level proficiency in python, sql, and machine learning techniques • advanced analytical competencies • proven experience in data science across power or broader energy industry (desirable) • experienced in the delivery of time series forecasting and analysis (desirable) about us securing energy for europe – it’s a simple statement, with a bold ambition. sefe is not just our name, but also encompasses everything that drives us. to accomplish this, we’re taking immediate action to secure gas supply – but also looking forward, to explore our role in the european energy transformation and how we can contribute to a stable and sustainable future. sefe, an international energy company, ensures the security of supply and drives the decarbonisation of its customers. sefe’s activities span the energy value chain, from origination and trading to sales, transport, and storage. through its decades-long expertise in trading and the development of its lng business, sefe has become one of the most important suppliers to industrial customers in europe, with an annual sales volume of 200 twh of gas and power. its 50,000 customers range from small businesses to municipalities and multinational organisations. by investing in clean energies and especially in the hydrogen ecosystem, sefe is contributing to the energy transition. the company employs around 2,000 people globally and is owned by the federal government of germany. our international teams work across locations in europe, asia, and north america. we’re passionate about energy and the important role it can play in shaping a better future. securing energy – now and for the future. our benefits in return we offer a competitive starting salary supported by a comprehensive range of financial, lifestyle and wellness benefits with the flexibility to follow a hybrid working model. • bonus earning potential • non-contributory pension with 10% employer contribution • 25 days holiday plus bank holidays and volunteering days • buy / sell holidays • life assurance • medical and dental insurance (family cover) • range of optional flexible benefits we are committed to supporting your career growth with opportunities to develop both your knowledge and experience through a blended approach to learning. join sefe and help us secure energy supply across europe and shape a better, more sustainable tomorrow. #li-jg1",manchester,Data Scientist,"['machine learning', 'python', 'r', 'sql', 'time series']","['machine learning', 'python', 'r', 'sql', 'time series']",
data scientist no experience necessary,newto training,"are you ready to start a new career in data analysis?the demand for data analysts has grown by 20% annually, with experienced professionals earning salaries upwards of £58,000.in today’s digital world, data is critical to business decision-making, making the role of a data analyst indispensable. as skills shortages continue to grow, the demand for qualified entry-level professionals is on the rise.with our data analytics career programme we will provide you with:8 training modules: excel, sql, python, r, tableau, power bi, comptia data+ & azure ai fundamentals3 official examinations: microsoft power bi data analyst, comptia data+, & microsoft azure ai fundamentals100+ hours of live instructor-led online classroom trainingreal-world data analyst project work & live labs to boost your cvexam & interview preparationjob guarantee with a salary up to £35,000course cost - £2495, or, £207.91 per monthwe guarantee you will be offered a job upon completion, or we will refund you 100% of your course fees.no prior industry experience is required - no matter your background, previous studies or work history - if you think you have the soft skills (communication skills, passion) needed then we can help you launch the career you want.click 'apply now’ to begin your new data career",united kingdom,Data Scientist,"['azure', 'data analysis', 'data analytics', 'excel', 'power bi', 'python', 'r', 'sql', 'tableau']","['azure', 'data analysis', 'data analytics', 'excel', 'power bi', 'python', 'r', 'sql', 'tableau']",£20k–£50k a year
data scientist,hays,"£43,001 - £47,779 per annum, flexible hybrid working pattern (2 days per week in office), 35-hour week, 39 days annual leave (including statutory days), good pension scheme and other generous benefits this post is subject to dbs clearance. hays technology are working in partnership with a large public sector organisation in coalville to recruit a data scientist to join their technology team on a permanent basis. the successful candidate will focus on leveraging data analytics to drive insights and improve the quality and efficiency of services by cleaning and organising data. this role involves working closely with various stakeholders to extract, analyse, and interpret complex data sets to inform decision-making and policy development. principal duties and responsibilities: • collect and analyse data from internal systems (tenancy, maintenance, finance) and external sources (e.g. census, public datasets). • clean, structure, and validate data to ensure accuracy and usability. • build models to forecast housing demand, rent arrears, and maintenance needs. • create dashboards and reports to communicate insights to non-technical stakeholders. • assess the impact of housing initiatives and recommend improvements. • use ml to optimise resource allocation, predict tenant behaviour, and automate processes like arrears risk scoring. • maintain data quality, security, and compliance with gdpr and other regulations. • work with housing officers and managers to translate operational needs into data-driven solutions. in order to apply, you must have the following skills and experience: • industry certifications in data science or related fields (e.g., microsoft certified: azure data scientist, google professional data engineer) or equivalent experience. • experience working as a data scientist, ideally within social housing, public sector, or a related industry. • experience working with social housing data systems (e.g., mri, northgate, civica, or orchard) and the ability to apply advanced analytics to operational challenges in housing (desirable). • demonstrated experience in using machine learning, predictive modelling, and statistical analysis to solve real-world problems. • expertise in statistical modelling, predictive analytics, clustering, classification, and regression techniques. • strong background in data mining, pattern recognition, and anomaly detection to improve service delivery. • proficient in python, r, or other relevant programming languages used for data science. • strong skills in sql and experience working with large databases and data warehouses. • ability to create intuitive and informative visualisations using tools such as power bi, tableau, or similar platforms. • familiarity with cloud-based data platforms (e.g., azure, aws) and deployment of models in a production environment. if you have the relevant experience and would like to apply, please submit your cv. #4712887 - daney bowen",coalville,Data Scientist,"['aws', 'azure', 'classification', 'cloud', 'clustering', 'dashboard', 'data analytics', 'data warehouse', 'machine learning', 'power bi', 'python', 'r', 'regression', 'sql', 'tableau']","['aws', 'azure', 'classification', 'cloud', 'clustering', 'dashboard', 'data analytics', 'data warehouse', 'machine learning', 'power bi', 'python', 'r', 'regression', 'sql', 'tableau']",
data scientist,seismic recruitment,"data scientist ‎‎desford - fully onsite initial 12 month contract, could be ongoing for the right person! 37.5 hours per week paye and umbrella rates available (discussed upon application) join our client - one of the largest off-highway machinery companies in the world, as a data scientist, where your expertise will play a key role in researching, developing, and delivering cutting-edge analytical and reporting solutions for a broad range of applications within the purchasing department. you'll work alongside a fantastic team where you can make a real impact! key responsibilities: • provide and maintain visually insightful live dashboards and reports; create and maintain dashboards and data presentations • create, clean, and analyse datasets using safe, reliable data capture methods where needed, using tools such as powerapps • extract information from internal systems using sql or edfl to push data into power bi • create and format data structures within safe databases and/or internal systems like snowflake or sharepoint • continuously enhance data collection, storage, and analysis methods what we are looking for: • strong understanding of statistical analysis and data visualization tools (e.g., tableau, power bi) • degree in computer science, data science, mathematics, or a related field • experience in coding and development using programming languages such as python or sql • strong ability to extract, interpret and understand data • high proficiency in powerapps, sharepoint and the microsoft office suite • exceptional problem-solving abilities with excellent attention to detail to apply, please submit a copy of your up to date cv clearly indicating your relevant experience. applicants must have an existing right to work in the uk and evidence of eligibility will be required. suitable candidates will be contacted. the above represents a summary of the contract assignment. a full description of this contract assignment is available. a full explanation of this rate and all deductions will be explained in a key information document (kid) supplied to registered candidates.",leicester,Data Scientist,"['dashboard', 'excel', 'power bi', 'python', 'r', 'snowflake', 'sql', 'tableau']","['dashboard', 'excel', 'power bi', 'python', 'r', 'snowflake', 'sql', 'tableau']",
senior data science consultant,experian,"we have a new vacancy for an experienced senior data science / analytics consultant to join our analytics team and supporting with our cloud-based ascend platform you will partner with clients to understand their business, identify what data is required and how clients can best use experian data analytics to improve business outcomes. responsibilities include: • design analytics solutions to client's problems in any area of consumer lending and credit risk management, using experian analytics solutions. • engage in a consultative way with the client, to identify problems and define, design and deliver analytics solutions, with expertise in credit risk modelling and optimisation techniques. • present proposals to clients for analytics solutions, including recommendations. • provide consultancy on the potential 'bigger picture' strategies. • co-ordinate with experian's analytics pre-sales team to contribute to sales opportunities and support the conversion of sales prospects. about experian experian is a global data and technology company, powering opportunities for people and businesses around the world. we help to redefine lending practices, uncover and prevent fraud, simplify healthcare, create marketing solutions, and gain deeper insights into the automotive market, all using our unique combination of data, analytics and software. we also assist millions of people to realise their financial goals and help them save time and money. we operate across a range of markets, from financial services to healthcare, automotive, agribusiness, insurance, and many more industry segments. we invest in people and new advanced technologies to unlock the power of data. as a ftse 100 index company listed on the london stock exchange (expn), we have a team of 22,500 people across 32 countries. our corporate headquarters are in dublin, ireland. learn more at experianplc.com experience and skills • strong analytical modelling and consultancy experienced gained in credit risk management or banking sector as a consultant, data scientist or machine learning engineer. • applied modelling and analytics experience to lead business decisions • expertise in credit risk decisioning. • deep coding knowledge in python with sas or r. • good stakeholder management skills. • subject matter expert on the mechanics of consumer lending (risk, data usag, outcomes) • knowledge of cloud / aws • product strategy experience desirable but not essential. additional information benefits package includes: • hybrid working • great compensation package • core benefits include pension, bupa healthcare, sharesave scheme and more • 25 days annual leave with 8 bank holidays and 3 volunteering days. you can purchase additional annual leave. our uniqueness is that we celebrate yours. experian's culture and people are important differentiators. we take our people agenda very seriously and focus on what truly matters; dei, work/life balance, development, authenticity, engagement, collaboration, wellness, reward & recognition, volunteering... the list goes on. experian's people first approach is award winning; great place to work™ in 24 countries, fortune best companies to work and glassdoor best places to work (globally 4.4 stars) to name a few. check out experian life on social or our careers site to understand why. experian is proud to be an equal opportunity and affirmative action employer. innovation is a critical part of experian's dna and practices, and our diverse workforce drives our success. everyone can succeed at experian and bring their whole self to work, irrespective of their gender, ethnicity, religion, colour, sexuality, physical ability or age. if you have a disability or special need that requires accommodation, please let us know at the earliest opportunity. grade: c/eb7 #li-dsi #li-hybrid experian careers - creating a better tomorrow together find out what its like to work for experian by clicking here",nottingham,Data Scientist,"['aws', 'cloud', 'data analytics', 'machine learning', 'python', 'r', 'recommendation', 'sas']","['aws', 'cloud', 'data analytics', 'machine learning', 'python', 'r', 'recommendation', 'sas']",
data scientist,kellanova,"at kellanova, data isn’t just numbers—it’s the fuel that powers smarter decisions and bold growth. we’re looking for a data scientist to join our dynamic team in manchester and help shape the future of how we leverage insights across e-commerce, sales, marketing, and revenue growth management (rgm). this is a 12-month fixed-term contract role that offers hybrid role, that offers flexibility and collaboration, giving you the chance to work on cutting-edge analytics projects that make a real impact. if you love turning complex data into actionable strategies, thrive in a fast-paced environment, and want to influence decisions that touch millions of consumers, this is your opportunity to shine. you’ll partner with cross-functional teams, apply advanced analytics, and bring fresh thinking to every challenge. a taste of what you’ll be doing e-commerce analytics: develop predictive models to optimise online sales performance, pricing strategies, and digital shelf visibility—helping our brands stand out in the digital marketplace. sales forecasting: build robust demand forecasting models to support trade planning, inventory optimisation, and flawless sales execution. marketing effectiveness: use advanced statistical and machine learning techniques to measure campaign roi, refine customer segmentation, and personalise experiences that drive engagement. revenue growth management (rgm): apply scenario modelling and elasticity analysis to optimise pricing, promotions, and assortment strategies for maximum impact. data integration: consolidate and harmonise data from multiple sources—pos, crm, digital platforms, syndicated data—into unified insights that empower smarter decisions. we’re looking for someone with master’s degree in a stem or related field (data science, mathematics, computer science, engineering). proficiency in python, r, sql, and data visualisation tools such as power bi. experience with machine learning frameworks (scikit-learn, tensorflow, pytorch). ability to present technical findings to non-technical stakeholders in a compelling and actionable way. what’s next after you apply, your application will be reviewed by a real recruiter, so it may take us a few weeks to get back to you by email or phone. visit our how we hire page to get insights into our hiring process and more about what we offer. need assistance throughout the application or hiring process? email european.recruitment@kellanova.com if you join our team, you’ll be rewarded for the difference you make. our comprehensive benefits offer you the support you need through your life events, big or small. visit our benefits page & be sure to ask your recruiter for more specific information. get to know us at kellanova, we are driven by our vision to be the world’s best-performing snacks-led powerhouse, unleashing the full potential of our differentiated brands and our passionate people. our portfolio of iconic, world-class brands include pringles, cheez-it, pop-tarts, morningstar farms, special k, krave, zucaritas, tresor, crunchy nut, among others. kellanova’s culture of best means we bring our best to all that we do in pursuit of our vision to be the world’s best performing snacks-led powerhouse. our culture celebrates boldness and empowers our people to challenge the status quo, achieve results, and win together. our focus on equity, diversity, and inclusion (ed&i) enables us to build a culture of belonging where all employees have a place at the table and are inspired to share their passion, talents and ideas to work. mars has agreed to acquire kellanova in a combination that will shape the future of snacking! the transaction is anticipated to close towards the end of 2025 (subject to customary closing conditions, including regulatory approvals). the companies remain separate until closing. you can learn more at www.futureofsnacking.com and our hiring teams will be happy to discuss further questions if your application advances in the hiring process. what does it take to be the best? someone like you. kellanova is an equal opportunity employer that strives to provide an inclusive work environment, a seat for everyone at the table, and embraces the diverse talent of its people. all qualified applicants will receive consideration for employment without regard to race, color, ethnicity, disability, religion, national origin, gender, gender identity, gender expression, marital status, sexual orientation, age, protected veteran status, or any other characteristic protected by law. for more information regarding our efforts to advance equity, diversity & inclusion, please visit our website here .",stretford,Data Scientist,"['machine learning', 'power bi', 'python', 'pytorch', 'r', 'scikit-learn', 'sql', 'tensorflow']","['machine learning', 'power bi', 'python', 'pytorch', 'r', 'scikit-learn', 'sql', 'tensorflow']",
data scientist,job spark,"role: data scientist location: remote we are hiring exceptional data scientists to support a leading ai initiative focused on building high-performance machine learning systems. you'll work on hands-on data science tasks—from designing experiments and preprocessing datasets to building, evaluating, and refining models. you'll also help develop and validate prompt-based questions that train advanced ai models. what you'll do • design experiments and run end-to-end data workflows • gather, clean, preprocess, and analyze large datasets • build and evaluate predictive and ml models • collaborate with engineering teams to support production-ready solutions • write, review, and refine prompts used for ai training what you'll bring • 3+ years of professional experience in data science or applied analytics • strong python skills (jupyter, numpy, pandas, scipy, scikit-learn, torch, tensorflow) • bachelor's degree in a quantitative field from the u.s., canada, uk, australia, or new zealand • expertise in eda, statistical inference, model evaluation, feature engineering, or experimentation • excellent communication and attention to detail opportunity highlights • 20–40 hours/week, with potential to scale up • fully remote and asynchronous • minimum 1-month engagement with possible extension • start date: early september compensation • independent contractor role • weekly payments via stripe connect apply now!",bradford,Data Scientist,"['excel', 'experimentation', 'feature engineering', 'machine learning', 'numpy', 'pandas', 'python', 'r', 'scikit-learn', 'tensorflow']","['excel', 'experimentation', 'feature engineering', 'machine learning', 'numpy', 'pandas', 'python', 'r', 'scikit-learn', 'tensorflow']",
senior data scientist - healthcare,kainos,"join kainos and shape the future at kainos, we’re problem solvers, innovators, and collaborators - driven by a shared mission to create real impact. whether we’re transforming digital services for millions, delivering cutting-edge workday solutions, or pushing the boundaries of technology, we do it together. we believe in a people-first culture, where your ideas are valued, your growth is supported, and your contributions truly make a difference. here, you’ll be part of a diverse, ambitious team that celebrates creativity and collaboration. ready to make your mark? join us and be part of something bigger. job profile description kainos is recognised as one of the uk’s leading ai and data businesses, with a decade-long track record of delivering impactful, production-grade ai solutions for clients across government, healthcare, defence, and commercial sectors. kainos is at the forefront of ai innovation, trusted by microsoft, aws, and others to deliver advanced ai and data solutions at citizen scale. our 150-strong ai and data practice brings together deep expertise in machine learning, generative ai, agentic ai and data. we are pioneers in responsible ai, having authored the uk government’s ai cyber security code of practice implementation guide and we partner with leading organisations to ensure ai is deployed ethically, securely and with measurable business value. our teams are at the cutting edge of ai research, and delivery, it is truly an exciting team to join kainos as we further grow our ai capability. main purpose of the role & responsibilities in the business: as a senior data scientist at kainos, you will be building advanced ai solutions leveraging state-of-the-art machine learning, generative and agentic ai technologies. you will drive the adoption of modern ai frameworks, aiops best practices and scalable cloud-native architectures. your role will involve hands-on technical delivery, collaborating with customers to translate business challenges into trustworthy ai solutions and ensuring responsible ai practices throughout. as a technical mentor, you will foster a culture of innovation, continuous learning, and engineering excellence. it is a fast-paced environment, so it is important for you to make sound, reasoned decisions. you will do this whilst learning about new technologies and approaches, with talented colleagues that will help you to develop and grow. you will support your colleagues and more junior developers, providing direction support as you solve challenging problems together. minimum (essential) requirements: • a minimum of a 2.1 degree in computer science, ai, data science, statistics or in a similar quantitative field. • have a deep understanding and developing of ai/ml models, including time series, supervised/unsupervised learning, reinforcement learning and llms. • experience with the latest ai engineering approaches such as prompt engineering, retrieval-augmented generation (rag), and agentic ai. • strong python skills with a grounding in software engineering best practices (ci/cd, testing, code reviews etc). • expertise in data engineering for ai: handling large-scale, unstructured, and multimodal data. • understanding of responsible ai principles, model interpretability, and ethical considerations. • strong interpersonal skills and team working. desirable: • demonstrable experience with modern deep learning frameworks (e.g. pytorch, tensorflow), fine-tuning or distillation of llms (e.g., gpt, llama, claude, gemini), machine learning libraries (e.g. scikit-learn, xgboost). • experience with data storage for ai, vector databases, semantic search, and knowledge graphs. • actively contributes to open-source ai projects, research publications, and industry events/websites. • familiarity with ai security, privacy, and compliance standards e.g. iso42001. embracing our differences at kainos, we believe in the power of diversity, equity and inclusion. we are committed to building a team that is as diverse as the world we live in, where everyone is valued, respected, and given an equal chance to thrive. we actively seek out talented people from all backgrounds, regardless of age, race, ethnicity, gender, sexual orientation, religion, disability, or any other characteristic that makes them who they are. we also believe every candidate deserves a level playing field. our friendly talent acquisition team is here to support you every step of the way, so if you require any accommodations or adjustments, we encourage you to reach out. we understand that everyone's journey is different, and by having a private conversation we can ensure that our recruitment process is tailored to your needs.",belfast (+3 others),Data Scientist,"['aws', 'cloud', 'deep learning', 'excel', 'machine learning', 'python', 'pytorch', 'r', 'scala', 'scikit-learn', 'statistics', 'tensorflow', 'time series', 'xgboost']","['aws', 'cloud', 'deep learning', 'excel', 'machine learning', 'python', 'pytorch', 'r', 'scala', 'scikit-learn', 'statistics', 'tensorflow', 'time series', 'xgboost']",
data scientist,teksystems,"description we are seeking a data scientist to join a growing r&d team focused on developing ai-driven solutions that deliver measurable business value. this role will involve working on a variety of use cases across core technology functions, with opportunities to collaborate on additional projects across the organization. key responsibilities • create and solution schemas and ontologies before moving into graph creation • highly experienced with creating knowledge graphs (rdf & property graphs) • knowledge of when to use the correct graph in specific situations • decision-making on technical stack when doing creation work • example: choosing google spanner for graph creation to enable vector store capabilities • strong understanding of fine-tuning and inferencing • adjusting model weights • experience working with gpus and expected loads • ability to collaborate with different business areas to understand data requirements and data pipelines required skills • vector stores • knowledge graphs • fine-tuning • gpu optimization and inferencing • data science • experience working with llm's and ai (generative/agentic) job title: data scientist location: sheffield, uk job type: contract trading as teksystems. allegis group limited, maxis 2, western road, bracknell, rg12 1rt, united kingdom. no. 2876353. allegis group limited operates as an employment business and employment agency as set out in the conduct of employment agencies and employment businesses regulations 2003. teksystems is a company within the allegis group network of companies (collectively referred to as ""allegis group""). aerotek, aston carter, easi, talentis solutions, teksystems, stamford consultants and the stamford group are allegis group brands. if you apply, your personal data will be processed as described in the allegis group online privacy notice available at https://www.allegisgroup.com/en-gb/privacy-notices. to access our online privacy notice, which explains what information we may collect, use, share, and store about you, and describes your rights and choices about this, please go to https://www.allegisgroup.com/en-gb/privacy-notices. we are part of a global network of companies and as a result, the personal data you provide will be shared within allegis group and transferred and processed outside the uk, switzerland and european economic area subject to the protections described in the allegis group online privacy notice. we store personal data in the uk, eea, switzerland and the usa. if you would like to exercise your privacy rights, please visit the ""contacting us"" section of our online privacy notice at https://www.allegisgroup.com/en-gb/privacy-notices for details on how to contact us. to protect your privacy and security, we may take steps to verify your identity, such as a password and user id if there is an account associated with your request, or identifying information such as your address or date of birth, before proceeding with your request. if you are resident in the uk, eea or switzerland, we will process any access request you make in accordance with our commitments under the uk data protection act, eu-u.s. privacy shield or the swiss-u.s. privacy shield.",sheffield,Data Scientist,"['data pipeline', 'r']","['data pipeline', 'r']",
data scientist iii - experimentation science (statistical methodologies),expedia group,"introduction to team: as a data scientist iii on our experimentation science team, you will play a critical role in shaping the statistical methodologies that underpin expedia group’s experimentation platform. your work will ensure that every feature change on our website—from button colors to booking flows—is rigorously tested to maximize business impact. this role blends research and practical application, requiring you to design, evaluate, and improve hypothesis testing frameworks and experimentation techniques. you will collaborate closely with platform engineers to implement scalable solutions, provide expert guidance to stakeholders, and drive innovation through cutting-edge statistical research. this position offers a unique opportunity to apply deep statistical expertise in a dynamic environment where your contributions directly influence product decisions and customer experiences worldwide. in this role, you will: • lead the development and validation of advanced statistical methodologies to support expedia group’s experimentation platform, ensuring robust and reliable a/b testing across the website’s features and user experiences • collaborate closely with the experimentation platform team to design, implement, and scale new testing frameworks and tools that enhance the platform’s capabilities and reporting accuracy • provide expert guidance and training to cross-functional teams on experimental design, hypothesis testing, and statistical best practices to optimize test duration, sample size, and overall experiment quality • conduct original research to explore and develop innovative approaches for experimentation challenges, including designing proof-of-concept studies and integrating new methodologies into production workflows • analyze complex experimental data, build simulation frameworks to quantify error rates, and communicate findings clearly to both technical and non-technical stakeholders to inform business decisions what we're looking for: • deep expertise in statistical methodologies, particularly hypothesis testing, design of experiments, and interpretation of p-values and confidence intervals, with the ability to apply these concepts rigorously in an experimentation context • strong analytical mindset with the capability to structure complex business problems and translate them into statistically sound experimental designs and solutions • proficiency in programming with experience in python, pyspark, r, or similar languages, sufficient to implement and test statistical models and simulations, with an emphasis on correctness over advanced coding skills • demonstrated ability to conduct independent research, quickly learn new statistical techniques, and develop proof-of-concept models that can be scaled and integrated into production experimentation platforms • excellent communication skills to clearly explain complex statistical concepts and experimental results to both technical and non-technical stakeholders, ensuring alignment and understanding across teams experience and qualifications: • a bachelor’s degree or higher in mathematics, statistics, or a closely related quantitative field, providing a strong foundation in statistical theory and experimental design • proven expertise in hypothesis testing, design of experiments, and statistical methodologies critical to experimentation science • solid programming skills in at least one language such as python, r, or pyspark, with the ability to write clean, maintainable code to support research and implementation • experience applying statistical rigor to real-world business problems, with the ability to independently lead and deploy methodologies at scale • strong analytical mindset with the capacity to research, evaluate, and implement new statistical approaches and simulation frameworks • excellent communication skills to clearly explain complex statistical concepts and results to both technical and non-technical stakeholders • ability to work collaboratively within a small, focused team and liaise effectively with platform engineers and business partners • prior experience in experimentation platforms or a/b testing environments is highly desirable • autonomy and accountability to serve as a point of contact for statistical methodologies within the team expedia group brands power global travel for everyone, everywhere. we design cutting-edge tech to make travel smoother and more memorable, and we create groundbreaking solutions for our partners. our diverse, vibrant, and welcoming community is essential in driving our success. why join us? to shape the future of travel, people must come first. guided by our values and leadership agreements, we foster an open culture where everyone belongs, differences are celebrated and know that when one of us wins, we all win. we provide a full benefits package, including exciting travel perks, generous time-off, parental leave, a flexible work model (with some pretty cool offices), and career development resources, all to fuel our employees' passion for travel and ensure a rewarding career journey. we’re building a more open world. join us. accommodation requests if you need assistance with any part of the application or recruiting process due to a disability, or other physical or mental health conditions, please reach out to our recruiting accommodations team through the accommodation request. we are proud to be named as a best place to work on glassdoor in 2024 and be recognized for award-winning culture by organizations like forbes, time, disability:in, and others. expedia group's family of brands includes: brand expedia®, hotels.com®, expedia® partner solutions, vrbo®, trivago®, orbitz®, travelocity®, hotwire®, wotif®, ebookers®, cheaptickets®, expedia group™ media solutions, expedia local expert®, carrentals.com™, and expedia cruises™. © 2024 expedia, inc. all rights reserved. trademarks and logos are the property of their respective owners. cst: 2029030-50 employment opportunities and job offers at expedia group will always come from expedia group’s talent acquisition and hiring teams. never provide sensitive, personal information to someone unless you’re confident who the recipient is. expedia group does not extend job offers via email or any other messaging tools to individuals with whom we have not made prior contact. our email domain is @expediagroup.com. the official website to find and apply for job openings at expedia group is careers.expediagroup.com/jobs. expedia is committed to creating an inclusive work environment with a diverse workforce. all qualified applicants will receive consideration for employment without regard to race, religion, gender, sexual orientation, national origin, disability or age.",united kingdom,Data Scientist,"['a/b testing', 'excel', 'experimentation', 'pyspark', 'python', 'r', 'scala', 'spark', 'statistics']","['a/b testing', 'excel', 'experimentation', 'pyspark', 'python', 'r', 'scala', 'spark', 'statistics']",US$72k–US$115k a year
data scientist,hays,"£43,001 - £47,779 per annum, flexible hybrid working pattern (2 days per week in office), 35-hour week, 39 days annual leave (including statutory days), good pension scheme and other generous benefits this post is subject to dbs clearance. hays technology are working in partnership with a large public sector organisation in coalville to recruit a data scientist to join their technology team on a permanent basis. the successful candidate will focus on leveraging data analytics to drive insights and improve the quality and efficiency of services by cleaning and organising data. this role involves working closely with various stakeholders to extract, analyse, and interpret complex data sets to inform decision-making and policy development. principal duties and responsibilities: • collect and analyse data from internal systems (tenancy, maintenance, finance) and external sources (e.g. census, public datasets). • clean, structure, and validate data to ensure accuracy and usability. • build models to forecast housing demand, rent arrears, and maintenance needs. • create dashboards and reports to communicate insights to non-technical stakeholders. • assess the impact of housing initiatives and recommend improvements. • use ml to optimise resource allocation, predict tenant behaviour, and automate processes like arrears risk scoring. • maintain data quality, security, and compliance with gdpr and other regulations. • work with housing officers and managers to translate operational needs into data-driven solutions. in order to apply, you must have the following skills and experience: • industry certifications in data science or related fields (e.g., microsoft certified: azure data scientist, google professional data engineer) or equivalent experience. • experience working as a data scientist, ideally within social housing, public sector, or a related industry. • experience working with social housing data systems (e.g., mri, northgate, civica, or orchard) and the ability to apply advanced analytics to operational challenges in housing (desirable). • demonstrated experience in using machine learning, predictive modelling, and statistical analysis to solve real-world problems. • expertise in statistical modelling, predictive analytics, clustering, classification, and regression techniques. • strong background in data mining, pattern recognition, and anomaly detection to improve service delivery. • proficient in python, r, or other relevant programming languages used for data science. • strong skills in sql and experience working with large databases and data warehouses. • ability to create intuitive and informative visualisations using tools such as power bi, tableau, or similar platforms. • familiarity with cloud-based data platforms (e.g., azure, aws) and deployment of models in a production environment. if you have the relevant experience and would like to apply, please submit your cv. hays specialist recruitment limited acts as an employment agency for permanent recruitment and employment business for the supply of temporary workers. by applying for this job you accept the t&c's, privacy policy and disclaimers which can be found at hays.co.uk",markfield,Data Scientist,"['aws', 'azure', 'classification', 'cloud', 'clustering', 'dashboard', 'data analytics', 'data warehouse', 'machine learning', 'power bi', 'python', 'r', 'regression', 'sql', 'tableau']","['aws', 'azure', 'classification', 'cloud', 'clustering', 'dashboard', 'data analytics', 'data warehouse', 'machine learning', 'power bi', 'python', 'r', 'regression', 'sql', 'tableau']",
lead data scientist - treasury liquidity,wise,"lead data scientist - treasury liquidity wise wisefinancial services lead data scientist - treasury liquidity united kingdom , londonapply now wise is a global technology company, building the best way to move and manage the world’s money. min fees. max ease. full speed. whether people and businesses are sending money to another country, spending abroad, or making and receiving international payments, wise is on a mission to make their lives easier and save them money. as part of our team, you will be helping us create an entirely new network for the world's money. for everyone, everywhere. more about our mission and what we offer. we are seeking a talented operating liquidity data scientist with expertise in liquidity management modelling to join our dynamic treasury team. this role focuses on driving our models and optimising their impact on our liquidity usage. your work will have a direct impact on wise’s mission and millions of our customers. about the role: as part of the team, you’ll be at the forefront of designing, implementing, and refining models that forecast and manage liquidity, and influencing decision-making processes across the organisation. your mission is to help us have enough cash in the right place at the right time and make sure we keep the liquidity risk under control. you will work closely with cross-functional teams to develop data-driven solutions that enhance our liquidity management and operational efficiency. here’s how you’ll be contributing: liquidity management • work closely with treasury operations to develop supply and demand forecasts and incorporate them into the real-time money movement processes across a multi-region portfolio of products and currencies. • conduct rigorous data analysis to support liquidity efficiency initiatives, ensuring a balance between sufficiency and excess. • collaborate with engineering teams to implement models within the treasury's operational backoffice, ensuring scalability and operational efficiency. • develop bespoke models and analyses in preparation for stress events and new product launches liquidity risk modelling and analysis • develop models and infrastructure for understanding liquidity consumption by company’s products. • partner with product and operational teams to translate complex liquidity risk scenarios into actionable insights for customer-focused solutions. • document and present model results and risk assessments to senior stakeholders, controllers and the risk team (the second line of defence). explain complex concepts and propose strategies that align with the company’s risk appetite and business objectives. qualificationsadditional information for everyone, everywhere. we're people building money without borders — without judgement or prejudice, too. we believe teams are strongest when they are diverse, equitable and inclusive. we're proud to have a truly international team, and we celebrate our differences. inclusive teams help us live our values and make sure every wiser feels respected, empowered to contribute towards our mission and able to progress in their careers. if you want to find out more about what it's like to work at wise visit wise.jobs. keep up to date with life at wise by following us on linkedin and instagram. create an accountor log in to save this jobcreate an accountlogin",united kingdom,Data Scientist,"['data analysis', 'r', 'scala']","['data analysis', 'r', 'scala']",
data scientist,apply4u,"job description ⚡ data scientist – applied ml in production 📍 nottingham (hybrid – 2 days onsite) join a fast-growing ai company delivering high-impact ml solutions for major uk brands. we’re looking for a technically strong data scientist with hands-on experience deploying models in production and a solid quantitative background. the role work end-to-end on applied ml problems, from data exploration and modelling to production deployment and post-launch analysis. build models that ship and shape real user outcomes. you’ll: • analyse and model large-scale behavioural and time-series data • build, validate, and deploy ml models for forecasting and prediction • develop scalable data workflows with engineering and product teams • communicate insights to influence business decisions you have: • strong quantitative background (master’s/phd advantageous) • production ml experience • excellent python and sql skills • confidence across the full ml lifecycle, from exploration to monitoring • ability to translate technical work into actionable outcomes why join: • work with a modern cloud-based ml stack on applied, high-impact problems • turn cutting-edge research into real operational deployments • collaborative, ambitious team shaping product direction • 30 days holiday + flexible working if you want to apply ml in production and see measurable impact, this is your chance.",nottingham,Data Scientist,"['cloud', 'excel', 'python', 'r', 'scala', 'sql']","['cloud', 'excel', 'python', 'r', 'scala', 'sql']",
data scientist,entech technical solutions limited,"data analystsql / python & powerbi dashboards essential £21 - £21.50 fcsa umbrella (inside ir35) £15.60 - £16.00 paye nr to leicester (around 15-20 mins from j21 leicester forest junction m1) fully office based 5 days per week may also suit business analyst with the correct software packages we are currently looking for data analyst with good solid data platform knowledge – python, r, sql, combined with dashboard creation in power bi, tableau or alteryx & strong excel skills do you have experience of category data administration, data / process analysis & automation, confident using excel & associated programs are you able to develop intuitive dashboards & programs to aid automation & user efficiency? we have an opportunity with a local company who are looking to source a person for a long term contract opportunity (min 12 months) we require a data analyst. the responsibilities and tasks for this job include the data analysis for all locations throughout this global business. you will provide analysis and input of the data, whilst verifying the results in question and providing process improvements through automation when needed. if you feel you can cover most of the below bullet point and can demonstrate experience of the opening points of this description we would love to hear from you. candidates will have gained the following skills and experience through previous roles: responsibilities • directing the data gathering, data mining, and data processing processes in huge volume; creating appropriate data models. • exploring, promoting, and implementing semantic data capabilities through natural language processing, text analysis and machine learning techniques. • leading to define requirements and scope of data analyses; presenting and reporting possible business insights to management using data visualization technologies. • conducting research on data model optimization and algorithms to improve effectiveness and accuracy on data analyses. • knowledge of the statistical tools, processes, and practices to describe business results in measurable scales; ability to use statistical tools and processes to assist in making business decisions. • relevant experience of working within a data critical environment • an undergraduate degree from a college or university, or equivalent experience. • confidence in creating reports, & updating databases within ms excel, ms access • support analysis of data, business process, then development of standard work, automation of data flows and understanding stakeholder • extensive experience of creating dashboards using most of the following - power bi, power app, power automate, alteryx, tableau software, snowflake databases & strong excel skills to £21.50 fcsa umbrella (this role is deemed inside ir35) to £16.00 paye required hours 8.00am – 4.45pm mon thurs, earlier finish friday please apply for further details",leicester,Data Scientist,"['dashboard', 'data analysis', 'excel', 'machine learning', 'natural language processing', 'power bi', 'python', 'r', 'snowflake', 'sql', 'tableau']","['dashboard', 'data analysis', 'excel', 'machine learning', 'natural language processing', 'power bi', 'python', 'r', 'snowflake', 'sql', 'tableau']",
lead data scientist - deep learning practitioner,posting date:21/ 11/2025,"white collar factory (95009), united kingdom, london, london lead data scientist - deep learning practitioner about this role our data science team focuses on the development of machine learning and deep learning solutions, to solve business problems and deliver actionable insights. we are a talented, collaborative and enthusiastic group, who use our expertise to derive insights from complex data, working in close collaboration with our business partners. this role will primarily focus on leading the development of proprietary deep learning models to address critical business challenges in underwriting. the role will also involve supporting our business partners as they develop advanced servicing products using large language models. what you’ll do • lead the development of new deep learning approaches to advance our current underwriting models, which form the heart of our lending business. apply these to new types of (multi-modal) data in order to stay at the forefront of innovation. • prioritise and own the roadmap for this work. balancing r&d with in-market results, you will drive ideas from prototypes through to production. • provide consultancy to our tech and product partners, to help design, develop and launch products powered by large language models (llms). this collaboration will help provide seamless experiences for our customers and associates. • use a combination of business acumen, coding and statistical skills to navigate large amounts of data and extract actionable solutions. • work cross-functionally on projects that support key business initiatives and drive sustainable growth. what we’re looking for • strong experience developing and deploying deep learning models, particularly for sequential data (e.g. time series, language) using techniques such as lstms or transformers. • a proven track record leading model development, including setting the technical direction, project management, stakeholder comms, and mentoring junior members of the team. • experience producing and managing reliable and maintainable code in python in a team setting, including code reviews and setting software engineering best practices • hands-on experience with modern machine/deep learning frameworks such as pytorch, tensorflow, or hugging face transformers. • familiarity with both pre-training and fine-tuning of large-scale models • experience working with structured and unstructured data, such as text, logs, or time series and tokenisation techniques. • a strong understanding of probability, statistics, machine learning and familiarity with large data set manipulation. • a drive for continued learning through an internal and external focus, and an ability to prototype new techniques to assess value we are committed to creating a level playing field and seek to create teams that are representative of our customers and the communities we serve. we’d love to hear from you if you identify with a typically under-represented group in our industry and are particularly keen to hear from women, the lgbtq+ community and ethnic minority candidates. where and how you'll work this is a permanent position based in our nottingham or london office. we have a hybrid working model, so you’ll be based in our office 3 days a week on tuesdays, wednesdays and thursdays, and can work from home on monday and friday. many of our associates have flexible working arrangements, and we're open to talking about an arrangement that works for you. what’s in it for you • bring us all this - and you’ll be well rewarded with a role contributing to the roadmap of an organisation committed to transformation • we offer high performers strong and diverse career progression, investing heavily in developing great people through our capital one university training programmes (and appropriate external providers) • immediate access to our core benefits including pension scheme, bonus, generous holiday entitlement and private medical insurance – with flexible benefits available including season-ticket loans, cycle to work scheme and enhanced parental leave • open-plan workspaces and accessible facilities designed to inspire and support you. our nottingham head-office has a fully-serviced gym, subsidised restaurant, mindfulness and music rooms. in london, you can heighten your mood with a run on our rooftop running track or an espresso at the workshop coffee café what you should know about how we recruit we pride ourselves on hiring the best people, not the same people. building diverse and inclusive teams is the right thing to do and the smart thing to do. we want to work with top talent: whoever you are, whatever you look like, wherever you come from. we know it’s about what you do, not just what you say. that’s why we make our recruitment process fair and accessible. and we offer benefits that attract people at all ages and stages. we also partner with organisations including the women in finance and race at work charters, stonewall and upreach to find people from every walk of life and help them thrive with us. we have a whole host of internal networks and support groups you could be involved in, to name a few: • reach – race equality and culture heritage group focuses on representation, retention and engagement for associates from minority ethnic groups and allies • outfront – to provide lgbtq+ support for all associates • mind your mind – signposting support and promoting positive mental wellbeing for all • women in tech – promoting an inclusive environment in tech • empowher - network of female associates and allies focusing on developing future leaders, particularly for female talent in our industry capital one is committed to diversity in the workplace. if you require a reasonable adjustment, please contact ukrecruitment@capitalone.com all information will be kept confidential and will only be used for the purpose of applying a reasonable adjustment. for technical support or questions about capital one's recruiting process, please send an email to careers@capitalone.com capital one does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site. capital one financial is made up of several different entities. please note that any position posted in canada is for capital one canada, any position posted in the united kingdom is for capital one europe and any position posted in the philippines is for capital one philippines service corp. (copssc). who we are at capital one, we're building a leading information-based technology company. still founder-led by chairman and chief executive officer richard fairbank, capital one is on a mission to help our customers succeed by bringing ingenuity, simplicity, and humanity to banking. we measure our efforts by the success our customers enjoy and the advocacy they exhibit. we are succeeding because they are succeeding. guided by our shared values, we thrive in an environment where collaboration and openness are valued. we believe that innovation is powered by perspective and that teamwork and respect for each other lead to superior results. we elevate each other and obsess about doing the right thing. our associates serve with humility and a deep respect for their responsibility in helping our customers achieve their goals and realize their dreams. together, we are on a quest to change banking for good.",nottingham (+1 other),Data Scientist,"['deep learning', 'machine learning', 'python', 'pytorch', 'r', 'statistics', 'tensorflow', 'time series']","['deep learning', 'machine learning', 'python', 'pytorch', 'r', 'statistics', 'tensorflow', 'time series']",
lead data scientist – marketing science,next,"job description we are looking for a lead data scientist to join the ecommerce data team! based from next head office in enderby, leicestershire with a competitive salary range and great benefits. let’s talk numbers. when it comes to uk retail, it’s hard to find a bigger name. we sell thousands of items an hour and are expanding our e-commerce business by the second. for anyone in tech, this is the place to learn. to grow. and to thrive. ecommerce data provides the department and the business the means to see what is working and what is not by drawing data and analysing patterns of shopping on our site and in general. about the role: as a lead data scientist within marketing science at next, you will build data-driven solutions through state-of-the-art machine learning, causal inference, and experiment design to measure true impact and make impactful recommendations to the business. as a lead, you are expected to handle end-to-end projects and mentor junior/senior data scientists whenever relevant. what else is involved? • working closely with marketing teams and agencies to understand challenges and opportunities in e-commerce marketing. • proactively lead the design, development, and deployment of data science solutions that inform marketing strategy and execution. • build marketing mix modeling (mmm), predictive modelling, lifetime value prediction, churn analysis, marketing attribution, and channel optimisation. • partner with marketing stakeholders to test, learn, and optimize campaigns through experimental design and/or causal inference methods. • create dashboards and data products that enable self-serve analytics and real-time insights for marketing teams. • drive data quality, model governance, and documentation standards across the marketing science function. about you: • proven experience in applied data science or analytics, with 2+ years in a leadership or mentoring role. • strong expertise in marketing analytics, building mmm model via meridian, robyn and/or pymc, multi-touch attribution, customer lifecycle modelling. • proficiency in sql, python, r, pyspark and dashboarding in powerbi/looker. • experienced running geolift test, causal inference model, and other forms of experimental design. • solid foundation in machine learning, statistics, econometric modelling, analytical and problem solving skills • strong communication skills with the ability to translate complex data insights into actionable business strategies. • preferred experience in the e-commerce, technology or marketing industry. • bachelor’s or master’s degree or higher education in statistics, computer science, data science, economics, or related field. you’ll be doing all this from our leicestershire head office on a hybrid basis. our offices are inspiring, yes. but we understand that life happens. so, we’re big on making sure your work works for you which is why we offer flexible working. bring your energy. play to your strengths. make things bigger and better than before. so if you’re an expert in data science, have strong python/sql/pyspark skills and considerable knowledge in test design, implementation and reporting, can work with initiative and can build great working relationships within the business, this is the place for you – and your career. about us you know next, but did you know we’re a ftse-100 retail company employing over 35,000 people across the uk and ireland. we’re the uk’s 2nd largest fashion retailer and for kidswear we’re the market leader. at the last count we have over 500 stores, plus the next online and it’s now possible to buy on-line from over 70 countries around the world! so we’ve gone global! about the team • 25% off most next, made*, lipsy*, gap* and victoria's secret* products (*when purchased through next) • company performance based bonus • sharesave scheme • on-site nursery available; ofsted outstanding in all areas • 10% off most partner brands & up to 15% off branded beauty • early vip access to sale stock • access to fantastic discounts at our staff shops • restaurants with great food at amazing prices • access a digital gp and other free health and wellbeing services • free on-site parking • financial wellbeing - save, track and enhance your financial wellbeing • apprenticeship - grow and develop on the job whilst gaining a qualification • direct to work - discount online and instore, collect your items the next day for free from your place of work or local store • support networks - access to network groups to empower and celebrate each other • wellhub - discounted flexible monthly gym memberships, with apps, pt sessions and more conditions apply to all benefits. these benefits are discretionary and subject to change. we aim to support all candidates during the application process and are happy to provide workplace adjustments when necessary. should you need support with your application due to a disability or long-term condition, feel free to get in touch with us by email headoffice_careers@next.co.uk (please include 'workplace adjustments' in the subject line), or call us on 0116 284 2486 and leave a voicemail.",coventry,Data Scientist,"['dashboard', 'looker', 'machine learning', 'pyspark', 'python', 'r', 'recommendation', 'spark', 'sql', 'statistics']","['dashboard', 'looker', 'machine learning', 'pyspark', 'python', 'r', 'recommendation', 'spark', 'sql', 'statistics']",
data scientist,flint bishop,"description flint bishop is one of the leading business-to-business debt recovery law firms in the uk. we are the recovery partner for many leading uk brands and the firm has won all three major national debt recovery awards for our strategy and results. data plays an increasingly important part of our business and as a result of continued growth and further investment within our d&a department, we now have an excellent opportunity for a data scientist to join our team based at derby. the firm uses an sql based case management system to manage the recovery of bulk debts and as the basis of internal and external reporting. this role will cover the full spectrum of internal and external data management, performance analytics and collaborating with both internal and external stakeholders to optimise operational strategies and deliver continuous improvement initiatives to maximise client return of investment. we are looking for a hands-on individual who is a natural problem solver with advanced analytical skills, experience of a/b testing & creating/maintaining ml models and delivering technical problems to non-technical audiences. reporting to the head of data & analytics the desired candidate will be keen to join a fast-paced and friendly team to help shape the departments future within flint bishop key responsibilities • champion data accuracy and quality, utilising external 3rd party relationships to enhance our portfolios • provide insight on client portfolios and identify trends across the business • create and maintain bespoke ml models for a variety of business and client needs • lead and manage champion/challenger strategies, to elevate performance on existing contact strategies • liaise with operational leaders, to help maximise contact centre resource to deliver for our clients, leading various projects end-to-end • support and mentor junior team members within d&a skills, knowledge and expertise you will: • 5+ years’ experience in a previous analytical role • experience in identifying performance trends and creating ml models to support operational contact strategies • be proficient in python, sql and microsoft excel • have great interpersonal skills and be able to communicate your ideas to a wider audience • be a good problem solver • have strong time management skills, with the ability to juggle multiple projects at once desirable: • a bachelor’s degree in a numeric subject (mathematics, statistics, etc.) or equivalent • experience working within credit risk/debt recovery industry • working knowledge of python and/or r • experience with legal systems such as liberate disclosure will be required in the event that a position is offered. financial checks will be required in the event that a position is offered. benefits what we offer? • competitive salary • bonus potential • life insurance • great working environment at our derby head offices • car parking provided by the firm • career development opportunities • 25 days holiday plus bank holidays • options to buy & sell 1 week’s holiday • 1 week’s holiday carry over • extra day off for your birthday • staff card discount scheme for local shops • social days and evenings on the firm • staff discounts on legal services about flint bishop flint bishop is a multi-award-winning, top 200 uk law firm. we have been providing legal services to businesses and individuals for over a century. recognised as a ‘leading firm’ by both chambers & partners and the legal 500, the outstanding quality of our staff and service levels is underlined by our iso quality assurance accreditations, industry awards and peer ranking. we have approximately 400 solicitors and legal experts who use their technical skills and expertise for one mission: to provide clients with clear advice that helps them to achieve their goals as effectively as possible.",derby,Data Scientist,"['a/b testing', 'excel', 'python', 'r', 'sql', 'statistics']","['a/b testing', 'excel', 'python', 'r', 'sql', 'statistics']",
data scientist – senior consultant,campion pickworth,"campion pickworth are working with a leading international professional services firm to recruit a data scientist – senior consultant to support the delivery of innovative analytics and machine learning solutions in a fast-paced, collaborative environment. this is a fantastic opportunity to work on a diverse range of high-impact data science projects, leveraging cutting-edge technologies and joining a highly skilled team. you’ll contribute to shaping data capabilities and delivering meaningful insights that underpin business-critical decisions. what you’ll do • develop and deploy advanced analytics, data science, and machine learning solutions across a variety of use cases. • use technologies such as python, r, azure, databricks, sql, power bi, and tableau to extract actionable insights from complex datasets. • contribute to the design and implementation of scalable data science workflows and best practices. • collaborate with cross-functional stakeholders to understand business needs and translate them into effective analytical solutions. • support multiple projects from exploration through to deployment within cloud-based environments. • maintain high standards of code quality, documentation, and testing within a devops-oriented workflow. • apply strong knowledge of ml techniques, from supervised/unsupervised learning through to emerging methods in generative ai and large language models. what we’re looking for essential skills & experience • proven experience solving complex, real-world problems using data science and analytics. • strong python skills (pandas, numpy, scikit-learn) with a solid grounding in probability and statistics. • deep understanding of machine learning methods and their practical application. • experience delivering multiple end-to-end data science projects across varied data types and business domains. • familiarity with devops practices and tools such as git. • cloud experience (e.g., azure, aws) and working with ml platforms and services. • excellent communication skills, with the ability to explain complex concepts to non-technical stakeholders. • ability to align analytical outputs with broader business objectives. desirable skills • experience using r and nlp or deep learning techniques (e.g., tf-idf, word embeddings, cnns, rnns). • familiarity with generative ai and prompt engineering. • experience with azure databricks, mlflow, azure ml services, docker, or kubernetes. • exposure to agile development environments and software engineering best practices. • experience working in large, complex organisations or regulated industries. • strong working knowledge of excel, sql, power bi, and tableau.",united kingdom,Data Scientist,"['aws', 'azure', 'cloud', 'databricks', 'deep learning', 'excel', 'machine learning', 'nlp', 'numpy', 'pandas', 'power bi', 'python', 'r', 'scala', 'scikit-learn', 'sql', 'statistics', 'tableau']","['aws', 'azure', 'cloud', 'databricks', 'deep learning', 'excel', 'machine learning', 'nlp', 'numpy', 'pandas', 'power bi', 'python', 'r', 'scala', 'scikit-learn', 'sql', 'statistics', 'tableau']",
data scientist,flutter uk & ireland,"company description: do you want to work somewhere extraordinary? from the people you spend your days with, to the ground-breaking projects, no two days will be the same. with a philosophy of ‘together we are more’ our 7,000+ colleagues come together to form an expert community across technology, product, commercial, data, infrastructure, marketing, and a myriad of subject areas. we listen without judgement, encourage & support, and help build others up. working at flutter uk&i means you can be yourself, work how and where suits you best -and let your personality shine! job description: an exciting opportunity has opened up for a passionate data scientist to join our data science team, part of flutter uk & ireland! the uki data science team is a cross business chapter sitting in the customer super tribe, working with other areas such as; safety, marketing, gaming, sports, and more. we work with a range of stakeholders to deliver cutting edge data science and ai modelling solutions to help solve business problems. what you'll do • work with the team to execute data science projects, including data collection, pre-processing, feature engineering, model development, validation, and deployment • engage with business stakeholders to understand domain-specific challenges and opportunities, and collaborate during development to ensure alignment with requirements • apply advanced statistical and machine learning algorithms to analyse large datasets, uncover patterns, and derive actionable insights to support business objectives • develop and implement predictive and prescriptive models using machine learning algorithms and techniques, such as regression, classification, clustering, time series analysis, natural language processing, and deep learning • create clear and compelling visualisations to communicate complex analytical findings effectively • collaborate with stakeholders (including business, product, and engineering) to understand requirements, identify opportunities, and provide data-driven recommendations • collaborate with cross-functional teams to integrate data science solutions into products and services • actively participate in knowledge sharing, technical, and non-technical discussions within the team • continuously improve the accuracy, efficiency, and scalability of existing models and algorithms, and identify areas for optimisation and innovation • maintain high standards of documentation, code quality, and application of best practices in data science and software development • actively participate in code reviews, pair programming, and knowledge sharing sessions, embracing feedback for continuous improvement • contribute to building a strong data science team through knowledge sharing and upskilling • keep abreast of the latest advancements in data science, machine learning, and related fields, and proactively identify opportunities to leverage emerging technologies required profile: how you'll do it • proficiency in python and sql. • good knowledge of machine learning techniques and algorithms. • a demonstrated track record of successfully delivering data science projects. • strong verbal and written communication, ability to explain technical concepts to nontechnical audiences. what we offer: what’s on offer • £1,000 learning fund • twice-yearly bonus (with part of it guaranteed!) • unlimited holiday • pension contribution scheme • private healthcare • hybrid working • access to thousands of udemy courses • invest via the company sharesave scheme",leeds,Data Scientist,"['classification', 'clustering', 'deep learning', 'feature engineering', 'machine learning', 'natural language processing', 'python', 'r', 'recommendation', 'regression', 'scala', 'sql', 'time series']","['classification', 'clustering', 'deep learning', 'feature engineering', 'machine learning', 'natural language processing', 'python', 'r', 'recommendation', 'regression', 'scala', 'sql', 'time series']",
data scientist,hiscox,"job type: permanent build a brilliant future with hiscox position: data scientist reporting to: lead data scientist location: york type: permanent as a data scientist at hiscox, you will take on a high-impact role, acting as a critical thinker and problem solver for the business. you’ll apply your core technical skills and innovative thinking to tackle complex challenges, identify opportunities, and help shape data-driven decision-making across the london market. you’ll operate across a wide variety of business functions, managing multiple priorities and delivering both ad hoc analysis and predictive/prescriptive models. your work will contribute directly to building hiscox’s data culture and enabling evidence-based decisions in a fast-paced, evolving environment. communicating the business value of your analytical solutions to stakeholders will be a key part of your role. you’ll be part of an award-winning team, recognised for its pioneering collaboration with google to deliver the market’s first ai-enhanced lead underwriting solution. this achievement reflects the team’s commitment to innovation, impact, and excellence in applying data science to real-world insurance challenges. as a data scientist, you’ll work within a wider technical team whose efforts span multiple business functions, bringing a multi-disciplinary approach to problem solving and analysis. this is an ideal role for someone who is passionate about using analytics to influence decisions and is keen to continue learning and delivering value through data. you’ll be expected to conceptualise new approaches, communicate your vision clearly to stakeholders, and see ideas through to implementation. key responsibilities: • leveraging industry standards, emerging methodologies and empirical research to develop critical inputs to business information, and helping business leaders develop innovative approaches to driving their business. • working on the end-to-end data solution including understanding complex business challenges, designing scientific solutions, working with large and small data sets (including 3rd party and internal data of a wide variety), using cutting-edge machine learning or statistical modelling techniques to derive insights • work collaboratively with data scientists, data engineers and other technical people including pricing and underwriting teams in order to help support maturation of analytics practice within the organisation. • work closely with other members of the data and analytics community at hiscox, contributing to delivering value though the use of a range of analytics techniques. person specification: • degree in a stem or closely related field or equivalent experience. a further degree is a plus. • experience of data science, advanced analytics or a genuine interest to learn. • experience of data science in finance or insurance is an advantage but not required. • ability to conduct high quality research in a suitably timely manner working in both independently and in small teams as required by the task. • familiarity with version control, agile working and other it delivery tools is required skills: • experience in developing predictive and prescriptive analysis (predictive modelling, machine learning or data mining) used to draw key business insights and clearly articulate findings for target audience. • experience with analytical tools / programming languages and databases (for example: python, r, sql). • experience with large language models and prompting, gcp experience is a plus. • interest in a variety of machine learning techniques from simple linear models and random forests to deep learning. • a particular interest in natural language processing or machine vision. • a strong grasp of foundational statistics is essential. • experience working both in small teams and independently on analytics projects. • strong verbal and written communications skills and effective presentation skills. this is absolutely essential since you will have a lot of exposure to different internal stakeholders. • willingness to learn best practice in software development. • knowledge of insurance is an advantage but not essential. apply now for further information you can follow hiscox on linkedin, glassdoor and instagram (@hiscoxinsurance) work with amazing people and be part of a unique culture",york,Data Scientist,"['deep learning', 'excel', 'gcp', 'machine learning', 'natural language processing', 'python', 'r', 'sql', 'statistics']","['deep learning', 'excel', 'gcp', 'machine learning', 'natural language processing', 'python', 'r', 'sql', 'statistics']",
artificial intelligence & machine learning systems engineer-cognitive electronic warfare (ew),caes,"delivering mission-critical, electronic solutions that protect lives. use your creativity and critical thinking to take our products from concept to customer. at caes by honeywell, we engineer solutions for the world’s most critical missions. we serve customers in the defense and aerospace markets. seeking a career that offers challenging, diverse projects and opportunities? looking for a position with a company that offers long-term professional advancement? searching for a place that values a diverse, team-based environment? one that values you. consider caes by honeywell. the most important thing we build is trust #customerfocus #values #leader #togetherwepioneer overview we’re seeking a highly skilled artificial intelligence & machine learning systems engineer to architect, design, and develop advanced ai/ml systems that power our next generation of products. in this leadership role, you’ll contribute to the technical roadmap, mentor engineering teams, and collaborate with cross-functional teams to deliver intelligent, scalable, and production-ready ai and machine learning technologies. you will be responsible for researching, creating, adapting and evaluating ai/ml techniques to solve complex customer problems with real-time solutions to support our defense customers. specifically, we are building next-generation cognitive electronic warfare systems that operate autonomously at the tactical edge in contested, low-swap (size, weight, and power), denied, and disconnected environments. this is not a prompt-engineering or genai role. we are looking for hardcore ai/ml systems engineers who treat machine learning as a component of a larger, mission-critical, real-time embedded system. responsibilities major duties & responsibilities: • design, implement, and harden on-line and continual-learning ml algorithms for rf signal classification, adaptive jamming, cognitive radar, and electronic attack/support decision engines. • port, optimize, and deploy ml inference algorithms to edge processors. • build and maintain low-latency, deterministic inference pipelines that integrate tightly with real-time rf front-ends and digital signal processing chains. • lead the systems integration of ai/ml techniques into mission-critical embedded platforms running real-time operating systems. • design and deliver warfighter-focused engineering visualizations and tactical displays (real-time spectrum awareness, threat emitter tracks, cognitive ew decision overlays, confidence heatmaps) using modern web stack frameworks that run natively on embedded tactical processors and dismounted soldier systems. • own the mlops and devsecops pipeline for classified ew programs: secure ci/cd, model versioning, containerized build/test/deploy, sbom generation, and compliance with dod zero-trust and cncf security standards. • architect and deploy kubernetes-based edge orchestration clusters (e.g. k3s) that operate in fully air-gapped tactical environments with strict latency and availability requirements. • perform end-to-end performance profiling (memory bandwidth, cache coherency, dma, gpu/tpu/npu utilization). • review code, guide architecture decisions, and mentor the ai/ml engineering team. • collaborate with product and engineering teams to identify ai/ml-driven opportunities. qualifications required qualifications: • bachelor’s or master’s degree in computer science, machine learning, artificial intelligence, data science, or related field • 7+ years of professional experience shipping production ai/ml systems, ideally in defense, aerospace, or autonomous systems • prior work on dod cognitive ew programs • deep expertise in high-performance and real-time applications (not just scripting wrappers) • real-time and embedded application programming (no python-only backgrounds) • proven track record of deploying ai/ml solutions to cloud and edge/constrained devices • strong systems engineering background: you understand clocks, interrupts, dma, cache hierarchies, memory-mapped i/o, and real-time scheduling • hands-on experience building and securing ci/cd pipelines for classified or regulated environments • expertise with docker, container hardening, and kubernetes in disconnected/edge configurations (k3s, microk8s, rancher harvester). • familiarity with rf/ml intersections: signal detection & classification, modulation recognition, emitter geolocation, fingerprinting, adaptive waveform design, or reinforcement learning for ew • proficiency with ml algorithms (including nlp, computer vision, time-series), libraries including foundational understanding and expertise in statistics probability theory and linear algebra • strong understanding of machine learning fundamentals: supervised/unsupervised learning, deep learning, model evaluation, optimization, feature engineering, etc • experience with data engineering workflows and building robust training datasets preferred qualifications • experience as the technical lead for establishing and accrediting classified ai/ml information systems under the dod risk management framework (rmf): • author and maintain system security plans (ssp), security conops, and ai/ml-specific risk annexes • build and harden multi-enclave classified development, integration, and operational environments (rhel 8/9, selinux enforcing, disa stigs, assured compliance assessment solution (acas)) • lead the creation of ai/ml-specific artifacts for emass packages, including model cards, data provenance, adversarial robustness testing, and continuous monitoring plans • obtain and maintain authority to operate (ato) for classified cognitive ew systems containing advanced gpu/npu-accelerated ai infrastructure • perform linux systems administration at the classified level: kernel tuning for real-time determinism, custom security hardening, cross-domain solution integration, auditd/elk stack management, and fips 140-3 compliant cryptography • deep linux systems administration and hardening experience in classified environments (rhel/centos, stig compliance, selinux policy authoring). • hands-on experience authoring rmf packages and obtaining atos for systems containing machine learning components for the u.s. government (army, navy, air force, or ic customer) • expertise with docker, container hardening (cis, oscap), and kubernetes in disconnected tactical environments • experience or exposure with implementing government reference architectures • experience with neuromorphic or spiking neural network hardware (intel loihi, brainchip akida) • experience with distributed training, gpu acceleration, and high-performance ml compute • strong background in foundation algorithms, transformers, or multimodal ai • knowledge of automated model monitoring, drift detection, and lifecycle management • experience integrating ml models into consumer or enterprise products preferred developer/admin skills • language: c/c++, golang, powershell, carbon, java, python, javascript, cuda, opencl, vhdl • orchestration/deployment: kubernetes/k3s, containerd, openvino, osgi • distributed: hazelcast, rest architecture, websockets, neo4j • devsecops: cmake, maven, ansible, google jib, gradle, jenkins, git, helm • visualization: node.js, react.js, material ui • system administration: linux, windows, vmware • genai: pytorch, tensorflow soft skills • strong leadership, communication, and technical mentorship abilities • thrives in fast-paced environments with a passion for continuous learning • ability to translate complex ai/ml concepts for non-technical partners what we offer • competitive salary + performance bonuses • equity opportunities • full benefits package (health, dental, vision, 401k) • flexible working environment • opportunity to shape the direction of ai/ml initiatives from the ground up why this role is different • you will own the entire stack from algorithm research to bare-metal deployment on platforms that fly, float, or roll into harm’s way • no python notebooks in production—everything is compiled, containerized, signed, and deployed with cryptographic integrity • real impact: your code will out-think and out-maneuver adversary emitters in real conflicts. if you live for the intersection of cutting-edge machine learning and extreme systems engineering under the harshest constraints, we want to talk to you salary range: $196,160.00 - $245,000.00 annually employees may be eligible for a discretionary bonus in addition to base bay. applicable pay within the posted range may vary based on factors including, but not limited to, geographical location, job function of the position, education, and experience of the successful candidate. caes provides a variety of benefits including health insurance coverage, life and disability insurance, 401k, paid holidays and vacation. the application period for the job is estimated to be 40 days from the job posting date; however, this may be shortened or extended depending on business needs and the availability of qualified candidates. job posting date: december 10, 2025. employment transparency benefits we take care of our people and provide competitive health, wealth and wellbeing benefits - from day one. you’ll also discover learning and development opportunities so you can take your career to the next level - and beyond. other benefits include • comprehensive pto, paid holiday and paid family leave programs. • student loan repayment program & tuition reimbursement • 9/80 alternate work week schedule • tailored management/leadership training • innovative medical programs, including family forming we are caes by honeywell caes by honeywell pioneers advanced electronics that underpin many of the world’s most critical missions. we design, engineer, test, and manufacture advanced electronic solutions for the u.s. aerospace and defense industry. from inception and development engineering, to full-rate production and sustainment, we work closely with customers as partners throughout the program lifecycle. we are an equal opportunities employer at caes by honeywell we welcome differences and celebrate new ideas. we believe the diversity of our people inspires our creativity and drives our innovation. everyone is welcome here, regardless of race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, protected veteran status, or genetic information. we are committed to working with and providing reasonable accommodations to individuals with disabilities. if you need a reasonable accommodation due to a disability for any part of the employment process, please email caescareers@caes.com.","arlington, va",Machine Learning Engineer,"['c++', 'classification', 'cloud', 'computer vision', 'deep learning', 'feature engineering', 'java', 'machine learning', 'nlp', 'python', 'pytorch', 'r', 'scala', 'statistics', 'tensorflow']","['c++', 'classification', 'cloud', 'computer vision', 'deep learning', 'feature engineering', 'java', 'machine learning', 'nlp', 'python', 'pytorch', 'r', 'scala', 'statistics', 'tensorflow']","196,160–245,000 a year"
generative ai & machine learning engineer,thomson reuters,"are you passionate about the chance to bring your data quality improvement experience to a world class organization that is leading the way in both content and technology to serve and protect our citizens home and abroad? do you have the skills necessary to manage, understand, and analyze inhouse and customer data including text mining, developing predictive systems, risk scoring, creating efficient algorithms, data quality improvement and other related activities? then thomson reuters special services (trss) is looking for you! about the role as a generative ai and machine learning engineer, you will be responsible for supporting product development efforts that range from generative ai and machine learning application development and deployment to model development and monitoring, ml management and devops. you will be a contributing member of a software engineering team that supports production applications and also helping to develop new features and models. the role will interface with internal and external stakeholders and provide continuity of technical and data-exploration expertise to ensure we are delivering a workable solution that meets the customer requirements and technical capabilities. the position requires a proactive, mission-oriented person who strives to produce the best possible work for the customer. as a generative ai and machine learning engineer, you will: • experiment and develop: you will drive the end-to-end model development lifecycle, championing best practices to ensure reproducible research and well-managed software delivery. • collaborate: working on a collaborative cross-functional team, you will share information, value diverse ideas, and partner effectively with colleagues across the globe. you will elevate and mentor teammates. you will work closely with product teams to see your work integrated and deployed into production environments. • deliver: with a sense of urgency and the desire to work in a fast-paced, dynamic environment, you will translate complex business problems into projects with clearly defined scope. our problems are complex; our solutions are right-sized. you will be accountable for timely, well-managed deliverables. • innovate: you will be empowered to try new approaches and learn new technologies. you will foster innovative ideas to solve real-world challenges. you will iterate on improvement rapidly with a mindset for failing fast and learning continuously from data. • inspire: you will be a proactive communicator who is excited to share your work. you will be articulate and compelling in describing ideas to both technical and non-technical audiences. you will help lead the way in the adoption of ai across the enterprise. about you you're a good fit for the role of generative ai & machine learning engineer if you have: • master’s or bachelor’s in a relevant discipline. relevant experience will be considered in lieu of a degree • proficiency in python • at least 3+ years practical, relevant experience building ai/ml products and applications and recent demonstratable experience using generative ai technologies such as rag patterns, react, langchain etc for performing document summarization, knowledge graphs, information extraction, or analysis. • solid software engineering skills and experience • experience as a technical leader, leading aspects of the following :aiding in the ideation with product stakeholders, comfort in dealing with uncertainty and ambiguity in problem statements and paths to success, breaking down complex problems into tractable components with iterative improvements, formulating research and development plans, coordinating and working with others, communication of progress and plans with varied stakeholders • hands on coding experience on ai/ml projects in current role and experience in designing, developing, and implementing machine learning models and algorithms • ability to obtain and maintain a u.s. national security clearance • u.s. citizenship essential to comply with government contract/agency or department of federal government requirements the following skills and tools are preferred, but not required • 6+ years practical experience using ai and ml to solve diverse and/or ambiguous problems in academia or industry • experience with ai agent frameworks and patterns • familiarity with common nlp use cases such as chatbot development and information retrieval (rag), language translation, ner, summarization and topic modeling • familiarity with llm training and efficient finetuning (peft, lora, quantization) • experience with nlp/cv libraries such as pytorch, tensorflow, scikit-learn, spacy, huggingface transformers/diffusers, openai, langchain, semantic kernel & opencv • experience using cloud solutions (aws, azure, etc.) for building and deploying applications • experience in a fast-paced, agile environment managing uncertainty and ambiguity. • proficiency in python and experience delivering minimum viable products in a large enterprise environment • experience developing libraries and apis for others to use • outstanding communication and data-driven decision-making collaboration with product + business stakeholders • diverse experiences across problem spaces, companies and cultures #li-cs1 what’s in it for you? • hybrid work model: we’ve adopted a flexible hybrid working environment (2-3 days a week in the office depending on the role) for our office-based roles while delivering a seamless experience that is digitally and physically connected. • flexibility & work-life balance: flex my way is a set of supportive workplace policies designed to help manage personal and professional responsibilities, whether caring for family, giving back to the community, or finding time to refresh and reset. this builds upon our flexible work arrangements, including work from anywhere for up to 8 weeks per year, empowering employees to achieve a better work-life balance. • career development and growth: by fostering a culture of continuous learning and skill development, we prepare our talent to tackle tomorrow’s challenges and deliver real-world solutions. our grow my way programming and skills-first approach ensures you have the tools and knowledge to grow, lead, and thrive in an ai-enabled future. • industry competitive benefits: we offer comprehensive benefit plans to include flexible vacation, two company-wide mental health days off, access to the headspace app, retirement savings, tuition reimbursement, employee incentive programs, and resources for mental, physical, and financial wellbeing. • culture: globally recognized, award-winning reputation for inclusion and belonging, flexibility, work-life balance, and more. we live by our values: obsess over our customers, compete to win, challenge (y)our thinking, act fast / learn fast, and stronger together. • social impact: make an impact in your community with our social impact institute. we offer employees two paid volunteer days off annually and opportunities to get involved with pro-bono consulting projects and environmental, social, and governance (esg) initiatives. • making a real-world impact: we are one of the few companies globally that helps its customers pursue justice, truth, and transparency. together, with the professionals and institutions we serve, we help uphold the rule of law, turn the wheels of commerce, catch bad actors, report the facts, and provide trusted, unbiased information to people all over the world. in the united states, thomson reuters offers a comprehensive benefits package to our employees. our benefit package includes market competitive health, dental, vision, disability, and life insurance programs, as well as a competitive 401k plan with company match. in addition, thomson reuters offers market leading work life benefits with competitive vacation, sick and safe paid time off, paid holidays (including two company mental health days off), parental leave, sabbatical leave. these benefits meet or exceeds the requirements of paid time off in accordance with any applicable state or municipal laws. finally, thomson reuters offers the following additional benefits: optional hospital, accident and sickness insurance paid 100% by the employee; optional life and ad&d insurance paid 100% by the employee; flexible spending and health savings accounts; fitness reimbursement; access to employee assistance program; group legal identity theft protection benefit paid 100% by employee; access to 529 plan; commuter benefits; adoption & surrogacy assistance; tuition reimbursement; and access to employee stock purchase plan. thomson reuters complies with local laws that require upfront disclosure of the expected pay range for a position. the base compensation range varies across locations. eligible office location(s) for this role include one or more of the following: new york city, san francisco, los angeles, and/or irvine, ca; mclean, va; washington, dc. the base compensation range for the role in any of those locations is $134,680 - $250,120. this role may also be eligible for an annual bonus based on a combination of enterprise and individual performance. base pay is positioned within the range based on several factors including an individual’s knowledge, skills and experience with consideration given to internal equity. base pay is one part of a comprehensive total reward program which also includes flexible and supportive benefits and other wellbeing programs. about us thomson reuters informs the way forward by bringing together the trusted content and technology that people and organizations need to make the right decisions. we serve professionals across legal, tax, accounting, compliance, government, and media. our products combine highly specialized software and insights to empower professionals with the data, intelligence, and solutions needed to make informed decisions, and to help institutions in their pursuit of justice, truth, and transparency. reuters, part of thomson reuters, is a world leading provider of trusted journalism and news. we are powered by the talents of 26,000 employees across more than 70 countries, where everyone has a chance to contribute and grow professionally in flexible work environments. at a time when objectivity, accuracy, fairness, and transparency are under attack, we consider it our duty to pursue them. sound exciting? join us and help shape the industries that move society forward. as a global business, we rely on the unique backgrounds, perspectives, and experiences of all employees to deliver on our business goals. to ensure we can do that, we seek talented, qualified employees in all our operations around the world regardless of race, color, sex/gender, including pregnancy, gender identity and expression, national origin, religion, sexual orientation, disability, age, marital status, citizen status, veteran status, or any other protected classification under applicable law. thomson reuters is proud to be an equal employment opportunity employer providing a drug-free workplace. we also make reasonable accommodations for qualified individuals with disabilities and for sincerely held religious beliefs in accordance with applicable law. more information on requesting an accommodation here. learn more on how to protect yourself from fraudulent job postings here. more information about thomson reuters can be found on thomsonreuters.com.","mclean, va",Machine Learning Engineer,"['aws', 'azure', 'classification', 'cloud', 'machine learning', 'nlp', 'python', 'pytorch', 'r', 'scikit-learn', 'tensorflow']","['aws', 'azure', 'classification', 'cloud', 'machine learning', 'nlp', 'python', 'pytorch', 'r', 'scikit-learn', 'tensorflow']",
machine learning engineer,signet jewelers ltd.,"we have many opportunities available on our other career site pages. click here to link to our careers page! signet jewelers is the world's largest retailer of diamond jewelry, operating more than 2,800 stores worldwide under the iconic brands: kay jewelers, zales, jared, h.samuel, ernest jones, peoples, banter by piercing pagoda, rocksbox, jamesallen.com and diamonds direct. we are a people-first company and this core value is at the heart of everything we do, from empowering our valued team members, to collaborating with our customers, to fostering the communities in which we live and serve. people - and the love their actions inspire - are what drive us. we're not only proud of the love we inspire outside our walls, we're especially proud of the diversity, inclusion and equity we're inspiring inside. there are dynamic career paths awaiting you - rewarding opportunities to impact the lives of others and inspire love. join us! we are looking for a hands-on machine learning engineer to operationalize advanced models across elasticity, uplift, forecasting, and other ai use cases. this role sits at the intersection of data engineering, mlops, and applied machine learning, ensuring that models developed by data science teams are production-ready, performant, and scalable. key responsibilities • design, build, and automate production-grade data pipelines to support elasticity, uplift, and other analytical models • implement clean, reusable data transformation logic that ensures consistency across modeling, analytics, and reporting layers • develop and maintain real-time inference services (e.g., aws sagemaker endpoints) to allow business teams and applications to consume model outputs seamlessly • establish mlops best practices, including: • model performance monitoring and drift detection • automated retraining and evaluation pipelines • feature / model versioning and lineage tracking • enable ci/cd for ml deployments, ensuring reliability, reproducibility, and rapid iteration • partner with data science teams to accelerate experimentation and automate recurring workflows • identify and drive automation opportunities across the broader ai & data science ecosystem to improve scalability, reliability, and cost efficiency preferred qualifications • bachelor's or master's degree in computer science, data engineering, applied ml, or equivalent experience • 3-6 years of industry experience in ml engineering or mlops • experience in retail analytics - such as demand forecasting, pricing, promotions, inventory optimization, customer segmentation, or e-commerce metrics - is highly preferred • strong programming skills in python (pandas, pyspark, fastapi, etc.) • experience building and managing etl/elt pipelines • hands-on experience deploying ml systems on aws (sagemaker, lambda, ecs/eks, s3, kinesis/streams, etc.) • experience with ci/cd tools (github actions, codepipeline, jenkins, etc.) • familiarity with monitoring and observability for ml (model drift, feature drift, inference latency, cost monitoring) • experience with containerization & orchestration (docker, kubernetes) is a plus nice to have • experience building data products or ml-powered apis that expose predictions or insights back to business applications. • experience with feature stores (sagemaker feature store / feast) • experience supporting batch + real-time inference workloads who you are • you enjoy solving problems at the intersection of data + ml + production systems • you care deeply about scalability, automation, and reliability • you love partnering with data scientists and analysts to turn prototypes into products benefits and perks: • comprehensive healthcare, dental, and vision insurance to keep you and your family covered • generous 401(k) matching after just one year to help secure your financial future • ample paid time off, plus seven holidays to recharge and unwind • exclusive discounts on premium merchandise just for you • dynamic learning & development programs to support your growth • and more!","akron, oh",Machine Learning Engineer,"['aws', 'data pipeline', 'elt', 'etl', 'experimentation', 'machine learning', 'pandas', 'pyspark', 'python', 'r', 'scala', 'spark']","['aws', 'data pipeline', 'elt', 'etl', 'experimentation', 'machine learning', 'pandas', 'pyspark', 'python', 'r', 'scala', 'spark']",90K–120K a year
machine learning compiler engineer,qualcomm,"company: qualcomm technologies, inc. job area: engineering group, engineering group > machine learning engineering general summary: if you’re interested in advancing and applying mathematics, programming languages theory, and advanced algorithms to program optimization for cutting-edge machine learning accelerators, then you really want to be talking to us! the compiler labs unit in qualcomm ai software department is looking for ml compiler engineers to join our team. we work tactically on improving existing ml compilers and strategically on developing new and innovative ml compilers. our technical approach to compilers emphasizes powerful representations for precisely and compactly modeling programs and the optimization challenges and using advanced mathematics and algorithms for performing optimizations. we are also solid in using “old school” compiler technologies as they apply to contemporary ml challenges, and in meticulous software engineering to produce beautiful compilers. we are also keen about seeing our compilers used and having large impacts on qualcomm’s business. mapping ml algorithms to ml accelerators is currently one of the most interesting and challenging problems for compilers. our compiler targets include the qualcomm neural signal processor, adreno gpus, low-power ml accelerators, and cpu accelerators. this job description spans multiple levels, from entry to experienced. our team is a good home for compiler developers with advanced degrees, and we have solid mentoring and give substantial responsibility quickly for entry level engineers. preferred qualifications: • master's degree in computer science, engineering, electrical engineering, or related field. • experience with compiler development and computer architecture ml experience • a degree in the field of computer science or applied mathematics • experience with software engineering • solid intellectual ability, motivation, and a strong history of achievement • excellent oral and written communication skills • experience with mlir, mlir dialects (linalg, affine), pytorch 2.0, tvm, triton, and/or llvm • sycl experience • ml applications and ml optimization experience • ml architecture experience • high performance computing experience • polyhedral compiler optimization experience • loop transformation and vectorization experience • gpu programming, parallel programming experience • general optimization experience principal duties and responsibilities: • work on a wide range of ml compilers • improve ml compiler optimization capabilities through benchmark analysis and profiling • innovate new ml compiler and optimization algorithms • upstream compiler algorithms to open-source compiler projects • author research publications and represent the company in conferences and industry forums • for senior levels - lead and manage projects while doing substantial technical work minimum qualifications: • bachelor's degree in computer science, engineering, information systems, or related field and 4+ years of hardware engineering, software engineering, systems engineering, or related work experience. or master's degree in computer science, engineering, information systems, or related field and 3+ years of hardware engineering, software engineering, systems engineering, or related work experience. or phd in computer science, engineering, information systems, or related field and 2+ years of hardware engineering, software engineering, systems engineering, or related work experience. qualcomm is an equal opportunity employer. if you are an individual with a disability and need an accommodation during the application/hiring process, rest assured that qualcomm is committed to providing an accessible process. you may e-mail disability-accomodations@qualcomm.com or call qualcomm's toll-free number found here. upon request, qualcomm will provide reasonable accommodations to support individuals with disabilities to be able participate in the hiring process. qualcomm is also committed to making our workplace accessible for individuals with disabilities. (keep in mind that this email address is used to provide reasonable accommodations for individuals with disabilities. we will not respond here to requests for updates on applications or resume inquiries). to all staffing and recruiting agencies: our careers site is only for individuals seeking a job at qualcomm. staffing and recruiting agencies and individuals being represented by an agency are not authorized to use this site or to submit profiles, applications or resumes, and any such submissions will be considered unsolicited. qualcomm does not accept unsolicited resumes or applications from agencies. please do not forward resumes to our jobs alias, qualcomm employees or any other company location. qualcomm is not responsible for any fees related to unsolicited resumes/applications. eeo employer: qualcomm is an equal opportunity employer; all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, veteran status, or any other protected classification. qualcomm expects its employees to abide by all applicable policies and procedures, including but not limited to security and other requirements regarding protection of company confidential information and other confidential and/or proprietary information, to the extent those requirements are permissible under applicable law. pay range and other compensation & benefits: $158,400.00 - $237,600.00 the above pay scale reflects the broad, minimum to maximum, pay scale for this job code for the location for which it has been posted. even more importantly, please note that salary is only one component of total compensation at qualcomm. we also offer a competitive annual discretionary bonus program and opportunity for annual rsu grants (employees on sales-incentive plans are not eligible for our annual bonus). in addition, our highly competitive benefits package is designed to support your success at work, at home, and at play. your recruiter will be happy to discuss all that qualcomm has to offer – and you can review more details about our us benefits at this link. if you would like more information about this role, please contact qualcomm careers.","san diego, ca",Machine Learning Engineer,"['classification', 'excel', 'machine learning', 'pytorch', 'r']","['classification', 'excel', 'machine learning', 'pytorch', 'r']",
senior machine learning engineer,keurig dr pepper,"job overview: at keurig dr pepper (kdp), we’re expanding the frontiers of data science, engineering, and applied ai to transform how we understand consumers, optimize our business, and power the future of coffee. our data professionals thrive in a large-scale, fast-paced organization that operates with the energy of a startup—building real-time data products, pipelines, and intelligent systems that drive immediate business value. as a machine learning engineer within our data & decision science team, you’ll combine technical rigor with business understanding—helping commercial, ecommerce, and personalization teams unlock insights from data. you’ll leverage your experience in python, sql, and modern ai/ml infrastructure to automate processes, deploy learning systems, and make data more actionable across kdp’s growing ecosystem. what you’ll do • independently run projects from conception to completion and manage stakeholder relationships and hit appropriate deadlines and communications • engineer scalable data solutions: design and optimize etl and ci/cd workflows across snowflake, azure vms, and other data platforms to enable high-velocity model development and deployment. • operationalize intelligence: partner with data scientists to productionize ltv and personalization models using agentic ai frameworks and automated training pipelines. • automate for impact: build scripts, services, and workflows that eliminate manual processes—enabling data and ml products to self-heal, self-update, and evolve with new data. • solve complex problems creatively: apply your experience in web scraping, data enrichment, and real-time ingestion to source and unify disparate datasets for modeling and decision support. • collaborate across commercialization and ecommerce: understand the challenges of scaling ml in consumer commerce—supporting smarter pricing, personalization, and marketing optimization. • advance our ai ecosystem: contribute to mlops best practices, observability frameworks, and continuous experimentation in python, sql, and azure ml environments. it is unlawful in massachusetts to require or administer a lie detector test as a condition of employment or continued employment. an employer who violates this law shall be subject to criminal penalties and civil liability. total rewards: • salary range: $96,800 - $130,000 • actual placement within the compensation range may vary depending on experience, skills, and other factors • benefits, subject to election and eligibility: medical, dental, vision, disability, paid time off (including paid parental leave, vacation, and sick time), 401k with company match, tuition reimbursement, and mileage reimbursement • annual bonus based on performance and eligibility requirements: • you have 5+ years of experience in data engineering, ml engineering, or applied data science. • you’re fluent in python and sql, and have experience working with snowflake, azure, or other modern data stacks. • you have hands-on experience with automation, scripting, and problem-solving at scale—from process optimization to model deployment. • you’re comfortable designing solutions in undefined environments and thrive on turning ambiguity into structure. • you hold a master’s degree in a quantitative field and are passionate about continuous learning. • you stay current on agentic ai, generative modeling, and emerging automation frameworks—and enjoy bringing new ideas to life. • you are skilled in consolidating data sources into robust pipelines • combine: you understand commercialization and ecommerce systems, and how data drives product, pricing, and consumer experience decisions. familiarity with cpg industry dynamics and business operations company overview: keurig dr pepper (nasdaq: kdp) is a leading beverage company in north america, with a portfolio of more than 125 owned, licensed and partner brands and powerful distribution capabilities to provide a beverage for every need, anytime, anywhere. we operate with a differentiated business model and world-class brand portfolio, powered by a talented and engaged team that is anchored in our values. we work with big, exciting beverage brands and the #1 single-serve coffee brewing system in north america at kdp, and we have fun doing it! together, we have built a leading beverage company in north america offering hot and cold beverages together at scale. whatever your area of expertise, at kdp you can be a part of a team that’s proud of its brands, partnerships, innovation, and growth. will you join us? we strive to be an employer of choice, providing a culture and opportunities that empower our team of ~29,000 employees to grow and develop. we offer robust benefits to support your health and wellness as well as your personal and financial well-being. we also provide employee programs designed to enhance your professional growth and development, while ensuring you feel valued, inspired and appreciated at work. keurig dr pepper is an equal opportunity employer and recruits qualified applicants and advances in employment its employees without regard to race, color, religion, gender, sexual orientation, gender identity, gender expression, age, disability or association with a person with a disability, medical condition, genetic information, ethnic or national origin, marital status, veteran status, or any other status protected by law. a.i. disclosure: kdp uses artificial intelligence to assist with initial resume screening and candidate matching. this technology helps us efficiently identify candidates whose qualifications align with our open roles. if you prefer not to have your application processed using artificial intelligence, you may opt out by emailing your resume and qualifications directly to kdpjobs@kdrp.com in lieu of clicking apply. please include the job title and location or job id # in the email subject line.","burlington, ma",Machine Learning Engineer,"['azure', 'etl', 'experimentation', 'machine learning', 'python', 'r', 'scala', 'snowflake', 'sql']","['azure', 'etl', 'experimentation', 'machine learning', 'python', 'r', 'scala', 'snowflake', 'sql']",
"principal machine learning engineer, content & trust recommendations",pinterest,"about pinterest: millions of people around the world come to our platform to find creative ideas, dream about new possibilities and plan for memories that will last a lifetime. at pinterest, we’re on a mission to bring everyone the inspiration to create a life they love, and that starts with the people behind the product. discover a career where you ignite innovation for millions, transform passion into growth opportunities, celebrate each other’s unique experiences and embrace the flexibility to do your best work. creating a career you love? it’s possible. we are looking for a principal machine learning engineer, a senior technical visionary, to be the principal technical lead for the content & trust engineering teams setting up overall technical strategy, unified technical architecture and defining a roadmap for industry leading methodology. strong hands on machine learning background including deep learning architectures, generative ai, low-resource ml (zero shot, few shot), responsible ai and large scale deployment and measurement is required. as the principal machine learning engineer you'll be responsible for the technical direction, strategy and health of our content and trust engineering org. you'll ensure that our technology can deliver on the business/product requirements necessary to keep pinterest safe and positive. this means working with other leads to set and execute a long-term strategy for content and trust, aligning the strategy with other clients where it makes sense and communicating to leadership our current status and path to having world-class content and trust capabilities. you'll also foster a healthy community where all ml engineers can learn best practices, collaborate effectively and understand our technical direction. what you’ll do • develop strong partnerships with product teams to understand and proactively address future technology needs and current developer pain points • champion and drive large-scale, cross-functional initiatives that improve the trust and safety of our platform • act as the ultimate “customer representative” for engineers on content and trust including representing needs to leadership and prioritizing projects on the platform teams that ensure high quality capabilities and a world-class pinner experience • scale your leadership through both direct mentorship and via best practices, processes, training and tools. • ensure solid technical plans are in place for projects within trust via direct review or delegation • be the technical point of contact for decisions that impact the whole pinterest platform and for cross-functional partners like policy, operations and legal what we’re looking for: • deep expertise building large scale ml systems at scale with modern frameworks • knowledge of (and a passion for) building responsible and quality first discovery surfaces • track record of delivering large, cross-functional projects across multiple organizations • strong written and verbal communication skills and proven ability to collaborate cross-functionally in-office requirement statement: • we let the type of work you do guide the collaboration style. that means we're not always working in an office, but we continue to gather for key moments of collaboration and connection. • this role will need to be in the office for in-person collaboration 1 time per quarter and therefore can be situated anywhere in the country. relocation statement: • this position is not eligible for relocation assistance. visit our pinflex page to learn more about our working model. #li-remote # li-ak7 at pinterest we believe the workplace should be equitable, inclusive, and inspiring for every employee. in an effort to provide greater transparency, we are sharing the base salary range for this position. the position is also eligible for equity. final salary is based on a number of factors including location, travel, relevant prior experience, or particular skills and expertise. information regarding the culture at pinterest and benefits available for this position can be found here. us based applicants only $267,393—$550,515 usd our commitment to inclusion: pinterest is an equal opportunity employer and makes employment decisions on the basis of merit. we want to have the best qualified people in every job. all qualified applicants will receive consideration for employment without regard to race, color, ancestry, national origin, religion or religious creed, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, age, marital status, status as a protected veteran, physical or mental disability, medical condition, genetic information or characteristics (or those of a family member) or any other consideration made unlawful by applicable federal, state or local laws. we also consider qualified applicants regardless of criminal histories, consistent with legal requirements. if you require a medical or religious accommodation during the job application process, please complete this form for support.",anywhere,Machine Learning Engineer,"['aws', 'deep learning', 'machine learning', 'r']","['aws', 'deep learning', 'machine learning', 'r']",
senior machine learning engineer – clinical foundations,athenahealth,"join us as we work to create a thriving ecosystem that delivers accessible, high-quality, and sustainable healthcare for all. we are looking for a machine learning engineer to join the clinical foundations data science zone. the zone through collaboration and innovation, identifies, develops accelerate the integration of machine learning technics in clinicals r&d - our clinicians and patients experience with athenaone. this role focuses on developing customized models and innovative solutions which can integrate into various workflows, ensuring high reliability, security and scalability by following best practices. about you you love to own important work and find it difficult to turn down a good challenge. you are excited about the latest developments in ml and keep abreast of the latest methods and technologies. you have experience building and evaluating ml models. you have strong communication skills and can work with colleagues from a variety of technical and non-technical backgrounds. finally, you have a strong interest in improving healthcare. about the team the athenaclinicals product is a vital component of the athenaone platform, enabling best experience for our clients and users. the data science team is bringing machine learning to bear in automating existing workflows and create more efficient ones. we are working with other engineering leaders in athenaclinicals product to build machine learning into our best in klas suite of products to drive innovation and increase automation. also, this team is responsible to deploy state-of-the-art machine learning models using cloud technologies. job responsibilities as a member of the athenaclinicals data science team, you are expected to work on projects within scrum teams of 2-4 people and execute the following: • identify opportunities for different machine learning techniques and evaluate which are best • assist in developing and deploying ml-based production services to our clients • understand and follow conventions and best practices for modeling, coding, architecture, and statistics; and hold other team members accountable for doing so • apply rigorous testing of statistics, models, and code • contribute to the development of internal tools and ai team standards • advocate for responsible ai—focusing on fairness, transparency, and data privacy • actively participate in technical design discussions and propose innovative solutions to complex challenges • provide timely and constructive code reviews • mentor and guide fellow engineers, fostering a culture of continuous learning and innovation typical qualifications: • bachelors or masters in relevant field: math, computer science, data science, statistics, or related field • 4-6 years of professional hands-on experience in developing evaluating & deploying machine learning models • strong software engineering foundations, with expertise in python and experience developing production-quality systems using best practices in testing, modularity, and documentation • provide technical & thought leadership, designing and implementing advanced machine learning techniques, focusing on robust & scalable solutions • experience with training & fine tuning llms & genai models is a bonus • familiarity with nlp or computer vision techniques • experience using the aws ecosystem a bonus, including kubernetes, kubeflow or eks experience • familiarity with large-scale data processing frameworks and etl pipelines (e.g., spark) • excellent verbal communication and writing skills • passion for staying current with ai research and applying novel techniques responsibly in production • understanding of ethics, fairness, and compliance implications of ai expected compensation $145,000 - $247,000 the base salary range shown reflects the full range for this role from minimum to maximum. at athenahealth, base pay depends on multiple factors, including job-related experience, relevant knowledge and skills, how your qualifications compare to others in similar roles, and geographical market rates. base pay is only one part of our competitive total rewards package - depending on role eligibility, we offer both short and long-term incentives by way of an annual discretionary bonus plan, variable compensation plan, and equity plans. about athenahealth our vision: in an industry that becomes more complex by the day, we stand for simplicity. we offer it solutions and expert services that eliminate the daily hurdles preventing healthcare providers from focusing entirely on their patients — powered by our vision to create a thriving ecosystem that delivers accessible, high-quality, and sustainable healthcare for all. our company culture: our talented employees — or athenistas, as we call ourselves — spark the innovation and passion needed to accomplish our vision. we are a diverse group of dreamers and do-ers with unique knowledge, expertise, backgrounds, and perspectives. we unite as mission-driven problem-solvers with a deep desire to achieve our vision and make our time here count. our award-winning culture is built around shared values of inclusiveness, accountability, and support. our dei commitment: our vision of accessible, high-quality, and sustainable healthcare for all requires addressing the inequities that stand in the way. that's one reason we prioritize diversity, equity, and inclusion in every aspect of our business, from attracting and sustaining a diverse workforce to maintaining an inclusive environment for athenistas, our partners, customers and the communities where we work and serve. what we can do for you: along with health and financial benefits, athenistas enjoy perks specific to each location, including commuter support, employee assistance programs, tuition assistance, employee resource groups, and collaborative workspaces — some offices even welcome dogs. we also encourage a better work-life balance for athenistas with our flexibility. while we know in-office collaboration is critical to our vision, we recognize that not all work needs to be done within an office environment, full-time. with consistent communication and digital collaboration tools, athenahealth enables employees to find a balance that feels fulfilling and productive for each individual situation. in addition to our traditional benefits and perks, we sponsor events throughout the year, including book clubs, external speakers, and hackathons. we provide athenistas with a company culture based on learning, the support of an engaged team, and an inclusive environment where all employees are valued. learn more about our culture and benefits here: athenahealth.com/careers https://www.athenahealth.com/careers/equal-opportunity","boston, ma",Machine Learning Engineer,"['aws', 'cloud', 'computer vision', 'etl', 'excel', 'machine learning', 'nlp', 'python', 'r', 'scala', 'spark', 'statistics']","['aws', 'cloud', 'computer vision', 'etl', 'excel', 'machine learning', 'nlp', 'python', 'r', 'scala', 'spark', 'statistics']",
senior machine learning engineer,adobe,"job level p50 employee role individual contributor the opportunity adobe stock’s content intelligence team is positioned at the center of adobe’s creative ecosystem. our mission is to employ machine learning to enhance our comprehension of the creative content that moves through adobe stock, behance, and onto adobe firefly. adobe boasts one of the largest pools of professional creative content worldwide, and content intelligence is pivotal in effectively catering to the world’s creatives. this outstanding opportunity allows you to be part of a world-class team dedicated to pushing the boundaries of machine learning and artificial intelligence to new heights. what you'll do as a senior machine learning engineer on the content intelligence team, you will lead the development of ml models and systems, to assist with content understanding. you will work closely with product and engineering management to align technical requirements and opportunities with product goals. your role will include: • leading the development and improvement of machine learning models that fuel innovation. • collaborating with cross-functional teams to deliver ambitious projects on time. • providing technical guidance and mentorship to team members, encouraging a collaborative and inclusive environment. • staying current with the latest advancements in ml and ai to ensure our solutions remain at the forefront. what you need to succeed • 5+ years of industry experience in developing, evaluating, and deploying ml models into production. • strong knowledge of python and python-based ml and data science frameworks, with proficiency in at least one deep learning framework such as pytorch or tensorflow. • proven experience leading major successful initiatives. • experience working with both research and product teams to ensure seamless integration of new technologies. • a degree in computer science, statistics, operations research, applied physics, engineering, or a related quantitative field. join us at adobe and help compose the future of creative content through the power of machine learning. together, we'll develop groundbreaking solutions that empower creatives around the world to achieve their ambitious visions! our compensation reflects the cost of labor across several u.s. geographic markets, and we pay differently based on those defined markets. the u.s. pay range for this position is $172,500 -- $306,625 annually. pay within this range varies by work location and may also depend on job-related knowledge, skills, and experience. your recruiter can share more about the specific salary range for the job location during the hiring process. at adobe, for sales roles starting salaries are expressed as total target compensation (ttc = base + commission), and short-term incentives are in the form of sales commission plans. non-sales roles starting salaries are expressed as base salary and short-term incentives are in the form of the annual incentive plan (aip). in addition, certain roles may be eligible for long-term incentives in the form of a new hire equity award. state-specific notices: california: fair chance ordinances adobe will consider qualified applicants with arrest or conviction records for employment in accordance with state and local laws and “fair chance” ordinances. colorado: application window notice if this role is open to hiring in colorado (as listed on the job posting), the application window will remain open until at least the date and time stated above in pacific time, in compliance with colorado pay transparency regulations. if this role does not have colorado listed as a hiring location, no specific application window applies, and the posting may close at any time based on hiring needs. massachusetts: massachusetts legal notice it is unlawful in massachusetts to require or administer a lie detector test as a condition of employment or continued employment. an employer who violates this law shall be subject to criminal penalties and civil liability. internal opportunities creativity, curiosity, and constant learning are celebrated aspects of your career growth journey. we’re glad that you’re pursuing a new opportunity at adobe! put your best foot forward: 1. update your resume/cv and workday profile – don’t forget to include your uniquely ‘adobe’ experiences and volunteer work. 2. visit the internal mobility page on inside adobe to learn more about the process and set up a job alert for roles you’re interested in. 3. check out these tips to help you prep for interviews. 4. if you are applying for a role outside of your current country, ensure you review the international resources for relocating employees on inside adobe, including the impacts to your benefits, aip, equity & payroll. once you apply for a role via workday, the talent team will reach out to you within 2 weeks. if you move into the official interview process with the hiring team, make sure you inform your manager so they can champion your career growth. at adobe, you will be immersed in an exceptional work environment that is recognized around the world. you will also be surrounded by colleagues who are committed to helping each other grow through our unique check-in approach where ongoing feedback flows freely. if you’re looking to make an impact, adobe's the place for you. discover what our employees are saying about their career experiences on the adobe life blog and explore the meaningful benefits we offer. adobe is proud to be an equal employment opportunity employer. we do not discriminate based on gender, race or color, ethnicity or national origin, age, disability, religion, sexual orientation, gender identity or expression, veteran status, or any other applicable characteristics protected by law. learn more. adobe aims to make adobe.com accessible to any and all users. if you have a disability or special need that requires accommodation to navigate our website or complete the application process, email accommodations@adobe.com or call (408) 536-3015.","seattle, wa (+2 others)",Machine Learning Engineer,"['aws', 'deep learning', 'machine learning', 'python', 'pytorch', 'r', 'statistics', 'tensorflow']","['aws', 'deep learning', 'machine learning', 'python', 'pytorch', 'r', 'statistics', 'tensorflow']",
"principal machine learning engineer, ad ranking",snapchat,"snap inc is a technology company. we believe the camera presents the greatest opportunity to improve the way people live and communicate. snap contributes to human progress by empowering people to express themselves, live in the moment, learn about the world, and have fun together. the company's three core products are snapchat, a visual messaging app that enhances your relationships with friends, family, and the world; lens studio, an augmented reality platform that powers ar across snapchat and other services; and its ar glasses, spectacles. snap engineering teams build fun and technically sophisticated products that reach hundreds of millions of snapchatters around the world, every day. we're deeply committed to the well-being of everyone in our global community, which is why our values are at the root of everything we do. we move fast, with precision, and always execute with privacy at the forefront. we're looking for a principal machine learning engineer to join the ad ranking team at snap! what you'll do: • drive the technical roadmap of the ad ranking team, advance the core ml capabilities of the ad optimization stack, and support various business verticals including app / web monetization and dynamic product ads • design, implement, and scale critical machine learning models to support snap's monetization strategies • collaborate with cross-functional teams to set and align on machine learning strategies to meet company objectives • stay up-to-date with the latest technology in machine learning and apply this knowledge to tackle complex problems in innovative ways • collaborate with leadership to up-level the ml tech stack and improve the performance of the organization • work across teams to understand product requirements, evaluate trade-offs, and deliver the solutions needed to build innovative products or services • advocate for and apply best practices when it comes to availability, scalability, operational excellence, and cost management • provide technical direction that influences the entire ml community knowledge, skills & abilities: • strong understanding of machine learning and deep learning approaches and algorithms, and their applications to advertising, recommendation, and/or search domain • experience setting the direction for a team whose primary output is online ranking / recommendation models • ability to design, train, and optimize advanced machine learning models • excellent programming and software design skills • ability to proactively learn new concepts and technology and apply them at work • skilled at solving ambiguous problems and leading and executing complex technical initiatives • strong collaboration and mentorship skills minimum qualifications: • bachelor's in a technical field such as computer science, mathematics, statistics or equivalent years of experience • 9+ years of post-bachelor's machine learning experience; or a master's degree in a technical field + 8+ year of post-grad ml experience; or a phd in a related technical field + 5+ years of post-grad ml experience • 2+ years of experience with technical leadership or acting as the domain-expert to a technical organization • experience developing and shipping performant and scalable machine learning models for recommendation or ranking use cases • experience with tensorflow, pytorch, or related deep learning frameworks preferred qualifications: • experience in online advertising, including ad targeting, ranking, auction, and/or marketplace optimization. • advanced degree in a related field such as machine learning, computer vision, or mathematics • experience partnering with cross-functional executives and management across a globally distributed organization and exercising sound judgment • track record of delivery in rapidly changing, highly collaborative, multi-site, multi-stakeholder environments • experience working with a diverse group of engineers • experience contributing to ai publications if you have a disability or special need that requires accommodation, please don't be shy and provide us some information. ""default together"" policy at snap: at snap inc. we believe that being together in person helps us build our culture faster, reinforce our values, and serve our community, customers and partners better through dynamic collaboration. to reflect this, we practice a ""default together"" approach and expect our team members to work in an office 4+ days per week. at snap, we believe that having a team of diverse backgrounds and voices working together will enable us to create innovative products that improve the way people live and communicate. snap is proud to be an equal opportunity employer, and committed to providing employment opportunities regardless of race, religious creed, color, national origin, ancestry, physical disability, mental disability, medical condition, genetic information, marital status, sex, gender, gender identity, gender expression, pregnancy, childbirth and breastfeeding, age, sexual orientation, military or veteran status, or any other protected classification, in accordance with applicable federal, state, and local laws. eoe, including disability/vets. we are an equal opportunity employer and will consider qualified applicants with criminal histories in a manner consistent with applicable law (by example, the requirements of the san francisco fair chance ordinance and the los angeles fair chance initiative for hiring, where applicable). our benefits: snap inc. is its own community, so we've got your back! we do our best to make sure you and your loved ones have everything you need to be happy and healthy, on your own terms. our benefits are built around your needs and include paid parental leave, comprehensive medical coverage, emotional and mental health support programs, and compensation packages that let you share in snap's long-term success! compensation in the united states, work locations are assigned a pay zone which determines the salary range for the position. the successful candidate's starting pay will be determined based on job-related skills, experience, qualifications, work location, and market conditions. the starting pay may be negotiable within the salary range for the position. these pay zones may be modified in the future. zone a (ca, wa, nyc): the base salary range for this position is $276,000-$414,000 annually. zone b: the base salary range for this position is $262,000-$393,000 annually. zone c: the base salary range for this position is $235,000-$352,000 annually. this position is eligible for equity in the form of rsus.","bellevue, wa",Machine Learning Engineer,"['aws', 'classification', 'computer vision', 'deep learning', 'excel', 'machine learning', 'pytorch', 'r', 'recommendation', 'scala', 'statistics', 'tensorflow']","['aws', 'classification', 'computer vision', 'deep learning', 'excel', 'machine learning', 'pytorch', 'r', 'recommendation', 'scala', 'statistics', 'tensorflow']",
artificial intelligence/machine learning (ai/ml) engineer,"base-2 solutions, llc","about the position requirements • select appropriate data sets. • perform statistical analysis. • run machine learning algorithms. • use results to improve models. • train and retrain systems when needed. • experience in working with various ml libraries and packages. • run standard test and evaluation protocols. • provide system integration oversight. • oversee test and evaluation of ai and ml algorithms through an iterative design process to meet verification and validation requirements. • research and implement a broad range of ai and ml algorithms and tools. • design or select appropriate data and knowledge representation methods. • recognize software architecture, data modelling, and data structures. • transform and convert data science prototypes into scalable solutions. • verify data and model output quality. • identify differences in data distribution that affect model performance. • five (5) years experience in applied machine learning in programs and contracts of similar scope, type, and complexity is required. • a master's or ph.d. degree in advanced math, artificial intelligence, data science, computer science or deep learning from an accredited college or university. • five (5) additional years of machine learning experience with a relevant bachelor's degree may be substituted for a master's degree. • experience with standard machine language frameworks, e.g. pytorch, tensorflow.",maryland,Machine Learning Engineer,"['deep learning', 'machine learning', 'pytorch', 'r', 'scala', 'tensorflow']","['deep learning', 'machine learning', 'pytorch', 'r', 'scala', 'tensorflow']",
artificial intelligence / machine learning (ai / ml) mid-career systems engineer,peraton,"required qualifications: • a high school diploma or ged plus twelve (12) years of general systems engineering experience, or • bachelor’s degree in a qualified engineering field or a related discipline from an accredited college or university plus seven (7) years of systems engineering experience, or • a master’s degree in a qualified engineering field or a related discipline from an accredited college or university plus five (5) years of systems engineering experience, or • a phd in a qualified engineering field or a related discipline from an accredited college or university plus three (3) years of systems engineering experience. • position requires an active ts/sci clearance with polygraph. candidate's most recent security polygraph must have been within the last 5 years. • experience with cloud architecture/ cloud engineer/ platform engineering (especially in aws) • experience with ai/ml system design and implementation. this should include: • high-level understanding of the ai life-cycle (development, training, inference, monitoring) • experience with ml pipeline development, model deployment, and devops/mlops • must have understanding system security and networking (particularly the implementation of system security on classified systems) • must have understanding of data engineering and data life-cycle management desired qualifications: • experience with amazon safemaker applicants selected will be subject to a government security investigation and must meet eligibility requirements for access to classified information. peraton offers enhanced benefits to employees working on this critical national security program, which includes heavily subsidized employee benefits coverage for you and your dependents, 25 days of pto accrued annually up to a generous pto cap and eligible to participate in an attractive bonus plan. peraton is hiring a mid career systems engineer with artificial intelligence / machine learning (ai / ml) experience to facilitate the government customer's deployment of solutions as part of a large high performance computing (hpc) related program. this program is on the cutting edge and includes everything from hpc test planning and execution, architecture design and prototyping, and vendor outreach and collaboration support. program technical areas include commercial cloud technologies, high performance computing, and enterprise architecture. the program is tactically important to the national security of the united states and the work on these missions are frequently recognized for their results and the program integrator will be directly involved in ensuring the government is successful in achieving their planned objectives of this growing, high-profile program. the ai/ml mid-career systems engineer will: • analyze user’s requirements, concept of operations documents, and high-level system architectures to develop system requirements specifications. • analyze system requirements and lead design and development activities. guide users in formulating requirements, advises alternative approaches, and conduct feasibility studies. • provide technical leadership for the integration of requirements, design, and technology. • incorporate new plans, designs and systems into ongoing operations. • develop technical documentation. • develop system architecture and system design documentation. • guide system development and implementation planning through assessment or preparation of system engineering management plans and system integration and test plans. • interact with the government regarding systems engineering technical considerations and for associated problems, issues or conflicts. • be responsible for the technical integrity of work performed and deliverables associated with the systems engineering area of responsibility. • communicate with other program personnel, government overseers, and senior executives. candidate must have excellent communication and collaboration skills with a variety of stakeholder groups, ability to proactively engage with these groups to understand requirements and inform system architecture decisions.#ajcm","fort meade, md",Machine Learning Engineer,"['aws', 'cloud', 'excel', 'machine learning', 'r']","['aws', 'cloud', 'excel', 'machine learning', 'r']",86K–138K a year
lead machine learning engineer -ml / ai,capital one,"lead machine learning engineer -ml / ai at capital one, we are changing banking for good by creating responsible and reliable ai-powered systems. our investments in technology infrastructure and world-class talent — along with our deep experience in machine learning — position us to be at the forefront of enterprises leveraging ai. from informing customers about unusual charges to answering their questions in real time, our applications of ai & ml are bringing humanity and simplicity to banking. we are committed to continuing to build world-class applied science and engineering teams to deliver our industry leading capabilities with breakthrough product experiences and scalable, high-performance ai infrastructure. at capital one, you will help bring the transformative power of emerging ai capabilities to reimagine exceptional products for our customers. in risk tech, we provide the foundation for capital one to thrive in an uncertain world. our engaged, empowered, and intelligent people produce outstanding products, working toward the common goal of transforming risk management with technology. we build data-driven tools that use machine learning to prevent risks & automatically detect issues before they impact our customers, our business, or our communities. in this role at risk tech, you will work with our internal audit team and partners across the company to build and deploy proprietary solutions that are powered by state-of-the-art ai technology. our products, enhanced with the transformative power of ai, are central to our business and deliver tremendous customer value. you will : partner with a cross-functional team of engineers, data scientists, product managers, and designers to deliver ai-powered products that change how our associates work and provide value to our customers. design, develop, test, deploy, and support ai software components utilizing machine learning models, including model evaluation and experimentation, large language model inference, similarity search, guardrails, governance, observability and agentic ai. fine-tune, develop and evaluate machine learning and foundation models, collaborate as part of a cross-functional agile team to create and enhance software that utilizes state-of-the-art ai and ml capabilities contribute thought leadership and technical vision to the long term roadmap of pioneering ai systems at capital one. leverage a broad stack of open source and saas ai technologies. inform your ml infrastructure decisions using your understanding of ml modeling techniques and issues. retrain, maintain, and monitor models in production. construct optimized data pipelines to feed ml models. ensure all code is well-managed to reduce vulnerabilities, models are well-governed from a risk perspective, and the ml follows best practices in responsible and explainable ai. the ideal candidate : you love to build systems, take pride in the quality of your work, and also share our passion to do the right thing. you want to work on problems that will help change banking for good. you are a passionate communicator, comfortable with explaining complex technical concepts to non-technical partners across the business, sometimes in front of large audiences. passion for staying abreast of the latest research, and an ability to intuitively understand scientific publications and judiciously apply novel techniques in production. you adapt quickly and thrive on bringing clarity to big, undefined problems. you love asking questions and digging deep to uncover the root of problems and can articulate your findings concisely with clarity. you have the courage to share new ideas even when they are unproven. you are deeply technical. you possess a strong foundation in engineering and mathematics, and your expertise in hardware, software, and ai enable you to see and exploit optimization opportunities that others miss. you are a resilient trail blazer who can forge new paths to achieve business goals when the route is unknown. passion for staying abreast of the latest ai research and ai systems, and judiciously applying novel techniques in production strategic & business-oriented : you think beyond the technology, deeply understanding business needs and how ai can solve them. you don't just build; you strategize and prioritize work that delivers the greatest business value. highly collaborative & transparent : you are a natural partner, working seamlessly across engineering, product, and data science teams. you communicate your progress, blockers, and decisions clearly and proactively, ensuring everyone is aligned. you love sharing knowledge and insights and are committed to the success of the entire team. technically mature & humble : you possess a strong foundation in engineering and mathematics. you are a resilient problem-solver who can bring clarity to complex, undefined problems and articulate your findings concisely. you demonstrate professional maturity by committing to and executing on team decisions. flexible & fungible : you are eager to roll up your sleeves and contribute wherever the team needs you most. you are comfortable working across different aspects of the tech stack and adapting to evolving priorities. a lifelong learner : you love staying current with the latest ai research and can apply novel techniques to production systems judiciously, always with a focus on business impact. basic qualifications : bachelor's degree at least 6 years of experience designing and building data-intensive solutions using distributed computing (internship experience does not apply) at least 4 years of experience programming with python, go, scala, or java at least 3 years of experience deploying scalable software solutions on cloud platforms (e.g. aws, google cloud, azure, or equivalent private cloud) preferred qualifications : master's degree in computer science, ai, electrical engineering, computer engineering, or related fields plus at least 2 years of experience developing ai and ml algorithms or technologies 6 years of experience designing, developing, delivering, and supporting ai services at scale 3 years of experience developing ai and ml algorithms or technologies using python 2 years of experience with retrieval augmented generation (rag) experience deploying scalable ai / ml solutions in a public cloud such as aws bedrock, google cloud, azure experience designing, implementing, and scaling complex data pipelines for ml models and evaluating their performance at this time, capital one will not sponsor a new applicant for employment authorization, or offer any immigration related support for this position (i.e. h1b, f-1 opt, f-1 stem opt, f-1 cpt, j-1, tn, e-2, e-3, l-1 and o-1, or any eads or other forms of work authorization that require immigration support from an employer) the minimum and maximum full-time annual salaries for this role are listed below, by location. please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount capital one is willing to pay at the time of this posting. salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked. cambridge, ma : $193,400 - $220,700 for lead machine learning engineer mclean, va : $193,400 - $220,700 for lead machine learning engineer new york, ny : $211,000 - $240,800 for lead machine learning engineer richmond, va : $175,800 - $200,700 for lead machine learning engineer candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate’s offer letter. this role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and / or long term incentives (lti). incentives could be discretionary or non discretionary depending on the plan. capital one offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. learn more at the capital one careers website . eligibility varies based on full or part-time status, exempt or non-exempt status, and management level. this role is expected to accept applications for a minimum of 5 business days.no agencies please. capital one is an equal opportunity employer (eoe, including disability / vet) committed to non-discrimination in compliance with applicable federal, state, and local laws. capital one promotes a drug-free workplace. capital one will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, article 23-a of the new york correction law; san francisco, california police code article 49, sections 4901-4920; new york city’s fair chance act; philadelphia’s fair criminal records screening act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries. if you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact capital one recruiting at 1-800-304-9102 or via email at recruitingaccommodation@capitalone.com . all information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. for technical support or questions about capital one's recruiting process, please send an email to careers@capitalone.com capital one does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site. capital one financial is made up of several different entities. please note that any position posted in canada is for capital one canada, any position posted in the united kingdom is for capital one europe and any position posted in the philippines is for capital one philippines service corp. (copssc).","petersburg, va",Machine Learning Engineer,"['aws', 'azure', 'cloud', 'data pipeline', 'experimentation', 'google cloud', 'java', 'machine learning', 'python', 'r', 'scala']","['aws', 'azure', 'cloud', 'data pipeline', 'experimentation', 'google cloud', 'java', 'machine learning', 'python', 'r', 'scala']",
machine learning engineer - sales engineering,apple,"imagine what you could do here. at apple, great ideas have a way of becoming great products, services and customer experiences very quickly. bring passion and dedication to your job and there's no telling what you could accomplish. apple is where individual imaginations gather together, committing to the values that lead to great work. every new product we build, service we create, or apple store experience we deliver is the result of us making each other’s ideas stronger. apple’s sales engineering team is shaping the future of channel sales with innovative, high-impact applications. we’re looking for a machine learning engineer to help us design and build the next generation of intelligent systems that power apple’s global partner ecosystem. in this role, you’ll develop and deploy machine learning solutions while leveraging generative ai and advanced ml capabilities to deliver scalable, production-ready systems that accelerate strategic, high-impact initiatives across apple channel sales. if you’re passionate about applying ai to solve complex business problems, experimenting with emerging genai technologies, and building products that make a real difference, join our collaborative team and help us move fast on game-changing ideas. description apple’s sales engineering rapid application development (rad) team is looking for a machine learning engineer to build intelligent, scalable solutions that power apple’s global channel sales. you’ll leverage generative ai and advanced machine learning technologies to deliver high-performance, production-ready systems that drive measurable business impact. the ideal candidate blends deep ml expertise with strong engineering skills, is passionate about applying ai to solve real-world problems, and thrives in fast-paced environments delivering value quickly. you’ll work side by side with product, design, and engineering teams to design, train, deploy, and optimize ml-powered applications that push the boundaries of innovation-whether enabling genai-driven workflows, implementing rag-based systems, or pioneering new intelligent capabilities. if you’re excited about shaping impactful ai solutions in a collaborative, experiment-driven environment, sales engineering rad team is where you’ll thrive."",""responsibilities"":""design, build, and deploy scalable machine learning and generative ai solutions that power apple’s global channel sales ecosystem. develop and optimize ml pipelines leveraging llms, lmms, and rag-based architectures for production-grade applications. collaborate with cross-functional teams to translate business needs into intelligent, data-driven systems and workflows. fine-tune and evaluate transformer-based models (e.g., gpt, llama, bert) for accuracy, performance, and scalability. prototype and productionize emerging ai capabilities, including agentic workflows and generative assistants. apply mlops best practices for model training, deployment, monitoring, and continuous improvement. ensure secure, compliant handling of sensitive data (including pii) while maintaining apple’s privacy standards. preferred qualifications proven ability to fine-tune, adapt, and deploy llms/lmms into real-world, production-grade applications. proficiency in python and leading ml frameworks such as pytorch and tensorflow. hands-on experience leveraging hugging face transformers and associated libraries. solid understanding of retrieval-augmented generation (rag) and practical experience with orchestration frameworks like langchain or llamaindex. familiarity with distributed computing, cloud platforms (aws, gcp, azure), and containerization/orchestration tools (docker, kubernetes). exceptional problem-solving skills and the ability to articulate complex ml/ai concepts clearly and effectively to diverse audiences. experience extending beyond traditional llms/lmms to include agent-based systems and agentic workflows. proficiency with advanced llm serving and inference frameworks, ensuring scalable and efficient model deployment. practical experience building sophisticated rag applications and orchestrating complex llm pipelines from inception to deployment. working knowledge of distributed systems and cloud-native infrastructure. expertise in optimizing transformer-based architectures (e.g., bert, gpt, llama) for low-latency, high-performance inference. demonstrated ability to communicate complex technical results and ml/llm concepts with clarity and impact to both technical and non-technical stakeholders. experience applying ml methodologies in specific domains, such as sales. minimum qualifications m.s. in computer science, machine learning, artificial intelligence, or a closely related technical field, or equivalent practical experience. 5+ years experience developing and deploying machine learning solutions, with a strong focus on large language models (llms) or large multimodal models (lmms). 5+ years experience with llms and transformer-based architectures (e.g., bert, gpt, llama). apple is an equal opportunity employer that is committed to inclusion and diversity. we seek to promote equal opportunity for all applicants without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, veteran status, or other legally protected characteristics. learn more about your eeo rights as an applicant .",california,Machine Learning Engineer,"['aws', 'azure', 'cloud', 'gcp', 'machine learning', 'python', 'pytorch', 'r', 'scala', 'tensorflow']","['aws', 'azure', 'cloud', 'gcp', 'machine learning', 'python', 'pytorch', 'r', 'scala', 'tensorflow']",
machine learning engineer/scientist jobs,precise systems,"as a solutions-based company, precise systems is focused on innovation in support of the warfighters and aligning our expertise to deliver the highest quality of service. powered by our highly skilled workforces, we provide expert consultation on network and weapons systems acquisition programs, maintenance/modernization programs, and sustainment programs with solutions in digital transformation, advanced engineering, physical sciences research, platform lifecycle support, and technical services. since our establishment, we have demonstrated and perfected our ability to manage the customer's needs, no matter how complex - offering exceptional service and the precise solution. precise systems seeks a machine learning engineer or scientist with experience in the physical sciences to support innovative chemical and biological defense projects that integrate automation, and physical science principles. the selected candidate will apply their academic and practical knowledge, experience, and expertise to optimize biomanufacturing data pipelines and perform data analytics using data science principles. the role involves integrating predictive performance metrics into laboratory processes using pythonic dashboards, adapting metrics and models to project needs, and executing them in laboratory settings to achieve desired outcomes. under the guidance of senior professionals, the individual will perform tasks encompassing research, development, testing, and evaluation of machine learning models towards chemical and biological datasets. the candidate will contribute to advancing chemical and biological research and development that drives innovation to out-pace emerging threats while simultaneously providing modern solutions specific tasks include but are not limited to: • collaborate with interdisciplinary teams to develop machine learning solutions for biomanufacturing challenges. • develop and apply state-of-the-art machine learning (ml) tools and algorithms to optimize dod research efforts. • perform internal verification and validation of models and code to ensure deployments for real laboratory use. • develop and implement dashboards or other graphical user interfaces for automation for laboratory experimentation. l. • analyze biological data using languages such as python and r to derive mathematical insights and improve system performance. • employ devsecops and mlops to integrate machine learning processes into secure data pipelines. • collaborate with academic and industry partners to test and refine algorithms and biological systems in diverse environments. • perform research to keep models and analysis up to date with the latest advancements. required education: • master's degree in computer science, computer engineering, computational biology, bioinformatics, , or a related field with three (3) years of experience in a laboratory and/or computational environment. • or a bachelor's degree in a relevant field with five (5) years of experience in a laboratory and/or computational environment. preferred education: • ph.d. in computational biology, machine learning, or a related field with one (1) year of experience in a laboratory and/or computational environment. required experience: • hands-on experience applying machine learning algorithms and data science techniques to physical science problems. • proficiency in python and other programming languages commonly used in machine learning and data analysis. • familiarity with physical science and laboratory experience. • proficiency with software development approaches such as agile, git, devsecops, and/or mlops. • advanced communication skills for interacting with interdisciplinary teams, government representatives, and industry partners. • willingness to learn new technologies and take on new challenges. • strong written and oral communication skills, excellent interpersonal skills, and proficiency in pc software packages for data analysis and visualization. preferred experience: • expertise in advanced machine learning frameworks (e.g., tensorflow, pytorch) and their application to biological data. • experience with bioinformatics tools and databases. • experience performing wet laboratory experimentation to validate software and models. • knowledge of automation systems and robotics for laboratory workflows. • collaboration with academic and industry partners on synthetic biology and machine learning projects must be able to obtain and maintain a secret security clearance. due to the sensitivity of customer-related requirements, u.s. citizenship is required. precise systems values employee contributions, promotes diverse opportunities for professional growth, and prioritizes overall well-being. our comprehensive professional services benefits package includes health insurance, life and accidental death and dismemberment coverage, disability insurance, retirement plans, holiday pay, employee-managed leave, and professional growth opportunities. we recognize exceptional performance and alignment with our core values through our star award recognition program. compensation at precise systems is determined by various factors, including education, experience, skills, competencies, and contract-specific requirements. the salary range for this position is $76,709.46 to $115,064.18 (annualized usd). this range represents the standard pay for this role and is just one component of precise systems' total compensation package. precise systems is committed to fair and equitable pay practices in alignment with applicable pay transparency laws and equal employment opportunity standards. precise systems is dedicated to a shared vision and core values of integrity, respect, and responsibility, which foster innovation and drive our continued success in the global marketplace. precise systems and its subsidiaries are equal opportunity /affirmative action employers. all qualified applicants will receive consideration for employment without regard to race, color, religion, age, sex, sexual orientation, gender identity, national origin, protected veteran status, status as an individual with a disability, or any legally protected status under federal, state, or local law. visit www.goprecise.com for a listing of current openings and our comprehensive, employee friendly benefits summary. precise systems participates in e-verify.","edgewood, md",Machine Learning Engineer,"['aws', 'dashboard', 'data analysis', 'data analytics', 'data pipeline', 'excel', 'experimentation', 'machine learning', 'python', 'pytorch', 'r', 'tensorflow']","['aws', 'dashboard', 'data analysis', 'data analytics', 'data pipeline', 'excel', 'experimentation', 'machine learning', 'python', 'pytorch', 'r', 'tensorflow']","76,709.46–115,064.18 a year"
ml/ai engineer,rillet,"what we do rillet serves finance teams. our customers are the financial brains of their companies. our job is to help them run the numbers with impossible speed, accuracy, and insight. rillet is an ai-native erp that can drive a zero-day close. we are different because of our unified source-of-truth data model, hundreds of best-in-class native integrations (stripe, ramp, salesforce, etc), automated & auditable workflows, multi-entity consolidation, and quickly expanding army of specialized ai agents (e.g., accrual, audit, p&l flux, board decks, etc). these earn us a consistently perfect customer satisfaction score. high-growth ai customers like windsurf, postscript, and finch love our ship velocity, because their financial stack needs to scale as quickly as they do. this huge market is ours to take. we have raised $100m from leading investors (including sequoia, a16z, iconiq, oak hc/ft, and first round) to help everyone run their numbers at the speed of ai. who we are rillet’s pace is not for everyone. intelligence is table stakes. to succeed here, you need extreme speed, agency, and flexibility. successful rilleteers do not wait for assignments. they internalize a mission, design a strategy, and bring back results that are better, faster, and more creative than a manager could have asked for. work revolves around our customers. successful rilleteers are energized by delivering the most important things, even those that weren’t in the original plan. in this role, you do not need to be an accountant. but you do need to appreciate the value that our customers can create for their own company when we equip them with the perfect financial tools. successful rilleteers love powering the financial core of the world’s fastest-growing companies. who we need: we're looking for a machine learning engineer to build the ai systems that will define the future of accounting software. you'll work directly with our cto to architect solutions that turn complex financial workflows into elegant, automated experiences—the kind of problems that actually matter. while we offer flexible work arrangements, we're building our presence in the san francisco bay area and new york city. we're looking for candidates who can work hybrid (in-office at least 2 days per week) from one of these locations or are open to relocating. what you'll do: • shape rillet's ml strategy and infrastructure for the long term • own problems end-to-end, from understanding user needs with product and design teams to deploying production systems • build automated pipelines for model training, evaluation, and deployment • ensure ml systems scale reliably under real-world conditions what we're looking for: • 3+ years building ml products at tech companies • strong python skills (numpy, pandas, sklearn, pytorch) and experience across ml techniques and backend systems • experience with llm-based agent frameworks • bachelor's in cs or related field (master's/phd in ai preferred) • someone who finds practical solutions without cutting corners and enjoys both independent work and tight collaboration life at rillet: • competitive pay & benefits: backed by world-class investors, we offer strong salaries plus equity so you share in our success. we've got you covered with top-tier health and dental insurance, premiums partially or fully covered for you, plus 90% coverage for dependents. • room to grow: we're building a team of ambitious, high-performing people who will grow with the company. as rillet scales, so will your role, responsibilities, and compensation. • flexibility that works: take the time you need with flexible pto and 9 company-wide holidays. we value both the flexibility of remote and hybrid work and the creativity and energy that comes from in-person collaboration at our hubs in san francisco, nyc, and barcelona. • build real connections: great work happens when people connect. join us for team offsites in incredible locations, our team has bonded everywhere from new york and san francisco to toronto, italy, france, and beyond.",anywhere,Machine Learning Engineer,"['machine learning', 'numpy', 'pandas', 'python', 'pytorch', 'r', 'sklearn']","['machine learning', 'numpy', 'pandas', 'python', 'pytorch', 'r', 'sklearn']",
machine learning engineer i - multimodal artificial intelligence for women's health (on site),mount sinai health systems,"description machine learning engineer i will be primarily responsible for contributing to the development and enhancement of machine learning applications and systems. they will work closely with other engineers and data scientists to design and implement scalable and efficient machine learning systems. we are recruiting a machine learning engineer i to support the lab’s core projects in multimodal ai for women’s health. the engineer will be responsible for building, optimizing, and deploying ml pipelines at scale, working with both postdocs and clinicians. this role is ideal for an applied researcher who is excited about translational machine learning and thrives in a collaborative, interdisciplinary environment. heavy menstrual bleeding affects nearly one in three women of reproductive age and is a leading cause of iron deficiency worldwide. yet it remains one of the most under-recognized challenges in medicine. our lab at the intersection between the artificial intelligence and human health department and the department for obstetrics, gynecology and reproductive sciences at mount sinai has been awarded a wellcome leap missed vital sign grant to change this. we are building a new, interdisciplinary group at the intersection of ai, human health, and obstetrics & gynecology. our mission is to harness state-of-the-art methods in machine learning and multimodal data integration to close critical gaps in women’s health—and to translate these advances into solutions that matter for patients and clinicians. as a founding member, you will help shape a lab designed for openness, collaboration, and translation. you will have access to unique resources including mount sinai’s genome-linked ehr biobank (the sinai million), airms (ai-ready mount sinai integrated data and analytics platform), the minerva hpc cluster, and ehive, a digital platform for wearable and real-world data collection. partnerships with the hasso plattner institute in germany create further opportunities for international collaboration. this is a chance to join at the ground level of a lab committed to impact: bringing computational innovation directly into women’s health. responsibilities • build, train and evaluate machine learning models on large scale multimodal datasets (wearables, imaging, genomics, ehr) • develop and maintain reproductible, scalable ml pipelines using pytorch • run experiments on hpc clusters (minerva) and support distributed learning (e.g. accelerate, lightning) • optimize workflows for compute and data efficiency • collaborate with post-doctoral fellows and clinical researchers to translate models into practice • contribute to codebases, documentation and open source tools • assist in the collection, cleaning, and curation of large data sets. • assist in the operationalization of machine learning models. • participate in evaluating model performance and contribute to model refinement. • work with other team members to deploy machine learning models. • contribute to maintaining clear and organized documentation of machine learning systems. • stay updated with the latest trends and technologies in the machine learning field. • work collaboratively with a multidisciplinary team to ensure the effectiveness of machine learning systems. • develop and maintain project work plans, including critical tasks, milestones, timelines, interdependencies, and contingencies. tracks and reports progress. keeps stakeholders apprised of project status and implications for completion. • prepare clear, well-organized project-specific documentation, including, at a minimum, analytic methods used, key decision points and caveats, with sufficient detail to support comprehension and replication. • share development and process knowledge with other analysts in order to assure redundancy and continuously builds a core of analytical strength within the organization. • adhere to corporate standards for performance metrics, data collection, data integrity, query design, and reporting format to ensure high quality, meaningful analytic output. • works closely with it on the ongoing improvement of mount sinai’s integrated data warehouse, driven by strategic and business needs, and designed to ensure data and reporting consistency throughout the organization. • demonstrates advanced level proficiency with the principles and methodologies of process improvement. applies these in the execution of responsibilities in support of a process focused approach. • other duties as assigned. qualifications requirements • bachelor's degree in computer science, statistics, mathematics, data science, biomedical informatics or related field. • experience in applied machine learning and deep learning using pytorch • experience in hpc environments, distributed training, and large scale data processing • familiarity with version control, containerization (docker, singularity) and reproducible research practices • experience with clinical data and biomedical informatics (omop, fhir) preferred • background in multi-modal machine learning, time series analysis, or computer vision preferred • azure cloud experience preferred • interest in translational applications in women's health preferred • knowledge of at least one programming language among scala, python, java, c, or c++. • knowledge of big data technologies (e.g., hadoop, spark) • knowledge of software development lifecycle. • self-motivated with a demonstrated ability to work independently, and to exercise independent judgment in developing complex techniques or programs in a dynamic environment. • act as the major contributor in the development and operationalization of four different applications. • play a key technical role in maintaining deployed products • understanding of machine learning algorithms (supervised, unsupervised ml algorithms). • familiarity with sql or other database languages. employer description strength through unity and inclusion the mount sinai health system is committed to fostering an environment where everyone can contribute to excellence. we share a common dedication to delivering outstanding patient care. when you join us, you become part of mount sinai’s unparalleled legacy of achievement, education, and innovation as we work together to transform healthcare. we encourage all team members to actively participate in creating a culture that ensures fair access to opportunities, promotes inclusive practices, and supports the success of every individual. at mount sinai, our leaders are committed to fostering a workplace where all employees feel valued, respected, and empowered to grow. we strive to create an environment where collaboration, fairness, and continuous learning drive positive change, improving the well-being of our staff, patients, and organization. our leaders are expected to challenge outdated practices, promote a culture of respect, and work toward meaningful improvements that enhance patient care and workplace experiences. we are dedicated to building a supportive and welcoming environment where everyone has the opportunity to thrive and advance professionally. explore this opportunity and be part of the next chapter in our history. about the mount sinai health system: mount sinai health system is one of the largest academic medical systems in the new york metro area, with more than 48,000 employees working across eight hospitals, more than 400 outpatient practices, more than 300 labs, a school of nursing, and a leading school of medicine and graduate education. mount sinai advances health for all people, everywhere, by taking on the most complex health care challenges of our time — discovering and applying new scientific learning and knowledge; developing safer, more effective treatments; educating the next generation of medical leaders and innovators; and supporting local communities by delivering high-quality care to all who need it. through the integration of its hospitals, labs, and schools, mount sinai offers comprehensive health care solutions from birth through geriatrics, leveraging innovative approaches such as artificial intelligence and informatics while keeping patients’ medical and emotional needs at the center of all treatment. the health system includes more than 9,000 primary and specialty care physicians; 13 joint-venture outpatient surgery centers throughout the five boroughs of new york city, westchester, long island, and florida; and more than 30 affiliated community health centers. we are consistently ranked by u.s. news & world report's best hospitals, receiving high ""honor roll"" status, and are highly ranked: no. 1 in geriatrics, top 5 in cardiology/heart surgery, and top 20 in diabetes/endocrinology, gastroenterology/gi surgery, neurology/neurosurgery, orthopedics, pulmonology/lung surgery, rehabilitation, and urology. new york eye and ear infirmary of mount sinai is ranked no. 12 in ophthalmology. u.s. news & world report’s “best children’s hospitals” ranks mount sinai kravis children's hospital among the country’s best in several pediatric specialties. the icahn school of medicine at mount sinai is ranked no. 11 nationwide in national institutes of health funding and in the 99th percentile in research dollars per investigator according to the association of american medical colleges. newsweek’s “the world’s best smart hospitals” ranks the mount sinai hospital as no. 1 in new york and in the top five globally, and mount sinai morningside in the top 20 globally. equal opportunity employer the mount sinai health system is an equal opportunity employer, complying with all applicable federal civil rights laws. we do not discriminate, exclude, or treat individuals differently based on race, color, national origin, age, religion, disability, sex, sexual orientation, gender, veteran status, or any other characteristic protected by law. we are deeply committed to fostering an environment where all faculty, staff, students, trainees, patients, visitors, and the communities we serve feel respected and supported. our goal is to create a healthcare and learning institution that actively works to remove barriers, address challenges, and promote fairness in all aspects of our organization. compensation the mount sinai health system (mshs) provides salary ranges that comply with the new york city law on salary transparency in job advertisements. the salary range for the role is $109000 - $163695 annually. actual salaries depend on a variety of factors, including experience, education, and operational need. the salary range or contractual rate listed does not include bonuses/incentive, differential pay or other forms of compensation or benefits.","new york, ny",Machine Learning Engineer,"['aws', 'azure', 'c++', 'cloud', 'computer vision', 'data warehouse', 'deep learning', 'excel', 'hadoop', 'java', 'machine learning', 'python', 'pytorch', 'r', 'scala', 'spark', 'sql', 'statistics', 'time series']","['aws', 'azure', 'c++', 'cloud', 'computer vision', 'data warehouse', 'deep learning', 'excel', 'hadoop', 'java', 'machine learning', 'python', 'pytorch', 'r', 'scala', 'spark', 'sql', 'statistics', 'time series']","109,000–163,695 a year"
machine learning engineer,full scope,"job title: machine learning engineer location: fort meade, md required clearance: ts/sci w/ full-scope poly salary: competitive we are seeking a highly skilled and motivated machine learning engineer to join our dynamic team. the ideal candidate will have a strong background in machine learning, data science, and software engineering. you will work closely with data scientists, engineers, and product managers to design, develop, and deploy machine learning models and solutions that drive business value. key responsibilities: • design, develop, and implement machine learning models and algorithms to solve real-world problems. • collaborate with cross-functional teams to understand business requirements and translate them into technical solutions. • conduct data analysis and preprocessing to ensure high-quality data for model training. • optimize and fine-tune models for performance, accuracy, and scalability. • deploy machine learning models into production and monitor their performance. • develop and maintain machine learning pipelines and infrastructure. • stay current with the latest research and advancements in machine learning and ai. • participate in code reviews, team meetings, and contribute to a collaborative development environment. • document processes, models, and findings comprehensively. qualifications: • bachelor’s or master’s degree in computer science, engineering, mathematics, or a related field. ph.d. is a plus. • proven experience as a machine learning engineer or in a similar role. • strong proficiency in programming languages such as python, r, or java. • experience with machine learning frameworks and libraries such as tensorflow, pytorch, scikit-learn, etc. • solid understanding of machine learning algorithms, including supervised and unsupervised learning, reinforcement learning, and deep learning. • experience with data processing tools like pandas, numpy, and data visualization tools such as matplotlib or seaborn. • familiarity with cloud platforms like aws, google cloud, or azure for model deployment and scaling. • strong problem-solving skills and the ability to think critically and analytically. • excellent communication and teamwork skills. preferred qualifications: • experience with natural language processing (nlp) and computer vision. • familiarity with big data technologies such as hadoop, spark, or kafka. • knowledge of software development best practices and version control systems like git. • experience with containerization tools like docker and orchestration tools like kubernetes. • previous experience in a fast-paced, startup environment.","fort meade, md",Machine Learning Engineer,"['aws', 'azure', 'cloud', 'computer vision', 'data analysis', 'deep learning', 'excel', 'google cloud', 'hadoop', 'java', 'kafka', 'machine learning', 'matplotlib', 'natural language processing', 'nlp', 'numpy', 'pandas', 'python', 'pytorch', 'r', 'scala', 'scikit-learn', 'seaborn', 'spark', 'tensorflow']","['aws', 'azure', 'cloud', 'computer vision', 'data analysis', 'deep learning', 'excel', 'google cloud', 'hadoop', 'java', 'kafka', 'machine learning', 'matplotlib', 'natural language processing', 'nlp', 'numpy', 'pandas', 'python', 'pytorch', 'r', 'scala', 'scikit-learn', 'seaborn', 'spark', 'tensorflow']",
machine learning engineer 100% remote,sherlocktalent,"location: 100% remote, work from anywhere in the world (company based in south florida) role: machine learning engineer compensation: depends on experience, us $170k+ doe job type: full time w2 or fulltime contract, c2c we are looking for an entrepreneurial minded machine learning engineer to help us disrupt the e-commerce industry. we believe it is time for technology and data to help merchants deliver the best pricing to their customers, no matter what. if you are looking for an opportunity to join a start-up led by a proven founder in an industry that is growing exponentially, and an opportunity to work alongside a chief data scientist and another ml engineer, then this is the opportunity to work on something not only exciting and fun, but also creative in ways that most people will never experience in their lifetime. what you will do you will work side by side with e-commerce experts and lead in the design and development of machine learning models to deliver optimal pricing for merchants. you will work on determining and capturing features best suited to achieve accurate predictions including but not limited to indicators of revenue, price and conversation rates. in this role you will develop and validate machine learning models, research relevant statistical methods, perform data analysis, and provide recommendations for operationalizing machine learning models into production. who you are you are passionate about statistics and machine learning modeling. you are pragmatic and effective, looking for all possible ways of improving your models. you attack complex business questions with data and curiosity, diving below the surface to identify the root cause. you have a deep understanding of the different parts of the data pipeline, having been exposed to data analysis, data engineering, analytics engineering and data science. however, machine learning engineering is what you want to focus on. you thrive working in a startup “get it done” environment and continuously delivering models into production to test makes you happy. you always deliver quality work but you embrace failure as a way to improve! a successful candidate must: ? be extremely hands-on and proficient working with python (pandas, numpy, sklearn, etc). ? experienced with plotting in python (matplotlib, etc). dashboarding experience is a plus. ? be self-motivated and comfortable owning the entire lifecycle from beginning to end. ? be able to understand the business needs first and then apply technology to solve those problems. ? have experience aggregating data, exploring data, building & validating predictive models, and deploying completed models with concept-drift monitoring and retraining to deliver business impact to the organization. ? have strong analytical skills. what you need ? minimum 3-5 years working professionally in the machine learning and data analytics field. ? a higher education degree in computer science, mathematics, physics or data science is a major plus. ? proven experience taking machine learning algorithms from r&d to product, especially using aws cloud services. ? proficient with sql and data wrangling. ? real world experience as an ml engineer or data scientist role building and deploying ml models or developing deep learning models, with experience in at least 3 of the 5 key machine learning algorithms: ridge regression and lasso, random forests, linear optimization models, sensitivity analysis, and boosting model. to apply, please send us a brief paragraph about why you are interested and what you would do in your week. github examples of your proudest projects or open-source contributions is a plus so please send along. sherlock loves to share $1,000 referral bonuses for successful placements!","hollywood, fl",Machine Learning Engineer,"['aws', 'cloud', 'dashboard', 'data analysis', 'data analytics', 'data pipeline', 'deep learning', 'machine learning', 'matplotlib', 'numpy', 'pandas', 'python', 'r', 'recommendation', 'regression', 'sklearn', 'sql', 'statistics']","['aws', 'cloud', 'dashboard', 'data analysis', 'data analytics', 'data pipeline', 'deep learning', 'machine learning', 'matplotlib', 'numpy', 'pandas', 'python', 'r', 'recommendation', 'regression', 'sklearn', 'sql', 'statistics']",
"tech lead, machine learning engineer, perception",waymo,"the perception team builds the system which learns the spatial-temporal representation and their semantic meanings of the surrounding environment of the self-driving car, i.e., the system that ""perceives"" the world around the car. we work jointly with downstream teams on the optimization and integration into the waymo driver. we conduct our own research to address real-world problems and collaborate with research teams at alphabet. we have access to millions of miles of driving data from a diverse set of sensors, enabling researchers like you to develop models for panoptic segmentation and occupancy in 3d and over time. in this hybrid role you will report to a sr staff technical lead manager. you will: • own the panoptic segmentation task (instance segmentation and semantic segmentation), take responsibility for task scaling and task performance, take ownership of aligning instance segmentation and object detection/tracking (in particular in an end-to-end architecture), create ml methods and recipes to scale and improve tasks • collaborate with ml engineering/research teams to develop efficient model architectures for representing occupancy and panoptic segmentation, and train and deploy such models with a focus on using large pre-trained models • collaborate with partner teams to establish efficient human and machine labeling workflows • analyze panoptic segmentation issues in real-world application, identify issues and root causes, advise or develop short- and long-term solutions. • develop and maintain metrics for 4d (3d + time) instance segmentation and semantic segmentation you have: • bachelors in computer science or a similar discipline, or an equivalent amount of deep learning experience • 7+ years experience in machine learning and/or computer vision • experience with python • experience with ml frameworks like pytorch or jax we prefer: • ms or phd degree in machine learning, robotics, computer science or a similar discipline • publications at top-tier conferences like cvpr, iccv, eccv, iclr, icml, icra, iros, rss, neurips, aaai, ijcv, pami • experience with c++","san diego, ca",Machine Learning Engineer,"['c++', 'computer vision', 'deep learning', 'machine learning', 'python', 'pytorch', 'r']","['c++', 'computer vision', 'deep learning', 'machine learning', 'python', 'pytorch', 'r']",
"junior machine learning engineer, remote",experian,"experian is a global data and technology company, powering opportunities for people and businesses around the world. they are seeking a junior machine learning engineer to join the experian assistant team, focusing on building workflows, improving prompt engineering strategies, and integrating the latest llms. responsibilities • architect agentic workflows allowing multi-agent orchestration across experian assistant features • design and improve prompts for llms, and analytics feedback systems • integrate and evaluate llms (e.g., gpt-4, gpt-5, claude, llama) for assistant use cases including code advisor, analytics expert, and data tutor • collaborate with teams to implement latency-optimized solutions and support parallel model onboarding • contribute to deployment documentation and release planning using confluence and internal pr tracking systems • ensure code through automated testing, code reviews, and continuous integration pipelines • maintain deployment pipelines and ensure testing for beta clients, including glossary expansion and sandbox integration skills • bachelor's degree in computer science, machine learning, or related field • 3+ years of experience in ml engineering or backend development with python (3.8–3.12) • hands-on experience with llm integration, prompt engineering, and agentic frameworks (e.g., openai responses api, langchain) • experience with restful apis, and microservices • experience with devops best practices, including infrastructure as code, observability, and security • knowledge of rbac, audit logging, and version control for ml functions and libraries • experience with aws, docker, and kubernetes benefits • great compensation package and bonus plan • core benefits including medical, dental, vision, and matching 401k • flexible work environment, ability to work remote, hybrid or in-office • flexible time off including volunteer time off, vacation, sick and 12-paid holidays company overview • experian is a data analytics and consumer credit reporting company. it was founded in 1980, and is headquartered in dublin, dublin, irl, with a workforce of 10001+ employees. its website is http://www.experianplc.com. company h1b sponsorship • experian has a track record of offering h1b sponsorships, with 8 in 2025, 18 in 2024, 9 in 2023, 16 in 2022, 19 in 2021, 23 in 2020. please note that this does not guarantee sponsorship for this specific role.","mount pleasant township, pa",Machine Learning Engineer,"['aws', 'data analytics', 'machine learning', 'python', 'r']","['aws', 'data analytics', 'machine learning', 'python', 'r']",64K–111K a year
ai/ml engineer – genai assistant & experience engineering,adobe,"our company changing the world through digital experiences is what adobe’s all about. we give everyone—from emerging artists to global brands—everything they need to design and deliver exceptional digital experiences! we’re passionate about empowering people to create beautiful and powerful images, videos, and apps, and transform how companies interact with customers across every screen. we’re on a mission to hire the very best and are committed to creating exceptional employee experiences where everyone is respected and has access to equal opportunity. we realize that new ideas can come from everywhere in the organization, and we know the next big idea could be yours! about the team the gen ai experience engineering team is defining the future of intelligent workflows across adobe experience cloud with the ai assistant. we operate like a startup within adobe—fast, iterative, and customer-focused. our mission is to build the intelligent systems, models, and services that power adobe’s ai assistant experience. the opportunity we’re looking for an ai/ml engineer who is excited to build intelligent features using whatever approach solves the problem best. some solutions will use llms and generative ai, some will rely on traditional ml, some will be heuristic or rules-based, and many will be hybrid systems that combine them. your job is to evaluate the problem, select the appropriate approach, and deliver high-quality, scalable solutions that improve user experiences. in this role, you’ll build models, design prompts, develop services, run evaluations, and ship features end-to-end. this is a hands-on applied engineering role with broad ownership, where you’ll work across modeling, service development, and lightweight ops. what you’ll do ai, ml & hybrid solution development • build and iterate on solutions using the full spectrum of approaches: llms, classical ml, heuristics, rules engines, retrieval systems, or combinations thereof. • design, train, fine-tune, and evaluate models for generative ai, classification, search, and content understanding. • develop prompting strategies, multi-step prompt workflows, and agents that power interactive ai experiences. • build hybrid pipelines that combine deterministic logic with ai/ml components for predictable, reliable outcomes. service & feature engineering • implement backend services and inference pipelines for the ai assistant across experience cloud. • build rag systems, model-serving layers, experimentation hooks, and scalable apis. • partner with frontend engineers and product teams to turn concepts into shipped features. evaluation, data, and light ops • build automated evaluation pipelines to measure quality, safety, latency, and reliability. • prepare datasets for evaluation, fine-tuning, and experimentation. • deploy models and services using ci/cd, containers, and cloud workflows. • monitor performance and iterate quickly based on data and user signals. while this is not a dedicated ops role you should be able to own and operate your work end-to-end as projects require. cross-functional collaboration • work closely with product, engineering, design and ml teams to explore new ideas and deliver customer-facing features. • help define guidelines for ai/ml development, evaluation, and hybrid system building. • contribute to shared tools that accelerate experimentation and improve developer productivity. what you bring • 5+ years experience in machine learning, applied ai engineering, ir or full-stack intelligent feature development. • hands-on experience with both llm-based and traditional ml techniques, and the judgment to choose the right tool for the job. • strong software engineering fundamentals and experience building production services (node, python, typescript, go, or similar). • experience building and deploying intelligent systems across cloud environments (azure, aws, gcp). • ability to design evaluation frameworks, run experiments, and iterate rapidly. • comfortable owning features from prototype → production, including monitoring and optimization. • excellent communication and collaboration skills; thrives in fast-paced environments with ambiguity and autonomy. nice to have • experience with hybrid llm + deterministic systems, vector search, or orchestration tools. • knowledge of adobe experience cloud or other enterprise saas ecosystems. • contributions to open-source ai/ml tools, model-serving frameworks, or evaluation libraries. • prior startup or high-velocity product development experience. our compensation reflects the cost of labor across several u.s. geographic markets, and we pay differently based on those defined markets. the u.s. pay range for this position is $142,700 -- $257,600 annually. pay within this range varies by work location and may also depend on job-related knowledge, skills, and experience. your recruiter can share more about the specific salary range for the job location during the hiring process. at adobe, for sales roles starting salaries are expressed as total target compensation (ttc = base + commission), and short-term incentives are in the form of sales commission plans. non-sales roles starting salaries are expressed as base salary and short-term incentives are in the form of the annual incentive plan (aip). in addition, certain roles may be eligible for long-term incentives in the form of a new hire equity award. state-specific notices: california: fair chance ordinances adobe will consider qualified applicants with arrest or conviction records for employment in accordance with state and local laws and “fair chance” ordinances. colorado: application window notice there is no deadline to apply to this job posting because adobe accepts applications for this role on an ongoing basis. the posting will remain open based on hiring needs and position availability. massachusetts: massachusetts legal notice it is unlawful in massachusetts to require or administer a lie detector test as a condition of employment or continued employment. an employer who violates this law shall be subject to criminal penalties and civil liability. adobe is proud to be an equal employment opportunity employer. we do not discriminate based on gender, race or color, ethnicity or national origin, age, disability, religion, sexual orientation, gender identity or expression, veteran status, or any other applicable characteristics protected by law. learn more. adobe aims to make adobe.com accessible to any and all users. if you have a disability or special need that requires accommodation to navigate our website or complete the application process, email accommodations@adobe.com or call (408) 536-3015.","san jose, ca",Machine Learning Engineer,"['aws', 'azure', 'classification', 'cloud', 'excel', 'experimentation', 'feature engineering', 'gcp', 'machine learning', 'python', 'r', 'scala']","['aws', 'azure', 'classification', 'cloud', 'excel', 'experimentation', 'feature engineering', 'gcp', 'machine learning', 'python', 'r', 'scala']",
distinguished machine learning engineer- capital one software (remote),capital one,"distinguished machine learning engineer- capital one software (remote) ever since our first credit card customer in 1994, capital one has recognized that technology and data can enable even large companies to be innovative and personalized. as one of the first large enterprises to go all-in on the public cloud, capital one needed to build cloud and data management tools that didn’t exist in the marketplace to enable us to operate at scale in the cloud. and in 2022, we publicly announced capital one software and brought our first b2b software solution, slingshot, to market. building on capital one’s pioneering adoption of modern cloud and data capabilities, capital one software is helping accelerate the data management journey at scale for businesses operating in the cloud. if you think of the kind of challenges that companies face – things like data publishing, data consumption, data governance, and infrastructure management – we’ve built tools to address these various needs along the way. capital one software will continue to explore where we can bring our solutions to market to help other businesses address these same needs going forward. we are seeking top tier talent to join our pioneering team and propel us towards our destination. you will be joining a team of innovative product, tech, and design leaders that tirelessly seek to question the status quo. as a capital one distinguished machine learning engineer, you’ll have the opportunity to be on the forefront of building this business and bring these tools to market. responsibilities: • build awareness, increase knowledge and drive adoption of modern technologies, sharing consumer and engineering benefits to gain buy-in • collaborate on capital one’s toughest issues, to deliver on business needs that directly impact the lives of our customers and associates • strike the right balance between lending expertise and providing an inclusive environment where others’ ideas can be heard and championed; leverage expertise to grow skills in the broader capital one team • promote a culture of engineering excellence, using opportunities to reuse and innersource solutions where possible • effectively communicate with and influence key stakeholders across the enterprise, at all levels of the organization • operate as a trusted advisor for a specific technology, platform or capability domain, helping to shape use cases and implementation in an unified manner • lead the way in creating next-generation talent for tech, mentoring internal talent and actively recruiting external talent to bolster capital one’s tech talent basic qualifications: • bachelors degree • at least 7 years of experience in software engineering • at least 5 years of experience in machine learning systems and infrastructure preferred qualifications: • masters’ degree • 7+ years of experience using python, java, go, and sql • 6+ year of experience deploying machine learning models • ability to understand deploying machine learning models in production environments at scale • experience using large language models (llms) to prompt engineering, fine-tune foundation models and evaluation and alignment of llms • 3+ years of experience with kafka, airflow, spark, aws glue/kinesis • 3+ years of experience with databricks or snowflake • experience using llm ops tooling and practices in model versioning, deployment, cost optimization and latency reduction at this time, capital one will not sponsor a new applicant for employment authorization for this position. the minimum and maximum full-time annual salaries for this role are listed below, by location. please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount capital one is willing to pay at the time of this posting. salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked. remote (regardless of location): $239,900 - $273,800 for distinguished machine learning engineer richmond, va: $239,900 - $273,800 for distinguished machine learning engineer candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate’s offer letter. this role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (lti). incentives could be discretionary or non discretionary depending on the plan. capital one offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. learn more at the capital one careers website. eligibility varies based on full or part-time status, exempt or non-exempt status, and management level. this role is expected to accept applications for a minimum of 5 business days. no agencies please. capital one is an equal opportunity employer (eoe, including disability/vet) committed to non-discrimination in compliance with applicable federal, state, and local laws. capital one promotes a drug-free workplace. capital one will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, article 23-a of the new york correction law; san francisco, california police code article 49, sections 4901-4920; new york city’s fair chance act; philadelphia’s fair criminal records screening act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries. if you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact capital one recruiting at 1-800-304-9102 or via email at recruitingaccommodation@capitalone.com. all information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. for technical support or questions about capital one's recruiting process, please send an email to careers@capitalone.com capital one does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site. capital one financial is made up of several different entities. please note that any position posted in canada is for capital one canada, any position posted in the united kingdom is for capital one europe and any position posted in the philippines is for capital one philippines service corp. (copssc).","richmond, va",Machine Learning Engineer,"['airflow', 'aws', 'cloud', 'databricks', 'excel', 'java', 'kafka', 'machine learning', 'python', 'r', 'snowflake', 'spark', 'sql']","['airflow', 'aws', 'cloud', 'databricks', 'excel', 'java', 'kafka', 'machine learning', 'python', 'r', 'snowflake', 'spark', 'sql']",
lead machine learning engineer (geometry / 3d / simulation) (sunnyvale),attis,"lead machine learning engineer generative ai for the physical world overview a rare opportunity has emerged for a visionary lead machine learning engineer to build the core intelligence for a stealth-mode, well-funded ai company. this foundational leadership role is for someone passionate about teaching machines to understand and engineer the physical world, moving beyond language and into the complex domain of 3d geometry and physics. why join? this is a chance to define a new category of software from the ground up. you will have the autonomy to set the technical vision, build a world-class team, and solve fundamental ai challenges that directly impact how next-generation products are designed and built. you will be instrumental in creating the brain that powers a revolutionary platform. the company my client is a pioneering startup developing a new class of generative ai. their platform uses autonomous agents to design, test, and optimize complex physical systems, aiming to radically accelerate today's slow and manual engineering cycles. backed by top-tier investors, the company is positioned to become a leader in the application of ai to advanced hardware design and manufacturing. the role as the lead machine learning engineer, you will architect the company's entire ml strategy. you will lead the research and development of the core models that allow ai agents to perceive and reason about 3d geometry. your primary focus will be on defining the technical roadmap, hiring and mentoring a high-performing team, and guiding the development of novel techniques in geometric deep learning and multimodal reasoning. the essential requirements • proven experience leading machine learning teams, setting technical direction, and mentoring senior engineers. • deep expertise in 3d perception, scene understanding, or geometric deep learning (e.g., from robotics, av, ar / vr). • a strong track record of architecting and deploying complex ml systems from research to production. • fluency with modern ml frameworks (e.g., pytorch) and models for geometric data (gnns, 3d cnns, nerfs). • ability to translate ambiguous product goals into a concrete technical roadmap and execute effectively. what will make you stand out • a phd in a relevant field such as computer science or robotics. • a background in mechanical, aerospace, or a related engineering discipline. • experience integrating perception models with llms or other agent-based systems. • a history of publications in top-tier machine learning conferences. benefits • up to $260,000 base • meaningful equity • a key leadership role with the autonomy to shape the companys technical culture and direction. • the opportunity to solve high-impact problems at the intersection of ai and engineering. if you are interested in this role, please apply with your resume through this site. seo keywords for search head of machine learning, head of ai, ai lead, ml lead, principal ml engineer, machine learning manager, ai research lead, 3d perception, geometric deep learning, computer vision, gnn, graph neural networks, 3d transformers, nerf, point clouds, cad, generative ai, autonomous agents, robotics, pytorch, mlops, llm, reinforcement learning, ai for engineering, simulation, computational geometry. disclaimer attis global ltd is an equal opportunities employer. no terminology in this advert is intended to discriminate on any of the grounds protected by law, and all qualified applicants will receive consideration for employment without regard to age, sex, race, national origin, religion or belief, disability, pregnancy and maternity, marital status, political affiliation, socio-economic status, sexual orientation, gender, gender identity and expression, and / or gender reassignment. m / f / d / v. we operate as a staffing agency and employment business. more information can be found at attisglobal.com.","sunnyvale, ca (+1 other)",Machine Learning Engineer,"['cloud', 'computer vision', 'deep learning', 'machine learning', 'pytorch', 'r']","['cloud', 'computer vision', 'deep learning', 'machine learning', 'pytorch', 'r']",
machine learning engineering lead,credit genie,"company credit genie is a mobile-first financial wellness platform designed to help individuals take control of their financial future. we leverage artificial intelligence to provide personalized insights and are building a financial ecosystem by offering tools and services that provide instant access to cash, and building credit. our goal is to empower every customer to achieve long-term financial stability. founded in 2019 by ed harycki, former swift capital founder (acquired by paypal in 2017). backed by khosla ventures and led by industry pioneers from companies such as; paypal, square, and cash app, we are well positioned to build the future of inclusive finance through cutting-edge technology and customer-centric solutions. overview as our machine learning engineering lead, you will be responsible for building and maintaining our machine learning (ml) and artificial intelligence (ai) infrastructure, and to help our team build, deploy, and monitor ml and ai models and services. additionally, you will provide hands-on technical leadership and mentorship for a growing team of ai/ml engineers. the successful candidate will have a strong background in ml, and significant prior experience in delivering end-to-end software development projects. this role is expected to collaborate very closely with our ai/ml team as well as with credit genie’s engineering, data engineering, and product teams. what you’ll do • responsible for building the engineering foundation for the data science and ml/ai team, and for setting the company’s ml/ai engineering roadmap in collaboration with our leadership team. • collaborate hands-on with data scientists and ml/ai engineers to deliver production-grade ml/ai systems that support critical areas of our business (e.g., risk and customer-facing in-app experiences). • collaborate with leads from engineering, data engineering, and product to expand the existing technological stack of the company and support ml/ai-specific requirements. • responsible for setting and maintaining the highest ml engineering standards in the data science and ml/ai team. • have fun working on hard and highly impactful problems. requirements • 7-10+ years of backend software development experience and demonstrated prior experience working with ml/ai teams in a technical leadership capacity. • strong foundation in software development and prior experience leading impactful ml software projects. • strong foundation and demonstrated prior experience with version control, infrastructure as code, testing, ci/cd, monitoring and observability. • demonstrated ability to deliver end-to-end production-grade ml solutions (e.g., experience with databases and data warehouses, streaming vs. batch data ingestion, feature engineering and versioning of features, jupyter notebooks and similar ml development environments, building and testing prediction services, versioning model artifacts, …). • significant prior experience with ml-specific technologies such as feature stores/feature platforms and model registries, and demonstrated knowledge of ml-specific architectural patterns and tradeoffs (real-time vs. batch prediction, batch features vs. streaming features). nice to have • proficiency in multiple programming languages beyond python (e.g., typescript, rust, go). • experience with the aws cloud. • experience with snowflake. • experience with ml/ai tools and frameworks (scikit-learn, pytorch, tensorflow, langgraph/crewai/pydanticai). benefits and perks our goal is to provide a comprehensive offering of benefits and perks that promote better financial, mental, and physical wellness. we believe working alongside each other in person is the best way to build a great product and foster a strong company culture. our expectation is that employees are in the office five days a week, allowing for optimal collaboration, inclusivity, and productivity. at the same time, we understand that life happens and recognize the importance of flexibility. we are committed to supporting our employees when circumstances arise that require remote work or adjusted schedules. our goal is to ensure everyone can effectively balance personal and professional responsibilities while maintaining our collaborative and productive environment. here are some highlights of our benefits and perks offerings, feel free to ask your recruiting partner for more details on our comprehensive offering for employees. • 100% company-paid medical, dental, and vision coverage for you and your dependents on your first day of employment. • monthly fitness reimbursement up to $100 or a full membership to lifetime fitness • 401(k) with a 2.5% match and immediate vesting • meal program for breakfast, lunch, and dinner • life and accidental insurance • flexible pto your actual level and base salary will be determined on a case-by-case basis and may vary based on the following considerations: job-related knowledge and skills, education, and experience. base salary is just one part of your total compensation and rewards package at credit genie. you may also be eligible to participate in the bonus and equity programs. you will also have access to comprehensive medical, vision, and dental coverage, a 401(k) retirement plan with company match, short & long term disability insurance, life insurance, and flexible pto along with many other benefits and perks. credit genie is a proud equal opportunity employer where we welcome and celebrate differences. we are committed to providing a workspace that is safe and inclusive, where everyone feels supported, connected, and inspired to do their best work. if you require any accommodations to participate in our recruitment process, please inform us of your needs when we contact you to schedule an interview.","pittsburgh, pa (+3 others)",Machine Learning Engineer,"['aws', 'cloud', 'data warehouse', 'feature engineering', 'machine learning', 'python', 'pytorch', 'r', 'scikit-learn', 'snowflake', 'tensorflow']","['aws', 'cloud', 'data warehouse', 'feature engineering', 'machine learning', 'python', 'pytorch', 'r', 'scikit-learn', 'snowflake', 'tensorflow']",200K–250K a year
cloud machine learning engineer - us remote,hugging face,"at hugging face, we’re on a journey to democratize good ai. we are building the fastest growing platform for ai builders with over 5 million users & 100k organizations who collectively shared over 1m models, 300k datasets & 300k apps. our open-source libraries have more than 400k+ stars on github. hugging face has become the most popular, community-driven project for training, sharing, and deploying the most advanced machine learning models. workload efficiency is key to our mission of democratizing state of the art and we are always looking to push the boundaries for faster, and more efficient ways to train and deploy models. about the role we are looking for a cloud machine learning engineer responsible to help build machine learning solutions used by millions leveraging cloud technologies. you will work on integrating hugging face's open-source libraries like transformers and diffusers, with major cloud platforms or managed saas solutions. you may want to take a look at these announcements to get a better sense of what this role might mean in practice : hugging face and aws partner to make ai more accessible hugging face and ibm partner on watsonx.ai, the next-generation enterprise studio for ai builders introducing safecoder hugging face collaborates with microsoft to launch hugging face model catalog on azure responsibilities we are looking for talented people with deep experience and passion for both machine learning (at the framework level) and cloud services: bridging and integrating • transformers/diffusers models with a different cloud provider. • ensuring the above models meet the expected performance • designing & developing easy-to-use, secure, and robust developer experiences & apis for our users. • write technical documentation, examples and notebooks to demonstrate new features • sharing & advocating your work and the results with the community. about you you'll enjoy working on this team if you have experience with and interest in deploying machine learning systems to production and build great developer experiences. the ideal candidate will have skills including: • deep experience building with hugging face technologies, including transformers, diffusers, accelerate, peft, datasets • expertise in deep learning framework, preferably pytorch, optionally xla understanding • strong knowledge of cloud platforms like aws and services like amazon sagemaker, ec2, s3, cloudwatch and/or azure and gcp equivalents. • experience in building mlops pipelines for containerizing models and solutions with docker • familiarity with typescript, rust, and mongodb, kubernetes are helpful • ability to write clear documentation, examples and definition and work across the full product development lifecycle • bonus: experience with svelte & tailwindcss more about hugging face we are actively working to build a culture that values diversity, equity, and inclusivity.we are intentionally building a workplace where people feel respected and supported—regardless of who you are or where you come from. we believe this is foundational to building a great company and community. hugging face is an equal opportunity employer and we do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. we value development.you will work with some of the smartest people in our industry. we are an organization that has a bias for impact and is always challenging ourselves to continuously grow. we provide all employees with reimbursement for relevant conferences, training, and education. we care about your well-being. we offer flexible working hours and remote options. we offer health, dental, and vision benefits for employees and their dependents. we also offer parental leave and flexible paid time off. we support our employees wherever they are. while we have office spaces in nyc and paris, we’re very distributed and all remote employees have the opportunity to visit our offices. if needed, we’ll also outfit your workstation to ensure you succeed. we want our teammates to be shareholders. all employees have company equity as part of their compensation package. if we succeed in becoming a category-defining platform in machine learning and artificial intelligence, everyone enjoys the upside. we support the community. we believe major scientific advancements are the result of collaboration across the field. join a community supporting the ml/ai community.",anywhere,Machine Learning Engineer,"['aws', 'azure', 'cloud', 'deep learning', 'elt', 'gcp', 'machine learning', 'pytorch', 'r']","['aws', 'azure', 'cloud', 'deep learning', 'elt', 'gcp', 'machine learning', 'pytorch', 'r']",
chief machine learning engineer - experience generation team,sap,"we help the world run better at sap, we keep it simple: you bring your best to us, and we'll bring out the best in you. we're builders touching over 20 industries and 80% of global commerce, and we need your unique talents to help shape what's next. the work is challenging – but it matters. you'll find a place where you can be yourself, prioritize your wellbeing, and truly belong. what's in it for you? constant learning, skill growth, great benefits, and a team that wants you to grow and succeed. about the team the experience generation team at sap accelerates the path from idea to outcome with three generative-ai services: deep research & planning (strategic plans from enterprise + market data), experience builder (ai-forged dashboards and micro-apps), and visual services (commerce imagery via diffusion). together they deliver business value across all sap lines of business, grounded in governed enterprise data and brand standards. our platform blends a react/next.js frontend, a python + fastapi backend orchestrating langgraph sub-agents, and a gardener-managed kubernetes runtime. with postgres, cassandra, hana cloud vector engines, and sap’s generative ai hub for model governance, we provide an enterprise-grade substrate for controllable, scalable ai workflows. what you’ll do as a chief machine learning engineer, you’ll be a technical leader helping architect, scale, and harden sap’s next generation of generative ai services. you’ll drive both design and implementation of ml pipelines, orchestrators, and agentic workflows—turning foundational model research into secure, production-ready enterprise solutions key responsibilities ideal qualifications why join us at sap, chief engineers have the chance to define architecture and strategy for some of the world’s most impactful enterprise ai services. you’ll shape how businesses plan, build, and visualize with ai—while working with a collaborative team, trusted cloud platforms, and the latest foundation models. your leadership will ensure sap’s ai innovations scale securely, reliably, and responsibly #sapaicareers #sapaiapp bring out your best sap innovations help more than four hundred thousand customers worldwide work together more efficiently and use business insight more effectively. originally known for leadership in enterprise resource planning (erp) software, sap has evolved to become a market leader in end-to-end business application software and related services for database, analytics, intelligent technologies, and experience management. as a cloud company with two hundred million users and more than one hundred thousand employees worldwide, we are purpose-driven and future-focused, with a highly collaborative team ethic and commitment to personal development. whether connecting global industries, people, or platforms, we help ensure every challenge gets the solution it deserves. at sap, you can bring out your best. we win with inclusion sap’s culture of inclusion, focus on health and well-being, and flexible working models help ensure that everyone – regardless of background – feels included and can run at their best. at sap, we believe we are made stronger by the unique capabilities and qualities that each person brings to our company, and we invest in our employees to inspire confidence and help everyone realize their full potential. we ultimately believe in unleashing all talent and creating a better world. sap is committed to the values of equal employment opportunity and provides accessibility accommodations to applicants with physical and/or mental disabilities. if you are interested in applying for employment with sap and are in need of accommodation or special assistance to navigate our website or to complete your application, please send an e-mail with your request to recruiting operations team: careers@sap.com. for sap employees: only permanent roles are eligible for the sap employee referral program, according to the eligibility rules set in the sap referral policy. specific conditions may apply for roles in vocational training. qualified applicants will receive consideration for employment without regard to their age, race, religion, national origin, ethnicity, age, gender (including pregnancy, childbirth, et al), sexual orientation, gender identity or expression, protected veteran status, or disability. compensation range transparency: sap believes the value of pay transparency contributes towards an honest and supportive culture and is a significant step toward demonstrating sap’s commitment to pay equity. sap provides the annualized compensation range inclusive of base salary and variable incentive target for the career level applicable to the posted role. the targeted combined range for this position is 274,300 - 609,200 (usd) usd. the actual amount to be offered to the successful candidate will be within that range, dependent upon the key aspects of each case which may include education, skills, experience, scope of the role, location, etc. as determined through the selection process. any sap variable incentive includes a targeted dollar amount and any actual payout amount is dependent on company and personal performance. please reference this link for a summary of sap benefits and eligibility requirements: sap north america benefits. ai usage in the recruitment process for information on the responsible use of ai in our recruitment process, please refer to our guidelines for ethical usage of ai in the recruiting process. please note that any violation of these guidelines may result in disqualification from the hiring process. requisition id: 441543 | work area: software-design and development | expected travel: 0 - 10% | career status: professional | employment type: regular part time | additional locations: #li-hybrid","palo alto, ca",Machine Learning Engineer,"['cloud', 'dashboard', 'machine learning', 'python', 'r', 'scala']","['cloud', 'dashboard', 'machine learning', 'python', 'r', 'scala']",
"machine learning engineer, end-to-end autonomy",woven by toyota,"woven by toyota is enabling toyota’s once-in-a-century transformation into a mobility company. inspired by a legacy of innovating for the benefit of others, our mission is to challenge the current state of mobility through human-centric innovation — expanding what “mobility” means and how it serves society. our work centers on four pillars: ad/adas, our autonomous driving and advanced driver assist technologies; arene, our software development platform for software-defined vehicles; woven city, a test course for mobility; and cloud & ai, the digital infrastructure powering our collaborative foundation. business-critical functions empower these teams to execute, and together, we’re working toward one bold goal: a world with zero accidents and enhanced well-being for all. team at woven by toyota, we are at the forefront of developing advanced machine learning solutions for autonomous driving. our team tackles groundbreaking challenges in designing state-of-the-art neural networks, pioneering innovative end-to-end architectures, and advancing ml techniques in perception, prediction, and motion planning. we're passionate about pushing the boundaries of autonomous systems through deep learning and optimization, particularly in complex 3d geometric computer vision scenarios. we're seeking passionate innovators and creative problem-solvers eager to redefine mobility through cutting-edge ai and robotics, contributing directly to shaping the future of self-driving technology. woven by toyota is developing a joint project between toyota research institute (tri) and woven by toyota to research and develop a fully end-to-end learned automated driving / adas stack. this cross-org collaborative project is synergistic with tri’s robotics division’s efforts in diffusion policy and large behavior models (lbm). responsibilities • support the design and development of ml models or model components for end-to-end autonomous driving: ranging from initial data strategy, design, development, experimentation, evaluation and deployment; • able to navigate ambiguities and address uncertainties arising from complex projects involving multiple teams and legacy codebases; • writes high-quality code while being rigorous with machine learning experimentation; • collaborate closely with stakeholders from multiple teams in different time-zones to define interfaces and requirements for an end-to-end stack; experience • ms, or higher degree, in related field, or equivalent industry experience • professional experience with ml frameworks such as pytorch, jax or tensorflow (pytorch preferred) • experience with data sampling and data curation pipelines for autonomous driving datasets • experience in state of the art architectures for end-to-end autonomous driving • experience in ml workflows: data sampling and curation, pre-processing, model training, ablation studies, evaluation, deployment, inference optimization • python and c++ experience • experience with infrastructure for large scale datasets and distributed model training • experience working with a modern cloud service (aws, gcp, azure etc.) nice to have • hands-on experience with autonomous driving systems • experience with model deployment with nvidia stack (e.g. onnx graphs, tensorrt, profiling) • familiarity with recent breakthroughs in ml (e.g. foundation models, pre-training and efficient fine-tuning, multimodal transformer architectures) • knowledge of autonomous driving large scale data curation pipelines for positions based in palo alto, ca, the base pay for this position ranges from $112,000- $184,000 a year. your base salary is one part of your total compensation. we offer a base salary, short term and long term incentives, and a comprehensive benefits package. the total compensation offered to an employee will be dependent upon the individual's skills, experience, qualifications, location, and level. what we offer we are committed to creating a modern work environment that supports our employees and their loved ones. we offer many options of the best programs to allow you to do your most meaningful work and to help you shape the future of mobility. ・excellent health, wellness, dental and vision coverage ・a rewarding 401k program ・flexible vacation policy ・family planning and care benefits our commitment ・we are an equal opportunity employer and value diversity. ・any information we receive from you will be used only in the hiring and onboarding process. please see our privacy notice for more details.","palo alto, ca (+1 other)",Machine Learning Engineer,"['aws', 'azure', 'c++', 'cloud', 'computer vision', 'deep learning', 'excel', 'experimentation', 'gcp', 'machine learning', 'python', 'pytorch', 'r', 'tensorflow']","['aws', 'azure', 'c++', 'cloud', 'computer vision', 'deep learning', 'excel', 'experimentation', 'gcp', 'machine learning', 'python', 'pytorch', 'r', 'tensorflow']",
"manager, machine learning",qualtrics,"at qualtrics, we create software the world’s best brands use to deliver exceptional frontline experiences, build high-performing teams, and design products people love. but we are more than a platform—we are the creators and stewards of the experience management category serving over 18k clients globally. building a category takes grit, determination, and a disdain for convention—but most of all it requires close-knit, high-functioning teams with an unwavering dedication to serving our customers. when you join one of our teams, you’ll be part of a nimble group that’s empowered to set aggressive goals and move fast to achieve them. strategic risks are encouraged and complex problems are solved together, by passing the mic and iterating until the best solution comes to light. you won’t have to look to find growth opportunities—ready or not, they’ll find you. from retail to government to healthcare, we’re on a mission to bring humanity, connection, and empathy back to business. join over 5,000 people across the globe who think that’s work worth doing. machine learning engineering manager - ai core why we have this role the ai core organization at qualtrics manages the entire model development lifecycle for ai/ml and agentic applications. it consists of ml engineers, applied scientists, and data and infrastructure engineers who provide the platforms, frameworks, tools, and services needed for safe, scalable, and accessible ai/ml development and deployment. the platform engineering team builds an enterprise-grade ai/ml system compliant with security and industry standards such as fedramp, iso42001, gdpr, hipaa, and pci. within this platform, you will lead a team responsible for the architecture, development, and operation of core reusable ai/ml components, including training pipelines, online model serving, feature stores, voice transcription, event stream processing, and llm serving. this team handles services that process billions of requests weekly across qualtrics. you will also lead projects and collaborate with ml practitioners and senior engineers to shape the future of qualtrics' ai and model serving infrastructure, reporting to the director of ai core’s platform engineering team. how you’ll find success • connect strategy to execution: translate organizational priorities into a clear, actionable technical vision and long-term roadmap for the ai core platform, aligning work with the broader mission. • drive technical excellence: build a scalable, highly-available, and cost-efficient ml platform services to deliver delightful user experiences and ensure community safety. • innovation & adoption: direct the evaluation and integration of new ml frameworks, including llms, generative ai, and agentic solutions, keeping qualtrics at the forefront of experience management (xm) ai. • build strong, trust-based relationships across product, data infrastructure, and ml/applied science teams. drive technical consensus and coordinate execution for large, cross-cutting platform initiatives and roadmap for our ai platform. • instill an mlops culture, standardizing best practices in feature engineering, model observability, governance, and infrastructure. how you’ll grow • context mastered and team embedded: you quickly develop a deep understanding of the team's structure, systems, workflows, and cultural norms, building the situational awareness needed to lead with clarity and effectiveness. • high-performing team building: own and evolve the engineering hiring process, focusing on quality, equity, and candidate experience. • lead, mentor, and grow high-performing machine learning engineers, providing clear career paths, constructive feedback, and growth opportunities. things you’ll do • define and gain alignment on a multi-quarter technical roadmap for one or more critical platform areas (e.g., llm serving or the feature store), demonstrating a clear plan for scalable, measurable impact. • run team operations with structure and empathy: lead execution through consistent team rituals (planning, retrospectives, one-on-ones), balancing structure with empathy for alignment and psychological safety. • stay hands-on by participating in code reviews, technical design discussions, and occasionally contributing to the codebase. • instill best practices for software development and documentation, ensuring high-quality deliverables on time. • actively manage, mentor, and coach your team, fostering professional development and strong team culture. • work closely with global clients to provide solutions and ensure outstanding customer experiences. • recruit, hire, and develop top engineering talent to build a world-class team. what we’re looking for on your resume • 10+ years of total experience in ml engineering, focusing on large-scale data, distributed systems, or ml infrastructure. • 3+ years of direct people management, leading high-performing ml teams, building models for business application experience and development of senior/staff talent. • proven ability to build and lead engineering teams with strong product ownership and end-to-end accountability. • deep technical knowledge in building and operating ai/ml platforms supporting the full model lifecycle, including orchestration. • experience building and shipping enterprise-grade ai solutions for global customers. • demonstrated success as an engineering manager, showing measurable outcomes like improved velocity, reduced cycle time, and scaling teams through high-growth ai apps. • successful track record leading complex, cross-functional initiatives across distributed teams, aligning engineering execution with product/business goals. • strong grasp of sre and mlops principles, expertise in system scalability, cloud cost optimization, ai observability, and production security. • experience programming in java, c++, python or related language • hands-on experience with real-time ai and agentic services (e.g., kubernetes, sagemaker, bedrock, langgraph) and deploying genai applications, llms, or agent frameworks. • bonus points: • experience establishing and managing cloud infrastructure budgets (e.g., aws, gcp, azure) by designing for efficiency and scale. • experience building or contributing to open-source tools within the ml ecosystem. • experience with fedramp, iso 42001, gdpr, pci, hipaa and other standards. • advanced degree (m.s. or ph.d.) in computer science, data science, or a related technical field. what you should know about this team • we are a group of talented and passionate engineers developing new products within the qualtrics platform that enable contact center use cases. our team, care, was created to help contact centers scale their impact by assisting agents during calls and providing post-call analytics to improve the quality of care for customers. we focus on ai/ml-driven services and applications that enhance the overall customer experience. • our engineers collaborate closely with platform teams to ensure our products are tightly integrated and consistent with core platform behavior. at the same time, we have the flexibility to choose our own architecture and the independence to quickly deliver on our business vision. • with broad product ownership and a true mandate for full-stack development, our team offers an amazing opportunity for career growth and building a strong foundational understanding across qualtrics systems. our team’s favorite perks and benefits • we participate in personalized career action planning. you will work with your manager to design a plan based on your life experiences, values, and career aspirations (inside and outside qualtrics) that helps you achieve your career goals at qualtrics • full time employees receive an annual experience bonus after their first year of employment. qualtrics experience bonus is a program designed to provide experiences to our employees they might not otherwise have. • amazing qgroup communities; mosaiq, green team, qualtrics pride, q&able, qualtrics salute, and women’s leadership development, which exist as places for support, allyship, and advocacy. the qualtrics hybrid work model: our hybrid work model is elegantly simple: we all gather in the office three days a week; mondays and thursdays, plus one day selected by your organizational leader. these purposeful in-person days in thoughtfully designed offices help us do our best work and harness the power of collaboration and innovation. for the rest of the week, work where you want, owning the integration of work and life. qualtrics is an equal opportunity employer meaning that all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, status as a protected veteran, or any other protected characteristic. ​​​​​​​applicants in the united states of america have rights under federal employment laws:family & medical leave act, equal opportunity employment, employee polygraph protection act qualtrics is committed to the inclusion of all qualified individuals. as part of this commitment, qualtrics will ensure that persons with disabilities are provided with reasonable accommodations. if reasonable accommodation is needed to participate in the job application or interview process, to perform essential job functions, and/or to receive other benefits and privileges of employment, please let your qualtrics contact/recruiter know. not finding a role that’s the right fit for now? qualtrics insiders is the one-stop shop for all things qualtrics life. sign up for exclusive access to content created with you in mind and get the scoop on what we have going on at qualtrics - upcoming events, behind the scenes stories from the team, interview tips, hot jobs, and more. no spam - we promise! you'll hear from us two times a month max with fresh, totally tailored info - so be sure to stay connected as you explore your best role and company fit. for full-time positions, this pay range is for base per year; however, base pay offered within this range may vary depending on location, job-related knowledge, education, skills, and experience. a sign-on bonus and restricted stock units may be included in an employment offer. full-time employees are eligible for medical, dental, vision, life and disability, 401(k) with match, paid time off, a wellness reimbursement, mental health benefits, and an experience bonus. for a detailed look at our benefits, visit qualtrics us benefits. washington state annual pay transparency range $222,000—$319,000 usd","seattle, wa",Machine Learning Engineer,"['aws', 'azure', 'c++', 'cloud', 'excel', 'feature engineering', 'gcp', 'java', 'machine learning', 'python', 'r', 'scala']","['aws', 'azure', 'c++', 'cloud', 'excel', 'feature engineering', 'gcp', 'java', 'machine learning', 'python', 'r', 'scala']",
machine learning engineer – robotics,bedrock robotics,"the role we’re looking for a highly motivated engineer with experience deploying machine learning algorithms to physical systems in the real world. the ideal candidate has hands-on experience in perception (e.g., object detection, semantic segmentation, depth estimation) and/or behavior learning methods (e.g., vision-language-action (vla) models, diffusion policies). more importantly, you’ve shipped ml models to robots in production environments, and you understand the complexities that come with it. what you’ll do: • develop and optimize real-time ml models for edge deployment on robotic systems • work with vendors to label data and build robust data extraction and labeling pipelines • design custom metrics to evaluate model performance in the field • reduce model latency using tools like onnx, tensorrt, or similar what we’re looking for: • practical experience applying machine learning with deep learning frameworks, such as pytorch, to solve real-world problems • proficiency in python and comfort with at least one systems language (e.g., c++, rust) • experience deploying ml models to robotic systems or other physical platforms • experience incorporating raw sensor data like camera, lidar, radar, imus, etc into deep learning algorithms. • bonus: practical application of incorporating 3d geometry into deep learning models • bonus: published work in conferences such as icra, iros, corl, cvpr, eccv, iccv, icml, neurips, we’re especially interested in engineers who thrive at the intersection of ml research and real-world robotics applications. our roles are often flexible. if you don't fit all the criteria, or are in another location (especially one where we have an office like sf of ny) please apply anyway! we'd love to consider you. join the team bringing advanced autonomy to the built world at bedrock, we've assembled one of the most experienced autonomous technology teams in the industry, with deep expertise scaling breakthroughs across transportation, infrastructure, and enterprise software. our leaders helped put the first self-driving cars on public roads at waymo, scaled systems for segment's $3.2b acquisition, and grew uber freight to $5b in revenue. while others debate the future of ai, we're deploying it in the real world. our systems are already installed on heavy machines across the country, learning on real construction sites and working to reshape the earth with survey-grade precision and exceptional safety. this isn't a simulation—it's autonomous intelligence working on billion-dollar infrastructure projects. in just over a year, we've raised $80m, put our equipment into the field, and established partnerships with forward-thinking contractors who are integrating our technology into their operations. we're working quickly to close the gap between america's surging demand for housing, data centers, manufacturing hubs, and the construction industry's growing labor shortage. here, algorithms meet steel-toed boots. you'll collaborate with both construction veterans and experienced engineers, tackling problems where your work directly impacts how the physical world get built. if you're interested in applying cutting-edge technology to solve meaningful problems alongside a talented team—we'd love to have you join us.","san francisco, ca",Machine Learning Engineer,"['c++', 'deep learning', 'machine learning', 'python', 'pytorch', 'r']","['c++', 'deep learning', 'machine learning', 'python', 'pytorch', 'r']",
senior software development engineer- machine learning,adobe,"job level p40 employee role individual contributor the opportunity we are search, discovery and content team and our search & discovery platform supports adobe products such as creative cloud, experience cloud, document cloud, lightroom among other search products. our platform is multi-modal supporting not just keywords but also image, video, audio and text searches. join this team to influence search, browse, machine learning, recommendations, gen ai, rag in creative suite of products. what you will do • be part of the core technical team for search/recommendation query services, search indexing, data streaming, search algorithms, visual search & deep learning in the context of the next generation search & discovery platform for adobe. • help build a highly scalable, highly available, performant & reliable search & discovery platform to index billions of images, documents and other assets in real-time & power millions of search queries from flagship adobe product experiences including adobe express, adobe photoshop, adobe acrobat, lightroom & all adobe asset management interfaces. • influence in a technical capacity the roadmap for new features and solutions in the realm of search & relevance to improve user experience for adobe users. what you need to succeed • ms or equivalent experience in engineering • 6+ years of proven experience working with java/python/c++ • experience building solutions employing large scale distributed systems for high efficiency, low latency applications • experience in web services and rest • experience in aws/microsoft azure or the like for server-side deployments • experience building backend api layers via soap, or rest api • machine learning exposure preferred • excellent communication skills across function and internationally distributed teams preferred experience or exposure to the following technologies: • messaging queues kafka/sqs • no-sql databases like hbase/cassandra/mongodb/couchdb or others • real-time ingestion systems like apache storm/apache flink/kafka streams • offline and batch processing systems like apache spark, hadoop and others our compensation reflects the cost of labor across several u.s. geographic markets, and we pay differently based on those defined markets. the u.s. pay range for this position is $142,700 -- $257,600 annually. pay within this range varies by work location and may also depend on job-related knowledge, skills, and experience. your recruiter can share more about the specific salary range for the job location during the hiring process. at adobe, for sales roles starting salaries are expressed as total target compensation (ttc = base + commission), and short-term incentives are in the form of sales commission plans. non-sales roles starting salaries are expressed as base salary and short-term incentives are in the form of the annual incentive plan (aip). in addition, certain roles may be eligible for long-term incentives in the form of a new hire equity award. state-specific notices: california: fair chance ordinances adobe will consider qualified applicants with arrest or conviction records for employment in accordance with state and local laws and “fair chance” ordinances. colorado: application window notice if this role is open to hiring in colorado (as listed on the job posting), the application window will remain open until at least the date and time stated above in pacific time, in compliance with colorado pay transparency regulations. if this role does not have colorado listed as a hiring location, no specific application window applies, and the posting may close at any time based on hiring needs. massachusetts: massachusetts legal notice it is unlawful in massachusetts to require or administer a lie detector test as a condition of employment or continued employment. an employer who violates this law shall be subject to criminal penalties and civil liability. internal opportunities creativity, curiosity, and constant learning are celebrated aspects of your career growth journey. we’re glad that you’re pursuing a new opportunity at adobe! put your best foot forward: 1. update your resume/cv and workday profile – don’t forget to include your uniquely ‘adobe’ experiences and volunteer work. 2. visit the internal mobility page on inside adobe to learn more about the process and set up a job alert for roles you’re interested in. 3. check out these tips to help you prep for interviews. 4. if you are applying for a role outside of your current country, ensure you review the international resources for relocating employees on inside adobe, including the impacts to your benefits, aip, equity & payroll. once you apply for a role via workday, the talent team will reach out to you within 2 weeks. if you move into the official interview process with the hiring team, make sure you inform your manager so they can champion your career growth. at adobe, you will be immersed in an exceptional work environment that is recognized around the world. you will also be surrounded by colleagues who are committed to helping each other grow through our unique check-in approach where ongoing feedback flows freely. if you’re looking to make an impact, adobe's the place for you. discover what our employees are saying about their career experiences on the adobe life blog and explore the meaningful benefits we offer. adobe is proud to be an equal employment opportunity employer. we do not discriminate based on gender, race or color, ethnicity or national origin, age, disability, religion, sexual orientation, gender identity or expression, veteran status, or any other applicable characteristics protected by law. learn more. adobe aims to make adobe.com accessible to any and all users. if you have a disability or special need that requires accommodation to navigate our website or complete the application process, email accommodations@adobe.com or call (408) 536-3015.","san jose, ca",Machine Learning Engineer,"['aws', 'azure', 'c++', 'cloud', 'deep learning', 'excel', 'hadoop', 'java', 'kafka', 'machine learning', 'python', 'r', 'recommendation', 'scala', 'spark', 'sql']","['aws', 'azure', 'c++', 'cloud', 'deep learning', 'excel', 'hadoop', 'java', 'kafka', 'machine learning', 'python', 'r', 'recommendation', 'scala', 'spark', 'sql']",
machine learning engineer,howmet aerospace,"howmet aerospace inc. (nyse: hwm), headquartered in pittsburgh, pennsylvania, is a leading global provider of advanced engineered solutions for the aerospace and transportation industries. our primary businesses focus on jet engine components, aerospace fastening systems, titanium structural parts and forged wheels. with $7.4 billion in revenue in 2024, our products play a crucial role in enabling fuel efficiency and lightweighting, contributing to our customers' success and making a positive impact on the world. to learn more about the way howmet aerospace inc. is advancing the sustainability of our customers, markets, and communities where we operate, review the 2024 environmental social and governance report at www.howmet.com/esg-report . follow: linkedin , twitter , instagram , facebook , and youtube . equal opportunity employer: howmet is proud to be an equal employment opportunity and affirmative action employer. we celebrate diversity and are committed to creating an inclusive environment for all employees. all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, status as a protected veteran, or other applicable legally protected characteristics. if you need assistance to complete your application due to a disability, please email talentacquisitioncoe_howmet@howmet.comminimum qualifications: • a ms or phd degree from an accredited university in data science, computer science, computer engineering, mathematics, statistics, analytics, or related. • minimum of 2 years of hands-on experience in deep learning for computer vision. • demonstrated success applying advanced statistical methods and machine learning algorithms to production/field data using python. • employees must be legally authorized to work in the united states. verification of employment eligibility will be required at the time of hire. visa sponsorship is not available for this position. preferred qualifications: • minimum of 5 years of professional experience in data science or machine learning. • strong statistical background with demonstrated expertise in analyzing industrial/manufacturing data. • familiarity with manufacturing or industrial plant environments and their unique challenges. • knowledge of software development life cycle (sdlc) • proficiency in visualization tools (e.g., power bi) • comprehensive knowledge of advanced analytics and machine learning techniques. • exceptional verbal and written communication skills with the ability to convey complex ideas clearly. • strong organizational skills and the ability to work independently and in a cross-functional team environment. why join us? • be at the forefront of innovation, applying machine learning to aerospace manufacturing. • collaborate with a supportive, cutting-edge team dedicated to solving challenging real-world problems. • access professional growth opportunities and a chance to directly impact howmet aerospace's success. howmet aerospace is hiring a machine learning engineer with expertise in deep learning for computer vision to join our innovative research and development team. this role involves building cutting-edge computer vision applications and collaborating with cross-functional teams to support our casting, alloy, core, and rings facilities. primary responsibilities: • design, develop, and evaluate advanced deep learning architectures (e.g., cnns, mask r-cnns, yolo) to address complex computer vision challenges. • train and optimize generative models to accelerate development. • build and refine machine learning algorithms to enhance howmet products across all business units. • construct, manipulate, and analyze large datasets using tools such as python and sql. • conduct statistical multi-factor analyses to uncover complex relationships and improve manufacturing processes. • present data-derived conclusions to a non-technical audience. • identify opportunities to optimize processes and implement continuous improvement tools using machine learning. • promote a data-driven culture across the organization by expanding machine learning applications and leading training initiatives. • collaborate with internal customers to validate trials, implement process enhancements, and integrate machine learning into production workflows. about the company: howmet aerospace","whitehall, mi",Machine Learning Engineer,"['computer vision', 'deep learning', 'machine learning', 'power bi', 'python', 'r', 'sql', 'statistics']","['computer vision', 'deep learning', 'machine learning', 'power bi', 'python', 'r', 'sql', 'statistics']",
machine learning engineer ii,glidewell dental,"essential functions: • designs, develops, and deploys machine learning models for real-world applications. • builds scalable pipelines for data ingestion, pre-processing, training, and inference. • owns end to end development of machine learning algorithms including data analysis, feature engineering, model development, training, validation, and performance evaluation. • designs, implements, and optimizes retrieval-augmented generation (rag) pipelines that combine large language models (llms) with vector search/retrieval systems. • builds data ingestion and embedding pipelines for efficient indexing and retrieval. • fine-tunes and adapts llms for domain-specific tasks such as instruction tuning, prompt engineering, low-rank adaptation (lora), etc. • engages in both engineering and research, exploring latest ml algorithms, solution architectures, and cutting-edge approaches to improve retrieval and generation performance. • works with stakeholders to translate business requirements into robust technical solutions that deliver measurable impact. • works with engineering teams to continuously scale and advance machine learning across the organization. • identifies new opportunities of applying ml technology to improve business workflows and processes. • builds a deep understanding of the company’s products, services, data, and customers to deliver impactful solutions. • performs other related duties and projects as business needs require at direction of management. education and experience: • master’s degree in machine learning, deep learning, or a computer science-related field. phd preferred. • minimum three (3) years of relevant work experience in the areas below, or any equivalent education and/or experience from which comparable knowledge, skills and abilities have been demonstrated/achieved: • understands fundamental concepts, practices, and procedures of machine learning field. • data discovery, data aggregation, and feature engineering with sql query writing skills. • training, evaluating, optimizing, deploying, and maintaining machine learning models on production systems. • logging, tracking, a/b testing, evaluating and analyzing the performance of different machine learning algorithms and models in production. • strong development skills in python programming language and have experience in developing data-driven, scalable, and reliable applications with amazon web services (aws). • applying machine learning algorithms to solve a wide range of optimization problems like customer sales prediction, recommendation engine, sentiment analysis, deep learning with image and natural language, customer segmentations/clustering, and object detection. • utilization of popular open-source machine learning/deep learning libraries like langchain, huggingface, tensorflow, scikit-learn, pandas, pytorch, and keras. • experience working with relational, non-relational, and high-scale data processing and storage frameworks like structured query language (sql), aws redshift, aurora, s3, dynamodb, mysql, postgresql. • experience with aws serverless architecture and aws native services like bedrock, ec2, lambda, step functions, sagemaker, rekognition, comprehend, lex/polly, and transcribe. pay range: $117,000 - $146,000/yr","irvine, ca",Machine Learning Engineer,"['a/b testing', 'aws', 'clustering', 'data analysis', 'deep learning', 'feature engineering', 'keras', 'machine learning', 'pandas', 'python', 'pytorch', 'r', 'recommendation', 'redshift', 'scala', 'scikit-learn', 'sql', 'tensorflow']","['a/b testing', 'aws', 'clustering', 'data analysis', 'deep learning', 'feature engineering', 'keras', 'machine learning', 'pandas', 'python', 'pytorch', 'r', 'recommendation', 'redshift', 'scala', 'scikit-learn', 'sql', 'tensorflow']",
senior machine learning engineer-en,cae,"about this role who we are: • cae vision: our vision is to be the worldwide partner of choice in defense and security, and civil aviation by revolutionizing our customers’ training and critical operations with digitally immersive solutions to elevate safety, efficiency and readiness. • cae defense & security mission: cae's defense and security business unit focuses on helping prepare military customers to develop and maintain the highest levels of mission readiness. • cae values: empowerment, innovation, excellence, integrity and onecae make us who we are and we strive to make a difference in the world while helping each other succeed. what we have to offer: • comprehensive and competitive benefits package and flexibility that promotes work-life balance • a work environment where all employees are valued, respected and safe • freedom to succeed by enabling team members to deliver, take initiatives and make decisions • recognition, professional development, advancement and having fun! summary we are seeking a highly skilled and experienced machine learning engineer to join our growing ai & data science team in r&d. this role is ideal for someone passionate about solving complex problems using data-driven approaches and deploying scalable machine learning solutions in production environments. additionally, this role will focus on designing scalable nlp systems powered by state-of-the-art transformer models, optimizing inference performance, and integrating llms into real-world products. you’ll collaborate with cross-functional teams to deliver intelligent, language-driven solutions that enhance user experience and business outcomes. this position is onsite with locations in tampa fl, arlington tx, or orlando fl. essential duties and responsibilities reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. • design, develop, and deploy machine learning models for real-world applications. • build scalable data pipelines and model training workflows using modern tools and frameworks. • conduct rigorous model evaluation, validation, and performance tuning. • monitor and maintain deployed models, ensuring reliability and accuracy over time. • design, fine-tune, and deploy llms (llama, mistral, etc.) for various nlp tasks such as summarization, question answering, semantic search, and chatbots. • develop scalable and efficient model serving infrastructure using tools like onnx, tensorrt, deepspeed, or vllm. • implement retrieval-augmented generation (rag) pipelines using vector databases (e.g., faiss, weaviate, pinecone, milvus). • optimize llm inference for latency, throughput, and cost across cloud and edge environments. • collaborate with cross-functional teams to understand business requirements and translate them into ml solutions. • stay current with the latest research and trends in machine learning, ai & llms. • mentor junior engineers and contribute to team knowledge sharing. • document processes, models, and decisions for transparency and reproducibility. qualifications and education requirements • bachelor’s or master’s degree in computer science, machine learning, or related field. phd is a plus. • 5+ years of software development experience, with at least 2 years focused on nlp or llms. • proficiency in ml frameworks (pytorch, tensorflow, scikit-learn, cuda). • good understanding of distributed systems, understanding of microservice architecture and rest apis. • strong understanding of mlops tools and practices (mlflow, airflow, dvc). • hands-on experience with hugging face transformers, langchain, and openai apis. • technology proficiency with cloud platforms (aws, gcp, azure), linux, and container orchestration (docker, kubernetes). • proven track record of deploying ml models in production environments. • experience in working with sql/nosql database systems such as mysql, mongodb or elasticsearch. • due to u.s. government contract requirements, only u.s. citizens are eligible for this role. preferred skills • experience with deep learning, nlp, computer vision, or reinforcement learning. • experience with feature engineering and model interpretability techniques. • knowledge of prompt engineering and prompt optimization strategies. • experience with multi-modal models (e.g., combining text with image or audio inputs). • familiarity with distributed training and model parallelism. • experience with fine-tuning llms using lora, qlora, or peft techniques is a plus. • familiarity with ci/cd pipelines and version control (git). • ability to work in a fast-paced, agile development environment. security responsibilities must comply with all company security and data protection / usage policies and procedures. personally responsible for proper marking and handling of all information and materials, in any form. shall not divulge any information, or afford access, to other employees not having a need-to-know. shall not divulge information outside company without management approval. all government and proprietary information will be accessed and stored electronically on company provided resources. • incumbent must be eligible for dod personal security clearance. due to u.s. government contract requirements, only u.s. citizens are eligible for this role. work environment and physical demands • duties performed in an office environment and manufacturing facility. • must be able to work overtime on and off-shifts as required. • may be required to sit down for long lengths of time. may be required to climb stairs. other duties please note this job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee for their job. duties, responsibilities, and activities may change at any time with or without notice. cae usa inc. is an equal opportunity employer, and all qualified applicants will be considered for employment without regard to any protected characteristic, including disability and protected veteran status, as defined under federal, state, or local laws. applicants needing reasonable accommodations should contact their recruiter at any point in the recruitment process. if you need assistance to submit your application because of incompatible assistive technology or a disability, please contact us at hrops@caemilusa.com position type regular cae thanks all applicants for their interest. however, only those whose background and experience match the requirements of the role will be contacted. equal opportunity employer cae is an equal opportunity employer committed to providing equal employment opportunities to all applicants and employees without regard to race, color, national origin, age, religion, sex, disability status, protected veteran status, or any other characteristic protected by federal, state or local laws. at cae, everyone is welcome to contribute to our success. applicants needing reasonable accommodations should contact their recruiter at any point in the recruitment process. if you need assistance to submit your application because of incompatible assistive technology or a disability, please contact us at caecarrieres-careers@cae.com.","orlando, fl",Machine Learning Engineer,"['airflow', 'aws', 'azure', 'cloud', 'computer vision', 'data pipeline', 'deep learning', 'excel', 'feature engineering', 'gcp', 'machine learning', 'nlp', 'pytorch', 'r', 'scala', 'scikit-learn', 'sql', 'tensorflow']","['airflow', 'aws', 'azure', 'cloud', 'computer vision', 'data pipeline', 'deep learning', 'excel', 'feature engineering', 'gcp', 'machine learning', 'nlp', 'pytorch', 'r', 'scala', 'scikit-learn', 'sql', 'tensorflow']",
senior machine learning engineer,clear capital | cubicasa,"we are a production team dedicated to supporting and enhancing production modeling systems. our focus is on building the most accurate models with the state-of-the-art modeling technology. we build core infrastructure and pipeline capabilities that ensure seamless data flow into our models. we also support the scaling and development of model prototypes in production environments. our goal is to create the top-notch machine learning products in the real estate industry. we are seeking a senior machine learning engineer to lead the design, development, and implementation of our machine learning products. what you will work on • lead the development of modeling complex, real-world problems. • lead the development of complex data pipelines to support production systems. • lead the development efforts to bring machine learning algorithms into production. • monitor and maintain production machine learning data pipelines. who we are looking for • minimum 5 years experience in machine learning model development, building data pipelines and robust machine learning software systems. • strong and in-depth knowledge of various modeling techniques and machine learning frameworks. • strong knowledge and understanding of machine learning pipelines from standardization, normalization, clustering, modeling, scoring, validation • strong knowledge of relational databases. • strong knowledge of python, r, and sql. • strong knowledge of distributed systems configuration and production maintenance. • understanding of etl engineering and tools so you can interface with data teams. • understanding of data design principles across multiple different storage systems. • working knowledge of aws. • parsing and manipulating a variety of different datasets and formats with glue magic • self starter with the ability to distill ambiguity into working results • you want autonomy and are willing to take responsibility • you iterate quickly and are a continuous learner and are comfortable in a face-paced environment • good communicator and willing to work cross-functionally with other engineering teams across the organization • strong problem-solving skills and the ability to work effectively in a team. what you can expect • compensation: the base salary for this position ranges from $136,000 to $178,000 annually, depending on your location, experience, and qualifications. additional compensation offerings include company profit-sharing bonus program, communication stipends, and referral bonuses. • inclusive benefits package offering: • comprehensive medical, dental, and company paid vision insurance, 401(k) retirement plan with employer match, voluntary life and ad&d insurance options, voluntary supplemental insurances for accident, critical illness, and legal services, paid time off (pto) and paid holidays, employee assistance and wellness programs, company paid short term disability coverage, company contributions to health saving funds (with participation in the high deductible health plan. we offer company paid access to galileo for virtual primary care and rula for virtual mental health resources. • through our anniversary program, we celebrate the meaningful milestones and long tenure that reflect how much we value your contributions and commitment to our team. • career and skill development resources to help advance your career and personal growth. • a mission-driven environment where your work makes a measurable impact on the real estate industry. what we value • wherever it leads, whatever it takes® - no matter how remote, complex, or unexpected. our commitment never wavers. • hire nice people - skills can be taught but character shines through. we seek those who bring integrity, kindness, and grit. • lift others up - we lead with empathy and strive to improve the lives of those around us. • sweat the details - excellence lives in the little things. getting it just so is how we make a big impact. • raise the bar - we don’t settle for industry standards, we redefine them. about us clear capital is a national real estate analytics, data solutions and valuation technology company with a simple purpose: to build confidence in real estate decisions to strengthen communities and improve lives. our goal is to provide customers with a complete understanding of nearly every u.s. property through our ai-driven analytics, data solutions, valuation services and automated appraisal review platforms. our commitment to excellence - wherever it leads, whatever it takes® - is embodied by our team members across our brands and has remained steadfast in this pursuit since our first order in 2001. clear capital is an equal-opportunity employer. to all recruitment agencies: clear capital does not accept agency resumes. please do not forward resumes to our jobs alias, clear capital employees, or any other company location. clear capital is not responsible for any fees related to unsolicited resumes.","reno, nv",Machine Learning Engineer,"['aws', 'clustering', 'data pipeline', 'etl', 'excel', 'machine learning', 'python', 'r', 'sql']","['aws', 'clustering', 'data pipeline', 'etl', 'excel', 'machine learning', 'python', 'r', 'sql']",
machine learning engineer,bebeedatascientist,"ai systems development role this is an opportunity to join a global community of talented professionals who work together to improve the intelligence and safety of ai models. we are looking for individuals who can contribute their expertise to refine cutting-edge ai systems and develop high-quality data. detailed understanding of analytical and problem-solving skills to develop complex prompts and evaluate ai reasoning. excellent writing skills to create clear, concise, and engaging content that explains decisions or critiques responses. able to commit to accuracy and assess technical aspects of model outputs.",arizona,Machine Learning Engineer,"['excel', 'r']","['excel', 'r']",
senior machine learning engineer,exact sciences corporation,"help us change lives at exact sciences, we’re helping change how the world prevents, detects and guides treatment for cancer. we give patients and clinicians the clarity needed to make confident decisions when they matter most. join our team to find a purpose-driven career, an inclusive culture, and robust benefits to support your life while you’re working to help others. position overview the sr. machine learning engineer, with minimal guidance from more experienced engineers, works independently and in collaboration with others on multiple projects which are complex in scope. this role applies deep knowledge of advanced artificial intelligence and machine learning algorithms and models to solve problems involving biological, genomic, clinical and healthcare data within a setting of advanced cancer screening and precision oncology. this role is involved at every step of the solution development process, from ideation, design, implementation and deployment. this role strongly contributes to the strategic vision for the application of cutting-edge ai methodologies at exact sciences and work as a partner with biostatisticians, bioinformatics scientists, and data scientists to further our goal of helping eradicate cancer by preventing it, detecting it earlier, and guiding personalized treatment. essential duties include, but are not limited to, the following: • maintain deep expertise in one or more ai technology areas. clearly present and communicate relevance of technological advances as relevant to our work in biostatistics, bioinformatics and data science at exact sciences. • develop perspectives on “ai technology affordance”: what do these advances in ai technology allow us to do (more efficiently and / or more effectively) that we couldn’t do before. • work with stakeholders to discover and refine requirements for ml- and ai-based solutions to business problems with a focus on bioinformatics, biostatistics and data engineering applications. • collaborate with other ml/ai engineers and relevant partners to design software solutions that utilize state-of-the art artificial intelligence and machine learning techniques. • work with other ai and ml engineers and software engineers to implement and deploy ml and ai solutions. • work in an agile framework focused on iterative, rapid delivery of proof-of-concept solutions. • contribute to the development of an ai strategy at exact sciences that empowers the team to leverage both existing ai and ml tools and those developed in-house to accelerate and enhance bioinformatics, biostatistics and data science processes and outcomes. • contribute to internal initiatives to build training resources, develop knowledge bases, and educate on advanced ai and ml technologies and best practices for leveraging them. • provide mentorship and coaching to more junior level team members. • act as resource and subject matter expert in core team and/or cross-functional meetings. • exercise discretion and judgment within broadly defined practices and policies in selecting methods and techniques and evaluation criteria for obtaining and interpreting results. • ability to apply strong communication skills and to explain difficult, sensitive, and/or complex information to audiences of peers. • uphold company mission and values through accountability, innovation, integrity, quality, and teamwork. • support and comply with the company’s quality management system policies and procedures. • maintain regular and reliable attendance. • ability to act with an inclusion mindset and model these behaviors for the organization. • ability to work designated schedule. • ability to work on a mobile device, tablet, or in front of a computer screen and/or perform typing for approximately 85% of a typical working day. • ability to travel 5% of working time away from work location, may include overnight/weekend travel. minimum qualifications • ph.d. in statistics, computational biology, computer science, or related quantitative field as outlined in the essential duties, or master’s degree in statistics, computational biology, computer science, or related quantitative field as outlined in the essential duties plus 4 years of experience in lieu of a ph.d. • 3+ years of experience in statistics, computational biology, applied mathematics, or related quantitative field as outlined in the essential duties. • 3+ years of experience with artificial intelligence and machine learning algorithms. • demonstrated knowledge and experience with advanced ai concepts, such as artificial neural networks, deep learning, and reinforcement learning. • demonstrated knowledge and experience using artificial intelligence and machine learning techniques within one or more of the following fields: natural language processing, image processing and computer vision, image and pattern recognition. • demonstrated knowledge and experience with large language models for generative ai and associated concepts, such as transformer architecture and retrieval augmented generation. • strong programming ability with demonstrated experience in python and one or more associated machine learning frameworks, such as tensorflow, pytorch, or sklearn. • knowledge of and experience working with open-source ai models. • demonstrated ability to perform the essential duties of the position with or without accommodation. • authorization to work in the united states without sponsorship. preferred qualifications • 2+ years of life sciences industry experience working with biological data. • 2+ years of industry experience in molecular diagnostics, preferably cancer diagnostics. • expertise in data mining approaches within healthcare settings generating insight from routinely collected healthcare data. • basic knowledge of ml-ops and processes for managing the versioning and deployment of machine learning models. • scientific understanding of cancer biology #li-sm1 salary range: $120,000.00 - $204,000.00 the annual base salary shown is for this position located in us - wi - madison on a full-time basis and may differ by hiring location. in addition, this position is bonus eligible, and is eligible to be considered for company stock at hire and on an annual basis. exact sciences is proud to offer an employee experience that includes paid time off (including days for vacation, holidays, volunteering, and personal time), paid leave for parents and caregivers, a retirement savings plan, wellness support, and health benefits including medical, prescription drug, dental, and vision coverage. learn more about our benefits. our success relies on the experiences and perspectives of a diverse team, and exact sciences fosters a culture where all employees can develop personally and professionally with a sense of respect and belonging. if you require an accommodation, please contact us here. not ready to apply? join our talent community to stay updated on the latest news and opportunities at exact sciences. we are an equal employment opportunity employer. all qualified applicants will receive consideration for employment without regard to disability, protected veteran status, and any other status protected by applicable local, state, or federal law. to view the right to work, e-verify employer, and pay transparency notices and federal, federal contractor, and state employment law posters, visit our compliance hub. the documents summarize important details of the law and provide key points that you have a right to know.","madison, wi",Machine Learning Engineer,"['computer vision', 'deep learning', 'machine learning', 'natural language processing', 'python', 'pytorch', 'r', 'sklearn', 'statistics', 'tensorflow']","['computer vision', 'deep learning', 'machine learning', 'natural language processing', 'python', 'pytorch', 'r', 'sklearn', 'statistics', 'tensorflow']",120K–204K a year
senior lead machine learning engineer,capital one,"senior lead machine learning engineer as a capital one machine learning engineer (mle), you'll be part of an agile team dedicated to productionizing machine learning applications and systems at scale. you’ll participate in the detailed technical design, development, and implementation of machine learning applications using existing and emerging technology platforms. you’ll focus on machine learning architectural design, develop and review model and application code, and ensure high availability and performance of our machine learning applications. you'll have the opportunity to continuously learn and apply the latest innovations and best practices in machine learning engineering. what you’ll do in the role: the mle role overlaps with many disciplines, such as ops, modeling, and data engineering. in this role, you'll be expected to perform many ml engineering activities, including one or more of the following: • design, build, and/or deliver ml models and components that solve real-world business problems, while working in collaboration with the product and data science teams. • inform your ml infrastructure decisions using your understanding of ml modeling techniques and issues, including choice of model, data, and feature selection, model training, hyperparameter tuning, dimensionality, bias/variance, and validation). • solve complex problems by writing and testing application code, developing and validating ml models, and automating tests and deployment. • collaborate as part of a cross-functional agile team to create and enhance software that enables state-of-the-art big data and ml applications. • retrain, maintain, and monitor models in production. • leverage or build cloud-based architectures, technologies, and/or platforms to deliver optimized ml models at scale. • construct optimized data pipelines to feed ml models. • leverage continuous integration and continuous deployment best practices, including test automation and monitoring, to ensure successful deployment of ml models and application code. • ensure all code is well-managed to reduce vulnerabilities, models are well-governed from a risk perspective, and the ml follows best practices in responsible and explainable ai. • use programming languages like python, scala, or java. basic qualifications: • bachelor’s degree • at least 8 years of experience designing and building data-intensive solutions using distributed computing (internship experience does not apply) • at least 4 years of experience programming with python, scala, or java • at least 3 years of experience building, scaling, and optimizing ml systems • at least 2 years of experience leading teams developing ml solutions preferred qualifications: • master's or doctoral degree in computer science, electrical engineering, mathematics, or a similar field • experience developing and deploying ml solutions in a public cloud such as aws, azure, or google cloud platform • 4+ years of on-the-job experience with an industry recognized ml framework such as scikit-learn, pytorch, dask, spark, or tensorflow • 3+ years of experience developing performant, resilient, and maintainable code • 3+ years of experience with data gathering and preparation for ml models • 3+ years of people management experience • ml industry impact through conference presentations, papers, blog posts, open source contributions, or patents • 3+ years of experience building production-ready data pipelines that feed ml models • ability to communicate complex technical concepts clearly to a variety of audiences capital one will consider sponsoring a new qualified applicant for employment authorization for this position. the minimum and maximum full-time annual salaries for this role are listed below, by location. please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount capital one is willing to pay at the time of this posting. salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked. boston, ma: $225,400 - $257,200 for sr. lead machine learning engineer mclean, va: $225,400 - $257,200 for sr. lead machine learning engineer new york, ny: $245,900 - $280,600 for sr. lead machine learning engineer san francisco, ca: $245,900 - $280,600 for sr. lead machine learning engineer san jose, ca: $245,900 - $280,600 for sr. lead machine learning engineer candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate’s offer letter. this role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (lti). incentives could be discretionary or non discretionary depending on the plan. capital one offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. learn more at the capital one careers website. eligibility varies based on full or part-time status, exempt or non-exempt status, and management level. this role is expected to accept applications for a minimum of 5 business days. no agencies please. capital one is an equal opportunity employer (eoe, including disability/vet) committed to non-discrimination in compliance with applicable federal, state, and local laws. capital one promotes a drug-free workplace. capital one will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, article 23-a of the new york correction law; san francisco, california police code article 49, sections 4901-4920; new york city’s fair chance act; philadelphia’s fair criminal records screening act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries. if you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact capital one recruiting at 1-800-304-9102 or via email at recruitingaccommodation@capitalone.com. all information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. for technical support or questions about capital one's recruiting process, please send an email to careers@capitalone.com capital one does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site. capital one financial is made up of several different entities. please note that any position posted in canada is for capital one canada, any position posted in the united kingdom is for capital one europe and any position posted in the philippines is for capital one philippines service corp. (copssc).","gaithersburg, md",Machine Learning Engineer,"['aws', 'azure', 'cloud', 'data pipeline', 'google cloud', 'java', 'machine learning', 'python', 'pytorch', 'r', 'scala', 'scikit-learn', 'spark', 'tensorflow']","['aws', 'azure', 'cloud', 'data pipeline', 'google cloud', 'java', 'machine learning', 'python', 'pytorch', 'r', 'scala', 'scikit-learn', 'spark', 'tensorflow']",
"engineer, machine learning",comcast,"about the position responsibilities • work as part of a team responsible for design, implementation and delivery of gateway api, frameworks and data plane that support our flagship products like xfinity voice remote, sky voice remote and the xfinity assistant. • develop, troubleshoot, and maintain web services in a microservice architecture at a large-scale handling millions of requests per day. • serve as a technical liaison for internal and external customers and business partners. • mentor junior team members. requirements • proficiency with programming in java and/or kotlin. • experience with spring, junit, maven, git and the like. • experience with rest web services and micro service frameworks like spring boot. nice-to-haves • experience with programming in kotlin (preferred) and/or python. • familiarity with container systems (docker, kubernetes), cicd tools (jenkins), cloud services (ec2, s3, lambda etc.), splunk, elastic search etc. • familiarity with natural language processing (nlp), machine learning, deep learning, optimization techniques and evaluation methodologies. benefits • best-in-class benefits to eligible employees. • an array of options, expert guidance and always-on tools, that are personalized to meet the needs of your reality.","philadelphia, pa",Machine Learning Engineer,"['cloud', 'deep learning', 'java', 'machine learning', 'natural language processing', 'nlp', 'python', 'r']","['cloud', 'deep learning', 'java', 'machine learning', 'natural language processing', 'nlp', 'python', 'r']",
machine learning / data science engineer,captech consulting,"company description captech is an award-winning consulting firm that collaborates with clients to achieve what’s possible through the power of technology. at captech, we’re passionate about the work we do and the results we achieve for our clients. from the outset, our founders shared a collective passion to create a consultancy centered on strong relationships that would stand the test of time. today we work alongside clients that include fortune 100 companies, mid-sized enterprises, and government agencies, a list that spans across the country. job description captech machine learning engineers are responsible for designing and implementing data-driven solutions for our clients, with a specific focus on building and deploying scalable machine learning systems in enterprise environments. captech employees enjoy a collaborative environment and have many opportunities to learn from and share knowledge with other captech analysts, architects, and our clients. specific responsibilities for the machine learning engineer position include: • strategizing with clients, data scientists, engineers, and other members of cross-functional teams to implement end-to-end machine learning solutions and identify new machine learning and data science approaches to meet business needs • deconstructing client needs into data-driven processes/models and analytical measures. • analyzing and transforming large datasets hosted on a variety of enterprise-level data platforms (e.g., aws, azure, gcp). • designing, developing, and deploying advanced analytical solutions leveraging client data (e.g., recommender systems, natural language processing, risk scoring). • productionizing ml systems with a focus on optimization and scalability to satisfy clients’ requirements. • growing captech’s machine learning and data science practices through delivering client presentations, writing proposals, attending various business development events, and leading teams of junior data scientists and engineers. qualifications • bachelor's degree or equivalent combination of education and experience. • hands-on experience manipulating and analyzing large (multi-billion record) data sets. • hands-on experience developing data-driven solutions using python, scala, or similar languages. • proficiency leveraging sql, spark, nosql, and/or cloud data processing frameworks in a production setting. • proficiency with containerization (e.g., docker) and microservices. • proficiency with data warehousing tools/environments such as snowflake, databricks, azure sql, amazon rds • comfort and proficiency in framing data-driven problems from cross-industry business requirements. • experience applying analytical methods across multiple business domains (e.g., customer analytics, marketing, finance, digital channels)hands-on experience implementing production-scale machine learning systems in one or more domains (i.e., personalization, natural language processing, computer vision). • knowledge of devops and automation best practices. • knowledge of statistics and statistical modeling methods. • knowledge of model management and model versioning best practices. additional information we want everyone at captech to be able to envision a lasting and rewarding career here, which is why we offer a variety of career paths based on your skills and passions. you decide where and how you want to develop, and we help get you there with customizable career progression and a comprehensive benefits package to support you along the way. alongside our suite of traditional benefits encompassing generous pto, health coverage, disability insurance, paid family leave and more, we’ve launched extended benefits to help meet our employees’ needs. • learning & development – programs offering certification and tuition support, digital on-demand learning courses, mentorship, and skill development paths • modern health –a mental health and well-being platform that provides 1:1 care, group support sessions, and self-serve resources to support employees and their families through life’s ups and downs • carrot fertility –inclusive fertility and family-forming coverage for all paths to parenthood – including adoption, surrogacy, fertility treatments, pregnancy, and more – and opportunities for employer-sponsored funds to help pay for care • fringe –a company paid stipend program for personalized lifestyle benefits, allowing employees to choose benefits that matter most to them – ranging from vendors like netflix, spotify, and grubhub to services like student loan repayment, travel, fitness, and more • employee resource groups – employee-led committees that embrace and incorporate diversity and inclusion into our day-to-day operations • philanthropic partnerships – opportunities to engage in partnerships and pro-bono projects that support our communities. • 401(k) matching – generous matching and no vesting period to help you continue to build financial wellness captech is an equal opportunity employer committed to fostering a culture of equality, inclusion and fairness — each foundational to our core values. we strive to create a diverse environment where each employee is encouraged to bring their unique ideas, backgrounds and experiences to the workplace. for more information about our diversity, inclusion and belonging efforts, click here. as part of this commitment, captech will ensure that persons with disabilities are provided reasonable accommodations. if reasonable accommodation is needed to participate in the job application or interview process, to perform essential job functions, and/or to receive other benefits and privileges of employment, please contact laura massa directly via email lmassa@captechconsulting.com. at this time, captech cannot transfer nor sponsor a work visa for this position. applicants must be authorized to work directly for any employer in the united states without visa sponsorship. #li-lm1","columbus, oh",Machine Learning Engineer,"['aws', 'azure', 'cloud', 'computer vision', 'databricks', 'gcp', 'machine learning', 'natural language processing', 'python', 'r', 'recommender', 'scala', 'snowflake', 'spark', 'sql', 'statistics']","['aws', 'azure', 'cloud', 'computer vision', 'databricks', 'gcp', 'machine learning', 'natural language processing', 'python', 'r', 'recommender', 'scala', 'snowflake', 'spark', 'sql', 'statistics']",
"senior machine learning engineer, computer vision - robotics",scale ai,"scale’s robotics business unit is dedicated to solving the data bottleneck in physical ai. this position will be a key contributor in conducting applied research in robotics and developing ml pipelines for training and fine-tuning on data collected by scale. in this role, you will have the opportunity to advance robotic research, shape scale’s robotics offerings, and expand the frontier of robotics data and model evaluation. we are seeking an exceptionally motivated and experienced senior machine learning engineer, computer vision to drive cutting-edge research and development in real-time and offline 2d and 3d algorithms. the successful candidate will be a hands-on technical leader responsible for translating complex computer vision algorithms from research papers into robust, production-ready systems that power our next-generation products. this role requires a deep theoretical background combined with substantial practical experience working with spatial and temporal data. you will: • pioneer core cv algorithms: lead the research, design, and implementation of novel computer vision and deep learning algorithms, with a specialized focus on 2d and 3d data (e.g point clouds). • focus area expertise: drive innovation in key perception areas, including: • 3d reconstruction and slam: advanced techniques for real-time 3d mapping, pose estimation, and environmental modeling from multi-modal sensor inputs (e.g., rgb-d, lidar). • hand/body tracking: developing robust and precise models for hand pose estimation, gesture recognition, and full-body tracking under various lighting and occlusion conditions. • object detection and tracking (mot/sot): designing high-performance deep learning models for accurate detection and persistent tracking of objects and people in video streams. • video processing: creating algorithms for temporal feature extraction, video-based action recognition, and motion analysis. • model optimization: optimize computationally intensive models for deployment on edge devices (low power, low latency) and/or large-scale cloud infrastructure. • technical leadership: serve as the subject matter expert in computer vision, providing technical direction and mentorship to junior engineers and cross-functional teams. • publication & ip: maintain state-of-the-art knowledge, evaluate recent academic publications (e.g., cvpr, iccv, eccv), and drive the filing of patents and publication of novel research. • cross-functional partnering: collaborate closely with software engineering, product, and hardware teams to define requirements, integrate vision systems, and ensure solutions meet performance targets. you have: • ph.d. in computer science, computer engineering, or a related quantitative field (mathematics, electrical engineering, etc.) or a master’s degree with 4+ years of equivalent professional experience in an applied research setting. • 5+ years of hands-on experience in algorithm development for 2d/3d computer vision and deep learning. • deep learning frameworks: expert proficiency in at least one major deep learning framework (pytorch, tensorflow or jax). • programming: mastery of python for machine learning and strong proficiency in c++ for performance-critical algorithm implementation. • 2d/3d fundamentals: in-depth knowledge of classical and modern computer vision fundamentals, including multi-view geometry, projective geometry, camera calibration, and 3d graphics/rendering principles. • building real-time and batch ml systems that analyze structured and unstructured signals • hands-on experience rapidly prototyping and iterating on ml systems with changing requirements nice to haves: • deep learning frameworks: expert proficiency in at least one major deep learning framework (pytorch, tensorflow or jax). • programming: mastery of python for machine learning and strong proficiency in c++ for performance-critical algorithm implementation. • 2d/3d fundamentals: in-depth knowledge of classical and modern computer vision fundamentals, including multi-view geometry, projective geometry, camera calibration, and 3d graphics/rendering principles. • building real-time and batch ml systems that analyze structured and unstructured signals • hands-on experience rapidly prototyping and iterating on ml systems with changing requirements compensation packages at scale for eligible roles include base salary, equity, and benefits. the range displayed on each job posting reflects the minimum and maximum target for new hire salaries for the position, determined by work location and additional factors, including job-related skills, experience, interview performance, and relevant education or training. scale employees in eligible roles are also granted equity based compensation, subject to board of director approval. your recruiter can share more about the specific salary range for your preferred location during the hiring process, and confirm whether the hired role will be eligible for equity grant. you’ll also receive benefits including, but not limited to: comprehensive health, dental and vision coverage, retirement benefits, a learning and development stipend, and generous pto. additionally, this role may be eligible for additional benefits such as a commuter stipend. the base salary range for this full-time position in the location of san francisco is: $218,400—$273,000 usd please note: our policy requires a 90-day waiting period before reconsidering candidates for the same role. this allows us to ensure a fair and thorough evaluation of all applicants. about us: at scale, our mission is to develop reliable ai systems for the world's most important decisions. our products provide the high-quality data and full-stack technologies that power the world's leading models, and help enterprises and governments build, deploy, and oversee ai applications that deliver real impact. we work closely with industry leaders like meta, cisco, dla piper, mayo clinic, time inc., the government of qatar, and u.s. government agencies including the army and air force. we are expanding our team to accelerate the development of ai applications. we believe that everyone should be able to bring their whole selves to work, which is why we are proud to be an inclusive and equal opportunity workplace. we are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability status, gender identity or veteran status. we are committed to working with and providing reasonable accommodations to applicants with physical and mental disabilities. if you need assistance and/or a reasonable accommodation in the application or recruiting process due to a disability, please contact us at accommodations@scale.com. please see the united states department of labor's know your rights poster for additional information. we comply with the united states department of labor's pay transparency provision. please note: we collect, retain and use personal data for our professional business purposes, including notifying you of job opportunities that may be of interest and sharing with our affiliates. we limit the personal data we collect to that which we believe is appropriate and necessary to manage applicants’ needs, provide our services, and comply with applicable laws. any information we collect in connection with your application will be treated in accordance with our internal policies and programs designed to protect personal data. please see our privacy policy for additional information.","san francisco, ca",Machine Learning Engineer,"['aws', 'c++', 'cloud', 'computer vision', 'deep learning', 'machine learning', 'python', 'pytorch', 'r', 'tensorflow']","['aws', 'c++', 'cloud', 'computer vision', 'deep learning', 'machine learning', 'python', 'pytorch', 'r', 'tensorflow']",
"director, machine learning engineering, programmatic ads",pinterest,"about pinterest: millions of people around the world come to our platform to find creative ideas, dream about new possibilities and plan for memories that will last a lifetime. at pinterest, we’re on a mission to bring everyone the inspiration to create a life they love, and that starts with the people behind the product. discover a career where you ignite innovation for millions, transform passion into growth opportunities, celebrate each other’s unique experiences and embrace the flexibility to do your best work. creating a career you love? it’s possible. pinterest is one of the fastest growing online ad platforms, and our success depends on mining rich user interest data that helps us connect users with highly relevant advertisers/ products. we’re looking for a leader with experience in machine learning, data mining, and information retrieval to lead a team that develops new data-driven techniques to show the most engaging and relevant promoted content to the users. you’ll be leading a world-class ml team that is growing quickly and laying the foundation for pinterest’s business success. what you’ll do: • manage and grow the engineering team, providing technical vision and long-term roadmap • design and implement algorithms for real-time bidding, ad scoring/ranking, inventory selection and yield optimization on the dsp • hire, mentor and grow a team of engineers. set technical direction, manage project roadmaps, and actively guide team execution. collaborate across product, engineering, marketing and sales – translating business goals into ml requirements and ensuring solutions meet cross-functional needs • product strategy and advocacy. define and evangelize the ml roadmap for programmatic products. present findings and roadmaps to senior leadership, and work with stakeholders to align analytics projects with company goals (e.g. privacy-first personalization, new dsp features) what we’re looking for: • m.s/ phd degree in computer science, statistics or related field • 10+ years of industry experience building production machine learning systems at scale, data mining, search, recommendations, and/or natural language processing • experience in programmatic advertising or information retrieval contexts (rtb, dsp/ssp, search). familiarity with ad campaign metrics, auction mechanics and yield optimization • excellent at cross-functional collaboration and stakeholder communication relocation statement: • this position is not eligible for relocation assistance. visit our pinflex page to learn more about our working model. in-office requirement statement: • we let the type of work you do guide the collaboration style. that means we're not always working in an office, but we continue to gather for key moments of collaboration and connection. • this role will need to be in the office for in-person collaboration 1-2 times per month and therefore needs to be in a commutable distance from one of the following offices: san francisco, palo alto, seattle, los angeles. #li-hybrid #li-sm4 at pinterest we believe the workplace should be equitable, inclusive, and inspiring for every employee. in an effort to provide greater transparency, we are sharing the base salary range for this position. the position is also eligible for equity. final salary is based on a number of factors including location, travel, relevant prior experience, or particular skills and expertise. information regarding the culture at pinterest and benefits available for this position can be found here. us based applicants only $294,000—$514,500 usd our commitment to inclusion: pinterest is an equal opportunity employer and makes employment decisions on the basis of merit. we want to have the best qualified people in every job. all qualified applicants will receive consideration for employment without regard to race, color, ancestry, national origin, religion or religious creed, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, age, marital status, status as a protected veteran, physical or mental disability, medical condition, genetic information or characteristics (or those of a family member) or any other consideration made unlawful by applicable federal, state or local laws. we also consider qualified applicants regardless of criminal histories, consistent with legal requirements. if you require a medical or religious accommodation during the job application process, please complete this form for support.","seattle, wa (+3 others)",Machine Learning Engineer,"['aws', 'excel', 'machine learning', 'natural language processing', 'r', 'recommendation', 'statistics']","['aws', 'excel', 'machine learning', 'natural language processing', 'r', 'recommendation', 'statistics']",
"machine learning systems engineer, rl engineering",anthropic,"about anthropic anthropic's mission is to create reliable, interpretable, and steerable ai systems. we want ai to be safe and beneficial for our users and for society as a whole. our team is a quickly growing group of committed researchers, engineers, policy experts, and business leaders working together to build beneficial ai systems. about the role: you want to build the cutting-edge systems that train ai models like claude. you're excited to work at the frontier of machine learning, implementing and improving advanced techniques to create ever more capable, reliable and steerable ai. as an ml systems engineer on our reinforcement learning engineering team, you'll be responsible for the critical algorithms and infrastructure that our researchers depend on to train models. your work will directly enable breakthroughs in ai capabilities and safety. you'll focus obsessively on improving the performance, robustness, and usability of these systems so our research can progress as quickly as possible. you're energized by the challenge of supporting and empowering our research team in the mission to build beneficial ai systems. our finetuning researchers train our production claude models, and internal research models, using rlhf and other related methods. your job will be to build, maintain, and improve the algorithms and systems that these researchers use to train models. you'll be responsible for improving the speed, reliability, and ease-of-use of these systems. you may be a good fit if you: • have 4+ years of software engineering experience • like working on systems and tools that make other people more productive • are results-oriented, with a bias towards flexibility and impact • pick up slack, even if it goes outside your job description • enjoy pair programming (we love to pair!) • want to learn more about machine learning research • care about the societal impacts of your work strong candidates may also have experience with: • high performance, large scale distributed systems • large scale llm training • python • implementing llm finetuning algorithms, such as rlhf representative projects: • profiling our reinforcement learning pipeline to find opportunities for improvement • building a system that regularly launches training jobs in a test environment so that we can quickly detect problems in the training pipeline • making changes to our finetuning systems so they work on new model architectures • building instrumentation to detect and eliminate python gil contention in our training code • diagnosing why training runs have started slowing down after some number of steps, and fixing it • implementing a stable, fast version of a new training algorithm proposed by a researcher deadline to apply: none. applications will be reviewed on a rolling basis. the expected base compensation for this position is below. our total compensation package for full-time employees includes equity, benefits, and may include incentive compensation. annual salary: $300,000-$405,000 usd logistics education requirements: we require at least a bachelor's degree in a related field or equivalent experience.location-based hybrid policy: currently, we expect all staff to be in one of our offices at least 25% of the time. however, some roles may require more time in our offices. visa sponsorship: we do sponsor visas! however, we aren't able to successfully sponsor visas for every role and every candidate. but if we make you an offer, we will make every reasonable effort to get you a visa, and we retain an immigration lawyer to help with this. we encourage you to apply even if you do not believe you meet every single qualification. not all strong candidates will meet every single qualification as listed. research shows that people who identify as being from underrepresented groups are more prone to experiencing imposter syndrome and doubting the strength of their candidacy, so we urge you not to exclude yourself prematurely and to submit an application if you're interested in this work. we think ai systems like the ones we're building have enormous social and ethical implications. we think this makes representation even more important, and we strive to include a range of diverse perspectives on our team. how we're different we believe that the highest-impact ai research will be big science. at anthropic we work as a single cohesive team on just a few large-scale research efforts. and we value impact - advancing our long-term goals of steerable, trustworthy ai - rather than work on smaller and more specific puzzles. we view ai research as an empirical science, which has as much in common with physics and biology as with traditional efforts in computer science. we're an extremely collaborative group, and we host frequent research discussions to ensure that we are pursuing the highest-impact work at any given time. as such, we greatly value communication skills. the easiest way to understand our research directions is to read our recent research. this research continues many of the directions our team worked on prior to anthropic, including: gpt-3, circuit-based interpretability, multimodal neurons, scaling laws, ai & compute, concrete problems in ai safety, and learning from human preferences. come work with us! anthropic is a public benefit corporation headquartered in san francisco. we offer competitive compensation and benefits, optional equity donation matching, generous vacation and parental leave, flexible working hours, and a lovely office space in which to collaborate with colleagues. guidance on candidates' ai usage: learn about our policy for using ai in our application process","san francisco, ca",Machine Learning Engineer,"['aws', 'machine learning', 'python', 'r', 'sas']","['aws', 'machine learning', 'python', 'r', 'sas']",300K–405K a year
machine learning engineer,populous,"machine learning engineer location: kansas city, mo who we are: we design places where people love to be together. populous is a global design firm that began with a singular focus: to draw people together around the things they love, through experiences that capture all the senses, amplifying the atmosphere of excitement and pure joy shared in human moments. we’ve designed and delivered some of the world's most memorable civic, sports and entertainment buildings, from iconic stadia to ground-breaking live music venues. populous is where architects and designers of all kinds create moments bigger than ourselves. we offer the best of both worlds: the resources and impact of the design giants and the tightly knit atmosphere and growth opportunities of smaller firms. what we offer: professional & personal development programs + summer hours + hybrid schedule + on-site gym + paid architect licensure & certifications exams + wedding pay + charitable match program + market leading wellness health and welfare benefits. who we are looking for: we’re seeking a machine learning engineer with at least 3 years of experience in applied ml to join our global ai technology team. are you an ml engineer who loves solving real-world problems with data and ai? you’ll thrive at populous if you’re hands-on, curious, and excited to bring new ai capabilities into tools that shape spaces and human experience. collaborating across time zones with full stack developers and our ai lead, you’ll help prototype, fine-tune, and integrate machine learning models—particularly in natural language processing (nlp), generative ai, and semantic search—into production systems that drive better outcomes in the built environment. you’ll join a passionate and forward-thinking team using the latest tools and cloud technologies to work on projects at the intersection of design, data, sports and entertainment, and ai. what your day could consist of: • working across the full ml lifecycle, from data prep and model experimentation to deployment and ongoing optimization. • adapt and integrate foundational models (e.g. anthropic, openai, cohere) for targeted use cases. • implement and maintain apis for inference, batch jobs, and model access within production systems. • collaborate with developers to embed ml capabilities in user-facing applications. • build end-to-end pipelines for data collection, preprocessing, feature engineering, and training. • work with structured, unstructured, and spatial data across a variety of formats and sources. • use ml frameworks such as pytorch, tensorflow, and hugging face transformers. • operate within cloud platforms (aws, azure, or gcp) for model training and deployment. • leverage tools like mlflow, weights & biases, or langchain for model tracking and orchestration. • help shape our ai architecture, staying current on research, tooling, and trends in ai/ml – sharing your perspective in technical planning and team discussions. requirements for success: core technical skills • strong python programming skills and familiarity with ml libraries (e.g. scikit-learn, pytorch, tensorflow). • solid understanding of vector search and embedding-based systems (e.g. faiss, pinecone, weaviate). • comfortable operationalizing models via rest apis (e.g. using fastapi or flask). • proficient in handling both structured and unstructured data (text, images, spatial data). • familiarity with retrieval-augmented generation (rag), prompt tuning, or hybrid search architectures is preferred. • exposure to mlops workflows or orchestration tools (e.g. airflow, argo) is preferred. development & collaboration • comfortable building and maintaining ml pipelines from prototype to production. • familiarity with tools for experiment tracking and version control (e.g. mlflow, git, w&b). • excellent communication skills – able to explain technical decisions to non-technical collaborators. • research-oriented and self-motivated with a desire to apply ai in tangible, impactful ways. • interest in the built environment – whether through urban design, spatial data, or large-scale civic infrastructure. • understanding of ai governance topics such as data privacy, fairness, and explainability is preferred. • comfortable collaborating across disciplines, time zones, and cultures in a hybrid or remote setting. essential qualifications: required • 3+ years of experience in machine learning engineering or applied ml roles. • experience integrating machine learning models into workflows and applications. • experience working in cloud-based environments (aws, azure, or gcp). preferred • experience in the aec (architecture, engineering, construction) industry or in location-aware applications. • experience with llm orchestration frameworks (e.g. longchain, haystack). • experience building internal tooling, design assistants, or custom ai interfaces for non-technical users. additional details: • travel may be required. compensation: populous offers a competitive salary and bonus packages. we strive to offer our staff the best benefits package in the industry, at the lowest cost to employees, including medical, dental and vision coverage, 401k, fsa/hsa, paid time off and continuing education benefits. populous is an equal opportunity employer. we consider all qualified applicants for employment without regard to race, religion, color, national origin, sex, age, genetic information, sexual orientation, veteran status, disability status, or any other characteristic protected under applicable federal, state, or local laws. no. 1 most innovative company in architecture by fast company no. 1 firm in cultural on interior design magazine’s top 100 giants no. 1 architecture firm in cultural facilities by bd+c no. 1 architecture firm in convention centers by bd+c no. 1 architecture firm in performing arts and concert venues by bd+c find us here - https://populous.com/careers instagram: @wearepopulous x: @populous","kansas city, mo",Machine Learning Engineer,"['airflow', 'aws', 'azure', 'cloud', 'excel', 'experimentation', 'feature engineering', 'gcp', 'machine learning', 'natural language processing', 'nlp', 'python', 'pytorch', 'r', 'sas', 'scikit-learn', 'tensorflow']","['airflow', 'aws', 'azure', 'cloud', 'excel', 'experimentation', 'feature engineering', 'gcp', 'machine learning', 'natural language processing', 'nlp', 'python', 'pytorch', 'r', 'sas', 'scikit-learn', 'tensorflow']",
ai lead software engineer - machine learning,jpmorgan chase & co.,"about the position responsibilities • collaborate with data scientists to facilitate training, fine-tuning, and deployment of ml models, including foundational and generative models. • integrate trained models into production applications (e.g., anomaly detection, automated reporting, agentic ai workflows). • develop apis, microservices, and user interfaces to expose model capabilities to business users and other systems. • design and implement prompt engineering strategies and agentic architectures for autonomous ai workflows. • monitor, troubleshoot, and optimize model performance, scalability, and reliability in production environments. • act as a technical liaison between data science, engineering, and product teams to ensure seamless integration and delivery. • document processes, workflows, and best practices for model deployment and application integration. requirements • formal training or certification on software engineering concepts and 5+ years applied experience • proficiency in python and experience building apis/microservices. • experience with ml frameworks (e.g., pytorch, tensorflow, hugging face) and foundational models (llms, generative ai). • familiarity with prompt engineering and agentic workflows. • strong understanding of cloud platforms (aws, gcp, azure) and mlops practices. • excellent communication and collaboration skills. nice-to-haves • experience with anomaly detection, automated reporting, or narrative generation systems. • exposure to vector databases, retrieval-augmented generation (rag), or semantic search. • experience with containerization (docker, kubernetes) and ci/cd pipelines. • knowledge of security and compliance in ai/ml deployments. • experience with databricks ml ops. • familiarity with regression/classification models and their integration into production systems.","jersey city, nj",Machine Learning Engineer,"['aws', 'azure', 'classification', 'cloud', 'databricks', 'excel', 'gcp', 'python', 'pytorch', 'r', 'regression', 'scala', 'tensorflow']","['aws', 'azure', 'classification', 'cloud', 'databricks', 'excel', 'gcp', 'python', 'pytorch', 'r', 'regression', 'scala', 'tensorflow']",
"service delivery center, ai & data, machine learning engineer (mle) - manager",ey,"location: manayunk, charlotte, san antonio, jacksonville, alpharetta at ey, we’re all in to shape your future with confidence. we’ll help you succeed in a globally connected powerhouse of diverse teams and take your career wherever you want it to go. join ey and help to build a better working world. manager - financial services organization – ai and data - service delivery center ey is the only professional services firm with a separate business unit (“fso”) that is dedicated to the financial services marketplace. our fso teams have been at the forefront of every event that has reshaped and redefined the financial services industry. this practice also has several service delivery centers that are made up of high-performing us-based resources who work closely with our experienced client-serving professionals to deliver project-based work and managed services to our us-based financial services clients. if you have a passion for rallying together to solve the most complex challenges in the financial services industry, come join our dynamic fso team! the opportunity data has yet to be utilized to its fullest potential. financial institutions are looking to build smarter and more efficient ways to operate their business, through new opportunities uncovered by their data. you’ll be solving complex problems, as well as giving deliverables to our clients in the ai and data space. with support from a highly talented team, you’ll have tremendous growth opportunities, with a focus on continuous learning and skills development to become a leader that can make significant contributions to companies. your key responsibilities you will be working with diverse teams of professionals with different skill sets to deliver a wide range of ai and data services. as part of that, you’ll need to be a team player with great communication skills, and have the desire to learn the technical skills required to fill the role. skills and attributes for success • collaborating, mentoring, and coaching teams with diverse skills and backgrounds • fostering an innovative and inclusive team-oriented work environment • being accountable for the success of your team and the project’s components • building strong relationships with your internal clients • great communication and influencing skills • demonstrating in-depth technical capabilities, professional knowledge, and interest in the financial industry and technology • leading and managing multi-disciplinary teams through a product life cycle – requirements, design, development, and testing • designing, building, and maintaining production-grade ml and dl models, machine learning workflows, and pipelines • helping clients make data-driven decisions by working with structured and unstructured data sets and building out predictive models • designing and developing solutions leveraging big data technology (hadoop, spark, etc.) to ingest, process, and analyze large, disparate data sets to qualify for the role you must have • a bachelor's or master’s degree in computer science, information systems, or related technology majors • minimum of 5 years of related work experience in ai/ml engineering or mle/ml ops • experience working with business, management, and technology concepts • proficiency in python programming and associated machine learning libraries/packages (sklearn, tensorflow, pytorch, etc.) • extensive experience with cloud platforms such as aws, azure, or google cloud platform (gcp) for deploying and managing ml and ai solutions • strong deep learning experience, particularly in applications of neural network architectures to natural language processing, computer vision, and machine intelligence • knowledge of mlops practices for continuous integration and continuous deployment (ci/cd) of machine learning models in a cloud environment • good written and verbal communication skills • a willingness and ability to travel at 0%-25% • valid driver’s license in the us • valid passport ideally, you’ll also have • certification in any database management system, reporting or data visualization, or programming/statistical language • working knowledge or certifications in cloud technologies such as snowflake, databricks, aws, or azure • practical experience in developing and deploying generative ai applications for various use cases such as text generation, image synthesis, and creative ai • familiarity with cloud-based machine learning services like aws sagemaker, google ai platform, or azure machine learning what we look for we’re interested in professionals that are passionate about technology, who already have an understanding of data and are comfortable analyzing or manipulating it while generating reports for clients. you’ll also need to be able to clearly articulate both problems and proposed solutions, and have the willingness to learn and quickly adapt to changing requirements. on top of this, we’re looking for team players and hard workers who are not afraid to take initiative to master their craft and produce high-quality work. if you have a proactive approach and want to be part of a group that continues to grow significantly, this role is for you. what working at ey offers we offer a competitive compensation package where you’ll be rewarded based on your performance and recognized for the value you bring to our business. in addition, our total rewards package includes medical and dental coverage, both pension and 401(k) plans, a minimum of 15 days of vacation plus ten observed holidays and three paid personal days, and a range of programs and benefits designed to support your physical, financial, and social well-being. plus, we offer: • opportunities to develop new skills and progress your career • support and coaching from some of the most engaging and knowledgeable colleagues • a collaborative environment where everyone works together to create a better working world • excellent training and development prospects about ey as a global leader in assurance, tax, transaction, and advisory services, we hire and develop the most passionate people in their field to help build a better working world. this starts with a culture that believes in giving you the training, opportunities, and creative freedom to make things better. so that whenever you join, however long you stay, the exceptional ey experience lasts a lifetime. join us in building a better working world. apply now. what we offer you at ey, we’ll develop you with future-focused skills and equip you with world-class experiences. we’ll empower you in a flexible environment, and fuel you and your extraordinary talents in a diverse and inclusive culture of globally connected teams. learn more. • we offer a comprehensive compensation and benefits package where you’ll be rewarded based on your performance and recognized for the value you bring to the business. the base salary range for this job in all geographic locations in the us is $76,200 to $174,100. the base salary range for new york city metro area, washington state and california (excluding sacramento) is $91,400 to $197,900. individual salaries within those ranges are determined through a wide variety of factors including but not limited to education, experience, knowledge, skills and geography. in addition, our total rewards package includes medical and dental coverage, pension and 401(k) plans, and a wide range of paid time off options. • join us in our team-led and leader-enabled hybrid model. our expectation is for most people in external, client serving roles to work together in person 40-60% of the time over the course of an engagement, project or year. • under our flexible vacation policy, you’ll decide how much vacation time you need based on your own personal circumstances. you’ll also be granted time off for designated ey paid holidays, winter/summer breaks, personal/family care, and other leaves of absence when needed to support your physical, financial, and emotional well-being. are you ready to shape your future with confidence? apply today. ey accepts applications for this position on an on-going basis. for those living in california, please click here for additional information. ey focuses on high-ethical standards and integrity among its employees and expects all candidates to demonstrate these qualities. ey | building a better working world ey is building a better working world by creating new value for clients, people, society and the planet, while building trust in capital markets. enabled by data, ai and advanced technology, ey teams help clients shape the future with confidence and develop answers for the most pressing issues of today and tomorrow. ey teams work across a full spectrum of services in assurance, consulting, tax, strategy and transactions. fueled by sector insights, a globally connected, multi-disciplinary network and diverse ecosystem partners, ey teams can provide services in more than 150 countries and territories. ey provides equal employment opportunities to applicants and employees without regard to race, color, religion, age, sex, sexual orientation, gender identity/expression, pregnancy, genetic information, national origin, protected veteran status, disability status, or any other legally protected basis, including arrest and conviction records, in accordance with applicable law. ey is committed to providing reasonable accommodation to qualified individuals with disabilities including veterans with disabilities. if you have a disability and either need assistance applying online or need to request an accommodation during any part of the application process, please call 1-800-ey-help3, select option 2 for candidate related inquiries, then select option 1 for candidate queries and finally select option 2 for candidates with an inquiry which will route you to ey’s talent shared services team (tss) or email the tss at ssc.customersupport@ey.com.",pennsylvania,Machine Learning Engineer,"['aws', 'azure', 'cloud', 'computer vision', 'databricks', 'deep learning', 'excel', 'gcp', 'google cloud', 'hadoop', 'machine learning', 'natural language processing', 'python', 'pytorch', 'r', 'sklearn', 'snowflake', 'spark', 'tensorflow']","['aws', 'azure', 'cloud', 'computer vision', 'databricks', 'deep learning', 'excel', 'gcp', 'google cloud', 'hadoop', 'machine learning', 'natural language processing', 'python', 'pytorch', 'r', 'sklearn', 'snowflake', 'spark', 'tensorflow']",76.2K–198K a year
"senior machine learning engineer, llm compressor and quantization",redhat,"job summary at red hat we believe the future of ai is open and we are on a mission to bring the power of open-source llms and vllm to every enterprise. red hat inference team accelerates ai for the enterprise and brings operational simplicity to genai deployments. as leading developers, maintainers of the vllm project, and inventors of state-of-the-art techniques for model quantization and sparsification, our team provides a stable platform for enterprises to build, optimize, and scale llm deployments. you would be joining the core team behind2025's most popular open source project on github. as a machine learning engineer focused on model optimization algorithms, you will work closely with our product and research teams to develop sota deep learning software. you will collaborate with our technical and research teams to develop training and deployment pipelines, implement model compression algorithms, and productize deep learning research. if you are someone who wants to contribute to solving challenging technical problems at the forefront of deep learning in the open source way, this is the role for you. join us in shaping the future of ai. what you will do • contribute to the design, development, and testing of various inference optimization algorithms in the vllm llm-compressor project • create and manage training and deployment pipelines • benchmark and evaluate different quantization approaches to determine the best performance for specific hardware and models • participate in technical design discussions and provide innovative solutions to complex problems • give thoughtful and prompt code reviews • mentor and guide other engineers and foster a culture of continuous learning and innovation • stay up-to-date with the latest advancements in quantization research what you will bring • strong understanding of machine learning and deep learning fundamentals with experience in one or more of computer vision, nlp, and reinforcement learning • strong python and pytorch experience • experience with tensor math libraries such as pytorch and numpy • strong programming skills with proven experience implementing python based machine learning solutions • ability to develop and implement research ideas and algorithms • experience with mathematical software, especially linear algebra • strong communications skills with both technical and non-technical team members • bs, or ms in computer science or computer engineering or a related field. a phd in a ml related domain is considered a plus #li-md2 the salary range for this position is $170,770.00 - $281,770.00. actual offer will be based on your qualifications. pay transparency red hat determines compensation based on several factors including but not limited to job location, experience, applicable skills and training, external market value, and internal pay equity. annual salary is one component of red hat's compensation package. this position may also be eligible for bonus, commission, and/or equity. for positions with remote-us locations, the actual salary range for the position may differ based on location but will be commensurate with job duties and relevant work experience. about red hat red hat is the world's leading provider of enterprise open source software solutions, using a community-powered approach to deliver high-performing linux, cloud, container, and kubernetes technologies. spread across 40+ countries, our associates work flexibly across work environments, from in-office, to office-flex, to fully remote, depending on the requirements of their role. red hatters are encouraged to bring their best ideas, no matter their title or tenure. we're a leader in open source because of our open and inclusive environment. we hire creative, passionate people ready to contribute their ideas, help solve complex problems, and make an impact. benefits comprehensive medical, dental, and vision coverage flexible spending account - healthcare and dependent care health savings account - high deductible medical plan retirement 401(k) with employer match paid time off and holidays paid parental leave plans for all new parents leave benefits including disability, paid family medical leave, and paid military leave additional benefits including employee stock purchase plan, family planning reimbursement, tuition reimbursement, transportation expense account, employee assistance program, and more! note: these benefits are only applicable to full time, permanent associates at red hat located in the united states. inclusion at red hat red hat's culture is built on the open source principles of transparency, collaboration, and inclusion, where the best ideas can come from anywhere and anyone. when this is realized, it empowers people from different backgrounds, perspectives, and experiences to come together to share ideas, challenge the status quo, and drive innovation. our aspiration is that everyone experiences this culture with equal opportunity and access, and that all voices are not only heard but also celebrated. we hope you will join our celebration, and we welcome and encourage applicants from all the beautiful dimensions that compose our global village. equal opportunity policy (eeo) red hat is proud to be an equal opportunity workplace and an affirmative action employer. we review applications for employment without regard to their race, color, religion, sex, sexual orientation, gender identity, national origin, ancestry, citizenship, age, veteran status, genetic information, physical or mental disability, medical condition, marital status, or any other basis prohibited by law. red hat does not seek or accept unsolicited resumes or cvs from recruitment agencies. we are not responsible for, and will not pay, any fees, commissions, or any other payment related to unsolicited resumes or cvs except as required in a written contract between red hat and the recruitment agency or party requesting payment of a fee. red hat supports individuals with disabilities and provides reasonable accommodations to job applicants. if you need assistance completing our online job application, email application-assistance@redhat.com. general inquiries, such as those regarding the status of a job application, will not receive a reply.","raleigh, nc",Machine Learning Engineer,"['cloud', 'computer vision', 'deep learning', 'machine learning', 'nlp', 'numpy', 'python', 'pytorch', 'r']","['cloud', 'computer vision', 'deep learning', 'machine learning', 'nlp', 'numpy', 'python', 'pytorch', 'r']","170,770–281,770 a year"
sr. machine learning engineer – context engineering,geico,"at geico, we offer a rewarding career where your ambitions are met with endless possibilities. every day we honor our iconic brand by offering quality coverage to millions of customers and being there when they need us most. we thrive through relentless innovation to exceed our customers’ expectations while making a real impact for our company through our shared purpose. when you join our company, we want you to feel valued, supported and proud to work here. that’s why we offer the geico pledge: great company, great culture, great rewards and great careers. position description geico is seeking an experienced sr. staff machine learning engineer to join our ai org. this person will play key roles for the development of geico’s virtual assistant platform that elevates the productivity for 30k+ internal associates and the customer experience for millions of policyholders. you will be collaborating with a dynamic team of ai and software engineers to design, develop and deploy systems that ensure scalability, reliability and usability of agentic workflows across geico. context engineers build centralized services that ensure seamless ingestion and consumption of internal knowledge and contextual data, giving ai agents comprehensive and up-to-update information to determine optimal next steps. the ideal candidate should demonstrate a proven track record of building high performance ai/ml platform & systems, with hands-on experience and deep enthusiasm for generative ai and related ecosystems. responsibilities: • own development of key platform components that power end-to-end genai agentic workflows. examples include knowledge curation & management, search, context management, workflow orchestration, etc. • collaborate with cross-functional teams, including data scientists, ml engineers, software engineers, product managers, designers to gather requirements, define project scope and prioritize feature backlogs for high impact business use cases. establish pragmatic visions & roadmaps that balance business outcome, product release timelines and engineering excellence. • contribute to the selection, evaluation, and implementation of software technologies, tools, and frameworks, balancing build vs. buy, speed to market, maintainability, etc. • lead a small team of engineers for feature & system implementation. troubleshoot and resolve complex software issues, ensuring optimal platform performance and reliability. • mentor and guide junior engineers via code reviews and design sessions, fostering a collaborative and high-performance team culture, elevating ai engineering best practices across the company. basic qualifications • 5+ years of experience designing and building aiml platform and systems utilizing components such as vectordb (e.g. qdrant, milvus, etc.), data warehouse (e.g. snowflake), streaming platform (e.g. kafka), relational database (e.g. postgres sql), knowledge graph (e.g. neo4j), workflow orchestration (e.g. airflow, temporal). • proficient in python, java and similar general-purpose programming languages. • 3+ years’ experience managing end-to-end software development life cycle (e.g. cicd pipelines, kubernetes-based deployments, testing, monitoring & alerting, production support etc.) for backend systems and apis • 2+ years’ experience building training, finetuning, real-time/batch inferencing and evaluation systems for aiml models and llms, esp. utilizing gpu-powered infrastructure • bachelor’s degree or above in computer science, engineering, statistics or a related field preferred qualifications: • 3+ years of experience building enterprise-level semantic knowledge-graph & related capabilities, such as ontology creation, knowledge ingestion, graph queries-based feature engineering, etc. • 2+ years’ experience utilizing and/or finetuning open source llms, llama, mistral, claude etc. to build conversational experiences and agentic workflows, esp. natural-language-based query engines (e.g. text2sql) • strong communication and problem-solving skills to excel in dynamic, cross-functional decision-making environments annual salary $115,000.00 - $230,000.00 the above annual salary range is a general guideline. multiple factors are taken into consideration to arrive at the final hourly rate/ annual salary to be offered to the selected candidate. factors include, but are not limited to, the scope and responsibilities of the role, the selected candidate’s work experience, education and training, the work location as well as market and business considerations. geico will consider sponsoring a new qualified applicant for employment authorization for this position. the geico pledge: great company: at geico, we help our customers through life’s twists and turns. our mission is to protect people when they need it most and we’re constantly evolving to stay ahead of their needs. we’re an iconic brand that thrives on innovation, exceeding our customers’ expectations and enabling our collective success. from day one, you’ll take on exciting challenges that help you grow and collaborate with dynamic teams who want to make a positive impact on people’s lives. great careers: we offer a career where you can learn, grow, and thrive through personalized development programs, created with your career – and your potential – in mind. you’ll have access to industry leading training, certification assistance, career mentorship and coaching with supportive leaders at all levels. great culture: we foster an inclusive culture of shared success, rooted in integrity, a bias for action and a winning mindset. grounded by our core values, we have an an established culture of caring, inclusion, and belonging, that values different perspectives. our teams are led by dynamic, multi-faceted teams led by supportive leaders, driven by performance excellence and unified under a shared purpose. as part of our culture, we also offer employee engagement and recognition programs that reward the positive impact our work makes on the lives of our customers. great rewards: we offer compensation and benefits built to enhance your physical well-being, mental and emotional health and financial future. • comprehensive total rewards program that offers personalized coverage tailor-made for you and your family’s overall well-being. • financial benefits including market-competitive compensation; a 401k savings plan vested from day one that offers a 6% match; performance and recognition-based incentives; and tuition assistance. • access to additional benefits like mental healthcare as well as fertility and adoption assistance. • supports flexibility- we provide workplace flexibility as well as our geico flex program, which offers the ability to work from anywhere in the us for up to four weeks per year. the equal employment opportunity policy of the geico companies provides for a fair and equal employment opportunity for all associates and job applicants regardless of race, color, religious creed, national origin, ancestry, age, gender, pregnancy, sexual orientation, gender identity, marital status, familial status, disability or genetic information, in compliance with applicable federal, state and local law. geico hires and promotes individuals solely on the basis of their qualifications for the job to be filled. geico reasonably accommodates qualified individuals with disabilities to enable them to receive equal employment opportunity and/or perform the essential functions of the job, unless the accommodation would impose an undue hardship to the company. this applies to all applicants and associates. geico also provides a work environment in which each associate is able to be productive and work to the best of their ability. we do not condone or tolerate an atmosphere of intimidation or harassment. we expect and require the cooperation of all associates in maintaining an atmosphere free from discrimination and harassment with mutual respect by and for all associates and applicants.","chevy chase, md",Machine Learning Engineer,"['airflow', 'data warehouse', 'excel', 'feature engineering', 'java', 'kafka', 'machine learning', 'python', 'r', 'scala', 'snowflake', 'sql', 'statistics']","['airflow', 'data warehouse', 'excel', 'feature engineering', 'java', 'kafka', 'machine learning', 'python', 'r', 'scala', 'snowflake', 'sql', 'statistics']",115K–230K a year
"dir, machine learning eng",adobe,"job level m60/p60 employee role people manager the opportunity we are seeking a highly motivated ai/ml and data leader to lead the evolution of a data platform organization into a world where the ai/ml innovation needs to be supported and to influence/fuel the capabilities within the data platform itself. as and ai/ml leader in the adobe experience platform you will be responsible for instilling the intelligence needed in the data lakehouse, data modeling and compute core platform capabilities to enable innovation in the application layers around simplifying and automating interaction with and management of the digital marketing tools to help our customers to easy work towards accomplishing their business goals. all this innovation in the ai space must happen within a well defined trust framework which will ensure that ai-driven decisions are happening in a governed and responsible manner, thus the expectation is for you to both align with these constraints and to help evolving the trust framework to support the ever-increasing needs around supporting the ai use-cases. what you'll do • technical leadership: lead a team of applied scientists and data / systems engineers in designing, building and maintaining core capabilities with adobe experience platform focused on big-data management (petabytes) and compute abstractions (spark, kubernetes, etc) • strategic leadership: drive the transformation of a data engineering group toward a world where ai/ml is an integral part and heavily influencing the innovation in the data engineering space. provide strong leadership to the engineering team, fostering a culture of collaboration, innovation, and continuous improvement. • define and maintain the best data, ml and engineering practices: drive technical excellence, operational maturity, and code quality within your team. up-to-date with any emerging trends to adjust the organization to always utilize the best data, ml and engineering practices. • talent scouting and training: attract, recruit, hire, and develop a high-performing group with the right mix of skills to support the ai/ml, data and operational excellence needs what you need to succeed • over 10 years of experience in software engineering with at least 5 years focused in designing machine learning solutions with direct application in the data engineering space • ability to lead and inspire a team, excellent interpersonal skills, and the capacity to communicate complex machine learning concepts and their influence / impact on the data space clearly and effectively to stakeholders at all levels. • a deep understanding of machine learning models, frameworks, and algorithms, with hands-on experience in model development, deployment, and optimization complemented with a good understanding of the big data technologies and challenges. • strategic thinking capabilities, with a proven track record of leveraging ai to solve complex business challenges and drive growth. • passionate about the ethical and responsible use of ai/ml, ensuring solutions meet the highest standards of transparency and fairness. • typically requires a master’s degree in applied machine learning, artificial intelligence, or data science. a phd is preferred. our compensation reflects the cost of labor across several u.s. geographic markets, and we pay differently based on those defined markets. the u.s. pay range for this position is $193,900 -- $373,600 annually. pay within this range varies by work location and may also depend on job-related knowledge, skills, and experience. your recruiter can share more about the specific salary range for the job location during the hiring process. at adobe, for sales roles starting salaries are expressed as total target compensation (ttc = base + commission), and short-term incentives are in the form of sales commission plans. non-sales roles starting salaries are expressed as base salary and short-term incentives are in the form of the annual incentive plan (aip). in addition, certain roles may be eligible for long-term incentives in the form of a new hire equity award. state-specific notices: california: fair chance ordinances adobe will consider qualified applicants with arrest or conviction records for employment in accordance with state and local laws and “fair chance” ordinances. colorado: application window notice if this role is open to hiring in colorado (as listed on the job posting), the application window will remain open until at least the date and time stated above in pacific time, in compliance with colorado pay transparency regulations. if this role does not have colorado listed as a hiring location, no specific application window applies, and the posting may close at any time based on hiring needs. massachusetts: massachusetts legal notice it is unlawful in massachusetts to require or administer a lie detector test as a condition of employment or continued employment. an employer who violates this law shall be subject to criminal penalties and civil liability. internal opportunities creativity, curiosity, and constant learning are celebrated aspects of your career growth journey. we’re glad that you’re pursuing a new opportunity at adobe! put your best foot forward: 1. update your resume/cv and workday profile – don’t forget to include your uniquely ‘adobe’ experiences and volunteer work. 2. visit the internal mobility page on inside adobe to learn more about the process and set up a job alert for roles you’re interested in. 3. check out these tips to help you prep for interviews. 4. if you are applying for a role outside of your current country, ensure you review the international resources for relocating employees on inside adobe, including the impacts to your benefits, aip, equity & payroll. once you apply for a role via workday, the talent team will reach out to you within 2 weeks. if you move into the official interview process with the hiring team, make sure you inform your manager so they can champion your career growth. at adobe, you will be immersed in an exceptional work environment that is recognized around the world. you will also be surrounded by colleagues who are committed to helping each other grow through our unique check-in approach where ongoing feedback flows freely. if you’re looking to make an impact, adobe's the place for you. discover what our employees are saying about their career experiences on the adobe life blog and explore the meaningful benefits we offer. adobe is proud to be an equal employment opportunity employer. we do not discriminate based on gender, race or color, ethnicity or national origin, age, disability, religion, sexual orientation, gender identity or expression, veteran status, or any other applicable characteristics protected by law. learn more. adobe aims to make adobe.com accessible to any and all users. if you have a disability or special need that requires accommodation to navigate our website or complete the application process, email accommodations@adobe.com or call (408) 536-3015.","san jose, ca",Machine Learning Engineer,"['aws', 'data lake', 'excel', 'machine learning', 'r', 'spark']","['aws', 'data lake', 'excel', 'machine learning', 'r', 'spark']",
consulting machine learning engineer,hca healthcare,"description introduction experience the hca healthcare difference where colleagues are trusted, valued members of our healthcare team. grow your career with an organization committed to delivering respectful, compassionate care, and where the unique and intrinsic worth of each individual is recognized. submit your application for the opportunity below:consulting machine learning engineerhca healthcare benefits hca healthcare, offers a total rewards package that supports the health, life, career and retirement of our colleagues. the available plans and programs include: • comprehensive medical coverage that covers many common services at no cost or for a low copay. plans include prescription drug and behavioral health coverage as well as free telemedicine services and free airmed medical transportation. • additional options for dental and vision benefits, life and disability coverage, flexible spending accounts, supplemental health protection plans (accident, critical illness, hospital indemnity), auto and home insurance, identity theft protection, legal counseling, long-term care coverage, moving assistance, pet insurance and more. • free counseling services and resources for emotional, physical and financial wellbeing • 401(k) plan with a 100% match on 3% to 9% of pay (based on years of service) • employee stock purchase plan with 10% off hca healthcare stock • family support through fertility and family building benefits with progyny and adoption assistance. • referral services for child, elder and pet care, home and auto repair, event planning and more • consumer discounts through abenity and consumer discounts • retirement readiness, rollover assistance services and preferred banking partnerships • education assistance (tuition, student loan, certification support, dependent scholarships) • colleague recognition program • time away from work program (paid time off, paid family leave, long- and short-term disability coverage and leaves of absence) • employee health assistance fund that offers free employee-only coverage to full-time and part-time colleagues based on income. learn more about employee benefits note: eligibility for benefits may vary by location. we are seeking a consulting machine learning engineer for our team to ensure that we continue to provide all patients with high quality, efficient care. did you get into our industry for these reasons? we are an amazing team that works hard to support each other and are seeking a phenomenal addition like you who feels patient care is as meaningful as we do. we want you to apply! job summary and qualifications the consulting machine learning engineer is responsible for developing and integrating machine learning model and pre trained api with parallon data engineering department large-scale data ecosystems and business usecase, including data management and the integration of structured and unstructured data to generate insights to help parallon unlock the value of enterprise technology and data platform investments. the parallon data engineering department serves multiple business units in the organization: from the coding and clinical data perspective and also from the revenue cycle side. the consulting machine learning engineer requires strong knowledge regarding cloud and data/ml platforms, to identify the best practical applicable solutions to resolve the business needs. considering multiple variables during proof of concept, including cost, long term functionality, company policies and procedures, organization vision and road maps for incoming technologies. the consulting machine learning engineer will report to the manager of data engineering and will work closely with the data scientist, data engineer and collaborate with strong communication skills, and team work mentality. this role is firmly customer facing, with an expectation of constant contact, communication and iteration with both clinical and business customers to define questions, explore solutions and present results. major responsibilities understanding of nlp techniques for text representation, semantic extraction techniques, data structures and modeling , nature language understanding. design and develop nlp applications on google cloud and data platform leveraging vertex ai , llm and doc ai study and transform data science prototypes design and develop nlp applications on google cloud and data platform leveraging vertex ai , llm and doc ai select appropriate annotated datasets for supervised learning methods use effective text representations to transform natural language into useful features find and implement the right algorithms and tools for nlp tasks integrate with data platform and source thru api integration. train the developed model and run evaluation experiments perform statistical analysis of results and refine models extend ml libraries and frameworks to apply in nlp tasks remain updated in the rapidly changing field of machine learning deep understanding of text representation techniques (such as n-grams, bag of words, sentiment analysis etc), statistics and classification algorithms knowledge and expertise of python and sql experience with machine learning frameworks and api what qualifications you will need: experience 7 or more years of experience in developing and integrating machine learning models and pre trained api in cloud platforms 7 or more years of experience as a data engineer or nlp ml engineer or similar role - preferred hca healthcare has been recognized as one of the world's most ethical companies® by the ethisphere institute more than ten times. in recent years, hca healthcare spent an estimated $3.7 billion in cost for the delivery of charitable care, uninsured discounts, and other uncompensated expenses. ""there is so much good to do in the world and so many different ways to do it.""- dr. thomas frist, sr. hca healthcare co-founder if you find this opportunity compelling, we encourage you to apply for our consulting machine learning engineer opening. we promptly review all applications. highly qualified candidates will be directly contacted by a member of our team. we are interviewing apply today! we are an equal opportunity employer. we do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.","murfreesboro, tn",Machine Learning Engineer,"['classification', 'cloud', 'google cloud', 'machine learning', 'nlp', 'python', 'r', 'sql', 'statistics']","['classification', 'cloud', 'google cloud', 'machine learning', 'nlp', 'python', 'r', 'sql', 'statistics']",
senior machine learning/ ai engineer- (hybrid- greenfield opportunity)),match made tech,"job title: senior ai / machine learning engineer sponsorship not available- must be us citizen/ green card holder location: irvine, ca (onsite). monday through thursday onsite, fridays remote. compensation: $75-95 an hour. this is a 2-year contract that will convert to full-time. about us we are on a mission to develop innovative ai solutions that will revolutionize the way workforces operate. as we embark on a new greenfield ai project , we’re seeking an exceptional ai/ml engineer to help design and deliver production-grade machine learning models from the ground up. position overview as an ai/ml engineer , you’ll play a pivotal role in shaping this high-priority ai initiative. you will design, build, and deploy models in collaboration with data scientists, engineers, and domain experts—helping to create solutions that will operate at scale across a global workforce responsibilities model development • collaborate with data scientists to define objectives and requirements. • design, develop, and train ml models using state-of-the-art techniques. data preparation • work with large datasets, ensuring quality through preprocessing and transformation. • implement pipelines and augmentation methods to support training. deployment & optimization • deploy ml models into production with a focus on scalability and performance. • conduct experiments, fine-tune algorithms, and optimize hyperparameters. • implement monitoring and ongoing maintenance of models. collaboration & documentation • work cross-functionally with engineers, scientists, and domain experts. • document processes, best practices, and share insights with the team. qualifications • bachelor’s or master’s in computer science, machine learning, or related field. • 3+ years of proven experience developing and deploying ml models. • strong programming skills in python . • proficiency in tensorflow, pytorch, or scikit-learn . • experience with data preprocessing, pipelines, and feature engineering . • familiarity with aws , containerization (docker, kubernetes). • strong problem-solving and analytical mindset. • excellent communication and collaboration skills. • experience with llm's why join us • greenfield opportunity – build from the ground up, no legacy systems. • impact solutions at global scale . • work alongside a talented, diverse team in a fast-paced environment. • competitive pay with clear path to full-time conversion . • professional growth and advancement opportunities.","irvine, ca",Machine Learning Engineer,"['aws', 'excel', 'feature engineering', 'machine learning', 'python', 'pytorch', 'r', 'scala', 'scikit-learn', 'tensorflow']","['aws', 'excel', 'feature engineering', 'machine learning', 'python', 'pytorch', 'r', 'scala', 'scikit-learn', 'tensorflow']",75–95 an hour
"2026 summer student opportunities rbc borealis -machine learning software engineer, 4 months - montreal",royal bank of canada,"job description location: montreal what is the opportunity? are you interested in working as a machine learning software engineer co-op on the innovation & tech team in technology & operations at the royal bank of canada? rbc`s [genai powered training team] is responsible for [providing a safe space where advisors can practice conversations with specific learning objectives. the application provides coaching and verifies that advisors have attained the learning objectives of each conversation scenario.]. what will you do? • build and integrate machine learning solutions to enhance functionality and user experience. • contribute to the backend engine of our application • prototype new features and showcase them to the team • address bugs end-to-end: from identification and solution design to deployment of fixes • collaborate with engineers and product managers to translate requirements into technical solutions what do you need to succeed? must-have • currently enrolled at a canadian post-secondary institution with a focus on computer science, computer engineering, software engineering, or equivalent • excellent interpersonal and highly developed communication skills (verbal and written) • creative and analytical thinker who is self-driven and capable of working in a fast-paced environment • strong ms office skills – word, outlook, excel and powerpoint • great python programming abilities • enthusiasm for learning on the job • a preference for elegant solutions nice-to-have • experience with deep learning packages such as onnx, keras or pytorch is an asset; • experience with ai engineering packages such as instructor, openai, transformers is an asset; • exposure to distributed computing frameworks as well as sql, nosql is an asset; • transferrable skills: communication, time management, analytical thinking, problem solving, results driven, curiosity, strategic thinking what’s in it for you? we thrive on the challenge to be our best, progressive thinking to keep growing, and working together to deliver trusted advice to help our clients thrive and communities prosper. we care about each other, reaching our potential, making a difference to our communities, and achieving success that is mutual. • network and build lasting relationships with students from diverse backgrounds from across canada • participate in fun events and gamification challenges to help build your career tool kit while enjoying work-life balance • leaders who support your development through coaching and learning opportunities • work in a dynamic, collaborative, progressive and high-performing team • ability to make a difference and lasting impact • enjoy a comfortable work environment with the option to dress casually about rbc borealis rbc borealis is the driving force behind royal bank of canada’s ai and data innovation. as part of canada’s largest financial institution, we bring together a team of architects, engineers, scientists, and product experts on a mission to revolutionize finance through world-class research, solutions, and a resilient data platform. with locations across toronto, waterloo, montreal, calgary, and vancouver, we’re at the forefront of ai research and platform development. with a focus on cutting-edge research in areas like time series forecasting, causal machine learning, and responsible ai, we are seamlessly integrating ai research and data engineering, to solve critical challenges in the financial industry. we are building intelligent, and scalable, data-driven solutions that will help communities thrive and drive innovation for our customers across the bank. inclusion and equal opportunity employment at rbc, we embrace diversity and inclusion for innovation and growth. we are committed to building inclusive teams and an equitable workplace for our employees to bring their true selves to work. we are taking actions to tackle issues of inequity and systemic bias to support our diverse talent, clients and communities. we also strive to provide an accessible candidate experience for our prospective employees with different abilities. please let us know if you need any accommodations during the recruitment process. we encourage you to apply as soon as possible as we accept applications on a rolling basis, but please note that the formal application deadline is january 18, 2026. should you be selected to progress, someone from our team will reach out directly to provide instructions on next steps. otherwise, feel free to check for progress updates by logging in to your rbc profile. if the status has not changed, it denotes the fact that your application is still under review. earlytechtalent job skills additional job details address: 6666 rue st urbain, floor 3:montréal city: montréal country: canada work hours/week: 37.5 employment type: full time platform: job type: student/coop (fixed term) pay type: salaried posted date: 2025-12-12 application deadline: 2026-01-19 note: applications will be accepted until 11:59 pm on the day prior to the application deadline date above inclusion and equal opportunity employment at rbc, we believe an inclusive workplace that has diverse perspectives is core to our continued growth as one of the largest and most successful banks in the world. maintaining a workplace where our employees feel supported to perform at their best, effectively collaborate, drive innovation, and grow professionally helps to bring our purpose to life and create value for our clients and communities. rbc strives to deliver this through policies and programs intended to foster a workplace based on respect, belonging and opportunity for all. join our talent community stay in-the-know about great career opportunities at rbc. sign up and get customized info on our latest jobs, career tips and recruitment events that matter to you. expand your limits and create a new future together at rbc. find out how we use our passion and drive to enhance the well-being of our clients and communities at jobs.rbc.com.","montreal, quebec",Machine Learning Engineer,"['deep learning', 'excel', 'keras', 'machine learning', 'python', 'pytorch', 'r', 'scala', 'sql', 'time series']","['deep learning', 'excel', 'keras', 'machine learning', 'python', 'pytorch', 'r', 'scala', 'sql', 'time series']",
"sr. machine learning engineer, ads ranking",pinterest,"about pinterest: millions of people around the world come to our platform to find creative ideas, dream about new possibilities and plan for memories that will last a lifetime. at pinterest, we’re on a mission to bring everyone the inspiration to create a life they love, and that starts with the people behind the product. discover a career where you ignite innovation for millions, transform passion into growth opportunities, celebrate each other’s unique experiences and embrace the flexibility to do your best work. creating a career you love? it’s possible. within the ads ranking team, we try to connect the dots between the aspirations of pinners and the products offered by our partners. in this role, you will be responsible for developing and executing a vision for the evolution of the machine learning technology stack for the conversion modeling team. you will work on tackling new challenges such as user sequence modeling, embedding features, model quantization, utility alignment, roas optimization and many more to advance the ml models that power the heavy ranker stage and delivery that bring together pinners and partners in this unique marketplace. what you’ll do: • execute the development of state-of-the-art applied machine learning projects for the ads conversion modeling team • coach and mentor engineers for the conversion modeling team • design features and build large-scale machine learning models to improve user ads action prediction with low latency • develop new techniques for inferring user interests from online activity • mine text, visual, user signals to better understand user intention • work with product and sales teams to design and implement new ad products what we’re looking for: • ms or phd degree in computer science, statistics or related field • 3+ years of industry experience building large-scale machine learning models, with experience in ranking and recommender systems (e.g., search, recommendations, personalization). • strong mathematical skills with knowledge of statistical methods • cross-functional collaborator and strong communicator • background in computational advertising is preferred, but not required in-office requirement statement: • we let the type of work you do guide the collaboration style. that means we're not always working in an office, but we continue to gather for key moments of collaboration and connection. • this role will need to be in the office for in-person collaboration 1-2 times/quarter and therefore can be situated anywhere in the provinces of ontario or british columbia. #li-hybrid #li-ak7 our commitment to inclusion: pinterest is an equal opportunity employer and makes employment decisions on the basis of merit. we want to have the best qualified people in every job. all qualified applicants will receive consideration for employment without regard to race, color, ancestry, national origin, religion or religious creed, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, age, marital status, status as a protected veteran, physical or mental disability, medical condition, genetic information or characteristics (or those of a family member) or any other consideration made unlawful by applicable federal, state or local laws. we also consider qualified applicants regardless of criminal histories, consistent with legal requirements. if you require a medical or religious accommodation during the job application process, please complete this form for support.","toronto, on",Machine Learning Engineer,"['aws', 'machine learning', 'r', 'recommendation', 'recommender', 'statistics']","['aws', 'machine learning', 'r', 'recommendation', 'recommender', 'statistics']",
"senior machine learning engineer, agentic ai",loblaws,"job id: r2000618089 job description: we are seeking a senior machine learning engineer to join our team, focusing on the development, deployment and testing of advanced ai systems and sophisticated search agents. this role involves leveraging ml and cutting-edge large language models (llms) for building robust, scalable ai applications in the retail vertical, as well as mentoring other ml developers. what you’ll do \tdesign, build and ship multiple components within an agentic ai system utilizing state of the art technologies to solve business problems. \tdevelop high-performance enterprise-level machine learning models and ai agents using python programming, leveraging massive structured and unstructured datasets and apis from various internal and external sources. \tchampion and lead best practices for mle and llmops. \tcollaborate with front end and back end engineering teams to build and deploy ml models and agentic components in production. \ttake ownership of system components, mentor other machine learning developers, and contribute to raising the technical bar within the team. \tdocument and share results and findings with stakeholders in a structured manner and drive technical discussions cross functionally. who you are: \tcustomer-first mindset – passion for building ai products that make people's lives easier and allow them to focus on what is important to them. \tstrong collaboration – excellent communication skills and ability to thrive in a cross-functional, collaborative, fast-paced environment. \tproblem-solving excellence – analytical thinker who can identify, diagnose, and resolve technical challenges in ml pipelines and scalability. \tautomation-first approach – you are always thinking about automation and are comfortable using ai for productivity. \tadaptability & innovation – willingness to quickly learn new technologies, mentor others and continuously improve the system. \townership & initiative – ability to take ownership of key product components and drive impactful improvements. qualifications: \tbachelor’s degree or equivalent in computer science or a related field alongside a strong foundation in ml algorithms, ml pipelines, and transformations, with 5+ years of hands-on experience building scalable ml products. \tsoftware engineering proficiency in python, sql, and design patterns, with proven experience building and deploying ml solutions in microservices architecture in production. \t2+ years experience using gcp tools in ml workflows like vertex ai, bigquery, cloud composer and cloud storage. \tproven experience in langchain ecosystem or other agentic frameworks, nlp, llms, rags and embedding models. \tskilled in ml workflow automation/deployment and mlops for seamless integration and deployment, and have supported ml products in production environments. \tcommitted to code quality at every stage of the ml lifecycle, with a strong mindset in testing methodologies (unit, integration, end-to-end) and container orchestration using docker and kubernetes. how you’ll succeed: at loblaw digital, we seek great people to continually strengthen our culture. we believe great people model our values, are authentic, build trust and make connections. we’re able to keep innovating because our colleagues are passionate about their work and excited about the future of ecommerce. if you have big ideas, undeniable enthusiasm, and thrive in a collaborative, creative, and diverse group, we’ll get along just fine. looking for a challenge? good. love an innovative work environment? even better. apply today. employment type: full time type of role: regular loblaw digital recognizes canada's diversity as a source of national pride and strength. we have made it a priority to reflect our nation’s evolving diversity in the products we sell, the people we hire, and the culture we create in our organization. accommodation is available upon request for applicants with disabilities in the recruitment and assessment process and when hired. in addition, we believe that compliance with laws is about doing the right thing. upholding the law is part of our code of conduct – it reinforces what our customers and stakeholders expect of us.","brampton, on",Machine Learning Engineer,"['aws', 'bigquery', 'cloud', 'excel', 'gcp', 'machine learning', 'nlp', 'python', 'r', 'scala', 'sql']","['aws', 'bigquery', 'cloud', 'excel', 'gcp', 'machine learning', 'nlp', 'python', 'r', 'scala', 'sql']",
rq09917 - machine learning engineer - senior,maarut inc,"responsibilities: • creates machine learning models and utilizes data to train models • focuses on analyzing data to find relations between the input and the desired output • understands business objectives and develops models that help achieve them, along with metrics to track their progress • designs and develops machine learning and deep learning systems • runs machine learning tests and experiments implements appropriate machine learning algorithms general skills: • experience managing available resources such as hardware, data, and personnel so that deadlines are met • experience analyzing the machine learning algorithms that could be used to solve a given problem and ranking them by their success probability • experience exploring and visualizing data to gain an understanding of it, then identifying differences in data distribution that could affect performance when deploying the model in the real world • experience verifying data quality, and/or ensuring it via data cleaning • experience supervising the data acquisition process if more data is needed • experience finding available datasets online that could be used for training • experience defining validation strategies • experience defining the preprocessing or feature engineering to be done on a given dataset • background in statistics and computer programming • a team player with a track record for meeting deadlines, managing competing priorities and client relationship management experience. requirements experience and skill set requirements: must haves: • deep understanding of machine learning concepts: proficiency in fundamental machine learning concepts, algorithms, and techniques. • expertise in natural language processing (nlp): knowledge of nlp techniques and models, especially bert and other transformer-based models, for tasks like text classification, sentiment analysis, and language understanding. • experience with deep learning frameworks: proficiency in deep learning libraries such as tensorflow or pytorch. experience with implementing, training, and fine-tuning bert models using these frameworks is crucial. • data preprocessing skills: ability to perform text preprocessing, tokenization, and understanding of word embeddings. • programming skills: strong programming skills in python, including experience with libraries like numpy, pandas, and scikit-learn. • model optimization and tuning: skills in optimizing model performance through hyperparameter tuning and understanding of trade-offs between model complexity and performance. • understanding of transfer learning: knowledge of how to leverage pre-trained models like bert for specific tasks and adapt them to custom datasets. skill set requirements: • deep understanding of machine learning concepts: proficiency in fundamental machine learning concepts, algorithms, and techniques. • expertise in natural language processing (nlp): knowledge of nlp techniques and models, especially bert and other transformer-based models, for tasks like text classification, sentiment analysis, and language understanding. • experience with deep learning frameworks: proficiency in deep learning libraries such as tensorflow or pytorch. experience with implementing, training, and fine-tuning bert models using these frameworks is crucial. • data preprocessing skills: ability to perform text preprocessing, tokenization, and understanding of word embeddings. • programming skills: strong programming skills in python, including experience with libraries like numpy, pandas, and scikit-learn. • model optimization and tuning: skills in optimizing model performance through hyperparameter tuning and understanding of trade-offs between model complexity and performance. • understanding of transfer learning: knowledge of how to leverage pre-trained models like bert for specific tasks and adapt them to custom datasets.",ontario,Machine Learning Engineer,"['classification', 'deep learning', 'feature engineering', 'machine learning', 'natural language processing', 'nlp', 'numpy', 'pandas', 'python', 'pytorch', 'r', 'scikit-learn', 'statistics', 'tensorflow']","['classification', 'deep learning', 'feature engineering', 'machine learning', 'natural language processing', 'nlp', 'numpy', 'pandas', 'python', 'pytorch', 'r', 'scikit-learn', 'statistics', 'tensorflow']",$70–$75 an hour
senior machine learning engineer,electronic arts,"general information locations: edmonton, alberta, canada • location: edmonton • country: canada role id 211532 worker type regular employee studio/department ea studios - quality verification work model hybrid description & requirements electronic arts creates next-level entertainment experiences that inspire players and fans around the world. here, everyone is part of the story. part of a community that connects across the globe. a place where creativity thrives, new perspectives are invited, and ideas matter. a team where everyone makes play happen. the atom team builds the future of ai for testing games. as a machine learning engineer reporting to the director of ai, you will fulfill high-impact applied research goals and help us bring ea's games to life. your mission is to discover and evaluate ai methods that increase the velocity and quality of next-generatio n interactive experiences. our team impacts every title in ea's portfolio, and you will work with all types of ai technology to improve our titles. responsibiliti es • prototype, train, and ship ai tools that improve game testing efficiency, such as autonomous play-testing agents, test-case generation, anomaly/bug detection, and bug triaging. • translate atom's technology roadmap into experiments and deliverables, with support from lead and senior ml scientists • build reliable data pipelines from gameplay logs, video/frames, and telemetry; ensure data quality, labelling strategies, and reproducibilit y. • stay up-to-date on advancements in deep learning and genai through self-study, internal workshops, and external conferences. • this job is onsite of hybrid remote/in-offi ce (3 days/week). qualifications • bsc degree in computer science, engineering or mathematics, or equivalent experience. • 5+ years of experience spanning across the entire ml lifecycle (frame, gather/curate data, model, evaluate, deploy, observe) • fluent in python and major ml frameworks (e.g., pytorch) and skill with software development practices. • experience training models at scale (multi-gpu or distributed), strong understanding of ml fundamentals, mlops, and best practices (e.g., reproducibilit y). preferences • graduate degree in computer science, engineering, mathematics, or related discipline. • experience with: r einforcement/i mitation learning, computer vision (for video), agents/llms, uncertainty q uantification, out-of-distrib ution detection. • experience with distributed ml (e.g., deepseed). compensation and benefits the ranges listed below are what ea in good faith expects to pay applicants for this role in these locations at the time of this posting. if you reside in a different location, a recruiter will advise on the applicable range and benefits. pay offered will be determined based on a number of relevant business and candidate factors (e.g. education, qualifications, certifications, experience, skills, geographic location, or business needs). pay ranges • british columbia (depending on location e.g. vancouver vs. victoria) *$138,700 - $199,900 cad in british columbia, we offer a package of benefits including vacation (3 weeks per year to start), 10 days per year of sick time, paid top-up to ei/qpip benefits up to 100% of base salary when you welcome a new child (12 weeks for maternity, and 4 weeks for parental/adoption leave), extended health/dental/vision coverage, life insurance, disability insurance, retirement plan to regular full-time employees. certain roles may also be eligible for bonus and equity. about electronic arts we’re proud to have an extensive portfolio of games and experiences, locations around the world, and opportunities across ea. we value adaptability, resilience, creativity, and curiosity. from leadership that brings out your potential, to creating space for learning and experimenting, we empower you to do great work and pursue opportunities for growth. we adopt a holistic approach to our benefits programs, emphasizing physical, emotional, financial, career, and community wellness to support a balanced life. our packages are tailored to meet local needs and may include healthcare coverage, mental well-being support, retirement savings, paid time off, family leaves, complimentary games, and more. we nurture environments where our teams can always bring their best to what they do. electronic arts is an equal opportunity employer. all employment decisions are made without regard to race, color, national origin, ancestry, sex, gender, gender identity or expression, sexual orientation, age, genetic information, religion, disability, medical condition, pregnancy, marital status, family status, veteran status, or any other characteristic protected by law. we will also consider employment qualified applicants with criminal records in accordance with applicable law. ea also makes workplace accommodations for qualified individuals with disabilities as required by applicable law.","edmonton, ab",Machine Learning Engineer,"['computer vision', 'data pipeline', 'deep learning', 'machine learning', 'python', 'pytorch', 'r']","['computer vision', 'data pipeline', 'deep learning', 'machine learning', 'python', 'pytorch', 'r']",$139K–$200K a year
machine learning specialist,telus,"join our team and what we'll accomplish together the ai accelerator team is on a continuous journey towards helping telus become a world-class leader in data solutions, doing so by delivering data analytics capabilities built upon unified scalable platforms, advanced ai solutions, high-quality data, and a data-product-oriented culture while always keeping an eye on the horizon, preparing for the next big thing. we are entrepreneurial and live by our ai manifesto of failing fast and being outcome vs technology-driven, creating value for our customers, team members, communities, and the environment. the team takes pride in our artificial intelligence and machine learning capabilities and takes ownership of each step of the process. from hypothesis generation, initial exploring of datasets, developing novel ai techniques to discover insights, to developing automation pipelines and web visualizations, we do it all! always wanted to work with a team of innovators touching all business units within telus, and be part of a culture that embraces creativity and collaboration? if so, we’d love to talk with you! you’ll be a part of the team and journey that will transform the way we do business across various domains. you’ll collaborate with teams across the company, seeking out various data sources to help identify new business opportunities while championing data-driven decision-making and the accelerated adoption of ai. as a machine learning engineer on the team, you will combine your expert knowledge of data science with your strong ml ops and software development skills to automate and facilitate data exploration, analytics, machine learning model development, training and deployment and will leverage your experience in building reusable algorithms, functions and libraries. what you'll do • lead the iterative development, validation, and deployment of ai/ml models • drive the development and deployment of generative ai applications, tools, and frameworks • collaborate on end-to-end automation efforts required to bring models to production • work with structured and unstructured raw data to design and develop innovative predictive models, metrics, and dashboards to uncover actionable insights • visualize and report data findings creatively in a variety of visual formats that provide insights to the organization • influence how we approach business challenges and opportunities by driving the adoption of a data-driven mindset • develop re-usable code aimed at delivering on future goals faster and more reliably • support and evolve the advanced analytics and data science roadmap by leveraging industry research, best practices and emerging tools/technology • build and maintain a strong engagement with key stakeholders to understand business needs and priorities what you bring • you are recognized for addressing business needs via your application of data mining and analysis, predictive modeling, statistics and other advanced analytical techniques in which you have previous hands-on work experience • you are an expert at designing and building genai workflows and applications to solve complex business problems • you are sought out for your skills in machine learning and ai, including regression, classification, clustering, time series analysis, nlp, and optimization and bring 4+ years of hands-on work and practical business experience in the above areas • you are a master communicator capable of breaking down technical and complex concepts in a way that is understood by non-technical audiences • you have advanced experience with python and you are comfortable using various data science libraries such as scikit-learn, pandas, numpy as well as frameworks like tensorflow, pytorch, keras and have applied these skills towards solving actual business matters • you are comfortable in and with a jupyter environment and infrastructure, and familiar with github • you possess strong knowledge in sql and distributed computing • you are familiar with at least one of the cloud computing platforms - gcp, aws, azure • you are well versed in software and ai development lifecycles, including ml ops • you are agile and have a bias for action, removing roadblocks to get results fast great-to-haves • masters or phd degree in a quantitative field such as math, statistics, computer science, economics, engineering, or data science • experience with agile methodology and work in a start-up environment • gcp or other cloud certifications","vancouver, bc",Machine Learning Engineer,"['aws', 'azure', 'classification', 'cloud', 'clustering', 'dashboard', 'data analytics', 'gcp', 'keras', 'machine learning', 'nlp', 'numpy', 'pandas', 'python', 'pytorch', 'r', 'regression', 'scala', 'scikit-learn', 'sql', 'statistics', 'tensorflow', 'time series']","['aws', 'azure', 'classification', 'cloud', 'clustering', 'dashboard', 'data analytics', 'gcp', 'keras', 'machine learning', 'nlp', 'numpy', 'pandas', 'python', 'pytorch', 'r', 'regression', 'scala', 'scikit-learn', 'sql', 'statistics', 'tensorflow', 'time series']",
machine learning lead,lumicity,"machine learning lead greater toronto area about the role: lumicity have partnered with an exciting start up to help source a technical lead specializing in machine learning and edge ai for their growing team in toronto. technical requirements: • 5+ years of ml experience • previous experience in a technical lead/senior position • high interest in edge ai and hardware architecture • founder mindset if you're interested in this position, please apply directly or contact me at max.pheysey@lumicity.io with your resume attached so that we can arrange a time to speak.",canada,Machine Learning Engineer,"['machine learning', 'r']","['machine learning', 'r']",
research engineer,royal bank of canada,"job description what's the opportunity? at rbc borealis, you’ll be joining a team of leading researchers and software engineering specializing in machine learning. you will have access to rich and massive datasets, and to computational resources to support novel product development touching machine learning areas such as generative ai, natural language processing, and time series analysis. we’re looking for an enthusiastic machine learning research engineer who’s excited by the opportunity of being at the forefront of applying machine learning technology to challenging problems. as a ml research engineer in the applied research team, you’ll be part of a collaborative group who aims to deliver ai projects end to end – everything from data pre-processing and exploration, to prototyping novel algorithmic solutions, to software implementations of machine learning-based products. the goal is to understand the needs of our business partners and bring to life these unique and efficient solutions that can only be achieved through the use of machine learning. your responsibilities include: • building machine learning-based software solutions; • collaborating with business stakeholders to prototype machine-learning solutions rapidly; • conducting comparisons to existing algorithms and baselines; • reviewing, extending, and optimizing prototype solutions; • collaborating with the engineering team to integrate algorithms into products; • developing reusable internal tools to facilitate research prototyping; • supporting projects with thorough documentation, design decisions, and capabilities. you’re our ideal candidate if you have: • a master’s or phd degree in computer science, mathematics, physics, economics or equivalent; • 2+ years of applied machine learning experience in a high-responsibility, minimal-supervision environment; • experience with writing modular, robust, scalable software in python 3.x; • expertise in a few of the following areas: deep learning, natural language processing, information retrieval; • experience with deep learning packages such as pytorch, jax, or tensorflow; • knowledge of professional software engineering best practices for the full software development life cycle, including testing methods, coding standards, code reviews, and source control management; • strong communication skills and a collaborative attitude. what’s in it for you? • become part of a team that thinks progressively and works collaboratively. we care about seeing each other reach full potential; • a comprehensive total rewards program including bonuses and flexible benefits, competitive compensation, commissions, and stock options where applicable; • leaders who support your development through coaching and managing opportunities; • ability to make a difference and lasting impact from a local-to-global scale. about rbc borealis rbc borealis, an rbc institute for research, is a curiosity-driven research centre dedicated to achieving state-of-the-art in machine learning. established in 2016, and with labs in toronto, montreal, waterloo, and vancouver, we support academic collaborations and partner with world-class research centres in artificial intelligence. with a focus on ethical ai that will help communities thrive, our machine learning scientists perform fundamental and applied research in areas such as reinforcement learning, natural language processing, deep learning, and unsupervised learning to solve ground-breaking problems in diverse fields. inclusion and equal opportunity employment rbc is an equal opportunity employer committed to diversity and inclusion. we are pleased to consider all qualified applicants for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, protected veterans status, aboriginal/native american status or any other legally-protected factors. disability-related accommodations during the application process are available upon request. #ll-post job skills big data management, data mining, data science, deep learning, machine learning (ml), machine learning algorithms, predictive analytics, programming languages, statistical analysis additional job details address: 777 bay st, th 27:toronto city: toronto country: canada work hours/week: 37.5 employment type: full time platform: technology and operations job type: regular pay type: salaried posted date: 2025-12-08 application deadline: 2026-04-27 note: applications will be accepted until 11:59 pm on the day prior to the application deadline date above inclusion and equal opportunity employment at rbc, we believe an inclusive workplace that has diverse perspectives is core to our continued growth as one of the largest and most successful banks in the world. maintaining a workplace where our employees feel supported to perform at their best, effectively collaborate, drive innovation, and grow professionally helps to bring our purpose to life and create value for our clients and communities. rbc strives to deliver this through policies and programs intended to foster a workplace based on respect, belonging and opportunity for all. join our talent community stay in-the-know about great career opportunities at rbc. sign up and get customized info on our latest jobs, career tips and recruitment events that matter to you. expand your limits and create a new future together at rbc. find out how we use our passion and drive to enhance the well-being of our clients and communities at jobs.rbc.com.","vancouver, bc (+1 other)",Machine Learning Engineer,"['deep learning', 'machine learning', 'natural language processing', 'python', 'pytorch', 'r', 'scala', 'tensorflow', 'time series']","['deep learning', 'machine learning', 'natural language processing', 'python', 'pytorch', 'r', 'scala', 'tensorflow', 'time series']",
senior machine learning engineer - growth,doordash canada,"about the team come help us build the world's most reliable on-demand logistics engine for last-mile delivery! we're seeking an experienced machine learning engineer to develop cutting-edge ml models that power doordash's pricing, selection, affordability, and growth strategies. about the role we're looking for a passionate machine learning expert to join our team. you'll leverage our robust data and machine learning infrastructure to build sophisticated ml systems that implement innovative ai solutions for consumer pricing, restaurant selection expansion, promotions (deals) optimization, and marketplace growth. you'll need to demonstrate strong production-level machine learning expertise while collaborating with multi-disciplinary teams to set strategy and drive business growth. you're excited about this opportunity because you will: • develop ml-driven systems that ensure doordash pricing remains affordable and fair across all restaurant selections and occasions • create ai solutions that optimize deals and promotions for relevancy and efficiency, intelligently adapting to evolving consumer ordering behaviors • partner with engineering and product leaders to shape product roadmaps through ai/ml innovations • own the complete modeling lifecycle from feature creation through deployment, experimentation, monitoring, explainability, and maintenance we're excited about you because you have: • 6+ years of industry experience developing and leading advanced machine learning initiatives with measurable business impact and production-deployed solutions • m.s. or ph.d. in statistics, computer science, mathematics, operations research, physics, economics, or another quantitative field • exceptional written and verbal communication skills • strong technical expertise in python, spark, and pytorch for machine learning applications • experience with statistics, causal inference, multi-objective optimization, or deep learning (preferred) notice to applicants for jobs located in nyc or remote jobs associated with office in nyc only we use covey as part of our hiring and/or promotional process for jobs in nyc and certain features may qualify it as an aedt in nyc. as part of the hiring and/or promotion process, we provide covey with job requirements and candidate submitted applications. we began using covey scout for inbound from august 21, 2023, through december 21, 2023, and resumed using covey scout for inbound again on june 29, 2024. the covey tool has been reviewed by an independent auditor. results of the audit may be viewed here: covey","toronto, on",Machine Learning Engineer,"['deep learning', 'experimentation', 'machine learning', 'python', 'pytorch', 'r', 'spark', 'statistics']","['deep learning', 'experimentation', 'machine learning', 'python', 'pytorch', 'r', 'spark', 'statistics']",
ai/ml software engineer,qualcomm,"company: qualcomm canada ulc job area: engineering group, engineering group > machine learning engineering general summary: as a leading technology innovator, qualcomm pushes the boundaries of what's possible to enable next-generation experiences and drives digital transformation to help create a smarter, connected future for all. as a qualcomm machine learning engineer, you will create and implement machine learning techniques, frameworks, and tools that enable the efficient discovery and utilization of state-of-the-art machine learning solutions over a broad set of technology verticals or designs. qualcomm engineers collaborate with cross-functional teams to enhance the world of mobile, edge, auto, and iot products through machine learning hardware and software. minimum qualifications: • bachelor's degree in computer science, engineering, information systems, or related field. job description summary: ai’s ability to solve complex problems across multiple domains is transformative. here in the artificial intelligence software group. we are building highly optimized on-device ai solutions. we combine high performance software with cutting edge hardware to run deep neural nets with that little computer you keep in your pocket, that car you drive, or that vacuum cleaner you unleash to clean your house. come join us if you want to work on bleeding edge ai technology. in this position you will build high performance software for ai engines which include snapdragon neural processing engine, android nn, tensorflow lite delegate and many more. we develop new tools and sdks to extend our ai solutions into industry leading customer use cases. responsibilities: • development of optimization algorithms for ml operators/layers for the qualcomm ai sw stack • development of ai sw stack framework enhancements for optimal resource usage while running a neural network on qualcomm hardware • development of software tools for profiling, and debugging to support rapid deployment of new neural networks in this fast-changing field • evaluating and optimizing neural networks’ runtime performance and accuracy • working with customer teams to enable state-of-the-art network models and new ai sw features to meet customer use-cases • collaborating with ai hardware and architecture teams to continuously improve our ai solution • being an active contributor to the development process to ensure commercial quality software releases • university grads welcome! ideal candidates will demonstrate the following: • enjoy software development with excellent programming capability and analytical, development, and debugging skills • curiosity and willingness to continually learn new things and propose new ideas • experience with c/c++, python • c++ work experience • android or embedded linux software development preferred • optimizing algorithms for hardware acceleration cores using opencl, opengl es, and / or dsps • experience with python numpy and deep learning frameworks such as tensorflow, pytorch • background in mathematical operations: linear algebra, fast math libraries, multi-threading and vector instruction sets • excellent communication skills (verbal, presentation, written) • ability to collaborate across a globally diverse team and multiple interests applicants: qualcomm is an equal opportunity employer. if you are an individual with a disability and need an accommodation during the application/hiring process, rest assured that qualcomm is committed to providing an accessible process. you may e-mail disability-accomodations@qualcomm.com or call qualcomm's toll-free number found here. upon request, qualcomm will provide reasonable accommodations to support individuals with disabilities to be able participate in the hiring process. qualcomm is also committed to making our workplace accessible for individuals with disabilities. (keep in mind that this email address is used to provide reasonable accommodations for individuals with disabilities. we will not respond here to requests for updates on applications or resume inquiries). qualcomm expects its employees to abide by all applicable policies and procedures, including but not limited to security and other requirements regarding protection of company confidential information and other confidential and/or proprietary information, to the extent those requirements are permissible under applicable law. to all staffing and recruiting agencies: our careers site is only for individuals seeking a job at qualcomm. staffing and recruiting agencies and individuals being represented by an agency are not authorized to use this site or to submit profiles, applications or resumes, and any such submissions will be considered unsolicited. qualcomm does not accept unsolicited resumes or applications from agencies. please do not forward resumes to our jobs alias, qualcomm employees or any other company location. qualcomm is not responsible for any fees related to unsolicited resumes/applications. if you would like more information about this role, please contact qualcomm careers.","markham, on",Machine Learning Engineer,"['c++', 'deep learning', 'excel', 'machine learning', 'numpy', 'python', 'pytorch', 'r', 'tensorflow']","['c++', 'deep learning', 'excel', 'machine learning', 'numpy', 'python', 'pytorch', 'r', 'tensorflow']",
"senior machine learning engineer, ad tech",loblaw,"come make your difference in communities across canada, where authenticity, trust and making connections is valued – as we shape the future of canadian retail, together. our unique position as one of the country's largest employers, coupled with our commitment to positively impact the lives of all canadians, provides our colleagues a range of opportunities and experiences to help canadians live life well®. at loblaw companies limited, we succeed through collaboration and commitment and set a high bar for ourselves and those around us. whether you are just starting your career, re-entering the workforce, or looking for a new job, this is where you belong. as a senior machine learning engineer in the retail media domain, you will use an abundance of data from the loblaw enterprise in order to build machine learning solutions. your team will collaborate with business stakeholders and engineers within loblaw digital to deliver models to production that help in driving our adtech and e-commerce business. you will design experiments to measure your success with kpis that include adoption, conversion & retention. you will use data every single day to uncover meaningful insights that inform your experimentation strategy and ultimately, your product roadmap. what you’ll do: • design, build, and maintain highly scalable, robust, and efficient cloud infrastructure using google cloud platform (gcp) services, including vertex ai, bigtable, bigquery, alloydb, and cloud composer. • develop automation and orchestration of ml pipelines, integrating data ingestion, feature engineering, training, and deployment processes. • collaborate with cross-functional teams to understand their needs and build solutions that improve platform usability, scalability, and the overall development experience. • optimize data processing pipelines and cloud resources to ensure low-latency, cost-effective operation. • implement monitoring, alerting, and failover strategies to ensure platform reliability. • stay updated with industry trends and best practices in cloud engineering, data engineering, and machine learning does this sound like you? • customer-centric mindset: passionate about delivering an exceptional experience for data scientists through a self-service platform, reducing friction in their workflows. • collaboration: strong communication skills to work closely with cross-functional teams, including data scientists and engineers, to ensure platform features meet user needs and expectations. • problem-solving: ability to identify and solve complex technical issues related to ml pipelines, cloud infrastructure, and scalability, ensuring an efficient and robust platform. • automation-first approach: commitment to streamlining and automating processes for scalability and reliability, enabling data scientists to focus on experimentation and model development. • adaptability: ability to quickly adjust to new technologies and evolving platform needs to keep the infrastructure cutting-edge and efficient. • ownership and initiative: comfortable taking ownership of key platform components, driving innovation and improvements that benefit the platform’s scalability and usability. • bachelor’s or master’s degree in computer science, engineering, or a related field. • 2+ years of experience in software engineering with a focus on cloud infrastructure and/or data engineering. • hands-on experience with google cloud platform services such as vertex ai, bigtable, bigquery, cloud composer, cloud storage, etc. • proficiency in one or more programming languages such as python, java, and sql. • experience with orchestration tools such as apache airflow (composer). • knowledge of ci/cd pipelines and devops tools for continuous integration and deployment. • familiarity with containerization and orchestration (docker, kubernetes). • strong problem-solving skills and attention to detail. • excellent communication skills and ability to work in a collaborative, fast-paced environment our commitment to sustainability and social impact is an essential part of the way we do business, and we focus our attention on areas where we can have the greatest impact. our approach to sustainability and social impact is based on three pillars – environment, sourcing and community – and we are constantly looking for ways to demonstrate leadership in these important areas. our core values – care, ownership, respect and excellence – guide all our decision-making and come to life through our blue culture. we offer our colleagues progressive careers, comprehensive training, flexibility, and other competitive benefits – these are some of the many reasons why we are one of canada’s top employers, canada’s best diversity employers, canada’s greenest employers & canada’s top employers for young people. if you are unsure whether your experience matches every requirement above, we encourage you to apply anyway. we are looking for varied perspectives which include diverse experiences that we can add to our team. we have a long-standing focus on diversity, equity and inclusion because we know it will make our company a better place to work and shop. we are committed to creating accessible environments for our colleagues, candidates and customers. requests for accommodation due to a disability (which may be visible or invisible, temporary or permanent) can be made at any stage of application and employment. we encourage candidates to make their accommodation needs known so that we can provide equitable opportunities. please note: candidates who are 18 years or older are required to complete a criminal background check. details will be provided through the application process. #en #ss #ld #on","toronto, on",Machine Learning Engineer,"['airflow', 'bigquery', 'cloud', 'excel', 'experimentation', 'feature engineering', 'gcp', 'google cloud', 'java', 'machine learning', 'python', 'r', 'scala', 'sql']","['airflow', 'bigquery', 'cloud', 'excel', 'experimentation', 'feature engineering', 'gcp', 'google cloud', 'java', 'machine learning', 'python', 'r', 'scala', 'sql']",
ml engineer,hays,"your new company join a leading retail company that is dedicated to providing exceptional customer experiences and innovative solutions. this organization is committed to leveraging cutting-edge technologies to drive business growth and enhance operational efficiency. your new role • architecting and implementing advanced deep learning models for multimodal recommendation systems, processing diverse data types including text, images, user behavior, item features, offer data, and contextual signals • leading the development and optimization of generative ai applications for personalized product discovery, search enhancement, and customer engagement • designing and maintaining highly scalable ml infrastructure using gcp services (vertex ai, bigtable, bigquery, cloud composer) capable of handling millions of daily predictions • building and optimizing end-to-end ml pipelines for model training, deployment, and monitoring at scale • driving architecture decisions for the personalization platform • collaborating with product managers, data scientists, and engineers to translate business requirements into robust technical solutions • leading experimentation strategies with measurable kpis including adoption, conversion, and retention what you'll need to succeed • master's or phd in computer science, machine learning, or related field • 7+ years of experience in machine learning engineering, with a focus on recommendation systems or personalization • strong expertise in deep learning frameworks (pytorch or tensorflow) and building production-grade ml systems • proven experience with gcp services and ml infrastructure at scale • proficiency in python, sql, and cloud-native development • experience with containerization (docker) and orchestration (kubernetes) • track record of deploying ml models to production at scale • experience with multimodal deep learning architectures and generative ai models • knowledge of modern recommendation system architectures (transformers, neural collaborative filtering) • expertise in building real-time inference systems what you'll get in return • competitive salary and benefits package • opportunities for professional growth and development • a collaborative and inclusive work environment • access to cutting-edge technologies and projects • the chance to work with industry leaders and innovators what you need to do now if you're interested in this role, click 'apply now' to forward an up-to-date copy of your cv, or call us now. if this job isn't quite right for you, but you are looking for a new position, please contact us for a confidential discussion on your career. #li-dni #1119198 - jamie dunne","toronto, on",Machine Learning Engineer,"['bigquery', 'cloud', 'deep learning', 'experimentation', 'gcp', 'machine learning', 'python', 'pytorch', 'r', 'recommendation', 'scala', 'sql', 'tensorflow']","['bigquery', 'cloud', 'deep learning', 'experimentation', 'gcp', 'machine learning', 'python', 'pytorch', 'r', 'recommendation', 'scala', 'sql', 'tensorflow']",
machine learning engineer (remote | $80 – $120/hr),call for referral,"machine learning engineer hourly contract | part-time remote | $80 –$120 per hour 1. about the role at mercor, we’re building the talent engine that helps leading labs and research organizations move ai forward. our newest initiative focuses on benchmarking and improving model performance and training speed across real machine learning workloads. if you’re an early-career machine learning engineer or an ml-focused graduate student/phd who values innovation, rigor, and meaningful impact, we’d love to meet you. this is a remote, asynchronous, part-time opportunity ideal for individuals who thrive on clear structure and measurable outcomes. 2. what to expect • schedule: remote and asynchronous — set your own working hours • commitment: ~20 hours per week • duration: through december 22nd, with potential extension into 2026 3. what you’ll do • draft detailed natural-language plans and code implementations for machine learning tasks • convert novel ml problems into agent-executable tasks for reinforcement learning environments • identify failure modes and apply golden patches to llm-generated trajectories for ml tasks 4. what you’ll bring • experience: • 0–2 years as a machine learning engineer or • graduate-level experience in computer science, ml, or related coursework • required skills: • strong python skills • familiarity with ml libraries: tensorflow, xgboost, scikit-learn, etc. • experience with model training, data preparation, and evaluation • bonus: • contributions to ml benchmarks or open-source ml tooling 5. compensation & terms • rate: $80–$120 per hour (based on experience and region) • payments: weekly via stripe connect • engagement: independent contractor 6. how to apply • submit your resume • complete the system design session (< 30 minutes) • fill out the machine learning engineer screen (< 5 minutes) ⚡ ps: mercor reviews applications daily. please complete your interview and onboarding steps to be considered for this opportunity. ⚡","vancouver, bc",Machine Learning Engineer,"['machine learning', 'python', 'r', 'scikit-learn', 'tensorflow', 'xgboost']","['machine learning', 'python', 'r', 'scikit-learn', 'tensorflow', 'xgboost']",US$80–US$120 an hour
"staff machine learning engineer, content mining",pinterest,"about pinterest: millions of people around the world come to our platform to find creative ideas, dream about new possibilities and plan for memories that will last a lifetime. at pinterest, we’re on a mission to bring everyone the inspiration to create a life they love, and that starts with the people behind the product. discover a career where you ignite innovation for millions, transform passion into growth opportunities, celebrate each other’s unique experiences and embrace the flexibility to do your best work. creating a career you love? it’s possible. content mining identifies the best sources to acquire content for pinterest (websites, merchants, social accounts), optimizes how we acquire it, and extracts structured attributes from that content at high scale. our work powers inspiring, accurate, and engaging pins. this is a high performing end to end ml team, with a recent paper in kdd: “cross-domain web information extraction at pinterest”. we’re hiring a staff ml engineer to serve as the technical lead for a 5‑engineer team (4 mles + you). as a tech lead, you will define the multi‑quarter technical vision and roadmap, lead execution and mentor engineers. the majority of your time will be spent with hands‑on designing, training, and shipping ml systems—especially llm/nlp models for extraction and what you’ll do: • technical leadership • own the long‑term architecture, roadmap, and execution for source discovery, acquisition optimization, and content understanding. • lead design reviews, set engineering standards, and drive cross‑team alignment with product, data, and infra. • mentor and uplevel mles through technical direction, pairing, and reviews. • modeling and systems • train/fine‑tune llms and nlp models for classification, extraction, and instruction‑following; design eval loops and guardrails. • design features and frameworks for sharing features across models. • productionize models for large-scale inference; drive latency, reliability, and cost efficiency (quantization, distillation, caching). • data, experimentation, and quality • establish offline/online evaluation, gold sets, and automated regressions; run a/b and canary/shadow launches. • work with human and automated labeling sources to define data labeling standards. • partner on data strategy, labeling/weak supervision, and feedback loops to expand coverage and improve precision/recall. • operational excellence • define and meet slos for data quality, model performance, and serving reliability; lead incident playbooks and postmortems. • measure and drive downstream impact on revenue and engagement. what we're looking for: • 5+ years building ml products end‑to‑end, including 2+ years as a tech lead driving multi‑quarter roadmaps and cross‑functional execution. • deep hands‑on experience with nlp/llm training and inference (pytorch, python); strong grounding in evaluation, prompt/data design, and fine‑tuning. • proven track record shipping models at scale: feature/data pipelines, online serving, monitoring/observability, and cost/perf trade‑offs. • strong software engineering in python with an eye for software engineering best practices. • experience mentoring senior engineers and influencing partner teams. • masters or phd in ml related studies. • llm efficiency techniques (lora/adapters, distillation, quantization, prompt caching) and cost control strategies. • mlops at scale with tools like airflow, spark/presto, triton, vllm. relocation statement: • this position is not eligible for relocation assistance. visit our pinflex page to learn more about our working model. in-office requirement statement: • we let the type of work you do guide the collaboration style. that means we're not always working in an office, but we continue to gather for key moments of collaboration and connection. • this role will need to be in the office for in-person collaboration 1-2 times/quarter and therefore can be situated anywhere in the province of ontario. #li-remote #li-ak7 our commitment to inclusion: pinterest is an equal opportunity employer and makes employment decisions on the basis of merit. we want to have the best qualified people in every job. all qualified applicants will receive consideration for employment without regard to race, color, ancestry, national origin, religion or religious creed, sex (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender, gender identity, gender expression, age, marital status, status as a protected veteran, physical or mental disability, medical condition, genetic information or characteristics (or those of a family member) or any other consideration made unlawful by applicable federal, state or local laws. we also consider qualified applicants regardless of criminal histories, consistent with legal requirements. if you require a medical or religious accommodation during the job application process, please complete this form for support.",canada (+1 other),Machine Learning Engineer,"['airflow', 'aws', 'classification', 'data pipeline', 'excel', 'experimentation', 'nlp', 'python', 'pytorch', 'r', 'regression', 'spark']","['airflow', 'aws', 'classification', 'data pipeline', 'excel', 'experimentation', 'nlp', 'python', 'pytorch', 'r', 'regression', 'spark']",
machine learning engineer | creative ads optimization,lumalabs.ai,"the opportunity luma ai is a full-stack ai lab building multimodal agi. to truly understand the world, models must learn from audio, video, and images. we train our own foundational models and build the products that utilize them. you will join a confidential, founding engineering team tasked with building a new category of creative tools from the ground up. we combine the massive compute resources and capital of a major tech company with the agility and individual impact of a focused startup. where you come in you will operate with high agency as a founding engineer, defining the technical architecture for a new generative product focused on ad creation. this is a role for a builder who thrives in ambiguity and prefers inventing solutions over maintaining existing platforms. you will serve as the bridge between our research team and the product, translating raw model capabilities into a powerful engine that optimizes creative work. what you will build the optimization engine : architect the core ml systems that translate the nuances of ad effectiveness into quantitative signals, guiding our ai models to generate high-performance content. novel capabilities : collaborate with researchers to apply state-of-the-art techniques, including efficient finetuning of vision-language models, to solve specific creative challenges. 0-to-1 architecture : design and build the initial backend infrastructure that powers our creative platform, ensuring it can scale from a prototype to a global product. the profile we are looking for founding mindset : you have a history of creating, launching, and building new systems from scratch and thrive in environments where you define the spec as you build. domain expertise : you possess a deep, quantitative understanding of the ads ecosystem, specifically regarding creative optimization, personalization, or relevance. technical fluency : you have expert-level command of python and hands-on experience finetuning or training generative models. #j-18808-ljbffr","toronto, on (+1 other)",Machine Learning Engineer,"['python', 'r']","['python', 'r']",
senior machine learning engineer - growth,doordash canada,"about the team come help us build the world's most reliable on-demand logistics engine for last-mile delivery! we're seeking an experienced machine learning engineer to develop cutting-edge ml models that power doordash's pricing, selection, affordability, and growth strategies. about the role we're looking for a passionate machine learning expert to join our team. you'll leverage our robust data and machine learning infrastructure to build sophisticated ml systems that implement innovative ai solutions for consumer pricing, restaurant selection expansion, promotions (deals) optimization, and marketplace growth. you'll need to demonstrate strong production-level machine learning expertise while collaborating with multi-disciplinary teams to set strategy and drive business growth. you're excited about this opportunity because you will: • develop ml-driven systems that ensure doordash pricing remains affordable and fair across all restaurant selections and occasions • create ai solutions that optimize deals and promotions for relevancy and efficiency, intelligently adapting to evolving consumer ordering behaviors • partner with engineering and product leaders to shape product roadmaps through ai/ml innovations • own the complete modeling lifecycle from feature creation through deployment, experimentation, monitoring, explainability, and maintenance we're excited about you because you have: • 6+ years of industry experience developing and leading advanced machine learning initiatives with measurable business impact and production-deployed solutions • m.s. or ph.d. in statistics, computer science, mathematics, operations research, physics, economics, or another quantitative field • exceptional written and verbal communication skills • strong technical expertise in python, spark, and pytorch for machine learning applications • experience with statistics, causal inference, multi-objective optimization, or deep learning (preferred) notice to applicants for jobs located in nyc or remote jobs associated with office in nyc only we use covey as part of our hiring and/or promotional process for jobs in nyc and certain features may qualify it as an aedt in nyc. as part of the hiring and/or promotion process, we provide covey with job requirements and candidate submitted applications. we began using covey scout for inbound from august 21, 2023, through december 21, 2023, and resumed using covey scout for inbound again on june 29, 2024. the covey tool has been reviewed by an independent auditor. results of the audit may be viewed here: covey about doordash at doordash, our mission to empower local economies shapes how our team members move quickly, learn, and reiterate in order to make impactful decisions that display empathy for our range of users—from dashers to merchant partners to consumers. we are a technology and logistics company that started with door-to-door delivery, and we are looking for team members who can help us go from a company that is known for delivering food to a company that people turn to for any and all goods. doordash is growing rapidly and changing constantly, which gives our team members the opportunity to share their unique perspectives, solve new challenges, and own their careers. we're committed to supporting employees' happiness, healthiness, and overall well-being by providing comprehensive benefits and perks including premium healthcare, wellness expense reimbursement, paid parental leave and more. our commitment to diversity and inclusion we're committed to growing and empowering a more inclusive community within our company, industry, and cities. that's why we hire and cultivate diverse teams of people from all backgrounds, experiences, and perspectives. we believe that true innovation happens when everyone has room at the table and the tools, resources, and opportunity to excel. if you need any accommodations, please inform your recruiting contact upon initial connection.","toronto, on",Machine Learning Engineer,"['deep learning', 'excel', 'experimentation', 'machine learning', 'python', 'pytorch', 'r', 'spark', 'statistics']","['deep learning', 'excel', 'experimentation', 'machine learning', 'python', 'pytorch', 'r', 'spark', 'statistics']",$120K–$150K a year
co-op engineer - machine learning,"huawei technologies canada co., ltd.","huawei canada has an immediate co-op opening for an engineer. about the team: the digital trust lab is dedicated to ensuring user data flows while maintaining privacy. researchers focus on key areas such as user identity authentication, data integrity, privacy protection, extensive model privacy assessment, multi-modal data identification, differential privacy, and federated learning. the lab supports deep research and encourages publications in leading journals. research outcomes are applied across various huawei product lines, including mobile phones, smart devices, and communications technologies. about the job: • develop a program on a mobile device or desktop device. • understand the machine learning mechanism, and master the model training and inference methods of machine learning. • efficient and timely communication/collaboration with other researchers from understanding a proposed method to its code implementation. about the ideal candidate: • currently enrolled in a university and registered with the school’s co-op program. • excellent programming skills in c/c++ and python (pytorch). • familiarity with linux, windows, and github. • understanding the concept of opencv, dsp and graphics pipeline. • strong knowledge of linux system fundamentals.","waterloo, on",Machine Learning Engineer,"['c++', 'excel', 'machine learning', 'python', 'pytorch', 'r']","['c++', 'excel', 'machine learning', 'python', 'pytorch', 'r']",
senior ai/machine learning engineer,okta,"we free everyone to safely use any technology, anywhere, on any device or app. our flexible and neutral products, okta platform and auth0 platform, provide secure access, authentication, and automation, placing identity at the core of business security and growth. we’re building a world where identity belongs to you. the intelligence accelerator team in the data platform group is the engine behind okta’s ai/ml evolution. we are responsible for building the foundational ai/ml services and systems that fast-track ai/ml for okta and deliver differentiated value to our users. working hand-in-glove with the data platform team, data scientists, product managers, and the sres, you will bridge the gap between data, research and platform offerings, playing a critical role in enabling the technical adoption of ml and generative ai across the company. we are assembling an elite unit designed to be fast, creative, and flexible. we expect great things from our engineers and reward them with stimulating challenges, novel technologies, and the chance to hold equity. join the intelligence accelerator and help us change the cloud security landscape forever. as a senior machine learning engineer within the intelligence accelerator team, you will contribute to the development of our next-generation ai/ml platform. you will work with the team to build the ""paved road"" that empowers okta’s applied ai teams to rapidly build and deploy intelligent features—enabling leveraging classical models, llms and autonomous agents. you will join a group that prioritizes engineering rigor—designing for scale, rigorous code reviews, automated testing, and ci/cd for ml (mlops). contribute to the foundation: help design and maintain scalable infrastructure for core machine learning lifecycles, including distributed training clusters, real-time inference serving, and high-performance feature stores. implement agentic infrastructure: work on the bleeding-edge stack for agentic ai, helping to implement standards like the model context protocol (mcp) to securely connect llms with internal tools, data, and apis. enable applied ai: build internal developer tooling that allow our product engineering teams to leverage generative ai workflows, rag pipelines, and vector databases without worrying about the underlying infrastructure. this is your opportunity to build the systems that power okta’s future, working with emerging tech stacks in a fast, flexible, and elite environment architect & deploy ml infrastructure: design, build, and maintain robust, scalable ml infrastructure using modern tools (airflow, mlflow, feature stores, vector databases) while implementing automated ci/cd workflows for model training, validation, and deployment. evaluate and adopt new ml technologies while ensuring data security, privacy, and compliance within the ml infrastructure build scalable data pipelines: design and maintain pipelines to ingest, process, and transform data from various sources, data warehouses, ensuring data quality, security, and compliance of data feeding into ml models and offline model artifact deployment deploy and monitor ml systems with data scientists and engineers, troubleshoot infrastructure issues, and optimize their performance collaborate & evangelize: work with cross-functional teams to align ai/ml platform strategies with company objectives while acting as an ai/ml evangelist and provide mentorship to analysts and stakeholders. engineering experience: 5+ years of software engineering experience with a strong focus on backend systems, distributed computing, or cloud infrastructure (aws/gcp/azure). ml infrastructure: experience building and scaling ml platforms using technologies like kubernetes, ray, kubeflow, or similar orchestration tools. you have a track record of implementing ci/cd pipelines, automated testing, and observability for machine learning workloads. langchain, vector databases, rag patterns) and an interest in emerging standards like model context protocol (mcp) and agentic workflows. language proficiency: expert-level proficiency in python and familiarity with go, java, or c++. experience working directly with data scientists or applied ai teams to translate research needs into robust production systems experience with java and/or java ee web applications. exposure to prompt engineering, llm ecosystems (hugging face, langchain, vector databases), and identity and access management or security focused ml applications. • *in addition, okta offers equity (where applicable), bonus, and benefits, including health, dental and vision insurance, 401(k), flexible spending account, and paid leave (including pto and parental leave) in accordance with our applicable plans and policies. the annual base salary range for this position for candidates located in the san francisco bay area is between: $130,000—$194,000 cad what you can look forward to as a full-time okta employee! making social impact developing talent and fostering connection + community at okta each organization is unique in the degree of flexibility and mobility in which they work so that all employees are enabled to be their most creative and successful versions of themselves, regardless of where they live. some roles may require travel to one of our office locations for in-person onboarding. all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, ancestry, marital status, age, physical or mental disability, or status as a protected veteran. okta is committed to complying with applicable data privacy and security laws and regulations.","toronto, on",Machine Learning Engineer,"['airflow', 'aws', 'azure', 'c++', 'cloud', 'data pipeline', 'data warehouse', 'gcp', 'java', 'machine learning', 'python', 'r', 'scala']","['airflow', 'aws', 'azure', 'c++', 'cloud', 'data pipeline', 'data warehouse', 'gcp', 'java', 'machine learning', 'python', 'r', 'scala']",$130K–$194K a year
machine learning engineer,align technology,"machine learning engineer department: information technology employment type: toàn thời gian location: canada-chq-ontario-toronto description at align technology, we believe a great smile can transform a person’s life, so we create technology that gives people the confidence to take on whatever’s next. we revolutionized the orthodontic industry with the introduction of the invisalign system, and we have never lost sight of that spirit of innovation. our diverse and collaborative teams are constantly pushing the boundaries of what’s possible. we are looking for a ml engineer to join a team responsible for integrating expert knowledge, statistical and machine learning models in the treatment plans. you will work in close collaboration with colleagues from a larger treatment planning group and with clinical experts, focusing on machine learning solutions for orthodontic problems. as a ml engineer, you will design, implement, and train models, and implement them in robust and reliable production solutions. role expectations • work in an agile team of developers, data engineers and sdets, and collaborate with other development teams • interact closely with clinical experts, project managers, and product owners to understand problems, analyze specifications, formalize functional requirements, and prepare and deliver technical presentations and demos. • perform statistical analysis and build machine learning models, perform feature engineering. • deliver models to production, monitor their quality, provide further training and incremental improvements. • write technical documentation for newly implemented functionality, contribute to verification and validation activities. • follow architectural, quality, and regulatory guidelines for medical device software development, demonstrate technical leadership, assist and mentor other developers, and uphold high standards of trust, responsibility, and professionalism. what we're looking for • bachelor or above degree in computer science, applied mathematics or relevant engineering disciplines. • 5+ years of professional experience in machine learning, data science or data analytics roles. • practical experience implementing ml solutions in production cloud environments. • excellent knowledge of python, including numpy, pandas, sklearn libraries. • excellent knowledge of sql. • basic familiarity with c++ or c#. • knowledge of the theoretical foundations of statistical data analysis and hypothesis testing apparatus. nice to haves... • experience working in highly regulated industries (such as banking, financial, healthcare, medical devices, aerospace, etc.). • familiarity with c++ or c#. • experience with aws cloud stack. • experience with distributed systems, spark, sagemaker, keras. • experience in cad/cam or 3d domain. • experience with power bi / tableau. • working knowledge of git and atlassian stack (jira, bitbucket, confluence, etc.). pay transparency if provided, base salary or wage rate ranges are the range in which align reasonably expects to set a candidate’s pay for the posted position. actual placement depends on the individual skills and experience level of a candidate plus the total compensation and equity across team members. for other locations outside of the primary location, the base salary range will be adjusted geographically. for field sales roles, the salary listed is the base pay only and does not include the applicable incentive compensation plan. a cost of living adjustment may be added to base pay for higher cost areas in the u.s. our internship hourly rates are a standard pay determined based on the position and your location, year in school, degree, and experience.","toronto, on",Machine Learning Engineer,"['aws', 'c#', 'c++', 'cloud', 'data analysis', 'data analytics', 'excel', 'feature engineering', 'keras', 'machine learning', 'numpy', 'pandas', 'power bi', 'python', 'r', 'sklearn', 'spark', 'sql', 'tableau']","['aws', 'c#', 'c++', 'cloud', 'data analysis', 'data analytics', 'excel', 'feature engineering', 'keras', 'machine learning', 'numpy', 'pandas', 'power bi', 'python', 'r', 'sklearn', 'spark', 'sql', 'tableau']",
machine learning engineer,pronavigator,"who is pronavigator? pronavigator helps insurance organizations centralize, manage, and deliver on-demand information to sales, service, underwriting, and brokers/agents. in 2019, we pioneered the first knowledge management platform for insurance. now, more than 10,000 insurance professionals use pronavigator to get quick, accurate answers. we continue to push the boundaries of what’s possible by integrating large language models that are accurate and secure. at pronavigator, we’re tackling and solving unique challenges. every day is an exciting new opportunity to collaborate with colleagues as we build a leading-edge platform and support our clients to do their best work. our culture is built on the values of hard work, creativity, collaboration and high performance. we’re looking for new team members who will try new things, share their opinion, move fast, think creatively and have fun. what’s this role all about? join our engineering team as they build the next generation of knowledge management products, powered by large language models. in this role you will play a leadership role in the design and development of ai software solutions and features, with a focus on researching, evaluating, training and tuning models to meet our product needs. this role is for you if you are a creative and scrappy problem solver who is looking to translate your technical depth in ml to provide technical direction and execution on language-based ai systems. our current technology stack includes javascript, react, python, fastapi, docker, aws and mysql, however we bring technologies in and out based on newly released tools, changes to requirements or solutions that more elegantly meet our objectives. your accountabilities in this role will include: • delivering ai/ml models from initial planning to data sourcing and preparation, building and training models, and deploying models to production in our cloud environment, ideally using and deploying open source models • creating ai-based prototypes to evaluate model and design options, working with internal stakeholders to analyze quality and relevance of results • training and tuning models to optimize outcomes of algorithms • integrating models to production, working with engineering team members as needed to integrate models into product architecture • working with internal stakeholders to understand product needs of ai and to scope potential solutions • conducting research into relevant ai models and technologies and staying well-connected to trends and developments in this rapidly evolving space this is a remote role that can be based from one of the following provinces in canada: british columbia, alberta, saskatchewan, manitoba, ontario, newfoundland, pei, new brunswick, nova scotia. what do you need to be successful? the experiences, skills and attributes we think are key to your success are listed below. if you’re excited about this role but don’t meet 100% of these qualifications, we still encourage you to apply – we know there is no match for hunger, curiosity and an ability to figure things out in an evolving environment. • 5+ years of software development experience with at least 3 years of experience implementing ml and ai technologies • deep knowledge of data modeling, and current ml and ai technologies with experience working with llms in particular to develop language-based solutions • highly analytical with the ability to think outside of the box, consider potential solutions from many different sources and make decisions with rapidly evolving or ambiguous data points • creative and scrappy problem solver who is excited about the prospect of creating greenfield solutions using new and evolving technologies • strong written and verbal communication skills with the ability to present complex ideas • highly collaborative with the ability to work cross-functionally to understand needs and execute against ideas • thrives in a startup environment where speed, agility and high performance are table stakes if you need accommodations during the hiring process, let us know at any time and we’ll make sure you’re set up for success. what’s in it for you? we’re looking for the best of the best to join our team and in return, we invest in the things that are important to our team members including: • 4 weeks to give you the time away from work you need to recharge and deliver at 100% • flexible time off policies to support you when things come up outside of work • remote-first work environment that gives you the flexibility to work wherever you’re most productive • annual professional development budget so you can pursue growth in the areas related to your career goals • competitive total compensation package across salary, variable compensation and benefits","kitchener, on",Machine Learning Engineer,"['aws', 'cloud', 'java', 'python', 'r', 'sas', 'sql']","['aws', 'cloud', 'java', 'python', 'r', 'sas', 'sql']",
senior machine learning engineer ii - applied ai,instacart,"we're transforming the grocery industry at instacart, we invite the world to share love through food because we believe everyone should have access to the food they love and more time to enjoy it together. where others see a simple need for grocery delivery, we see exciting complexity and endless opportunity to serve the varied needs of our community. we work to deliver an essential service that customers rely on to get their groceries and household goods, while also offering safe and flexible earnings opportunities to instacart personal shoppers. instacart has become a lifeline for millions of people, and we’re building the team to help push our shopping cart forward. if you’re ready to do the best work of your life, come join our table. instacart is a flex first team there’s no one-size fits all approach to how we do our best work. our employees have the flexibility to choose where they do their best work—whether it’s from home, an office, or your favorite coffee shop—while staying connected and building community through regular in-person events. learn more about our flexible approach to where we work. overview about the team -ai special projects is a newly formed, agile team driving the next wave of innovation at instacart. the team’s charter is to reimagine our core consumer experience with ai. we are taking on some of the company’s biggest growth bets across generative recommendation systems, hyper-personalized marketing content, and unique customer experiences such as personalized meals. further, we are paving the way for ai-native development across the company, driving event-driven architectures and other paradigms forward that will impact a wide range of applications. we build with a modern, ai-native python stack with minimal legacy tech debt. this allows us to move quickly, experiment, and ruthlessly prioritize the highest-impact opportunities. about the role - as a technical lead on the team, you'll play a pivotal role in shaping the future of our platform offerings. this role is your opportunity to elevate ai at instacart by applying the most advanced industry architectures, innovating upon them, and deploying them to directly improve our product for our customers and retailers. you will arrive from day 1 as a leader on the team. this means substantial scope and team influence through your product mindset, technical vision, strong execution, and mentorship. we are fueled by a culture of bottom‑up innovation and deep ownership – if that resonates, we’re excited to meet you! about the job in this role, you will: • build the team vision: partner with cross‑functional leaders to define key business challenges and design ai‑first solutions that unlock value for customers and stakeholders. • drive hands‑on execution: lead the development of personalized content and recommendation systems. this includes both generative and traditional ml architectures, leveraging the best tool for the job to drive core business outcomes. • deploy ai systems at scale: drive end‑to‑end software development, building ml services and agentic workflows, managing experiment lifecycles, and deploying to production at scale. • innovate: stay at the forefront of ai innovation by conducting literature reviews, exploring novel paradigms in recommendation systems (recsys), and identifying where they can be applied and improved upon to meet instacart’s unique needs. • elevate the team: mentor and guide junior team members, fostering engagement, learning, and technical growth within the team. about you minimum qualifications expertise in generative ai technologies, including prompt engineering, model fine‑tuning, aiqa processes, and familiarity with emerging trends. strong background in traditional machine learning and deep learning fundamentals, with hands‑on experience building performant models in frameworks like pytorch or tensorflow. professional software development skills, including experience building data pipelines in sql/spark, managing ci/cd workflows, and deploying models to production. product‑minded, with a bias for action and proven ability to define and solve complex business problems with cross‑functional collaboration. exceptional communication skills, with the ability to influence stakeholders and translate technical findings into actionable business insights. an innovation mindset — driven by curiosity and a desire to push the boundaries of what's possible in applied ai. preferred qualifications experience building scalable ranking and recommendation systems. application of ml models within an e‑commerce environment. cloud infrastructure experience (e.g. aws, gcp), particularly managing large gpu workloads for training and inference. track record of success as a technical leader, with proven ability to guide teams toward strategic, high‑impact solutions. demonstrated technical influence through contributions to blogs or published papers. instacart provides highly market‑competitive compensation and benefits in each location where our employees work. this role is remote and the base pay range for a successful candidate is dependent on their permanent work location. please review our flex first remote work policy here. currently, we are only hiring in the following provinces: ontario, alberta, british columbia, and nova scotia. offers may vary based on many factors, such as candidate experience and skills required for the role. additionally, this role is eligible for a new hire equity grant as well as annual refresh grants. please read more about our benefits offerings here. for canadian based candidates, the base pay ranges for a successful candidate are listed below. can $203,000 — $225,000 cad #j-18808-ljbffr","montreal, quebec",Machine Learning Engineer,"['aws', 'cloud', 'data pipeline', 'deep learning', 'gcp', 'machine learning', 'python', 'pytorch', 'r', 'recommendation', 'scala', 'spark', 'sql', 'tensorflow']","['aws', 'cloud', 'data pipeline', 'deep learning', 'gcp', 'machine learning', 'python', 'pytorch', 'r', 'recommendation', 'scala', 'spark', 'sql', 'tensorflow']",
machine learning software engineer,royal bank of canada,"job description what's the opportunity? we’re looking for an enthusiastic software developer who’s excited by the opportunity of working on challenging problems at the intersection of machine learning and the financial services industry. as a machine learning software engineer, you’ll be responsible for owning and delivering a project end to end – everything from data pre-processing and exploration, to building and scaling ml algorithms and pipelines, to deployment and monitoring of production systems. at rbc borealis, you’ll be joining a team that works directly with leading researchers in machine learning, has access to rich and massive datasets, and offers the computational resources to support cutting-edge machine learning r&d. your responsibilities include: • to build cutting edge ml solutions throughout the research and product development lifecycle; • to play a key role in the design and development of borealis’ machine learning products; • to partner with rbc borealis’s research and product teams to ensure the seamless delivery of these products; • to apply engineering and data best practices to build robust and scalable large-scale machine learning software systems; • to support projects with thorough documentation, design decisions, and technical advisory. you're our ideal candidate if you have: • experience building modular and robust software systems in python or similar language; • knowledge of professional software engineering best practices for the full software development life cycle, including testing methods, coding standards, code reviews and source control management; • experience working across the entire ml research and product lifecycle from prototyping to production is a plus; • experience building microservices, data pipelines and using relational and non-relational databases is a plus; • experience working with data science tooling and deep learning frameworks is a plus; • experience with devops engineering (ci/cd pipelines, observability, containers etc) is a plus. what's in it for you? • be part of a dynamic & flexible working environment; • become part of a team that thinks progressively and works collaboratively. we care about seeing each other reach full potential; • leaders who support your development through coaching and managing opportunities; • ability to make a difference and lasting impact from a local-to-global scale. about rbc borealis rbc borealis is the driving force behind royal bank of canada’s ai and data innovation. as part of canada’s largest financial institution, we bring together a team of architects, engineers, scientists, and product experts on a mission to revolutionize finance through world-class research, solutions, and a resilient data platform. with locations across toronto, waterloo, montreal, calgary, and vancouver, we’re at the forefront of ai research and platform development. with a focus on cutting-edge research in areas like time series forecasting, causal machine learning, and responsible ai, we are seamlessly integrating ai research and data engineering, to solve critical challenges in the financial industry. we are building intelligent, and scalable, data-driven solutions that will help communities thrive and drive innovation for our customers across the bank. inclusion and equal opportunity employment rbc is an equal opportunity employer committed to diversity and inclusion. we are pleased to consider all qualified applicants for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, protected veterans status, aboriginal/native american status or any other legally-protected factors. disability-related accommodations during the application process are available upon request. #ll-post job skills big data analytics, critical thinking, decision making, industry knowledge, machine learning, software engineering, software product design additional job details address: 401 georgia st w:vancouver city: vancouver country: canada work hours/week: 37.5 employment type: full time platform: technology and operations job type: regular pay type: salaried posted date: 2025-04-29 application deadline: 2026-01-26 note: applications will be accepted until 11:59 pm on the day prior to the application deadline date above inclusion and equal opportunity employment at rbc, we believe an inclusive workplace that has diverse perspectives is core to our continued growth as one of the largest and most successful banks in the world. maintaining a workplace where our employees feel supported to perform at their best, effectively collaborate, drive innovation, and grow professionally helps to bring our purpose to life and create value for our clients and communities. rbc strives to deliver this through policies and programs intended to foster a workplace based on respect, belonging and opportunity for all. join our talent community stay in-the-know about great career opportunities at rbc. sign up and get customized info on our latest jobs, career tips and recruitment events that matter to you. expand your limits and create a new future together at rbc. find out how we use our passion and drive to enhance the well-being of our clients and communities at jobs.rbc.com.","calgary, ab (+3 others)",Machine Learning Engineer,"['data analytics', 'data pipeline', 'deep learning', 'machine learning', 'python', 'r', 'scala', 'time series']","['data analytics', 'data pipeline', 'deep learning', 'machine learning', 'python', 'r', 'scala', 'time series']",
ai / machine learning engineer,thri5,"about thri5 thri5 is the ai-powered system of actions for the modern retailer. despite massive investments in planning, forecasting, and analytics, retailers still face the same operational issues—out-of-stocks, bad master data, margin leakage, and inconsistent execution across stores and channels. the gap isn’t in intelligence; it’s in execution. thri5 continually scans data across the business, detects and prioritizes opportunities, evaluates impact, and orchestrates execution through both humans and ai agents. from store managers and dc leaders to category and supply chain teams, thri5 routes the right actions to the right owners—with clear context, recommendations, and workflows—closing the gap between plan and real-world performance. our vision is to become the trusted ai operating layer for retail execution, making every operator 10x more effective and freeing them to focus on what matters most: serving customers and growing the business. founded by a team with deep retail and retail-technology experience, thri5 is venture-backed by some of canada’s most prominent vc and angel investors. your role as an ai / machine learning engineer at thri5, you’ll help build the agent layer that powers our system of actions. you’ll design and implement multi-agent co-pilot systems that orchestrate complex workflows, call tools and apis, and automate operational tasks at scale. you’ll also develop deterministic, data-driven detection models to reliably identify operational issues and opportunities—and then layer llm-based capabilities on top to generate high-quality alerts, recommended actions, and explanations grounded in real retail data. you’ll work closely with the founding team to turn messy, real-world retail problems into robust, production agent workflows that operators actually trust and use every day. key responsibilities agent framework & orchestration • design and build the core frameworks that power thri5’s ai agents: task decomposition, routing, tool calling, multi-step workflows, and human-in-the-loop escalation. • implement agents that coordinate across operators (store, dc, category, supply chain) and systems to drive real actions, not just insights. llm-driven intelligence • develop and fine-tune llm-based components to detect anomalies and opportunities that impact commercial and operational performance. • build prompt, retrieval, and grounding patterns that produce reliable behaviour in noisy, real-world data. • combine deterministic signals with llms to produce contextual narratives, explanations, and recommended actions. deterministic detection & scoring • design and implement deterministic and semi-deterministic detection models (e.g., statistical anomaly detection, rules + ml hybrids, scoring systems) to identify out-of-stocks, bad master data, and execution gaps. • build evaluation frameworks (precision/recall, false positive control, business impact, backtests) to ensure detections are trustworthy and stable in production. • collaborate with product and domain experts to translate heuristics and business rules into robust, maintainable detection logic. data & recommendation pipelines • build and optimize pipelines that leverage real-time and batch customer data (transactions, inventory, operations) to power agent decisions and recommendations. • own end-to-end ml workflows—data preprocessing, feature engineering, training, evaluation, and production inference. mlops / llmops & reliability • implement robust mlops practices for ci/cd, experimentation, and monitoring of models and agents. • instrument and monitor agent behaviour (latency, cost, quality, safety) and continuously iterate to improve performance, accuracy, and scalability. collaboration & product • partner with product and engineering to translate customer problems into concrete agent capabilities and use cases. • contribute to technical decision-making and architecture as we scale the thri5 platform. requirements • ai fluency: 5+ years of software development experience with deep exposure to modern ai/ml, including both classical ml / data science and llms, gpt-style models, and agent/tool-calling ecosystems. • ml / data science proficiency: strong background in supervised/unsupervised learning and anomaly detection, with hands-on experience designing deterministic or semi-deterministic detection systems (statistical models, rules + ml, scoring). comfortable with model evaluation, experimentation, and translating business heuristics into data-driven logic. • programming & frameworks: proficient in python and familiar with ml frameworks such as pytorch or tensorflow. experience with genai tooling (e.g., langchain, llamaindex, custom agent frameworks) and vector databases is an asset. • data handling: comfortable working with large-scale datasets, complex schemas, and event-driven data. strong sql skills and experience building data pipelines into production systems. • startup mindset: thrive in a fast-paced, ambiguous environment; able to bring structure to open-ended problems. enjoy high accountability and end-to-end ownership from idea to production impact. • teamwork: collaborative, low-ego, and comfortable working across a small, high-performing team (founders, engineers, product, and customers). • domain experience (nice to have): experience in retail, supply chain, predictive analytics, time-series modeling, or operational optimization. • education: bachelor’s, master’s or ph.d. in computer science, data science, machine learning, or a related field (or equivalent practical experience).","toronto, on",Machine Learning Engineer,"['data pipeline', 'experimentation', 'feature engineering', 'machine learning', 'python', 'pytorch', 'r', 'recommendation', 'scala', 'sql', 'tensorflow']","['data pipeline', 'experimentation', 'feature engineering', 'machine learning', 'python', 'pytorch', 'r', 'recommendation', 'scala', 'sql', 'tensorflow']",
machine learning engineer ii,loblaw,"come make your difference in communities across canada, where authenticity, trust and making connections is valued – as we shape the future of canadian retail, together. our unique position as one of the country's largest employers, coupled with our commitment to positively impact the lives of all canadians, provides our colleagues a range of opportunities and experiences to help canadians live life well®. at loblaw companies limited, we succeed through collaboration and commitment and set a high bar for ourselves and those around us. whether you are just starting your career, re-entering the workforce, or looking for a new job, this is where you belong. does working with some of canada’s most talented minds in innovation supporting retail, digital consumer solutions and analytical platforms excite you? loblaw technology powers some of canada’s most game-changing retail solutions, giving our customers the ability to live their lives well. come work with a team that values diverse ideas, fosters a culture of inclusion and develops our talent from within. loblaw technology gives you the chance to excel and helps you to strive for success in a big way. keep reading to learn more! machine learning engineer ii, brampton, on as a machine learning engineer ii, you will move beyond just building models; you will be required to design, build, and deploy the robust, end-to-end mlops pipelines that power our most critical initiatives, including our next-generation genai-driven loyalty program. ideally, you are an software engineer who has specialized in a production-focused ml role and is passionate about building scalable, high-quality systems, especially within the gcp ecosystem. what you’ll do: • own the design, development, deployment and support of complex ml components and solutions, contributing significantly to the full ml lifecycle. • design, build, and maintain end-to-end mlops pipelines to automate and streamline model deployment, versioning, advanced monitoring, and governance. • work closely with cross-functional teams (product, engineering, analytics) to translate business requirements into robust technical solutions and deliver high-impact ml projects, including genai initiatives. • uphold and champion best practices for data integrity, governance, security, and compliance in all ml systems. • proactively monitor the performance of production ml systems, implementing robust monitoring strategies and driving continuous optimization efforts. • contribute to code reviews and provide technical guidance to junior engineers, helping to raise the team's bar for code quality and software design. • participate actively in project delivery and technical planning, helping to scope, estimate, and break down complex tasks. what you bring: • bachelor’s or master’s degree in computer science, engineering, or a related field. • 3-5 years of hands-on experience in software engineering and/or machine learning engineering, with an emphasis on building and deploying production-grade ml systems. • expert-level proficiency in python and software engineering fundamentals (e.g., data structures, algorithms, design patterns, unit/integration testing). • proven experience in designing and building production-grade ml systems and data pipelines on cloud platforms, preferably gcp (vertex ai, gke, bigquery, etc.). • hands-on experience with ml frameworks (e.g., tensorflow, pytorch, scikit-learn) and hands-on experience with mlops tools (e.g., mlflow, kubeflow, vertex ai pipelines, airflow). • demonstrable experience building complex ci/cd pipelines (e.g., gitlab ci, jenkins, cloud build) for ml applications. · genai (bonus): experience with generative ai, including building rag pipelines, fine-tuning, and deploying llms (e.g., using langchain, llamaindex, or open-source models). · full-stack (bonus): experience with full-stack development (e.g., building rest apis with fastapi/flask, or frontend experience with react/vue) to create end-user-facing ml applications. what loblaw offers you we offer flexibility and balance, and an environment that sets you up for success no matter where your workspace is located. here, you will find a great team to help you achieve your goals as you help us achieve ours! work in our fast-paced, exciting technology environment, helping our stores, colleagues and customers every day. loblaw colleagues also enjoy: • work perks program • on-site goodlife fitness, basketball & volleyball courts, ice rink, groceries delivered to work via pc express, dry cleaning services (1pcc office) • tuition reimbursement & online learning • pension & benefits • paid vacation if you’re up to the challenge, then we would love to hear from you. apply today, and get the process started. loblaw recognizes canada's diversity as a source of national pride and strength. we have made it a priority to reflect our nation’s evolving diversity in the products we sell, the people we hire, and the culture we create in our organization. at loblaw, we celebrate diversity and strive to build a culture of inclusion where differences are embraced, valued and supported. we are committed to being an equal opportunity employer and encourage people from all backgrounds and identities to apply to our jobs. accommodation in the recruitment, assessment, and hiring process is available upon request for applicants with disabilities. we thank all candidates for their interest but please note, those candidates who meet the minimum requirements for the position will be contacted. www.loblaw.ca/careers our commitment to sustainability and social impact is an essential part of the way we do business, and we focus our attention on areas where we can have the greatest impact. our approach to sustainability and social impact is based on three pillars – environment, sourcing and community – and we are constantly looking for ways to demonstrate leadership in these important areas. our core values – care, ownership, respect and excellence – guide all our decision-making and come to life through our blue culture. we offer our colleagues progressive careers, comprehensive training, flexibility, and other competitive benefits – these are some of the many reasons why we are one of canada’s top employers, canada’s best diversity employers, canada’s greenest employers & canada’s top employers for young people. if you are unsure whether your experience matches every requirement above, we encourage you to apply anyway. we are looking for varied perspectives which include diverse experiences that we can add to our team. we have a long-standing focus on diversity, equity and inclusion because we know it will make our company a better place to work and shop. we are committed to creating accessible environments for our colleagues, candidates and customers. requests for accommodation due to a disability (which may be visible or invisible, temporary or permanent) can be made at any stage of application and employment. we encourage candidates to make their accommodation needs known so that we can provide equitable opportunities. please note: candidates who are 18 years or older are required to complete a criminal background check. details will be provided through the application process. #en #ss #ltna #on","brampton, on",Machine Learning Engineer,"['airflow', 'bigquery', 'cloud', 'data pipeline', 'excel', 'gcp', 'machine learning', 'python', 'pytorch', 'r', 'scala', 'scikit-learn', 'tensorflow']","['airflow', 'bigquery', 'cloud', 'data pipeline', 'excel', 'gcp', 'machine learning', 'python', 'pytorch', 'r', 'scala', 'scikit-learn', 'tensorflow']",
machine learning engineer,infoya inc.,"job description seeking a highly skilled and motivated ai/ml engineer with expertise in machine learning, statistics, and generative ai to join our team. the ideal candidate will have extensive experience in building and productionizing data science/genai use cases and a strong understanding of ml ops and cloud-based ml orchestration services. as an ai/ml engineer, you will be responsible for developing cutting-edge ai/ml solutions, particularly in the field of anomaly detection, natural language processing (nlp) and large language models (llms). key responsibilities: • develop and deploy machine learning models to solve real-world business challenges. • design and implement generative ai solutions with a focus on nlp and llms. • work on end-to-end ml workflows, including data preprocessing, model training, evaluation, and deployment. • leverage ml ops best practices to automate and optimize model deployment, monitoring, and retraining. • utilize cloud-based ml orchestration services (e.g., vertex ai, aws sagemaker, or azure ml) for scalable model deployment. • develop and analyze sql queries for data extraction, transformation, and aggregation. • stay updated with the latest advancements in generative ai, nlp, and data science. requirements • 5+ years of experience in developing and deploying machine learning models in production. • strong expertise in time series forecasting, generative ai, nlp, and llms. • proficiency in python and relevant ml/genai frameworks (e.g., tensorflow, langchain, llamaindex, adk). • hands-on experience with ml ops tools and cloud-based ml platforms (preferably on gcp). • strong background in developing deep learning and neural network-based models for various ai applications. • solid understanding of probability, statistics, and mathematical modeling. • experience with different types of ml modeling approaches, including supervised, unsupervised, and reinforcement learning techniques. • experience working with big data processing frameworks (apache beam, pyspark or similar) is a plus. • experience with scheduling tools (cloud composer or similar) is a plus • proactively solutioning the problems benefits • competitive salary and benefits package. • opportunity to work with a diverse and talented team. • professional development and growth opportunities. • a dynamic and collaborative work environment. • hybrid work options job types: full-time, permanent pay: $80,000.00-$100,000.00 per year benefits: • dental care • extended health care • paid time off work location: hybrid remote in oakville, on l6h 6x7","oakville, on",Machine Learning Engineer,"['aws', 'azure', 'cloud', 'deep learning', 'gcp', 'machine learning', 'natural language processing', 'nlp', 'pyspark', 'python', 'r', 'scala', 'spark', 'sql', 'statistics', 'tensorflow', 'time series']","['aws', 'azure', 'cloud', 'deep learning', 'gcp', 'machine learning', 'natural language processing', 'nlp', 'pyspark', 'python', 'r', 'scala', 'spark', 'sql', 'statistics', 'tensorflow', 'time series']",$80K–$100K a year
"principal/senior principal machine learning engineer, generative ai",autodesk canada co.,"• set the strategic technical vision for autodesk’s generative ai capabilities in the aec domain, influencing both short-term priorities and long-term investments • lead the design and development of intelligent data processing and characterization systems that transform unstructured inputs (e.g., text, images, geometry) into structured, ml-ready formats • architect and implement scalable, production-grade data and ml pipelines that support training and fine-tuning of models • drive strategic technical planning across the team—identifying bottlenecks, proposing long-term architectural improvements, and aligning data/ml infrastructure with product goals • collaborate closely with data engineers, applied scientists, and product teams to integrate large-scale data and related attributes into model development workflows • perform hands-on development of data preprocessing, feature extraction, and transformation modules optimized for downstream ml model performance • define and establish best practices for model experimentation, evaluation, and deployment in high-throughput environments • investigate and apply advanced techniques including self-supervised learning, active learning, and weak supervision to maximize the value of unlabeled data • own and evolve the model/data feedback loop by monitoring model quality, diagnosing failure modes, and guiding iterative improvements • mentor and support a team of ml engineers, fostering a culture of engineering excellence, curiosity, and technical ownership • stay current with advances in generative ai, foundation models, and data-centric ai—translating research into practical, scalable solutions minimum qualifications • a master's degree (or higher) in computer science, machine learning, artificial intelligence, mathematics, statistics or a related field • 10+ years of work experience in machine learning, data science, ai, or a related field with a proven track record of technical leadership and hands-on implementation • deep understanding of data modelling, system architectures, and processing techniques, including 2d/3d geometric data representations • expertise in deep learning architectures (e.g., transformers, cnns, gans) and modern ml frameworks (e.g., pytorch, lightning, ray) • experience with large models (llms and/or vlms) and related technologies, including frameworks, embedding models, vector databases, and retrieval-augmented generation (rag) systems, in production settings • experience with aws cloud services and sagemaker studio for scalable data processing and model development • strong foundation in computer science fundamentals, distributed computing, and algorithmic efficiency • proven ability to translate theoretical concepts into practical solutions and prototype implementations • ability to work autonomously while effectively collaborating across teams, bridging the gap between research and practical implementation • excellent technical writing and communication skills for documentation, presentations, and influencing cross-functional teams other qualifications • background in architecture, engineering, or construction • extensive experience in system design for data preparation, hyperparameter selection, acceleration techniques, and optimization methods • proficiency in parallel and distributed computing techniques, with hands-on experience using platforms like spark, ray, or similar distributed systems for large-scale data processing and model training • familiarity with responsible ai principles, including bias mitigation, explainability, and ethical ai practices the ideal candidate • is passionate about solving problems for aec customers (architecture, engineering, and construction) by applying machine learning techniques • is a strategic thinker, capable of shaping and executing long-term data-driven initiatives that align with business objectives • is comfortable working in newly forming ambiguous areas where learning, experimentation and adaptability are key skills • actively contributes to a learning-driven culture, sharing knowledge, mentoring peers, and fostering an environment of continuous growth • is bold and iterative, unafraid to share ideas, experiment, and fail fast learn more about autodesk welcome to autodesk! amazing things are created every day with our software – from the greenest buildings and cleanest cars to the smartest factories and biggest hit movies. we help innovators turn their ideas into reality, transforming not only how things are made, but what can be made. we take great pride in our culture here at autodesk – it’s at the core of everything we do. our culture guides the way we work and treat each other, informs how we connect with customers and partners, and defines how we show up in the world. when you’re an autodesker, you can do meaningful work that helps build a better world designed and made for all. ready to shape the world and your future? join us! salary transparency salary is one part of autodesk’s competitive compensation package. for u.s.-based roles, we expect a starting base salary between $166,600 and $269,500. offers are based on the candidate’s experience and geographic location, and may exceed this range. in addition to base salaries, our compensation package may include annual cash bonuses, commissions for sales roles, stock grants, and a comprehensive benefits package. salary is one part of autodesk’s competitive compensation package. for canada-bc based roles, we expect a starting base salary between $161,300 and $236,500. offers are based on the candidate’s experience and geographic location, and may exceed this range. in addition to base salaries, our compensation package may include annual cash bonuses, commissions for sales roles, stock grants, and a comprehensive benefits package. equal employment opportunity at autodesk, we're building a diverse workplace and an inclusive culture to give more people the chance to imagine, design, and make a better world. autodesk is proud to be an equal opportunity employer and considers all qualified applicants for employment without regard to race, color, religion, age, sex, sexual orientation, gender, gender identity, national origin, disability, veteran status or any other legally protected characteristic. we also consider for employment all qualified applicants regardless of criminal histories, consistent with applicable law. diversity & belonging we take pride in cultivating a culture of belonging where everyone can thrive. learn more here: https://www.autodesk.com/company/diversity-and-belonging are you an existing contractor or consultant with autodesk? please search for open jobs and apply internally (not on this external site).",british columbia,Machine Learning Engineer,"['aws', 'cloud', 'deep learning', 'excel', 'experimentation', 'machine learning', 'pytorch', 'r', 'scala', 'spark', 'statistics']","['aws', 'cloud', 'deep learning', 'excel', 'experimentation', 'machine learning', 'pytorch', 'r', 'scala', 'spark', 'statistics']",$161K–$236K a year
senior machine learning engineer,tundra technical - community,"job title: senior machine learning engineer location: surrey, bc (hybrid) estimated duration: 6 months job description: our client, a software developer in the medical industry, is looking for a senior machine learning engineer for their surrey bc location key responsibilities • architect, build, and maintain scalable and reliable data pipelines (etl/elt) to process and transform large-scale datasets for model training and analysis. • lead the design and implementation of production-grade mlops systems, including ci/cd/ct pipelines for automated data validation, model training, and deployment. • develop and manage data architectures, including databases, data warehouses, and data lakes, to ensure data quality, integrity, and accessibility. • optimize ml model inference for low latency and high throughput to meet the demands of real-time clinical use. • implement comprehensive monitoring for both data pipelines (data quality, freshness, lineage) and ml models (performance, concept/data drift). • manage the end-to-end data and ml infrastructure on cloud platforms using containerization (docker) and orchestration (kubernetes). • collaborate with data scientists to productionalize prototype models and with software engineers to integrate them into user-facing applications. • establish and enforce company-wide best practices and documentation for both data engineering and mlops. • mentor other engineers and data scientists on building efficient data solutions and robust ml systems. required skills and qualifications: • bachelor’s or master’s degree in computer science, engineering, or a related quantitative field. • 7+ years of professional experience in data engineering and/or machine learning engineering, with at least 4+ years of hands-on experience building production data pipelines and deploying ml models. • expert-level proficiency in python and a strong command of sql. • deep hands-on experience with big data processing frameworks like apache spark, hadoop, or dask. • strong experience with ml frameworks like tensorflow, pytorch, or scikit-learn. • proven experience with mlops tools and practices (e.g., mlflow, kubeflow, kubeflow pipelines). • expertise in cloud platforms (aws, gcp, or azure) and proficiency with containerization and orchestration technologies (docker, kubernetes). • in-depth knowledge of various database technologies, including sql (e.g., postgresql) and nosql (e.g., mongodb). • excellent problem-solving skills with a demonstrated ability to architect complex, end-to-end data and ml systems. • strong leadership, mentorship, and communication skills. preferred qualifications: • experience in the healthcare, life sciences, or medical imaging domain. • knowledge of real-time data streaming technologies (kafka, apache flink, or kinesis). • familiarity with infrastructure as code (iac) tools like terraform or cloudformation. • knowledge of data governance and data privacy standards (e.g., hipaa, gdpr).","surrey, bc",Machine Learning Engineer,"['aws', 'azure', 'cloud', 'data lake', 'data pipeline', 'data warehouse', 'elt', 'etl', 'excel', 'gcp', 'hadoop', 'kafka', 'machine learning', 'python', 'pytorch', 'r', 'scala', 'scikit-learn', 'spark', 'sql', 'tensorflow']","['aws', 'azure', 'cloud', 'data lake', 'data pipeline', 'data warehouse', 'elt', 'etl', 'excel', 'gcp', 'hadoop', 'kafka', 'machine learning', 'python', 'pytorch', 'r', 'scala', 'scikit-learn', 'spark', 'sql', 'tensorflow']",
"principal/senior principal machine learning engineer, generative ai",autodesk,"job requisition id # 25wd89883 position overview autodesk is leading the transformation of the aec industry, integrating ai technology into our products. we're enhancing our applications with cloud-native capabilities, including data at scale, edge computing, ai-based solutions, and advanced 3d modeling and graphics. this innovation is happening across our flagship products—autocad, revit, and construction cloud—and forma, our new industry cloud. as a principal machine learning engineer on the aec solutions team, you will join a team of technologists to help build cutting‑edge foundation models and generative ai tools for the aec industry. you will collaborate across organizations with a versatile group of ai researchers, ml engineers, software architects, and experience designers to generate and interpret design data that can augment design and engineering workflows. the ideal candidate is someone who isn’t afraid of technical complexity and is constantly looking for ways to leverage the latest and greatest advancements in the field of artificial intelligence and machine learning to deliver next‑generation experiences for our aec customers. report: you will report to an ml development manager for the generative ai team location: we support hybrid work or remote work in canada or united states. east coast preferred responsibilities • set the strategic technical vision for autodesk’s generative ai capabilities in the aec domain, influencing both short‑term priorities and long‑term investments • lead the design and development of intelligent data processing and characterization systems that transform unstructured inputs (e.g., text, images, geometry) into structured, ml‑ready formats • architect and implement scalable, production‑grade data and ml pipelines that support training and fine‑tuning of models • drive strategic technical planning across the team—identifying bottlenecks, proposing long‑term architectural improvements, and aligning data/ml infrastructure with product goals • collaborate closely with data engineers, applied scientists, and product teams to integrate large‑scale data and related attributes into model development workflows • perform hands‑on development of data preprocessing, feature extraction, and transformation modules optimized for downstream ml model performance • define and establish best practices for model experimentation, evaluation, and deployment in high‑throughput environments • investigate and apply advanced techniques including self‑supervised learning, active learning, and weak supervision to maximize the value of unlabeled data • own and evolve the model/data feedback loop by monitoring model quality, diagnosing failure modes, and guiding iterative improvements • mentor and support a team of ml engineers, fostering a culture of engineering excellence, curiosity, and technical ownership • stay current with advances in generative ai, foundation models, and data‑centric ai—translating research into practical, scalable solutions minimum qualifications • a master's degree (or higher) in computer science, machine learning, artificial intelligence, mathematics, statistics or a related field • 10+ years of work experience in machine learning, data science, ai, or a related field with a proven track record of technical leadership and hands‑on implementation • deep understanding of data modelling, system architectures, and processing techniques, including 2d/3d geometric data representations • expertise in deep learning architectures (e.g., transformers, cnns, gans) and modern ml frameworks (e.g., pytorch, lightning, ray) • experience with large models (llms and/or vlms) and related technologies, including frameworks, embedding models, vector databases, and retrieval‑augmented generation (rag) systems, in production settings • experience with aws cloud services and sagemaker studio for scalable data processing and model development • strong foundation in computer science fundamentals, distributed computing, and algorithmic efficiency • proven ability to translate theoretical concepts into practical solutions and prototype implementations • ability to work autonomously while effectively collaborating across teams, bridging the gap between research and practical implementation • excellent technical writing and communication skills for documentation, presentations, and influencing cross‑functional teams other qualifications • background in architecture, engineering, or construction • extensive experience in system design for data preparation, hyperparameter selection, acceleration techniques, and optimization methods • proficiency in parallel and distributed computing techniques, with hands‑on experience using platforms like spark, ray, or similar distributed systems for large‑scale data processing and model training • familiarity with responsible ai principles, including bias mitigation, explainability, and ethical ai practices the ideal candidate • is passionate about solving problems for aec customers (architecture, engineering, and construction) by applying machine learning techniques • is a strategic thinker, capable of shaping and executing long‑term data‑driven initiatives that align with business objectives • is comfortable working in newly forming ambiguous areas where learning, experimentation and adaptability are key skills • actively contributes to a learning‑driven culture, sharing knowledge, mentoring peers, and fostering an environment of continuous growth • is bold and iterative, unafraid to share ideas, experiment, and fail fast about autodesk welcome to autodesk! amazing things are created every day with our software – from the greenest buildings and cleanest cars to the smartest factories and biggest hit movies. we help innovators turn their ideas into reality, transforming not only how things are made, but what can be made. we take great pride in our culture here at autodesk – it’s at the core of everything we do. our culture guides the way we work and treat each other, informs how we connect with customers and partners, and defines how we show up in the world. when you’re an autodesker, you can do meaningful work that helps build a better world designed and made for all. ready to shape the world and your future? join us! salary transparency salary is one part of autodesk’s competitive compensation package. for u.s.-based roles, we expect a starting base salary between $166,600 and $269,500. offers are based on the candidate’s experience and geographic location, and may exceed this range. in addition to base salaries, our compensation package may include annual cash bonuses, commissions for sales roles, stock grants, and a comprehensive benefits package. for canada-bc based roles, we expect a starting base salary between $161,300 and $236,500. offers are based on the candidate’s experience and geographic location, and may exceed this range. in addition to base salaries, our compensation package may include annual cash bonuses, commissions for sales roles, stock grants, and a comprehensive benefits package. equal employment opportunity at autodesk, we’re building a diverse workplace and an inclusive culture to give more people the chance to imagine, design, and make a better world. autodesk is proud to be an equal opportunity employer and considers all qualified applicants for employment without regard to race, color, religion, age, sex, sexual orientation, gender, gender identity, national origin, disability, veteran status or any other legally protected characteristic. we also consider for employment all qualified applicants regardless of criminal histories, consistent with applicable law. diversity & belonging we take pride in cultivating a culture of belonging where everyone can thrive. learn more here: are you an existing contractor or consultant with autodesk? please search for open jobs and apply internally (not on this external site). #j-18808-ljbffr","montreal, quebec",Machine Learning Engineer,"['aws', 'cloud', 'deep learning', 'excel', 'experimentation', 'machine learning', 'pytorch', 'r', 'scala', 'spark', 'statistics']","['aws', 'cloud', 'deep learning', 'excel', 'experimentation', 'machine learning', 'pytorch', 'r', 'scala', 'spark', 'statistics']",
ml engineer – generative ai & llms (remote),ample insight inc.,"company description you will join a world-class team of engineers and data scientists from facebook, uber, amazon and google. we are a fast growing consulting firm based in toronto with clients ranging from leading startups building impactful technologies to fortune 500 companies looking to scale their engineering and data capabilities. job description we’re looking for machine learning engineers who are passionate about building cutting-edge systems with llms and real-world data. in this role, you’ll work closely with clients and teammates to design, prototype, and productize scalable machine learning solutions. you’ll be part of a collaborative, high-performing team that values clear thinking, pragmatic execution, and continuous learning. if you thrive in fast-paced environments, enjoy tackling open-ended problems, and care deeply about the quality and impact of your work, we’d love to connect. qualifications • must have: expertise in llm engineering, including familiarity with popular llm providers and their best practices. experience building or working with agentic systems (e.g. tool use, memory, planning, multi-agent coordination) is a strong plus. show us your llm projects and detail your ownership and contributions. • strong ability to rapidly prototype cutting-edge tools and research ideas, with a track record of turning prototypes into production-ready services. • hands-on experience with statistics and machine learning. comfortable working with the python ml stack: pandas, numpy, scikit-learn, xgboost, pytorch, etc. proficient with development tools such as git, docker, sql, bash, and fastapi. • strong analytical mindset and business acumen; able to think critically about data and its impact on product or business outcomes. • bachelor's degree or higher (e.g., ms or phd) in computer science or a related engineering field involving coding. • bonus: familiarity with aws or azure, github actions, spark, neo4j cypher, and graph databases. additional information we have competitive compensation. we believe in accountability and not micro-management.","toronto, on",Machine Learning Engineer,"['aws', 'azure', 'machine learning', 'numpy', 'pandas', 'python', 'pytorch', 'r', 'scala', 'scikit-learn', 'spark', 'sql', 'statistics', 'xgboost']","['aws', 'azure', 'machine learning', 'numpy', 'pandas', 'python', 'pytorch', 'r', 'scala', 'scikit-learn', 'spark', 'sql', 'statistics', 'xgboost']",
"machine learning engineer intern technical · toronto, canada",multiverse computing,"technical · toronto, canada machine learning engineer intern multiverse computing multiverse computing is collaborating with ictc’s wil digital program to provide exciting internship opportunities for post-secondary students. wil digital, funded by the government of canada’s student work placement program (swpp), supports employers with wage subsidies to hire students, while giving students valuable hands-on experience and access to exclusive e-learning courses in technology, business, and entrepreneurship. multiverse is a well-funded, fast-growing deep-tech company founded in 2019. we are the largest quantum software company in the eu and have been recognized by cb insights (2023 and 2025) as one of the 100 most promising ai companies in the world. with 180+ employees and growing, our team is fully multicultural and international. we deliver hyper-efficient software for companies seeking a competitive edge through quantum computing and artificial intelligence. our flagship products, compactifai and singularity, address critical needs across various industries: • compactifai is a groundbreaking compression tool for foundational ai models based on tensor networks. it enables the compression of large ai systems—such as language models—to make them significantly more efficient and portable. • singularity is a quantum- and quantum-inspired optimization platform used by blue-chip companies to solve complex problems in finance, energy, manufacturing, and beyond. it integrates seamlessly with existing systems and delivers immediate performance gains on classical and quantum hardware. you’ll be working alongside world-leading experts to develop solutions that tackle real-world challenges. we’re looking for passionate individuals eager to grow in an ethics-driven environment that values sustainability and diversity. we’re committed to building a truly inclusive culture—come and join us. as a machine learning intern, you will • work with fortune-500 customers from government and private sectors. • collaborate with the founding team in a fast paced startup environment. • tackle high value open problems from industry. • collaborate with quantum experts to design and implement new machine learning models. required qualifications • you are a canadian citizen, a permanent resident or a protected person as defined by the immigration and refugee protection act • you are registered or enrolled at a canadian post-secondary education institution and are able to provide proof of full-time or part-time enrolment during placement • you are legally entitled to work in canada • currently pursuing or recent graduate in masters or phd degree in computer science, statistics, operations research, or related field. • background in machine learning, both theoretical and practical. • knowledge of python, scikit, and tensorflow. • perfect command of english language perks & benefits • relocation package (if applicable, it will be included in the offer). • hybrid work policy with 3 days in office and 2 days home. • working in a high paced environment, working on cutting edge technologies. • career plan. opportunity to learn and teach. • progressive company. happy people culture. as an equal opportunity employer, multiverse computing is committed to building an inclusive workplace. the company welcomes people from all different backgrounds, including age, citizenship, ethnic and racial origins, gender identities, individuals with disabilities, marital status, religions and ideologies, and sexual orientations to apply. department technical role machine learning engineer locations toronto, canada employment type internship workplace type hybrid seniority level internship about multiverse computing come and join our multicultural team! 5 locations +27 languages founded in 2019 coworkers +150 technical · toronto, canada machine learning engineer intern already working at multiverse computing? let’s recruit together and find your next colleague.","toronto, on",Machine Learning Engineer,"['machine learning', 'python', 'r', 'statistics', 'tensorflow']","['machine learning', 'python', 'r', 'statistics', 'tensorflow']",
senior machine learning research engineer - acceleration of ai models,"huawei technologies canada co., ltd.","huawei canada has an immediate permanent opening for a senior research engineer. about the team: the computing data application acceleration lab aims to create a leading global data analytics platform organized into three specialized teams using innovative programming technologies. this team focuses on full-stack innovations, including software-hardware co-design and optimizing data efficiency at both the storage and runtime layers. this team also develops next-generation gpu architecture for gaming, cloud rendering, vr/ar, and metaverse applications. one of the goals of this lab are to enhance algorithm performance and training efficiency across industries, fostering long-term competitiveness. about the job: • track the trend of ai theory and technology development in the world and generate research report and proposals for promoting ascend system accordingly. • lead or participate in research of algorithms in accelerating the training of the market-driven ai models (cv/nlp/gnn/…), reaching/exceeding the state of the art accuracy, and develop a proof of concept of the algorithms. those algorithms include but are not limited to the following: optimizers, loss functions, new model architecture, mix precision, model compression, learning technologies (e.g., meta-learning), etc. • publish relevant high-quality ai research papers when necessary and approved, and attend conferences for increasing public awareness of huawei’s ascend products; file high-value patents on critical algorithms/processes that are of potential business gain. • team up with other departments/teams from huawei’s global research centers for collaboration. • assist the team lead on the planning of projects and definition of technology/products development road map. about the ideal candidate： • master or phd in computer science, math/statistics, focusing on ai & deep learning with solid publication records. • 2+ years working experience in optimizing performance of training deep learning models and/or their applications to cv/nlp/gnn domains. • solid skills in programming in tensorflow/keras/pytorch/mxnet. • hands-on skills in c++/python programming. • excellent documentation skills in writing internal reports and/or publishing research papers. • excellent communication skills in internal and external presentation. • working knowledge of ai accelerators or the full stack of ai acceleration system is an asset. • strong math background in optimization (e.g., gradient descending) is an asset. #li-sz2","markham, on",Machine Learning Engineer,"['c++', 'cloud', 'data analytics', 'deep learning', 'excel', 'keras', 'nlp', 'python', 'pytorch', 'r', 'statistics', 'tensorflow']","['c++', 'cloud', 'data analytics', 'deep learning', 'excel', 'keras', 'nlp', 'python', 'pytorch', 'r', 'statistics', 'tensorflow']",
sr. machine learning scientist & engineer,laivly,"about laivly seeking curious and creative types! we are an ambitious company of innovators building and shaping the future of customer service technology. our solutions help the world’s biggest brands leverage artificial intelligence, machine learning, and digital automation in their contact centers to deliver better customer experiences. led by a team of established contact center experts, laivly addresses the unique needs and challenges of customer service programs, with an emphasis on ethics in ai and the customer service agent experience. about the role the team at laivly is looking for a sr. machine learning scientist & engineer to lead our prompt optimization and validation efforts for llm-powered features. as a sr. machine learning scientist & engineer, you’ll work closely with engineering and product teams to design robust, accurate, and scalable experiments and monitoring systems that support safe and reliable deployment of ai features. you’ll own the end-to-end design and evaluation of prompts, build synthetic datasets, and develop pre- and post-release validation frameworks that measure performance, safety, and cost-effectiveness. this remote role is open to all canadian residents. as sr. ml scientist & engineer, you will… • lead prompt design, optimization, versioning, and documentation • generate and maintain synthetic datasets for pre-release validation and testing • design and run experiments to measure accuracy, robustness, and safety of llm behavior • define and monitor key metrics such as hallucination rates, latency, token cost, and regression risk • build evaluation pipelines, dashboards, and drift detection tools • implement rigorous statistical methods and produce reproducible reports with effect sizes and confidence intervals • investigate regressions or prompt failures in production and coordinate mitigation with stakeholders • collaborate with cross-functional teams on model integration, rate limiting, and cost controls • maintain prompt libraries, test suites, and operational runbooks as sr. ml scientist & engineer, you have… • bachelor’s degree in computer science, computational linguistics, statistics, or related field; master’s preferred • 2+ years working directly with llms, prompt design, or applied nlp in production • experience creating synthetic datasets using programmatic generation or controlled sampling • deep knowledge of experimental design, statistical inference, and power analysis • strong python programming skills and familiarity with statistical/ml frameworks • experience defining observability for model outputs and instrumenting alerting systems • familiarity with cloud environments (aws, azure), kubernetes, and production deployment practices • proficiency in sql and experience with reproducible analysis pipelines • excellent communication skills and strong documentation practices life at laivly laivly gives you the opportunity to collaborate and grow your career with a creative, diverse, and passionate team. we work hard and play often, with a flexible environment that works with you. a career at laivly means being part of a fun-loving, dedicated team of creatives, risk takers and game changers. it’s about sharing your talent and imagination to develop innovative tech that’s revolutionizing the way top brands interact with the world. we’ve got a shared mission—and a laivly future. join us today! laivly provides equal employment opportunities in accordance with all provincial and federal laws. laivly is committed to ensuring equality of opportunity in all aspects of employment and does not discriminate based on protected characteristics. laivly is committed to accommodating persons with disabilities. if you need accommodation at any stage of the application process or want more information on our accommodation policies, please let us know.","winnipeg, mb",Machine Learning Engineer,"['aws', 'azure', 'cloud', 'dashboard', 'excel', 'machine learning', 'nlp', 'python', 'r', 'regression', 'scala', 'sql', 'statistics']","['aws', 'azure', 'cloud', 'dashboard', 'excel', 'machine learning', 'nlp', 'python', 'r', 'regression', 'scala', 'sql', 'statistics']",
ai scientist / machine learning engineer,ml analytix,"responsibilities train and fine‑tune ai models (vision and language) for document layout understanding, ocr enhancement, visual data extraction, and captioning / alt‑text generation. optimize ai pipelines for real‑time or near‑real‑time performance, replacing high‑latency models with efficient alternatives. develop and integrate models trained on real‑world and synthetic datasets. create or annotate datasets for graph / table classification, layout segmentation, and text extraction. evaluate models for accuracy, generalization, and inference speed. collaborate with software engineers to deploy models in production environments. participate in the development of ai‑assisted user interfaces for manual correction, qa, and accessibility validation. maintain research rigor through documentation, testing, and performance benchmarks. requirements phd / msc in computer science, artificial intelligence, or related field. strong research and hands‑on experience in computer vision (opencv, object detection, layout parsing), nlp (captioning, text generation, alt‑text generation), and deep learning (pytorch, tensorflow). strong programming skills in python, with experience building end‑to‑end ai pipelines (data ingestion, preprocessing, training, inference, deployment). model optimization and performance tuning (e.g., quantization, pruning, batch inference). training and evaluating models on noisy, scanned, or complex real‑world documents. dataset creation / augmentation for domain‑specific tasks. familiarity with or willingness to learn tools like layoutparser, layoutlmv3, blip / llava, yolo, tesseract. experience with lightweight front‑end frameworks for ml interaction (e.g., streamlit). bonus skills knowledge of accessibility standards (e.g., wcag, en 301‑549). experience deploying ai in secure, high‑throughput production environments. background in ocr qa or document remediation tools. working knowledge of containerized api deployment (docker, fastapi, hugging face transformers). to apply a brief cover letter explaining your relevant experience in the domains mentioned above. (optional) links to publications, github, or portfolio projects. #j-18808-ljbffr","toronto, on",Machine Learning Engineer,"['classification', 'computer vision', 'deep learning', 'nlp', 'python', 'pytorch', 'r', 'tensorflow']","['classification', 'computer vision', 'deep learning', 'nlp', 'python', 'pytorch', 'r', 'tensorflow']",
senior machine learning engineer - discovery (ml + backend engineering),scribd,"about the company: at scribd inc. (pronounced “scribbed”), our mission is to spark human curiosity. join our team as we create a world of stories and knowledge, democratize the exchange of ideas and information, and empower collective expertise through our four products: everand, scribd, slideshare, and fable. we support a culture where our employees can be real and be bold; where we debate and commit as we embrace plot twists; and where every employee is empowered to take action as we prioritize the customer. when it comes to workplace structure, we believe in balancing individual flexibility and community connections. it’s through our flexible work benefit, scribd flex, that employees – in partnership with their manager – can choose the daily work-style that best suits their individual needs. a key tenet of scribd flex is our prioritization of intentional in-person moments to build collaboration, culture, and connection. for this reason, occasional in-person attendance is required for all scribd inc. employees, regardless of their location. so what are we looking for in new team members? well, we hire for “grit”. the textbook definition of grit is demonstrating the intersection of passion and perseverance towards long term goals. at scribd inc., we are inspired by the potential that this can unlock, and ask each of our employees to pursue a grit-ty approach to their work. in a tactical sense, grit is also a handy acronym that outlines the standards we hold ourselves and each other to. here’s what that means for you: we’re looking for someone who showcases the ability to set and achieve goals, achieve results within their job responsibilities, contribute innovative ideas and solutions, and positively influence the broader team through collaboration and attitude. about the recommendations team the recommendations team powers personalized discovery across scribd’s products, delivering relevant and engaging suggestions to millions of users. we operate at the intersection of large-scale data, cutting-edge machine learning, and product innovation — collaborating across brands and platforms to enhance user experiences in reading, listening, and learning. our team is a blend of frontend, backend, and ml engineers who partner closely with product managers, data scientists, and analysts. we: • prototype 0→1 solutions in collaboration with product and engineering teams. • build and maintain end-to-end, production-grade ml systems for recommendations, search, and generative ai features. • develop and operate services in go, python, and ruby that power high-traffic recommendation and personalization pipelines. • run large-scale a/b and multivariate experiments to validate models and feature improvements. • transform scribd’s massive, diverse dataset into actionable insights that drive measurable business impact. • explore and implement generative ai for conversational recommendations, document understanding, and advanced search capabilities. about the role we’re looking for a machine learning engineer who will design, build, and optimize ml systems that scale to millions of users. you’ll work across the entire lifecycle — from data ingestion to model training, deployment, and monitoring — with a focus on creating fast, reliable, and cost-efficient pipelines. you’ll also play a key role in delivering next-generation ai features like doc-chat and ask-ai that expand how users interact with scribd’s content. key responsibilities: • data pipelines – collaborate with engineering and analytics teams to build large-scale ingestion, transformation, and validation pipelines on databricks. • model development & deployment – train, evaluate, and deploy ml models (including generative models) to production using scribd’s internal platform and industry-standard frameworks. • experimentation – design and run a/b and n-way experiments to measure the impact of model and feature changes. • cross-functional collaboration – partner with product managers, data scientists, and analysts to identify opportunities, define requirements, and deliver solutions that solve real user problems. requirements must have • 4+ years of post qualification experience as a professional ml or software engineer, with a proven track record of delivering production ml systems at scale. • proficiency in at least one key programming language (preferably python or golang; scala or ruby also considered). • expertise in designing and architecting large-scale ml pipelines and distributed systems. • deep experience with distributed data processing frameworks (spark, databricks, or similar). • strong cloud expertise (aws, azure, or gcp) and experience with deployment platforms (ecs, eks, lambda). • proven ability to optimize system performance and make informed trade-offs in ml model and system design. • experience leading technical projects and mentoring engineers. • bachelor’s or master’s degree in computer science or equivalent professional experience. nice to have • experience with embedding-based retrieval, large language models, advanced recommendation or ranking systems. • expertise in experimentation design, causal inference, or ml evaluation methodologies. why work with us • high-impact environment: your contributions will power recommendations, search, and next-generation ai features used by millions of readers, learners, and listeners worldwide. • cutting-edge projects: tackle challenging ml and ai problems with a forward-thinking team, building novel generative features on top of scribd’s massive and unique dataset. • collaborative culture: join a culture that values debate, fresh perspectives, and a willingness to learn from each other. • flexible workplace: benefit from scribd flex, which offers autonomy in choosing your daily work style, while still prioritizing in-person collaboration. at scribd, your base pay is one part of your total compensation package and is determined within a range. our pay ranges are based on the local cost of labor benchmarks for each specific role, level, and geographic location. san francisco is our highest geographic market in the united states. in the state of california, the reasonably expected salary range is between $146,500 [minimum salary in our lowest geographic market within california] to $228,000 [maximum salary in our highest geographic market within california]. in the united states, outside of california, the reasonably expected salary range is between $120,000 [minimum salary in our lowest us geographic market outside of california] to $217,000 [maximum salary in our highest us geographic market outside of california]. in canada, the reasonably expected salary range is between $153,000 cad[minimum salary in our lowest geographic market] to $202,000 cad[maximum salary in our highest geographic market]. we carefully consider a wide range of factors when determining compensation, including but not limited to experience; job-related skill sets; relevant education or training; and other business and organizational needs. the salary range listed is for the level at which this job has been scoped. in the event that you are considered for a different level, a higher or lower pay range would apply. this position is also eligible for a competitive equity ownership, and a comprehensive and generous benefits package. working at scribd, inc. are you currently based in a location where scribd is able to employ you? employees must have their primary residence in or near one of the following cities. this includes surrounding metro areas or locations within a typical commuting distance: united states: atlanta | austin | boston | dallas | denver | chicago | houston | jacksonville | los angeles | miami | new york city | phoenix | portland | sacramento | salt lake city | san diego | san francisco | seattle | washington d.c. canada: ottawa | toronto | vancouver mexico: mexico city benefits, perks, and wellbeing at scribd • benefits/perks listed may vary depending on the nature of your employment with scribd and the geographical location where you work. • healthcare insurance coverage (medical/dental/vision): 100% paid for employees • 12 weeks paid parental leave • short-term/long-term disability plans • 401k/rsp matching • onboarding stipend for home office peripherals + accessories • learning & development allowance • learning & development programs • quarterly stipend for wellness, wifi, etc. • mental health support & resources • free subscription to the scribd inc. suite of products • referral bonuses • book benefit • sabbaticals • company-wide events • team engagement budgets • vacation & personal days • paid holidays (+ winter break) • flexible sick time • volunteer day • company-wide employee resource groups and programs that foster an inclusive and diverse workplace. • access to ai tools: we provide free access to best-in-class ai tools, empowering you to boost productivity, streamline workflows, and accelerate bold innovation. want to learn more about life at scribd? www.linkedin.com/company/scribd/life we want our interview process to be accessible to everyone. you can inform us of any reasonable adjustments we can make to better accommodate your needs by emailing accommodations@scribd.com about the need for adjustments at any point in the interview process. scribd is committed to equal employment opportunity regardless of race, color, religion, national origin, gender, sexual orientation, age, marital status, veteran status, disability status, or any other characteristic protected by law. we encourage people of all backgrounds to apply, and believe that a diversity of perspectives and experiences create a foundation for the best ideas. come join us in building something meaningful.","vancouver, bc (+22 others)",Machine Learning Engineer,"['aws', 'azure', 'cloud', 'data pipeline', 'databricks', 'experimentation', 'gcp', 'machine learning', 'python', 'r', 'recommendation', 'scala', 'spark']","['aws', 'azure', 'cloud', 'data pipeline', 'databricks', 'experimentation', 'gcp', 'machine learning', 'python', 'r', 'recommendation', 'scala', 'spark']",
"senior machine learning engineer, ads remote - ontario, canada","reddit, inc.","reddit is a community of communities. it’s built on shared interests, passion, and trust and is home to the most open and authentic conversations on the internet. every day, reddit users submit, vote, and comment on the topics they care most about. with 100,000+ active communities and approximately 101m+ daily active unique visitors, reddit is one of the internet’s largest sources of information. for more information, visit redditinc.com. reddit has a flexible workforce! if you happen to live close to one of our physical office locations, our doors are open for you to come into the office as often as you'd like. don’t live near one of our offices? no worries: you can apply to work remotely in any country in which we have a physical presence. team description ads ml serving team part of reddit’s ads ml platform, this team builds a highly reliable, scalable, and efficient ml serving stack. they focus on long-term architecture, tight integration with the ads serving stack, cpu/gpu performance optimization, and model velocity tools like observability libraries and quality gating. attribution & identity team this team builds attribution systems and identity solutions that help advertisers measure the impact of their campaigns. they create experimentation tools and platforms that improve usability, transparency, and performance insights. ads measurement modeling team a horizontal ml team in the ads measurement org focused on proving reddit ads value while maintaining privacy compliance. their work includes modeled identity, modeled conversions, and att opt-out utility enhancements. ads targeting and retrieval team this team designs and implements large-scale ml systems to improve targeting products. they work on offline and online retrieval systems to enhance contextual and behavioral targeting. advertiser optimization team composed of two horizontal teams, this group focuses on advertiser outcomes. the recommendations and forecasting team builds ml-driven tools for advertisers and sales. the bidding/pacing team develops algorithms and products like tcpa, troas, and performance advertising solutions, while driving innovations in marketplace dynamics. ads marketplace quality team this team optimizes reddit’s ads marketplace by building algorithms for auction and pricing efficiency. they also work on supply optimization and ad relevance, ensuring ads reach the right users at the right time in the right context. app ads and conversion modeling teams formed in early 2024, these teams focus on app ads modeling, including app install models and deep neural network models for ios and android conversions. they work on in-app event optimization and return on ad spend (roas) optimization, and are running experiments on top of dnn architectures to improve prediction accuracy. ads prediction team this team drives innovation across signals, features, model architecture, and infrastructure to improve marketplace efficiency and revenue. it includes: • core ads ranking (car) : builds reusable, scalable features and ranking models that integrate across the ads ecosystem, improving quality and iteration speed. • engagement modeling (ev) : develops click, long-click, and video engagement models for upper- and middle-funnel ad products. the ads creative effectiveness team this team is a newly formed group aimed at improving ad creative at reddit through generative and predictive products. we train, adapt and finetune llms/vlms to help advertisers make impactful images, videos and text. we build performance predictors to understand and rank ad components for best possible campaigns. we construct insight and recommendation engines to guide advertisers towards best practices and key enhancements, distilling knowledge about what works at reddit to supercharge their performance. this team is at the heart of reddit’s creative strategy, a core priority for the organization. reddit ads offers the opportunity to work on large-scale systems that directly impact advertisers, users, and revenue. we have openings across multiple teams and are looking for engineers and ml experts at all levels. role description join the ads team as a machine learning engineer and become a key contributor to reddit’s business. in this hands‑on role, you will be responsible for the full lifecycle of our ml systems, from initial research and modeling to deployment and optimization in production. your work will directly impact how we deliver relevant ads and drive value for our advertisers across areas like ad ranking, bidding, measurement, and optimization . responsibilities • design, build, and deploy industrial-level machine learning models to solve critical problems in ad ranking, bidding, and optimization. • take full ownership of the ml lifecycle, from ideation and research to building scalable serving systems and maintaining models in production. • perform systematic feature engineering to transform raw, diverse data into high-quality features that drive model performance. • work closely with product managers, data scientists, and engineers to translate business challenges into effective ml solutions. • improve the reliability and stability of our ml systems by building robust monitoring, alerting, and automated retraining pipelines. • research new algorithms, stay up-to-date with state-of-the-art ml techniques, and contribute to the team’s strategy and roadmap. required qualifications • experience working in the ads domain • at least 3-5+ years of end-to-end experience in training, evaluating, and deploying machine learning models in a production environment. • proficient in one or more general-purpose programming languages (e.g., python, scala) and have a solid understanding of software development best practices. • hands‑on experience with a major machine learning framework (e.g., tensorflow, pytorch) and a deep understanding of core ml concepts and algorithms. • proven ability to work effectively with cross‑functional teams, including product managers and data scientists, to translate business needs into technical solutions. • track record of using machine learning to drive key performance indicator (kpi) wins and solve complex, real‑world problems. bonus points • experience or interest in the advertising business and understanding customer needs • familiarity with distributed systems and large‑scale data processing technologies (e.g., spark, kafka). in select roles and locations, the interviews will be recorded, transcribed and summarized by artificial intelligence (ai). you will have the opportunity to opt out of recording, transcription and summarization prior to any scheduled interviews. during the interview, we will collect the following categories of personal information: identifiers, professional and employment‑related information, sensory information (audio/video recording), and any other categories of personal information you choose to share with us. we will use this information to evaluate your application for employment or an independent contractor role, as applicable. we will not sell your personal information or disclose it to any third party for their marketing purposes. we will delete any recording of your interview promptly after making a hiring decision. for more information about how we will handle your personal information, including our retention of it, please refer to our candidate privacy policy for potential employees and contractors. reddit is proud to be an equal opportunity employer, and is committed to building a workforce representative of the diverse communities we serve. reddit is committed to providing reasonable accommodations for qualified individuals with disabilities and disabled veterans in our job application procedures. if, due to a disability, you need an accommodation during the interview process, please let your recruiter know. reddit canada equal employment information reddit is proud to be an equal opportunity employer, and is committed to building a workforce representative of the diverse communities we serve. to bring community and belonging to everyone in the world, reddit’s employees must represent communities and redditors on our platform. the company is committed to treating all people in a way that allows them to maintain their dignity and independence. we believe in integration and equal opportunity. accommodations are available throughout the recruitment process and applicants with a disability may request to be accommodated throughout the recruitment process. we will work with all applicants to accommodate their individual accessibility needs. our vision at reddit is to have a workforce representative of people with different perspectives and experiences, including but not limited to, gender, race and ethnicity, sexual orientation, age, national origin, religion, and political views. we invite you to self-identify across the identities below so we can better understand our talent pools and assess our effectiveness in attracting and recruiting people to reddit from all backgrounds. answering these questions will not impact your application, nor will this information be shared with anyone making a hiring decision. #j-18808-ljbffr","montreal, quebec",Machine Learning Engineer,"['experimentation', 'feature engineering', 'kafka', 'machine learning', 'python', 'pytorch', 'r', 'recommendation', 'scala', 'spark', 'tensorflow']","['experimentation', 'feature engineering', 'kafka', 'machine learning', 'python', 'pytorch', 'r', 'recommendation', 'scala', 'spark', 'tensorflow']",
machine learning engineer for telecom digital churn predictions project,architech,"join us in building the future at architech, we don’t just ship software. we partner with north america’s leading brands to modernize legacy platforms, embed ai into real operations, and launch digital products that transform business outcomes. our engineers and designers harness cloud-native tools, autonomous agents, data-driven insights, and genai to drive measurable impact - replatforming systems in the cloud, optimizing customer journeys, or accelerating ai adoption across the enterprise. you’ll work at the intersection of strategy and execution, solving complex problems alongside smart, curious teammates across canada and poland. backed by 20+ years of experience, a drive for excellence, and a culture rooted in growth and collaboration, this is where you thrive if you’re looking to deliver meaningful, high-stakes software solutions. we’re building a more inclusive tech industry we believe diversity leads to better outcomes. nearly half of our team was born outside of canada, and we speak 19+ languages. we’re 31% women, 57% bipoc, and 14% lgbtqia+. we’ve doubled the number of women in tech roles in the past year, and maintain a 0% gender pay gap across our delivery and technology teams. inclusion here isn’t a buzzword, it’s backed by data, policy, and accountability. how we work together we’re a close-knit, collaborative group who care about doing excellent work, and doing it with integrity. our values shape how we show up every day: • think big – dream it, plan it, ship it • be open & collaborate – diverse minds build better solutions • never fail a client – own the outcome • grow our people – feedback, learning, leadership • do the right thing – even when it’s hard • embrace change – adapt fast, stay curious role overview: lead the development and optimisation of churn prediction models to identify customers at risk of leaving. responsibilities include building scalable machine learning models, designing feature engineering pipelines, monitoring performance, and collaborating with business teams to translate insights into retention strategies. key skills • experience with customer churn predictions • proficiency in python/r, sql, databricks and data manipulation libraries. • experience with machine learning frameworks (scikit-learn, tensorflow, pytorch). • strong background in feature engineering and large-scale data processing. • knowledge of statistical modelling and predictive analytics. • familiarity with cloud platforms (azure, aws, gcp) and big data tools (spark, databricks). • excellent communication for technical and non-technical audiences. architech is an equal opportunity employer committed to diversity. should you require any accommodations prior to or during the interview process, please indicate this during the interview process. we strongly encourage applications from racialized people, people with disabilities, people from gender and sexually diverse communities and/or people with intersectional identities.","toronto, on",Machine Learning Engineer,"['aws', 'azure', 'cloud', 'databricks', 'excel', 'feature engineering', 'gcp', 'machine learning', 'python', 'pytorch', 'r', 'scala', 'scikit-learn', 'spark', 'sql', 'tensorflow']","['aws', 'azure', 'cloud', 'databricks', 'excel', 'feature engineering', 'gcp', 'machine learning', 'python', 'pytorch', 'r', 'scala', 'scikit-learn', 'spark', 'sql', 'tensorflow']",$70–$85 an hour
machine learning engineer ii,"remitly canada operations, inc.","job description: at remitly, we believe everyone deserves the freedom to access, move, and manage their money wherever life takes them. since 2011, we've tirelessly delivered on our promise to customers sending money globally, providing secure, simple, and reliable ways to manage their money, ensuring true peace of mind. whether it's supporting loved ones back home, growing a business across continents, or pursuing new opportunities abroad, we're not just here to move money— we're here to move our global customers forward. we're looking for builders, reimaginers, and global thinkers who want to work at the intersection of technology, trust, and transformation. if that's you and you're ready to do the most meaningful work of your career—we invite you to join over 2,800 passionate remitlians worldwide who are united by our vision to transform lives with trusted financial services that transcend borders. about the role: at remitly, we are on a mission to transform lives by providing trusted financial services that transcend borders. we serve millions of customers sending billions of dollars each year across 170+ countries, and our work impacts communities on a truly global scale. as a machine learning engineer in remitly's core ai/ml team, you'll work at the heart of our ai strategy. the core ai/ml team is responsible for building the foundational machine learning systems, frameworks, and services that power remitly's next generation of products and operations. we work horizontally across domains at remitly—global money movement, treasury, identity & trust, promotions, referrals, etc. you'll collaborate with experienced engineers and data scientists to design, develop, and deploy machine learning models that help scale remitly's mission and impact worldwide. you will: implement and maintain ml models across the full lifecycle—from data exploration and feature engineering to training, evaluation, and deployment. design, build, and scale feature pipelines for batch and real-time ml applications. partner with data scientists, product owners, and engineers across verticals to turn prototypes into reliable, customer-facing ml systems. optimize models and pipelines using mlops best practices: automated retraining, drift detection, ci/cd, monitoring, observability. contribute to technical discussions and design reviews to improve team practices and systems. you have: degree in computer science, data science, statistics, mathematics or equivalent practical experience. 2+ years delivering machine learning systems in production. strong programming skills in python, go, scala or a similar language. hands-on experience with ml frameworks (e.g., pytorch, scikit-learn, numpy). familiarity working with major cloud platforms (aws, gcp, or azure). experience with one or more of deep learning algorithms, large language models, casual inference, personalization, knowledge graphs, natural language processing would be desirable. compensation details. the starting base salary range for this position is $132,000-$165,000. in canada, remitly employees are shareholders in our company and equity is part of our total compensation plan. your recruiter can share more information about medical benefits offered, as well as other financial benefits and total compensation components offered with this role. our benefits four weeks vacation health benefits mental health & family forming benefits rrsp plan with company match employee stock purchase plan (espp) life insurance & disability continuing education and travel benefits our connected work culture: driving innovation, together at remitly, we believe that true innovation sparks when we come together. our connected work culture fosters dynamic in-person collaboration, where ideas ignite and challenging problems find solutions faster. for corporate team members, we have an in-office expectation of at least 50% of the time monthly, typically achieved by coming in three days a week. this creates a consistent, meaningful overlap that supports team norms and business needs. managers also have the flexibility to set higher expectations based on their team's specific needs. these intentional in-office moments are vital for deepening relationships, fueling creativity, and ensuring your impact is felt where it matters most. remitly is an e-verify employer at remitly, we are dedicated to ensuring that our workplace offers equal employment opportunities to all employees and candidates, in full compliance with applicable laws and regulations. remitly is an equal opportunity employer. we celebrate diversity and are committed to creating an inclusive environment for all employees. remitly is a leading digital financial services provider for immigrants and their families in over 170 countries around the world. remitly helps immigrants send money home in a safe, reliable and transparent manner. its digitally-native, cross-border remittance app eliminates the long wait times, complexities and fees typical of traditional remittance processes. building on its strong foundation, remitly is expanding its suite of products to further its mission and transform financial services for immigrants all around the world. search for jobs click here to find additional opportunities at remitly!","burnaby, bc",Machine Learning Engineer,"['aws', 'azure', 'cloud', 'deep learning', 'elt', 'feature engineering', 'gcp', 'machine learning', 'natural language processing', 'numpy', 'python', 'pytorch', 'r', 'scala', 'scikit-learn', 'spark', 'statistics']","['aws', 'azure', 'cloud', 'deep learning', 'elt', 'feature engineering', 'gcp', 'machine learning', 'natural language processing', 'numpy', 'python', 'pytorch', 'r', 'scala', 'scikit-learn', 'spark', 'statistics']",
"senior machine learning engineer, agentic ai",loblaw,"at loblaw digital, we know that our customers expect the best from us. whether that means building the best, most innovative online shopping experience, or designing an app that will impact the lives of people across the country, we’re up for the challenge. from our office in downtown toronto, we’ve created leading ecommerce experiences in the online grocery shopping, beauty, pharmacy, and apparel spaces, and we’re only just getting started. why is this role important? what you’ll do we are seeking a senior machine learning engineer to join our team, focusing on the development, deployment and testing of advanced ai systems and sophisticated search agents. this role involves leveraging ml and cutting-edge large language models (llms) for building robust, scalable ai applications in the retail vertical, as well as mentoring other ml developers. who you are: • design, build and ship multiple components within an agentic ai system utilizing state of the art technologies to solve business problems. • develop high-performance enterprise-level machine learning models and ai agents using python programming, leveraging massive structured and unstructured datasets and apis from various internal and external sources. • champion and lead best practices for mle and llmop. • collaborate with front end and back end engineering teams to build and deploy ml models and agentic components in production • take ownership of system components, mentor other machine learning developers, and contribute to raising the technical bar within the team. • document and share results and findings with stakeholders in a structured manner and drive technical discussions cross functionally does this sound like you? • bachelor’s degree or equivalent in computer science or a related field alongside a strong foundation in ml algorithms, ml pipelines, and transformations, with 5+ years of hands-on experience building scalable ml products. • software engineering proficiency in python, sql, and design patterns, with proven experience building and deploying ml solutions in microservices architecture in production. • 2+ years experience using gcp tools in ml workflows like vertex ai, bigquery, cloud composer and cloud storage. • proven experience in langchain ecosystem or other agentic frameworks, nlp, llms, rags and embedding models. • skilled in ml workflow automation/deployment and mlops for seamless integration and deployment, and have supported ml products in production environments. • committed to code quality at every stage of the ml lifecycle, with a strong mindset in testing methodologies (unit, integration, end-to-end) and container orchestration using docker and kubernetes. how you’ll succeed: at loblaw digital, we seek great people to continually strengthen our culture. we believe great people model our values, are authentic, build trust and make connections. we’re able to keep innovating because our colleagues are passionate about their work and excited about the future of ecommerce. if you have big ideas, undeniable enthusiasm, and thrive in a collaborative, creative, and diverse group, we’ll get along just fine. looking for a challenge? good. love an innovative work environment? even better. apply today. employment type: full time type of role: regular loblaw digital recognizes canada's diversity as a source of national pride and strength. we have made it a priority to reflect our nation’s evolving diversity in the products we sell, the people we hire, and the culture we create in our organization. accommodation is available upon request for applicants with disabilities in the recruitment and assessment process and when hired. in addition, we believe that compliance with laws is about doing the right thing. upholding the law is part of our code of conduct – it reinforces what our customers and stakeholders expect of us. #en #ss #ld #on","toronto, on",Machine Learning Engineer,"['aws', 'bigquery', 'cloud', 'gcp', 'machine learning', 'nlp', 'python', 'r', 'scala', 'sql']","['aws', 'bigquery', 'cloud', 'gcp', 'machine learning', 'nlp', 'python', 'r', 'scala', 'sql']",
machine learning engineer,manulife,"machine learning engineer are you enthusiastic about machine learning and keen to demonstrate your skills in an innovative environment? join manulife as a machine learning engineer and become an integral part of our u.s. advanced analytics team! this is a groundbreaking opportunity to work on pioneering projects that generate significant business value for our insurance division. you'll collaborate closely with a variety of teams, including underwriting, pricing, it, data office, operations, sales, and distribution, to successfully implement world-class analytics solutions! position responsibilities: • design and implement scalable machine learning pipelines in collaboration with data teams. • optimize, deploy, and monitor ml models in production environments. • build and maintain data science infrastructure using azure cloud services. • develop generative ai applications, including retrieval-augmented generation (rag) systems and fine-tuned large language models (llms). • select appropriate technical tools and frameworks for project implementation. • collaborate with multi-functional teams to integrate ml solutions into existing systems. • document ml architecture designs, model integration procedures, and deployment workflows in a clear, comprehensive manner. • develop automation scripts and tools to ease the deployment, scaling, and management of ml systems within the cloud environment. required qualifications: • 3+ years of experience in ml engineering, devops, or data science roles. • 3+ years of hands-on experience with azure/cloud technologies (databricks, mlflow, azure ai studio). • strong python programming skills and advanced sql knowledge. • experience deploying models as rest apis. • proficiency with docker, kubernetes, and cloud infrastructure. • working knowledge of llms (gpt models, bert, llama) and timely engineering. • experience with devops practices (git, ci/cd pipelines). • ability to deliver pragmatic solutions under tight deadlines. • degree or equivalent experience in computer science, data science, engineering, or related field. preferred qualifications: • experience in the insurance industry, particularly in underwriting and product development. when you join our team: • we’ll empower you to learn and grow the career you want. • we’ll recognize and support you in a flexible environment where well-being and inclusion are more than just words. • as part of our global team, we’ll support you in shaping the future you want to see. à propos de manuvie et de john hancock la société financière manuvie est un chef de file mondial des services financiers qui aide les gens à prendre leurs décisions plus facilement et à vivre mieux. pour en apprendre plus à notre sujet, rendez vous à l’adresse www.manuvie.com. manuvie est un employeur qui souscrit au principe de l’égalité d’accès à l’emploi chez manulife/john hancock nous valorisons notre diversité. nous nous efforçons d’attirer, de perfectionner et de maintenir une main d'oeuvre qui est aussi diversifiée que nos clients, et de favoriser la création d’un milieu de travail inclusif qui met à profit la diversité de nos employés et les compétences de chacun. nous nous engageons à assurer un recrutement, une fidélisation, une promotion et une rémunération équitables, et nous administrons toutes nos pratiques et tous nos programmes sans discrimination en raison de la race, de l’ascendance, du lieu d’origine, de la couleur, de l’origine ethnique, de la citoyenneté, de la religion ou des croyances ou des convictions religieuses, du genre (y compris grossesse et affection liée à une grossesse), de l’orientation sexuelle, des caractéristiques génétiques, du statut d’ancien combattant, de l’identité de genre, de l’expression de genre, de l’âge, de l’état matrimonial, de la situation de famille, d’une invalidité ou de tout autre motif protégé par la loi applicable. nous nous sommes donné comme priorité d’éliminer les obstacles à l’accès égalitaire à l’emploi. c’est pourquoi un représentant des ressources humaines collaborera avec les candidats qui demandent accommodement raisonnable pendant le recrutement. tous les renseignements communiqués pendant le processus de demande d'accommodement seront stockés et utilisés conformément aux lois et aux politiques applicables de manuvie. pour demander une mesure d’accommodement raisonnable dans le cadre du recrutement, écrivez à recruitment@manulife.com. région de référence du salaire toronto, ontario modalités de travail hybride l’échelle salariale devrait se situer entre $75,880.00 cad - $140,920.00 cad si vous posez votre candidature à ce poste en dehors de la région principale, veuillez écrire à recruitment@manulife.com pour obtenir l’échelle salariale correspondant à votre région. le salaire varie en fonction des conditions du marché local, de la géographie et de facteurs pertinents liés au poste telles les connaissances, les compétences, les qualifications, l’expérience et l’éducation ou la formation. les employés ont également la possibilité de participer à des programmes de motivation et de toucher une rémunération incitative liée au rendement de l’entreprise et au rendement individuel. manuvie offre aux employés admissibles une vaste gamme d’avantages sociaux personnalisables, notamment une assurance soins médicaux, soins dentaires, santé mentale, soins de la vue, invalidité de courte et de longue durée, assurance vie et assurance dma, assurance adoption, de maternité de substitution et de soins médicaux non urgents ainsi que des programmes d’aide aux employés et leur famille. nous proposons également aux employés admissibles différents régimes d’épargne-retraite (y compris des régimes de rente et un programme international d’actionnariat assortie de cotisations patronales de contrepartie) ainsi que des ressources en matière d’éducation et de conseils financiers. notre généreux programme de congés rémunérés au canada comprend les jours fériés, les congés annuels, les congés personnels et les congés de maladie, et nous offrons toute la gamme des congés autorisés prévus par la loi. si vous posez votre candidature à ce poste aux états-unis, veuillez écrire à recruitment@manulife.com pour obtenir de plus amples renseignements sur les dispositions relatives aux congés rémunérés spécifiques aux états-unis.","toronto, on (+1 other)",Machine Learning Engineer,"['azure', 'cloud', 'databricks', 'machine learning', 'python', 'r', 'scala', 'sql']","['azure', 'cloud', 'databricks', 'machine learning', 'python', 'r', 'scala', 'sql']",
senior machine learning engineer - mlops,spotify,"the hendrix ml platform team is dedicated to developing a robust, spotify-wide platform for training and serving machine learning models. this platform streamlines the productionization of ai and ml models by mitigating the incidental complexities involved in creating backend services for serving predictions and training models. what you'll do • contribute to spotify ml platform sdk and build tools for various ml operations. • collaborate with machine learning engineers (mle), researchers, and various product teams to deliver scalable ml platform tooling solutions that meet the timelines and specifications of given requirements. • work independently and collaboratively on squad projects that often requires learning and applying new technologies that may go beyond existing skillsets. • manage and maintain large scale production kubernetes clusters for ml workloads, including ml platform infrastructure and necessary dev ops. • designs, documents and implements reliable, testable and maintainable solutions ml infrastructure capabilities. who you are • you have 6+ years of hands-on experience implementing production ml infrastructure at scale in python, go or similar languages. • you have knowledge of deep learning fundamentals, algorithms, and open-source tools such as huggingface, ray, pytorch or tensorflow • you have an understanding of distributed training leveraging gpus and kubernetes • you have a general understanding of data processing for ml • you have experience with agile software processes and modular code design following industry standards where you'll be • this role is based in toronto, canada • we offer you the flexibility to work where you work best! there will be some in person meetings, but still allows for flexibility to work from home. spotify is an equal opportunity employer. you are welcome at spotify for who you are, no matter where you come from, what you look like, or what’s playing in your headphones. our platform is for everyone, and so is our workplace. the more voices we have represented and amplified in our business, the more we will all thrive, contribute, and be forward-thinking! so bring us your personal experience, your perspectives, and your background. it’s in our differences that we will find the power to keep revolutionizing the way the world listens. at spotify, we are passionate about inclusivity and making sure our entire recruitment process is accessible to everyone. we have ways to request reasonable accommodations during the interview process and help assist in what you need. if you need accommodations at any stage of the application or interview process, please let us know - we’re here to support you in any way we can. spotify transformed music listening forever when we launched in 2008. our mission is to unlock the potential of human creativity by giving a million creative artists the opportunity to live off their art and billions of fans the chance to enjoy and be passionate about these creators. everything we do is driven by our love for music and podcasting. today, we are the world’s most popular audio streaming subscription service.","toronto, on",Machine Learning Engineer,"['deep learning', 'machine learning', 'python', 'pytorch', 'r', 'scala', 'tensorflow']","['deep learning', 'machine learning', 'python', 'pytorch', 'r', 'scala', 'tensorflow']",
machine learning engineer - 360° video/image processing,bmad technologies,"the production team at bmad is looking for a highly motivated machine learning engineer with expertise in 360° video and image processing to lead the development of ai tools designed to merge images captured from multiple cameras into seamless 360-degree images or videos, creating an immersive viewing experience as a machine learning engineer focused on generating high-quality equirectangular panoramas, your daily responsibilities will center around independently designing and building ai-driven tools and processing pipelines that transform images from multiple cameras into seamless 360° panoramas. this involves developing algorithms for image stitching, blending, and inpainting to ensure smooth transitions and minimize visible seams or distortions, particularly in challenging areas like the poles and edges. you will also work on optimizing projection techniques to accurately map 2d images onto a spherical surface, converting them into the standard 2:1 equirectangular format required for vr and immersive viewing experiences. the role requires expertise in handling large image datasets, applying advanced computer vision and deep learning methods, and ensuring that the final panoramas are visually coherent and high-resolution, ready for integration into vr platforms or further editing workflows this is a hands-on, independent role suited for a passionate engineer with a strong home-lab ethos; excellent knowledge of linux-based containerized technologies, and a fascination with bleeding-edge gpu hardware and genai systems. responsibilities: • design, develop, potentially train, and deploy ai models for 360° image and video generation. • develop pipelines to stitch images/videos from multiple cameras into seamless 360° visuals. • optimize diffusion models using pytorch, tensorrt, and related toolkits for maximum performance, accuracy, and scalability. • procure, manage, and curate large 360° image/video datasets for model training. • integrate ai solutions into vr-focused platforms and services. • deploy and manage ai services using docker, kubernetes, and other containerized technologies on linux systems. • stay current with cutting-edge research and emerging trends in 360° imaging, spherical cnns, and generative ai. • collaborate with the production team and contribute as the ai subject matter expert on project direction and technical decision-making. requirements: • at least 4 years of experience in a development or research role focused on 360° image and video processing. • proven experience stitching multi-camera content into seamless 360° images/videos. • bachelor’s or master’s degree in computer science, mathematics, engineering, or a computational science-related field. • hands-on experience building and managing home servers, homelabs, and deep knowledge of linux systems and gpu hardware. • deep experience developing stable diffusion pipelines, model checkpoints, and loras, particularly for 360° equirectangular panorama generation. • proficiency in python, with familiarity in c++, and node.js being a plus. • strong command of ai/ml tools and frameworks: pytorch, hugging face diffusers, and tensorrt. • experience with docker, kubernetes, and containerized ai deployment on linux systems. • strong ability to work autonomously in an independent, self-directed environment. job types: full-time, permanent benefits: • extended health care • paid time off work location: remote",canada,Machine Learning Engineer,"['c++', 'computer vision', 'deep learning', 'excel', 'machine learning', 'python', 'pytorch', 'r', 'scala']","['c++', 'computer vision', 'deep learning', 'excel', 'machine learning', 'python', 'pytorch', 'r', 'scala']",
senior machine learning engineer,instacart,"this is a general posting for multiple senior machine learning roles open across our 4-sided marketplace. you’ll get the chance to learn about the problems the different ml teams solve as you go through the process. towards the end of your process, we’ll do a team-matching exercise to determine which of the open roles/teams you’ll join. you can find a blurb on each team at the bottom of this page. core experience: the core experience organization at instacart is at the forefront of applying cutting-edge ai technologies, including large language models (llms), to revolutionize how customers find products. working with world-class engineers, data scientists, and product managers, the team builds sophisticated machine learning and ai systems that power the future of search and recommendations at instacart. by leveraging state-of-the-art transformer architectures, multimodal ai, and generative models, the team enhances the relevance across all shopping surfaces. they innovate with advanced neural retrieval methods, llm-powered ranking algorithms, and ai-driven personalization systems, delivering highly contextual and intuitive results to users throughout the instacart ecosystem. the team tackles some of the most critical aspects of the business—helping customers connect with exactly the right products through ai. they solve complex, large-scale search challenges using the latest in deep learning, natural language understanding, and llm fine-tuning techniques. their dedication to ai innovation is reflected in their recent publications and contributions to the field. about the job • design, develop, and deploy advanced ai and machine learning solutions, including llms and neural networks, to solve complex challenges in our dynamic marketplace environment. • architect and implement state-of-the-art deep learning systems that leverage transformer models, multimodal ai, and generative techniques to create intelligent, adaptive solutions. • collaborate closely with product managers, data scientists, and backend engineers to translate business requirements into cutting-edge ai applications that deliver measurable impact. • pioneer foundation model applications, including prompt engineering and fine-tuning, to create ai systems with unparalleled understanding of context and user intent. • ensure ethical implementation and strong integration of ai solutions, aligning them with strategic business objectives. • drive continuous innovation by researching, testing, and implementing the latest advancements in machine learning, such as embeddings, vector databases, and rlhf (reinforcement learning from human feedback). • enhance operational efficiency through intelligent automation, predictive modeling, and algorithmic optimization driven by custom-trained ai systems. about you minimum qualifications • graduate degree (master’s or phd) in artificial intelligence, machine learning, or equivalent self-study and experience. • 7+ years of industry experience using machine learning to solve real-world problems with large datasets. • strong programming skills in python with fluency in data manipulation tools (sql, pandas) and machine learning frameworks (scikit-learn, xgboost, keras/tensorflow). • strong analytical skills and problem-solving abilities. • excellent communicator, capable of collaborating with diverse stakeholders across all levels. preferred qualifications • extensive expertise with modern deep learning frameworks (pytorch, tensorflow, jax) and advanced llm architectures, including transformer models and multimodal ai systems. • demonstrated experience implementing and fine-tuning large language models, including prompt engineering, embedding techniques, and optimizing inference in production environments. • strong foundation in ai fundamentals, including neural network architectures, generative models, and foundation model adaptation methodologies like peft, lora, and rlhf. • proven track record of designing and deploying sophisticated ml/ai systems in production environments that improve recommendations, search relevance, and user engagement metrics. • experience optimizing ai model performance across the stack, from architecture design and training workflows to distributed inference and serving infrastructure. • self-motivated and innovative with a strong sense of ownership, capable of navigating the rapidly evolving ai landscape to evaluate and implement novel techniques. • passionate about applying cutting-edge ai research to real-world problems while considering the practical aspects of building responsible, efficient ai systems at scale. instacart provides highly market-competitive compensation and benefits in each location where our employees work. this role is remote and the base pay range for a successful candidate is dependent on their permanent work location. please review our flex first remote work policy here . currently, we are only hiring in the following provinces: ontario, alberta, british columbia, and nova scotia. offers may vary based on many factors, such as candidate experience and skills required for the role. additionally, this role is eligible for a new hire equity grant as well as annual refresh grants. please read more about our benefits offerings here . for canadian based candidates, the base pay ranges for a successful candidate are listed below. can $176,000 — $225,000 cad",alberta,Machine Learning Engineer,"['deep learning', 'excel', 'keras', 'machine learning', 'pandas', 'python', 'pytorch', 'r', 'recommendation', 'scikit-learn', 'sql', 'tensorflow', 'xgboost']","['deep learning', 'excel', 'keras', 'machine learning', 'pandas', 'python', 'pytorch', 'r', 'recommendation', 'scikit-learn', 'sql', 'tensorflow', 'xgboost']",$176K–$225K a year
machine learning engineer,pinnacle,"job title: machine learning engineer reports to: machine learning technical lead location: north york about us: welcome to pinnacle, the ultimate destination for sports enthusiasts seeking an exhilarating sportsbook and gaming experience! established in 1998, we have solidified our position as one of the globe's foremost licensed online gaming companies. with our cutting-edge offerings, we guarantee an electrifying experience that will keep you on the edge of your seat. pinnacle invites you to join our team and become an instrumental figure in the exciting realm of sports betting. our vibrant team is fueled by passion and driven by innovation, working together to redefine the landscape of sports betting and gaming. together, we constantly strive to surpass limitations and deliver unparalleled experiences to sports enthusiasts worldwide. prepare yourself for a thrilling journey and discover sports in an entirely new dimension with pinnacle! job purpose: we are seeking a machine learning engineer for a role encompassing sports modelling, algorithm development, and both building and utilizing tools to support model evaluation and performance analysis. the ideal candidate will have experience working with traditional statistical learning techniques, probabilistic graphical models, and a deep understanding of data analysis and visualization. proficiency in r is strongly preferred, as is a demonstrated ability to work with large datasets, develop intuitive visualization tools, and deliver insights to inform decision-making. a passion for sports, sports analytics, sports betting, and fantasy sports is also beneficial. the ideal candidate should have: • a degree in mathematics, computer science, statistics, or a related field. advanced degrees (ms/phd) specializing in machine learning or statistics are highly desirable. • experience with statistical computing in r, with proficiency in modern r packages and technologies such as data.table, dplyr, tidyr is preferred. • strong experience building and utilizing existing tools for model evaluation and performance analysis, ideally in r utilizing visualization tools such as ggplot2, shiny, and plotly. • deep knowledge of traditional machine learning techniques, such as linear regression, generalized linear models (glms), and generalized additive models (gams). • expertise in probabilistic graphical models, such as directed acyclic graphs (dags) and markov chains, is a strong plus. • demonstrated ability to work with large datasets efficiently and develop optimized code, with tools such as rcpp, microbenchmark, and profviz, is a big plus. • experience working with relational databases such as sql server, postgresql or google bigquery. • a passion for sports, sports analytics, sports betting, or fantasy sports is highly beneficial. working environment: we foster a collaborative environment that emphasizes skill development, mentorship, and tackling a combination of theoretical and real-world problems in sports analytics and betting. with opportunities for continuous learning and growth, team members expand their technical expertise, gain exposure to new technologies, and deliver impactful solutions in a fast-paced, dynamic industry, supported by open communication and mutual respect. core responsibilities: this is a key role within the team, focusing primarily on building and iterating on predictive models, and developing tools to support model evaluation and performance analysis. these models are released to real production environments, analyzed by expert traders, and eventually form the basis of the initial layer of our odds engine. the performance of these models are analyzed against our sharp betting clientele and constantly iterated upon based on the changing dynamics of market trends, data feed quality, sport-specific gameplay, and rule changes. technical expertise: a strong background in statistics, machine learning, and algorithm development is essential. candidates will ideally have advanced proficiency in r, with experience coding for production environments. however, for those who are less experienced with r directly, we offer a highly supportive internal ecosystem with dedicated guidance to help develop r skills. development expertise: understanding of software principles and functional coding principles are necessary. experience deploying models to production in real world applications with feedback from senior teammates and key stakeholders is a strong plus. experience with code optimization, rcpp/c++, and r package development are highly valued. experience using tools like git, jira, along with participating in peer reviews to ensure high quality, maintainable code in a collaborative development environment is highly desired. data and evaluation focus: demonstrated ability to work with large datasets, optimize code for performance, and deliver insights through well-designed tools is critical. experience with shiny for building tools and ggplot2 or similar libraries for data visualization is highly valued. soft skills: strong communication skills (both verbal and written) are required to convey complex information effectively. the ideal candidate will also have strong analytical, conceptual, and problem-solving abilities, with attention to detail. domain knowledge: a thorough understanding and passion for sports, sports analytics, and sports betting markets is highly desirable. experience working with sports betting markets, financial markets, or similar applications is a plus. bonus skills: experience with bayesian techniques and their implementation to solve practical problems with software like stan or nimble, is a plus, though not required. familiarity with rcpp, c++, and tools like profvis, microbenchmark, parallel is a strong plus we are an equal opportunity employer dedicated to fostering an inclusive and diverse workplace. we prioritize hiring the best candidates based on their skills and qualifications, irrespective of race, gender, age, religion, or any other characteristic. our strength lies in our diverse teams, and we proudly celebrate and empower everyone to embrace and promote diversity throughout their time with us.",ontario,Machine Learning Engineer,"['bayesian', 'bigquery', 'c++', 'data analysis', 'ggplot', 'machine learning', 'r', 'regression', 'sql', 'sql server', 'statistics']","['bayesian', 'bigquery', 'c++', 'data analysis', 'ggplot', 'machine learning', 'r', 'regression', 'sql', 'sql server', 'statistics']",
machine learning / deep learning engineer - coding wizard ( 70% - 100% ) (remote/onsite),imnoo ag,"do you feel like you’re just coding toy problems without making a real impact? join us for a fulfilling career where you’ll tackle concretely defined, real-world challenges. at imnoo, you’ll make a significant impact on automating manufacturing processes, gaining invaluable experience and deep satisfaction along the way! we’re looking for an experienced software engineer / machine learning – python/cloud services wizard who is ready to develop and roll out new and existing services into production systems. why join us? • a clear path for career growth and advancement within the company. • the opportunity to solve real and meaningful challenges. • a dynamic and flexible work environment. • opportunities for professional development and certification support. • a platform to share your ideas and opinions, where they are highly valued. your playground • work on real/hands-on problems • develop efficient tools directly impacting the day-to-day life of our manufacturers • develop advanced feature extraction and data-processing solutions for managing cad, 2d, and batch data (filtered/unfiltered) for machine learning • design, engineer, and take responsibility for cloud services, apis, automated model training/deployment, and build tools. • own services end-to-end, from proof of concept to production-ready solutions. • gain in-depth knowledge of interconnected cloud, data, computer vision, and ml services across multiple languages and platforms. • maintain and further develop optimization/machine learning services. • enhance 2d/3d/cad tools/solutions our offer • learn a loooooooooooot all the time!!!! • work on tangible, real-world challenges that make an impact. • fully flexible work style. • snacks of your choice. • support for professional certifications and development. • exposure to a broad stack of machine learning and data analysis tools. • opportunities to grow your hard and soft skills within a diverse team. • collaboration in a dynamic startup environment with a motivated and young team. • 25 days of vacation annually. • unlimited home office or offsite work flexibility. • excellent work-life balance with regular social team events. (middlemen such as recruiting agencies are not welcome and will be automatically disqualified) best to have: • strong software / coding skills • 5+ years in both dynamically typed and statically typed languages • strong problem-solving skills • 3d / geometry / game development / fluid simulations / rendering / cuda or similar - foundations/experience • experience in systems running in production under heavy load nice to have: • educational background in mathematics or statistics with strong dedication and experience in applied technologies.(nice to have) • cad data usage experience (nice to have) • industry/mechanical experience in cnc and related fields (optional) • hands-on experience in frontend (e.g., angular) and backend development . (optional) • full-stack development experience.(optional) • familiarity with popular machine learning/deep learning libraries, such as scikit-learn , pytorch , and pytorch lightning . (nice to have) • experience with industrialization tools for machine learning models, including quantization , onnx , and docker .(nice to have) • knowledge of mlops practices and pipeline development.(nice to have) • expertise in processing, clustering, filtering, indexing, and querying large datasets efficiently. (nice to have) • proficiency in automated model training and deployment. (optional) • strategic data analysis and research skills, including statistical testing, error propagation, and identifying clusters or outliers. (optional) • experience with cloud services, particularly aws and azure cloud services . (optional) • strong database skills: sql and nosql . (optional) • deep understanding of data structures. (optional) • familiarity with compiled languages like c# , c++ , or similar.(nice to have)",canada,Machine Learning Engineer,"['aws', 'azure', 'c#', 'c++', 'cloud', 'clustering', 'computer vision', 'data analysis', 'deep learning', 'excel', 'machine learning', 'python', 'pytorch', 'r', 'scikit-learn', 'sql', 'statistics']","['aws', 'azure', 'c#', 'c++', 'cloud', 'clustering', 'computer vision', 'data analysis', 'deep learning', 'excel', 'machine learning', 'python', 'pytorch', 'r', 'scikit-learn', 'sql', 'statistics']",
senior machine learning research engineer,royal bank of canada,"job description what's the opportunity? at rbc borealis, you’ll be joining a team of leading researchers and software engineering specializing in machine learning. you will have access to rich and massive datasets, and to computational resources to support novel product development touching machine learning areas such as generative ai, natural language processing, and time series analysis. we’re looking for an enthusiastic senior machine learning research engineer who’s excited by the opportunity of being at the forefront of applying machine learning technology to challenging problems. as a senior ml research engineer in the applied research team, you’ll be part of a collaborative group who aims to deliver ai projects end to end – everything from data pre-processing and exploration, to prototyping novel algorithmic solutions, to software implementations of machine learning-based products. the goal is to understand the needs of our business partners and bring to life these unique and efficient solutions that can only be achieved through the use of machine learning. your responsibilities include: • lead development of machine learning-based software solutions; • collaborating with business stakeholders to prototype machine-learning solutions rapidly; • conducting comparisons to existing algorithms and baselines; • reviewing, extending, and optimizing prototype solutions; • collaborating with the engineering team to integrate algorithms into products; • developing reusable internal tools to facilitate research prototyping; • supporting projects with thorough documentation, design decisions, and capabilities. you’re our ideal candidate if you have: • a master’s or phd degree in computer science, mathematics, physics, economics or equivalent; • 5+ years of applied machine learning experience as an individual contributor driving projects; • experience with writing modular, robust, scalable software in python 3.x; • expertise in a few of the following areas: deep learning, natural language processing, information retrieval; • experience with deep learning packages such as pytorch, jax, or tensorflow; • knowledge of professional software engineering best practices for the full software development life cycle, including testing methods, coding standards, code reviews, and source control management; • strong communication skills and a collaborative attitude. what’s in it for you? • become part of a team that thinks progressively and works collaboratively. we care about seeing each other reach full potential; • a comprehensive total rewards program including bonuses and flexible benefits, competitive compensation, commissions, and stock options where applicable; • leaders who support your development through coaching and managing opportunities; • ability to make a difference and lasting impact from a local-to-global scale. about rbc borealis rbc borealis, an rbc institute for research, is a curiosity-driven research centre dedicated to achieving state-of-the-art in machine learning. established in 2016, and with labs in toronto, montreal, waterloo, and vancouver, we support academic collaborations and partner with world-class research centres in artificial intelligence. with a focus on ethical ai that will help communities thrive, our machine learning scientists perform fundamental and applied research in areas such as reinforcement learning, natural language processing, deep learning, and unsupervised learning to solve ground-breaking problems in diverse fields. inclusion and equal opportunity employment rbc is an equal opportunity employer committed to diversity and inclusion. we are pleased to consider all qualified applicants for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, protected veterans status, aboriginal/native american status or any other legally-protected factors. disability-related accommodations during the application process are available upon request. job skills analytical thinking, decision making, detail-oriented, long term planning, machine learning, product development design, programming languages, quantitative research, research and development operations, research documents additional job details address: 777 bay st, th 27:toronto city: toronto country: canada work hours/week: 37.5 employment type: full time platform: technology and operations job type: regular pay type: salaried posted date: 2025-11-27 application deadline: 2026-02-23 note: applications will be accepted until 11:59 pm on the day prior to the application deadline date above inclusion and equal opportunity employment at rbc, we believe an inclusive workplace that has diverse perspectives is core to our continued growth as one of the largest and most successful banks in the world. maintaining a workplace where our employees feel supported to perform at their best, effectively collaborate, drive innovation, and grow professionally helps to bring our purpose to life and create value for our clients and communities. rbc strives to deliver this through policies and programs intended to foster a workplace based on respect, belonging and opportunity for all. join our talent community stay in-the-know about great career opportunities at rbc. sign up and get customized info on our latest jobs, career tips and recruitment events that matter to you. expand your limits and create a new future together at rbc. find out how we use our passion and drive to enhance the well-being of our clients and communities at jobs.rbc.com.","toronto, on",Machine Learning Engineer,"['deep learning', 'machine learning', 'natural language processing', 'python', 'pytorch', 'r', 'scala', 'tensorflow', 'time series']","['deep learning', 'machine learning', 'natural language processing', 'python', 'pytorch', 'r', 'scala', 'tensorflow', 'time series']",
"senior machine learning engineer, recommendations",lyft,"at lyft, our purpose is to serve and connect. we aim to achieve this by cultivating a work environment where all team members belong and have the opportunity to thrive. with over half a billion rides and counting, lyft is solving hard problems in a rapidly growing domain with a lot of data and creative solutions in marketplace, mapping, fraud, trust & safety, growth and beyond. while traditional approaches to optimization and problem decomposition are sufficient to disrupt transportation, building next-generation platform for low-cost, ultra-immersive transportation to improve people's lives warrants modern ml utilizing peta-byte scale data. our highly motivated machine learning engineers work on these challenging problems and define solutions to directly impact various aspects of our core business. if you are a critical thinker with experience in machine learning workflows, passionate about solving business problems using data and working in a dynamic, creative, and collaborative environment, we are searching for you. as a machine learning engineer, you will be developing and launching the algorithms that power the platform's core services and impactful products. compared to similarly-sized technology companies, the set of problems that we tackle is incredibly diverse. they cut across transportation, economics, forecasting, mapping, safety, personalization, and adaptive control. we are hiring motivated experts in each of these fields. we're looking for someone who is passionate about solving problems with data, building reliable ml systems, and is excited about working in a fast-paced, innovative, and collegial environment. responsibilities: • partner with engineers, data scientists, product managers, and business partners to apply machine learning for business and user impact • perform data analysis and build proof-of-concept to explore and propose ml solutions to both new and existing problems • develop statistical, machine learning, or optimization models • write production quality code to launch machine learning models at scale • evaluate machine learning systems against business goal experience: • b.s., m.s., or ph.d. in computer science or other quantitative fields or related work experience • 5+ years of machine learning experience • passion for building impactful machine learning models leveraging expertise in one or multiple fields. • proficiency in python, golang, or other programming language • excellent communication skills and fluency in english • strong understanding of machine learning methodologies, including supervised learning, forecasting, recommendation systems, reinforcement learning, and multi-armed bandits benefits: • extended health and dental coverage options, along with life insurance and disability benefits • mental health benefits • family building benefits • child care and pet benefits • access to a lyft funded health care savings account • rrsp plan to help save for your future • in addition to provincial observed holidays, salaried team members are covered under lyft's flexible paid time off policy. the policy allows team members to take off as much time as they need (with manager approval). hourly team members get 15 days paid time off, with an additional day for each year of service • lyft is proud to support new parents with 18 weeks of paid time off, designed as a top-up plan to complement provincial programs. biological, adoptive, and foster parents are all eligible. • subsidized commuter benefits lyft is committed to creating an inclusive workforce that fosters belonging. lyft believes that every person has a right to equal employment opportunities without discrimination because of race, ancestry, place of origin, colour, ethnic origin, citizenship, creed, sex, sexual orientation, gender identity, gender expression, age, marital status, family status, disability, pardoned record of offences, or any other basis protected by applicable law or by company policy. lyft also strives for a healthy and safe workplace and strictly prohibits harassment of any kind. accommodation for persons with disabilities will be provided upon request in accordance with applicable law during the application and hiring process. please contact your recruiter if you wish to make such a request. lyft highly values having employees working in-office to foster a collaborative work environment and company culture. this role will be in-office on a hybrid schedule — team members will be expected to work in the office at least 3 days per week, including on mondays, wednesdays, and thursdays. lyft considers working in the office at least 3 days per week to be an essential function of this hybrid role. your recruiter can share more information about the various in-office perks lyft offers. additionally, hybrid roles have the flexibility to work from anywhere for up to 4 weeks per year. #hybrid the expected base pay range for this position in the toronto area is cad $149,600 - cad $187,000. salary ranges are dependent on a variety of factors, including qualifications, experience and geographic location. range is not inclusive of potential equity offering, bonus or benefits. your recruiter can share more information about the salary range specific to your working location and other factors during the hiring process.","toronto, on",Machine Learning Engineer,"['data analysis', 'excel', 'machine learning', 'python', 'r', 'recommendation']","['data analysis', 'excel', 'machine learning', 'python', 'r', 'recommendation']",$150K–$187K a year
senior software engineer - machine learning,arm,"arm's machine learning (ml) group is seeking a highly motivated and creative senior software engineer to join a team of brilliant engineers located in cambridge, uk who specialise in ml compilers. this role presents an opportunity to contribute to advance ml compilation technology. you will help to build the software that enables development of deep learning applications that form the basis of many ground-breaking technologies like self-driving cars, generative ai engines and ml-powered wearables. job description: arm machine learning (ml) team is looking for a software engineer who would build a range of innovative compiler solutions for a variety of markets. you will apply your experience and insight within this domain to craft and optimise compilers for machine learning networks that target arm’s cpus, gpus and npus. if you are interested in this opportunity, make sure to apply soon! we look forward to receiving your application and welcoming you to arm. you could be joining our highly motivated team and have a marked impact on both strategy and implementation! responsibilities: • contribute to deliver production-grade software and push the boundaries of machine learning compilation • build, extend and collaborate on innovative ml compilation software projects, such as tosa and the broader mlir ecosystem • work with other groups in arm to expand support for arm architecture and ecosystem required skills and experience: • a passion for software development and quality. experience with the full software development lifecycle - planning, designing, developing, testing, delivering, and maintaining production-quality software • experience with c++, understanding of python is a plus • experience with or interest in compilers such as llvm and the mlir ecosystem • high degree of initiative and problem solving skills • ability to own team's delivery and lead others on large or more sophisticated tasks • good interpersonal and communication skills. 'nice to have' skills and experience: • knowledge or curiosity about large language models (llms), machine learning, their applications and frameworks • experience with contributing to open-source projects and working with a broader open-source community • experience with python packaging, linux and scripting languages, such as shell-scripting in return: on top of the already compelling life at arm, we offer strong team culture, learning opportunities, regular career conversations, emphasis on diversity, equity and inclusion and a continuous improvement mentality. accommodations at arm at arm, we want to build extraordinary teams. if you need an adjustment or an accommodation during the recruitment process, please email accommodations@arm.com. to note, by sending us the requested information, you consent to its use by arm to arrange for appropriate accommodations. all accommodation or adjustment requests will be treated with confidentiality, and information concerning these requests will only be disclosed as necessary to provide the accommodation. although this is not an exhaustive list, examples of support include breaks between interviews, having documents read aloud, or office accessibility. please email us about anything we can do to accommodate you during the recruitment process. hybrid working at arm arm’s approach to hybrid working is designed to create a working environment that supports both high performance and personal wellbeing. we believe in bringing people together face to face to enable us to work at pace, whilst recognizing the value of flexibility. within that framework, we empower groups/teams to determine their own hybrid working patterns, depending on the work and the team’s needs. details of what this means for each role will be shared upon application. in some cases, the flexibility we can offer is limited by local legal, regulatory, tax, or other considerations, and where this is the case, we will collaborate with you to find the best solution. please talk to us to find out more about what this could look like for you. equal opportunities at arm arm is an equal opportunity employer, committed to providing an environment of mutual respect where equal opportunities are available to all applicants and colleagues. we are a diverse organization of dedicated and innovative individuals, and don’t discriminate on the basis of race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or status as a protected veteran.",cambridge,Machine Learning Engineer,"['c++', 'deep learning', 'machine learning', 'python', 'r']","['c++', 'deep learning', 'machine learning', 'python', 'r']",
machine learning engineer,testq technologies,"we are looking for a skilled document ai / idp engineer to build production-ready ml solutions for content extraction, classification, and automation using python, ocr, and generative ai. key responsibilities: • build and deploy production ml/ocr/genai models for document processing • work with business teams to design and iterate scalable solutions • develop python, ml, and ocr components end-to-end • test, deploy, and support applications in production • collaborate with product managers and engineering teams must-have experience: strong hands-on experience in document ai / idp using traditional + genai approaches production experience in python, ml, dl, nlp, ocr, image processing knowledge of frameworks: tensorflow, pytorch, huggingface, spacy, opencv, regex experience delivering secure, resilient code and apis nice to have: postgresql experience exposure to azure (vms, aks, azure cli, azure devops) experience working in fast, frequent-release engineering teams",sheffield,Machine Learning Engineer,"['azure', 'classification', 'nlp', 'python', 'pytorch', 'r', 'scala', 'sql', 'tensorflow']","['azure', 'classification', 'nlp', 'python', 'pytorch', 'r', 'scala', 'sql', 'tensorflow']",
principal machine learning engineer,qodea,"work where work matters. elevate your career at qodea, where innovation isn't just a buzzword, it's in our dna. we are a global technology group built for what's next, offering high calibre professionals the platform for high stakes work, the kind of work that defines an entire career. when you join us, you're not just taking on projects, you're solving problems that don't even have answers yet. you will join the exclusive roster of talent that global leaders, including google, snap, diageo, paypal, and jaguar land rover call when deadlines seem impossible, when others have already tried and failed, and when the solution absolutely has to work. forget routine consultancy. you will operate where technology, design, and human behaviour meet to deliver tangible outcomes, fast. this is work that leaves a mark, work you’ll be proud to tell your friends about. qodea is built for what’s next. an environment where your skills will evolve at the frontier of innovation and ai, ensuring continuous growth and development. we are looking for a principal machine learning engineer to shape the next generation of our data and machine learning capabilities, focusing on data quality, enrichment, and the intelligent linking of products and information. this role offers the opportunity to define architectural strategy, lead transformative initiatives, and work at scale on platforms infused with machine learning and semantic intelligence to unlock deep insights from complex data. we look for people who embody: innovation to solve the hardest problems. ‍accountability for every result. ‍integrity always. about the role • lead the architecture and evolution of scalable, high-performance data pipelines and ml systems, focusing on data ingestion, transformation, quality checks, and enrichment. • provide technical leadership and mentorship to a cross-functional team of ml engineers, data scientists, and infrastructure engineers, ensuring alignment with architectural standards and driving a culture of high quality and operational excellence. • drive cross-functional initiatives to integrate modern machine learning and ai technologies (including semantic understanding, natural language processing, and potentially large language models) to automate data quality, link canonical products, and create intelligent data enrichment solutions. • define strategies to enhance the performance, reliability, and observability of data and ml services, ensuring robust, high-quality data outputs. • design and implement frameworks for evaluating data quality and the effectiveness of ml models through both offline metrics and online validation. • champion engineering best practices and mentor engineers across teams, raising the bar for code quality, data governance, and ml system design. • shape long-term technical direction by staying ahead of trends in ai, ml, data engineering, and distributed systems and bringing these innovations into production within the knowledge domain. this role is designed for impact, and we believe our best work happens when we connect. while we operate a flexible model, we expect you to spend time on site (at our offices or a client location) for collaboration sessions, customer meetings, and internal workshops. requirements what success looks like • extensive experience designing and leading the development of large-scale distributed data and/or ml backend systems. • hands-on experience with etl pipeline design and optimization for complex data sets is a strong advantage. • deep familiarity with technologies such as apache beam, pub/sub, redis, and other large-scale data processing frameworks. • expertise in backend development with python and scala; knowledge of node.js or golang is a plus. • proficient with both sql and nosql databases, and experience with data warehousing solutions. • demonstrated experience building robust apis (rest, graphql) and operating in modern cloud environments (gcp preferred), using kubernetes, docker, ci/cd, and observability tools. • proven ability to lead and influence engineering direction across teams and functions, particularly in a data-centric and ml-driven environment. • strong communication skills and the ability to align diverse technical stakeholders around a cohesive vision for data quality and knowledge extraction. benefits we believe in supporting our team members both professionally and personally. here's how we invest in you: compensation and financial wellbeing • competitive base salary. • matching pension scheme (up to 5%) from day one. • discretionary company bonus scheme. • 4 x annual salary death in service coverage from day one. • employee referral scheme. • tech scheme. health and wellness • private medical insurance from day one. • optical and dental cash back scheme. • help@hand app: access to remote gps, second opinions, mental health support, and physiotherapy. • eap service. • cycle to work scheme. work-life balance and growth • 36 days annual leave (inclusive of bank holidays). • an extra paid day off for your birthday. • ten paid learning days per year. • flexible working hours. • market-leading parental leave. • sabbatical leave (after five years). • work from anywhere (up to 3 weeks per year). • industry-recognised training and certifications. • bonusly employee recognition and rewards platform. • clear opportunities for career development. • length of service awards. • regular company events. diversity and inclusion at qodea, we champion diversity and inclusion. we believe that a career in it should be open to everyone, regardless of race, ethnicity, gender, age, sexual orientation, disability, or neurotype. we value the unique talents and perspectives that each individual brings to our team, and we strive to create a fair and accessible hiring process for all.",greater manchester,Machine Learning Engineer,"['cloud', 'data pipeline', 'etl', 'excel', 'gcp', 'machine learning', 'natural language processing', 'python', 'r', 'scala', 'sql']","['cloud', 'data pipeline', 'etl', 'excel', 'gcp', 'machine learning', 'natural language processing', 'python', 'r', 'scala', 'sql']",
lead machine learning engineer,ge vernova,"we are seeking a lead machine learning (ml) engineer with solid experience typically gained over at least 5 years in large multinational manufacturing environments, ideally within the energy, smart infrastructure, or industrial automation sectors. the ideal candidate has a proven track record of independently leading and delivering ml projects in complex, data-intensive ecosystems. in this position, you will be responsible for leading end-to-end ml initiatives, from problem framing and data preparation to model development, optimization, and deployment across edge and cloud platforms. you will independently drive ml project execution, ensuring technical excellence, scalability, and measurable business impact. you will collaborate closely with r&d, product teams, and other business units to support the development of innovative, reliable, and high-performance data-driven solutions. job description essential responsibilities: • lead the design, development, and deployment of scalable ai/ml models for grid innovation applications in the energy, smart infrastructure, or industrial automation sectors. • create innovative analytics to optimize grid system performance and product differentiation. • develop ai/ml applications for customer-driven use cases, including predictive maintenance and load forecasting. • validate and verify ai/ml proof-of-concepts in real-world environments, ensuring they meet the diverse needs of our customers. • monitor, maintain, and optimize deployed ai/ml models to continuously enhance their accuracy and performance. • manage the collection, structuring, and analysis of data to enable seamless ai/ml applications. • ensure that models are production-ready and continuously improve in line with emerging needs and technologies. • embrace mlops principles to streamline the deployment and updating of ml models in production. • collaborate closely with cross-functional teams to identify business challenges and deliver ai-driven solutions that are efficient, equitable, and scalable. • integrate ai/ml solutions effortlessly into grid automation systems, whether in the cloud or at the edge. must-have requirements: • experience typically gained over +5 years in large multinational companies within the energy sector or related industrial domains such as smart infrastructure or industrial automation. • master’s or phd in computer science, information technology, electrical engineering, or a related field. • solid foundation in ai/ml techniques, including supervised, unsupervised, and reinforcement learning, deep learning, and large language models (llms). • experience with ml frameworks such as tensorflow, pytorch, and scikit-learn. • hands-on experience deploying ml models in production environments using mlops principles. • expertise in relevant ai/ml applications, such as predictive maintenance, load forecasting, or optimization. • proficiency in programming languages such as python, r, matlab, or c++. • familiarity with cloud platforms (aws, azure, google cloud) and microservices architecture. nice-to-have requirements: • experience with data modeling, containerization (docker, kubernetes), and distributed computing (spark, scala). • familiarity with graphdb, mongodb, sql/nosql, and other dbms technologies. • understanding of system automation, protection, and diagnostics in relevant sectors. • experience with deep learning algorithms, reinforcement learning, nlp, and computer vision in applicable domains. • excellent communication, organizational, and problem-solving skills, with a strong emphasis on teamwork, collaboration, and fostering inclusive environments. at ge vernova - grid automation, you will have the opportunity to work on cutting-edge projects that shape the future of energy. we offer a collaborative environment where your expertise will be valued, and your contributions will make a tangible impact. join us and be part of a team that is driving innovation and excellence in control systems. about gev grid solutions: at gev grid solutions we are electrifying the world with advanced grid technologies. as leaders in the energy space our goal is to accelerate the transition for a more energy efficient grid to full fill the needs of tomorrow. with a focus on growth and sustainability ge grid solutions plays a pivotable role in integrating renewables onto the grid to drive to carbon neutral. in grid solutions we help enable the transition for a greener more reliable grid. ge grid solutions has the most advanced and comprehensive product and solutions portfolio within the energy sector. why we come to work: at gev, our engineers are always up for the challenge - and we’re always driven to find the best solution. our projects are unique and interesting, and you’ll need to bring a solution-focused, positive approach to each one to do your best. surrounded by committed, loyal colleagues, if you can dare to bring your ingenuity and desire to make an impact, you’ll be exposed to game-changing, diverse projects that truly allow you to play your part in the energy transition. what we offer: a key role in a dynamic, international working environment with a large degree of flexibility of work agreements competitive benefits, and great development opportunities - including private health insurance. additional information relocation assistance provided: no",west midlands,Machine Learning Engineer,"['aws', 'azure', 'c++', 'cloud', 'computer vision', 'deep learning', 'excel', 'google cloud', 'machine learning', 'matlab', 'nlp', 'python', 'pytorch', 'r', 'scala', 'scikit-learn', 'spark', 'sql', 'tensorflow']","['aws', 'azure', 'c++', 'cloud', 'computer vision', 'deep learning', 'excel', 'google cloud', 'machine learning', 'matlab', 'nlp', 'python', 'pytorch', 'r', 'scala', 'scikit-learn', 'spark', 'sql', 'tensorflow']",
machine learning engineer - tech lead,kaluza,"job title: machine learning engineer - tech lead location: london, bristol, edinburgh salary: £72,000 - £85,250 reporting to: head of data science & products kaluza is the energy intelligence platform, turning energy complexity into seamless coordination. we help energy companies overcome today's challenges while accelerating the shift to a clean, electrified future. our platform orchestrates millions of real-time decisions across homes, devices, markets and grids. by combining predictive algorithms with human-centred design, kaluza makes clean energy dependable, affordable and adaptive to everyday life. with teams across europe, north america, asia and australia, and a joint venture with mitsubishi corporation in japan, we power leading companies including ovo, agl and engie, as well as innovators like volvo and volkswagen. at kaluza we embrace a flexible, hybrid work model that balances autonomy with the power of in-person connection. many of our teams find value in coming together regularly to collaborate, strengthen relationships, and accelerate progress. we're focused on shaping thoughtful, team-driven approaches that support both business impact and individual well-being. where in the world of kaluza will i be working? you'll be part of the centralised kaluza ml team and wider data community where you'll share knowledge, support other mles, analysts and product teams. you'll be developing optimisation, ml algorithms and genai solutions across kaluza. what will i be doing? data is the foundation of everything we do, and to deliver our vision we need curious, tenacious people who can turn this data into strategy and actions with their expertise. as an mle at kaluza, you'll help product teams identify patterns and solve challenges with data. projects include forecasting, recommenders and helpdesk ticket classification. key responsibilities include: • develop ml and genai solutions: design and implement machine learning using python, leveraging data technologies such as databricks, kafka, and the aws cloud environment. our architecture is based on microservices, allowing for dynamic deployment of new features. • productionise algorithms: deploy algorithms into production environments where they can be tested with customers and continuously improved. you'll automate workflows, monitor performance, and maintain data science products using best practices for tooling, alerting, and version control (e.g., git). • contribute to a collaborative data science culture: share your knowledge and experience with the wider team. you'll play a key role in fostering an ml / ai community that values openness, collaboration, and innovation. • identify opportunities for impact: help uncover new opportunities where ml/ai can add value across our products and services. this includes asking the right questions, identifying high-impact areas, and contributing to the broader data strategy. ideally you will have: • proven experience leading teams in real-world ml / ai projects, with a strong understanding of core algorithms, data structures, and model performance evaluation. • proficiency in python, including libraries such as scikit-learn, pandas, numpy, and others commonly used in the ml ecosystem. • hands-on experience with genai apis and tools, including deployment and integration of genai solutions into production systems. • strong analytical and problem-solving skills, with the ability to guide teams through complex challenges while keeping business impact in focus. • experience across the full ml lifecycle, including data preprocessing, model training, evaluation, deployment, and monitoring in production environments. • expertise with mlops tools and practices (e.g., mlflow, sagemaker, docker, ci/cd pipelines), and the ability to set standards and best practices for the team. • excellent communication and presentation skills, capable of clearly articulating technical results and strategic implications to both technical and non-technical stakeholders, including senior leadership. • demonstrated track record of stakeholder engagement, leading cross-functional collaboration with product, engineering, and business teams. • solid foundation in statistics, including techniques such as hypothesis testing, significance testing, and probability theory. • comfortable working in an agile environment, driving iterative development cycles and mentoring cross-functional teams. • some experience with scala is a plus. why might this role not suit you? we are going through a period of significant evolution which is exciting and with it brings lots of opportunity and challenging work, which is not for everyone. to be successful in this role, you will be excellent at operating in ambiguous, changing environments, balancing multiple priorities simultaneously and get enjoyment from making the complex, simple. we're on a mission, we build together, we're inclusive, we get it done, we communicate with purpose.* our values are not words on a wall — they are at the heart of our culture. they are how we make decisions and how we treat each other. they are concrete and clear, and reflect what we as people, and as a business, really care about. kaluza's vision is to power a world where net-zero is within everyone's reach. would you be interested in joining us to help achieve this? from us you'll get • pension scheme • discretionary bonus scheme • private medical insurance + virtual gp • life assurance • access to furthr - a climate action app • free mortgage advice and eye tests • perks at work - access to thousands of retail discounts • 5% flex fund to spend on the benefits you want most • 26 days holiday • flexible bank holidays, giving you an additional 8 days which you can choose to take whenever you like • progressive leave policies with no qualifying service periods, including 26 weeks full pay if you have a new addition to your family • dedicated personal learning and home office budgets • flexible working — we trust you to work in a way that suits your lifestyle • and more… even better? you'll have access to these benefits from day 1 when you join! we want the best people we're keen to meet people from all walks of life — our view is that the more inclusive we are, the better our work will be. we want to build teams which represent a variety of experiences, perspectives and skills, and we recognise talent on the basis of merit and potential. we understand some people may not apply for jobs unless they tick every box. but if you're excited about joining us and think you have some of what we're looking for, even if you're not 100% sure, we'd still love to hear from you. find out more about working in kaluza on our careers page and linkedin.",london,Machine Learning Engineer,"['aws', 'classification', 'cloud', 'databricks', 'excel', 'kafka', 'machine learning', 'numpy', 'pandas', 'python', 'r', 'recommender', 'scala', 'scikit-learn', 'statistics']","['aws', 'classification', 'cloud', 'databricks', 'excel', 'kafka', 'machine learning', 'numpy', 'pandas', 'python', 'r', 'recommender', 'scala', 'scikit-learn', 'statistics']","£72,000–£85,250 a year"
machine learning engineer (applied ai) (100% remote in emea),artificial intelligence jobs,"about the job testlio’s fully managed crowdsourced testing platform, powered by our proprietary intelligence technology – leoai enginetm, integrates expert, on‑demand testers directly into your release process. ship faster and more confidently with global coverage across 600,000+ devices, 800+ payment methods, 150+ countries, and 100+ languages. to learn more, visit www.testlio.com . we are hiring a staff machine learning scientist (applied ai) to help design, build, and scale testlio’s next generation of ai‑powered data products. you’ll join our product team and will apply advanced machine learning, data science, and statistical methods to transform the rich data from our software testing platform into actionable insights for our customers. your work will directly influence how global engineering and product teams understand product quality, user experience, and business impact. this is a hands‑on, high‑impact opportunity to shape the future of ai at testlio while working in a remote, collaborative, and fast‑paced environment. why will you love this job? • * build from the ground up. you’ll be part of the team shaping testlio’s new ai product line, creating real‑world solutions that customers will use to make smarter product decisions. • high‑impact data. our platform generates unique and complex datasets from software testing at scale, giving you access to rich, real‑world data to design and deploy meaningful models. • experiment + innovate. you’ll have freedom to test, prototype, and deploy new approaches in nlp, predictive modeling, and data‑driven insights. • cross‑functional collaboration. you’ll work closely with engineers and customer‑facing teams to ensure your models deliver measurable business value. why will you love being a part of testlio? • * great culture: testlio is a female‑founded company, and half our team identifies as women. we’re proud of our inclusive, purpose‑driven culture where people genuinely enjoy collaborating. as part of our team (we call ourselves testlions), you’ll help create exceptional digital experiences for our customers, while also contributing to our freelance network. • remote work: our culture is built around remote work. we’ve created systems to allow us to successfully work together asynchronously as a fully remote and globally distributed team. testlio provides the tools and guidance for everyone to succeed in their careers in a fully remote setting. our working style encourages everyone to make decisions, communicate effectively, and work at a sustainable pace. • investment in you: your growth and well‑being matter to us. you’ll have flexible paid time off—including national holidays, personal days, and sick days—plus stock options so you can grow with testlio. we also provide a $300 annual learning stipend to support your personal and professional development. • winning business: testlio is growing, profitable, and cash‑strong. we are leading our industry with exceptional clients who provide us with a high nps score and a 4.7 rating on g2. our business model is global, enterprise, and subscription‑based. several of our largest clients have been with us for 7+ years, and many spend $500k+ / year with testlio. what would your day look like? • * partner with engineering leaders to define, design, and deliver ai data products. • explore and model complex datasets from testlio’s platform using statistical, ml, and deep learning techniques. • prototype and validate models, then work with engineering to deploy them into production at scale. • research and implement techniques in areas like nlp, anomaly detection, recommendation systems, and predictive analytics. • translate raw outputs into clear, actionable insights that are easy for customers to understand and use. • measure and improve model performance continuously to ensure accuracy, fairness, and scalability. • contribute to building testlio’s data science practice — influencing standards, tools, and best practices. what do you need to succeed? technical skills • * advanced degree (master’s or phd) in computer science, data science, statistics, or a related field. • 10+ years applying end‑to‑end machine learning and statistical modeling solutions to real‑world problems, ideally in saas or data products. • strong python skills and experience with deep learning frameworks such as pytorch and tensorflow. • hands‑on experience with nlp, predictive modeling, recommendation systems, anomaly detection, and training ml and deep learning models from scratch. • expertise in data wrangling, feature engineering, and managing large, complex datasets. • knowledge of large language models (llms) and small language models (slms), including architecture, training, fine‑tuning (lora, qlora, sft), and deployment strategies. • proven experience designing, building, and maintaining end‑to‑end ml pipelines and mlops frameworks, including model training, deployment, monitoring, and lifecycle management. • hands‑on experience designing, deploying, and maintaining ai agents, including multi‑agent systems, in production with robust apis and error handling. • proficiency with sql, data visualization tools, and cloud platforms such as aws or azure. human skills • * curiosity + creativity. you’re eager to experiment, learn, and push the boundaries of what’s possible with data. • business orientation. you can translate technical outputs into meaningful customer value. • collaborative spirit. you enjoy working across teams to deliver impact. • adaptability. you thrive in fast‑changing environments where priorities evolve as we scale. • mentorship mindset. you enjoy sharing knowledge and uplifting teammates. • growth mindset. you continuously refine your craft, stay current with advances in ai/ml, and seek feedback to improve. what is the application process? we do our best to bring on individuals who will be excited about their role and have the potential for a great future with testlio. since we are fully distributed, we’d like you to meet with multiple people from our organization to give you an idea of who you would be working with, your role expectations, etc. our interview process can take about 3 to 4 weeks to complete. • * application • recruiter interview • testgorilla assessment • ~3 team and stakeholder interviews, inclusive of skills demonstration • reference checks • offer diversity and inclusion testlio is an equal‑opportunity employer deeply committed to creating an inclusive environment for people of all backgrounds and identities. we are female‑founded, and 46% of our team members identify as women. for more information, see the dei section of our website. #li-remote #j-18808-ljbffr",glasgow,Machine Learning Engineer,"['aws', 'azure', 'cloud', 'deep learning', 'feature engineering', 'machine learning', 'nlp', 'python', 'pytorch', 'r', 'recommendation', 'scala', 'sql', 'statistics', 'tensorflow']","['aws', 'azure', 'cloud', 'deep learning', 'feature engineering', 'machine learning', 'nlp', 'python', 'pytorch', 'r', 'recommendation', 'scala', 'sql', 'statistics', 'tensorflow']",
"machine learning engineer | python | pytorch | natural language processing | llm | large language models | remote, europe",enigma,"machine learning engineer | python | pytorch | natural language processing | llm | large language models | remote, europe about us we build ai assistants that make technical knowledge instantly accessible. as a research engineer, you will work on advancing our system's ability to answer increasingly complex technical questions. our technology is already deployed on real-world developer documentation, where users can query information directly through an integrated ai assistant. the challenges you'll work on • evaluating a retrieval-augmented-generation (rag) system in production without labelled data • designing your own benchmarks from scratch • building an agentic retrieval pipeline that adapts between fast and more thorough query strategies • fine-tuning embeddings or reranking models what you'll do • collaborate closely with the core team and software engineers • stay up-to-date with the latest research and apply new ideas to real product challenges • design, run and analyse experiments to push system performance you might be a great fit if you have • a master's or phd in computer science, machine learning, mathematics, statistics, or a related field • strong knowledge of machine learning, deep learning (including llms), and natural language processing • hands-on experience training, fine-tuning, and deploying llms • experience working with vector databases, search indices, or data stores for retrieval use cases • significant experience building evaluation systems for search or language models • familiarity with information retrieval techniques (e.g., lexical search, dense vector search) • comfort working in a fast-moving environment with ambiguous problem spaces • a desire to learn more about ml research please note that this position is fully remote in europe but you must have the right to work in your country of residence. machine learning engineer | python | pytorch | natural language processing | llm | large language models | remote, europe",watford,Machine Learning Engineer,"['deep learning', 'machine learning', 'natural language processing', 'python', 'pytorch', 'r', 'statistics']","['deep learning', 'machine learning', 'natural language processing', 'python', 'pytorch', 'r', 'statistics']",
"machine learning engineer | python | pytorch | natural language processing | llm | large language models | remote, europe",enigma,"machine learning engineer | python | pytorch | natural language processing | llm | large language models | remote, europe about us we build ai assistants that make technical knowledge instantly accessible. as a research engineer, you will work on advancing our system’s ability to answer increasingly complex technical questions. our technology is already deployed on real-world developer documentation, where users can query information directly through an integrated ai assistant. the challenges you’ll work on • evaluating a retrieval-augmented-generation (rag) system in production without labelled data • designing your own benchmarks from scratch • building an agentic retrieval pipeline that adapts between fast and more thorough query strategies • fine-tuning embeddings or reranking models what you’ll do • collaborate closely with the core team and software engineers • stay up-to-date with the latest research and apply new ideas to real product challenges • design, run and analyse experiments to push system performance you might be a great fit if you have • a master's or phd in computer science, machine learning, mathematics, statistics, or a related field • strong knowledge of machine learning, deep learning (including llms), and natural language processing • hands-on experience training, fine-tuning, and deploying llms • experience working with vector databases, search indices, or data stores for retrieval use cases • significant experience building evaluation systems for search or language models • familiarity with information retrieval techniques (e.g., lexical search, dense vector search) • comfort working in a fast-moving environment with ambiguous problem spaces • a desire to learn more about ml research please note that this position is fully remote in europe but you must have the right to work in your country of residence. machine learning engineer | python | pytorch | natural language processing | llm | large language models | remote, europe",high wycombe,Machine Learning Engineer,"['deep learning', 'machine learning', 'natural language processing', 'python', 'pytorch', 'r', 'statistics']","['deep learning', 'machine learning', 'natural language processing', 'python', 'pytorch', 'r', 'statistics']",
machine learning research engineer - nlp / llm,redtech recruitment,"machine learning research engineer - nlp / llm an incredible opportunity for a machine learning research engineer to work on researching and investigating new concepts for an industry-leading, machine-learning software company in cambridge, uk. this unique opportunity is ideally suited to those with a ph.d. relating to classic machine learning and natural language processing and its application to an ever-advancing technical landscape. on a daily basis you will be working on the very cutting-edge of machine-learning including prototyping, building and implementing new approaches to ai problems. location: cambridge – 3 days in office / 2 days remote salary: highly competitve salary + comprehensive benefits requirements for machine learning research engineer • you will have a ph.d from a world-leading university in a computer science, physics, maths or similar (we are very keen to hear from those with a ph.d. directly related to nlp) • experience weighted more towards classic machine learning than ai engineering • strong knowledge in llms, nlp and machine learning / ai • excellent academics throughout including a minimum of a 2.1 degree from a leading university, aab at a-level • published papers • good understanding of software engineering concepts • experience with machine learning python frameworks such as pytorch, tensorflow and scikit-learn • data science experience including working with large data sets responsibilities for machine learning research engineer • join a team responsible for investigating the latest research in machine learning algorithms – largely within the nlp field along with some computer vision • make suggestions based on the feasibility of research to help shape future projects and products • prototyping research concepts • determining output quality, efficiency, and feasibility compared with other techniques what this offers • working for an industry-leading software company who have a fantastic track record of successfully hiring and training graduates • highly interesting work researching the next phases of cutting-edge, machine-learning software • a good remuneration and benefits package applications if you would like to apply for this unique ml engineer opportunity, we would love to hear from you. please send an up-to-date cv via the relevant link. we’re committed to creating an inclusive and accessible recruitment process. if you require reasonable adjustments for your application or during the review process, please highlight this by emailing (if this email address has been removed by the job-board, full details for contact are available on our website). keywords: python/ deep learning / machine learning / nlp / natural language processing / computer vision / research / post-doc / ph.d. / dphil / computer science / mathematics / physics / engineering/ text analytics / speech processing / software engineer / research / researching / journals / ai / artificial intelligence / machine vision / neural networks / developer / algorithm • ******************************************************************************* redtech recruitment ltd focus on finding roles for people leaving academia entering industry. even if the above role isn’t of interest, please visit our website to see our other opportunities. we are an equal opportunity employer and value diversity at our company. we do not discriminate on the basis of race, religion, colour, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.",cambridge,Machine Learning Engineer,"['computer vision', 'deep learning', 'excel', 'machine learning', 'natural language processing', 'nlp', 'python', 'pytorch', 'r', 'scikit-learn', 'tensorflow']","['computer vision', 'deep learning', 'excel', 'machine learning', 'natural language processing', 'nlp', 'python', 'pytorch', 'r', 'scikit-learn', 'tensorflow']",
"machine learning engineer, scaling world models, accelerated learning loop",wayve,"at wayve we're committed to creating a diverse, fair and respectful culture that is inclusive of everyone based on their unique skills and perspectives, and regardless of sex, race, religion or belief, ethnic or national origin, disability, age, citizenship, marital, domestic or civil partnership status, sexual orientation, gender identity, veteran status, pregnancy or related condition (including breastfeeding) or any other basis as protected by applicable law. about us founded in 2017, wayve is the leading developer of embodied ai technology. our advanced ai software and foundation models enable vehicles to perceive, understand, and navigate any complex environment, enhancing the usability and safety of automated driving systems. our vision is to create autonomy that propels the world forward. our intelligent, mapless, and hardware-agnostic ai products are designed for automakers, accelerating the transition from assisted to automated driving. in our fast-paced environment big problems ignite us—we embrace uncertainty, leaning into complex challenges to unlock groundbreaking solutions. we aim high and stay humble in our pursuit of excellence, constantly learning and evolving as we pave the way for a smarter, safer future. at wayve, your contributions matter. we value diversity, embrace new perspectives, and foster an inclusive work environment; we back each other to deliver impact. make wayve the experience that defines your career! the role science is the team that is advancing our end-to-end autonomous driving research. the team’s mission is to accelerate our journey to av2.0 and ensure the future success of wayve by incubating and investing in new ideas that have the potential to become game-changing technological advances for the company. the goal of this role is to build, scale, and optimise next-generation world model architectures (e.g. gaia and successors) and bridge them into high-throughput training infrastructure, enabling synthetic data and simulation to dramatically accelerate autonomy development. you will live at the intersection of model research, large-scale ml systems, and real-world deployment. you will both invent new generative architectures and make them trainable at scale (efficiently and reliably) so that synthetic environments can exceed reality in utility. key responsibilities: • design and implement performance improvements (tensor parallelism, pipeline parallelism etc) for large scale training. • profile and diagnose large-scale model training jobs to identify bottlenecks (gpu/compute, memory, i/o, communication) and optimise performance. • train large-scale temporal models on multi-modal data (video, lidar, vehicle telemetry), learning representations of complex real-world dynamics. • design experiments to understand model generalization, scaling behavior, and performance trade-offs between synthetic and real data. • define and track metrics and benchmarks for long-horizon prediction, scene fidelity, and planner integration. • challenge assumptions and drive innovation: propose bold ideas, conduct ablation studies, and question conventional approaches to training and evaluation. • collaborate with platform/engineering teams to align research prototypes with production-level infrastructure. about you: in order to set you up for success as an applied scientist at wayve, we’re looking for the following skills and experience. • established background in ml engineering or applied research. • hands-on experience optimizing large-scale training workloads (multi-gpu / multi-node), including parallelism, kernel-level optimizations, memory and i/o bottlenecks. • proven experience working cross-functionally between research teams and platform / infrastructure teams. • demonstrated background working with high-dimensional temporal or spatial-temporal data (e.g., video, multi-sensor fusion). • strong python and pytorch engineering fundamentals, and experience building research-grade production tools. • ability to take bold ideas, run experiments, and iterate quickly. • ability to work collaboratively in a fast-paced, innovative, interdisciplinary team environment. desirable: • deep knowledge of generative modelling (e.g., auto-regressive, diffusion, or vaes) • experience in avs, robotics, simulation, or other embodied ai domains. why join us • work on transformative technology with real-world impact on mobility, safety, and ai. • access massive driving datasets, cutting-edge infrastructure, and world-class research talent. • be part of a high-trust, high-autonomy team that values creativity, experimentation, and deep thinking. • publish, share, and shape the future of generative ai for autonomy. we understand that everyone has a unique set of skills and experiences and that not everyone will meet all of the requirements listed above. if you’re passionate about self-driving cars and think you have what it takes to make a positive impact on the world, we encourage you to apply. for more information visit careers at wayve. to learn more about what drives us, visit values at wayve disclaimer: we will not ask about marriage or pregnancy, care responsibilities or disabilities in any of our job adverts or interviews. however, we do look to capture information about care responsibilities, and disabilities among other diversity information as part of an optional dei monitoring form to help us identify areas of improvement in our hiring process and ensure that the process is inclusive and non-discriminatory.",london,Machine Learning Engineer,"['excel', 'experimentation', 'python', 'pytorch', 'r']","['excel', 'experimentation', 'python', 'pytorch', 'r']",
machine learning engineer,get2talent,"our client is a leading technology company developing groundbreaking laser communications systems and software-defined networking platforms for the aerospace industry. with technology acquired from google, they’re at the forefront of innovation in satellite and airborne mesh networks, cislunar, and deep-space communications transforming how the world connects across land, sea, air, and space.the opportunitywe’re looking for an experienced machine learning engineer to join our client’s spacetime team in the uk. this is a hybrid role combining ml research and development, where you’ll apply cutting-edge algorithms to solve complex temporospatial networking and resource management challenges.you’ll work in a highly collaborative, international environment — developing real-world ai applications that help shape the future of planetary-scale communication systems.key responsibilitiesresearch and develop state-of-the-art machine learning algorithms for network orchestration problemsbuild and manage ml training infrastructure using kubernetes clusters and modern mlops toolingwrite clear documentation and reports for novel algorithms developed by the teamintegrate ai models with the broader spacetime platform to ensure seamless functionalityact as a technical communication expert, interacting with customers and partners on ml-related technologiesrequired skills & experiencemaster’s or phd in computer science, mathematics, statistics, or a related ml disciplineproficiency in python and at least one deep learning library (pytorch, tens",exeter,Machine Learning Engineer,"['deep learning', 'machine learning', 'python', 'pytorch', 'r', 'statistics']","['deep learning', 'machine learning', 'python', 'pytorch', 'r', 'statistics']",£100k–£150k a year
machine learning engineer,corsearch,"at corsearch, we are dedicated to creating a world where consumers can trust the choices they make. as a global leader in trademark and brand protection, we partner with businesses to safeguard their most valuable assets in an increasingly complex digital environment. our comprehensive solutions, powered by ai-driven data and deep analytics, enable brands to establish, monitor, and protect their presence against infringement and counterfeiting. why choose corsearch? • innovative solutions: we combine cutting-edge technology with expert judgment to deliver market-leading services in trademark clearance, brand protection, and anti-counterfeiting. • global impact: trusted by over 5,000 customers worldwide, including 73 of fortune's top 100 companies, our work has a meaningful impact on businesses and consumers alike. • collaborative culture: with a team of over 1,900 professionals across multiple global offices, you'll be joining an inclusive environment where diverse perspectives thrive. • mission-driven purpose: our commitment to protecting consumers and their trust in brands drives everything we do, making corsearch a force for good in the world. the role as an ml engineer you'll play a crucial role in advancing our shared mission of protecting and enhancing the world's most valuable brands. you will design and optimize ai models that power our state-of-the-art image search engine and other innovative solutions. collaborating with corsearch's global team of data scientists, engineers, and brand protection experts, you'll help drive innovation in brand protection and online content monitoring, ensuring our clients stay ahead in a rapidly evolving digital landscape. responsibilities and duties • develop and optimize machine learning models with a focus on computer vision (e.g., image recognition, object detection) and nlp (e.g., text classification, sentiment analysis, entity recognition). • train and fine-tune deep learning models (e.g., cnns, rnns, transformers) for tasks such as reverse image search, image-text matching, and online content analysis. • collaborate with cross-functional teams to improve product and system performance. • research and implement state-of-the-art algorithms and techniques to improve the accuracy and performance of ai solutions. • build scalable data pipelines for the ingestion and processing of image and text data for model training and evaluation. • continuously monitor the performance of models in production, identify bottlenecks, and propose optimizations. essential • proven experience as a machine learning engineer with at least 3 years of professional experience. • strong proficiency in machine learning frameworks such as catboost, tensorflow, pytorch, or keras. • experience with image processing, feature extraction, and text preprocessing techniques. • solid understanding of algorithms for tasks such as object detection, image classification, text extraction, and sentiment analysis. • hands-on experience with large datasets and data pipelines for training and deploying ml models. • strong analytical and problem-solving skills, with the ability to translate business challenges into technical solutions. • full professional english proficiency corsearch is an equal opportunity and inclusive employer and does not tolerate discrimination of any kind. we are committed to creating a diverse and inclusive workplace where all employees feel valued, respected, and supported. we welcome applications from all individuals regardless of race, nationality, religion, gender, gender identity or expression, sexual orientation, age, disability, or any other protected characteristic. together, we are working proactively to build a workplace where everyone can belong and be at their best selves. together, we make an impact. #ukfrance trusted by over 5,000 customers worldwide, corsearch delivers ai-powered data, deep analytics, and professional services that support brands to market their assets, drive growth, and optimise brand presence against infringement. corsearch enables brand owners to discover, monitor and control the use and misuse of their brands and associated products and services online and is the industry market leader. corsearch does this in an increasingly complex global digital environment, with online brand protection solutions that cover everything from anti-counterfeiting to anti-piracy.",wakefield,Machine Learning Engineer,"['classification', 'computer vision', 'data pipeline', 'deep learning', 'keras', 'machine learning', 'nlp', 'pytorch', 'r', 'scala', 'tensorflow']","['classification', 'computer vision', 'data pipeline', 'deep learning', 'keras', 'machine learning', 'nlp', 'pytorch', 'r', 'scala', 'tensorflow']",
senior machine learning engineer - applied ai,testlio,"about the role join testlio, a pioneering crowdsourced testing platform, to drive the creation of cutting-edge ai-powered data products! as a senior machine learning engineer specializing in applied ai, you will be part of a dynamic product team dedicated to transforming extensive software testing data into invaluable insights for our clients. this role is hands-on and significantly impacts how engineering and product teams perceive product quality, user experience, and business outcomes. why you will love this opportunity: • innovative environment: collaborate with a brilliant team on building our next-generation ai solutions that empower customers to make informed product decisions. • access to unique data: work with rich datasets from diverse software testing scenarios, enabling you to design and deploy impactful models. • room for creativity: experiment with and implement cutting-edge techniques in natural language processing, predictive analytics, and more. • collaborative culture: partner with engineers and customer-facing teams to ensure measurable business outcomes from your models. what makes testlio special: • inclusive culture: testlio is a female-founded company, proudly fostering an environment where diverse voices thrive. • remote-first work: embrace a global, fully remote working culture designed for effective asynchronous collaboration. • personal growth investment: enjoy flexible paid time off and an annual learning stipend to enhance your personal and professional growth. • thriving business: join a profitable firm with a stellar reputation and long-standing clientele, driving a strong growth trajectory. your typical day: • collaborate with engineering leaders to define and deliver ai-driven data products. • analyze complex datasets using statistical models and advanced machine learning techniques. • prototype and validate models, and oversee their deployment at scale with engineering support. • investigate and apply methodologies in nlp, anomaly detection, and predictive modeling. • transform raw data into user-friendly, actionable insights for clients. • continuously monitor and refine model performance to maintain accuracy and scalability. • help shape our data science practices, contributing to best practices and standards. requirements for success: • educational background: advanced degree (master’s or phd) in computer science, data science, or a relevant field. • experience: minimum of 10 years in machine learning and statistical modeling, particularly in saas environments. • technical proficiency: proficient in python, with experience in frameworks like pytorch and tensorflow. • expertise: strong hands-on experience with nlp, predictive modeling, and building ml pipelines. • data management skills: adept in data wrangling, feature engineering, and handling complex datasets. • knowledge of llms: familiarity with large language models and deployment strategies. • cloud integration: experience working with cloud platforms such as aws or azure. soft skills: • curiosity: a passion for experimentation and exploring new data possibilities. • business acumen: ability to link technical solutions to customer value. • team player: enjoy collaborating across diverse teams for impactful results. • adaptability: thrive in a fast-paced, evolving environment. • mentorship: committed to uplifting teammates through knowledge sharing. • growth-oriented: stay current with ai/ml trends and actively seek constructive feedback. application process: we value candidates who are excited about their role and have the potential to grow with testlio. as a fully distributed team, you'll engage with multiple colleagues to understand your prospective role and our expectations. our interview process usually spans 3 to 4 weeks, including: • application submission • recruiter interview • assessment • multiple team interviews with skills demonstration • reference checks • offer discussion diversity and inclusion: testlio is an equal-opportunity employer committed to fostering an inclusive workplace for individuals of all backgrounds. we value diversity and are proud of our inclusive company culture. #li-remote",leeds,Machine Learning Engineer,"['aws', 'azure', 'cloud', 'experimentation', 'feature engineering', 'machine learning', 'natural language processing', 'nlp', 'python', 'pytorch', 'r', 'scala', 'tensorflow']","['aws', 'azure', 'cloud', 'experimentation', 'feature engineering', 'machine learning', 'natural language processing', 'nlp', 'python', 'pytorch', 'r', 'scala', 'tensorflow']",
machine learning engineer,faculty,"why faculty? we established faculty in 2014 because we thought that ai would be the most important technology of our time. since then, we’ve worked with over 350 global customers to transform their performance through human-centric ai. you can read about our real-world impact here. we don’t chase hype cycles. we innovate, build and deploy responsible ai which moves the needle - and we know a thing or two about doing it well. we bring an unparalleled depth of technical, product and delivery expertise to our clients who span government, finance, retail, energy, life sciences and defence. our business, and reputation, is growing fast and we’re always on the lookout for individuals who share our intellectual curiosity and desire to build a positive legacy through technology. ai is an epoch-defining technology, join a company where you’ll be empowered to envision its most powerful applications, and to make them happen. about the team our defence team is focused on building and embedding human-centered ai solutions which give our nation a competitive edge in the defence sector. we collaborate with our clients to bring ethical, reliable and cutting-edge ai to high-stakes situations and maintain the balance of global powers essential to our liberty. because of the nature of the work we do with our defence clients, you will need to be eligible for uk security clearance (sc) and willing to work up to three days per week on site with these customers, which may require travel to locations outside of our london base. about the role join us as a machine learning engineer to deliver bespoke, impactful ai solutions for our diverse clients. you will be instrumental in bringing machine learning out of the lab and into the real world, contributing to scalable software architecture and defining best practices. working with clients, and cross-functional teams, you'll ensure technical feasibility and timely delivery of high-quality, production-grade ml systems. what you'll be doing: • building and deploying production-grade ml software, tools, and infrastructure. • creating reusable, scalable solutions that accelerate the delivery of ml systems. • collaborating with engineers, data scientists, and commercial leads to solve critical client challenges. • leading technical scoping and architectural decisions to ensure project feasibility and impact. • defining and implementing faculty’s standards for deploying machine learning at scale. • acting as a technical advisor to customers and partners, translating complex ml concepts for stakeholders. who we're looking for: • you understand the full machine learning lifecycle and have experience operationalising models built with frameworks like scikit-learn, tensorflow, or pytorch. • you possess strong python skills and solid experience in software engineering best practices. • you bring hands-on experience with cloud platforms and infrastructure (e.g., aws, azure, gcp), including architecture and security. • you've worked with container and orchestration tools such at docker & kubernetes to build and manage applications at scale • you are comfortable with core ml concepts, including probability, statistics, and common learning techniques. • you're an excellent communicator, able to guide technical teams and confidently advise non-technical stakeholders. • you thrive in a fast-paced environment, and enjoy the autonomy to own scope, solve and delivery solutions our interview process • talent team screen (30 minutes) • pair programming interview (90 minutes) • system design interview (90 minutes) • commercial interview (60 minutes) our recruitment ethos we aim to grow the best team - not the most similar one. we know that diversity of individuals fosters diversity of thought, and that strengthens our principle of seeking truth. and we know from experience that diverse teams deliver better work, relevant to the world in which we live. we’re united by a deep intellectual curiosity and desire to use our abilities for measurable positive impact. we strongly encourage applications from people of all backgrounds, ethnicities, genders, religions and sexual orientations. some of our standout benefits: • unlimited annual leave policy • private healthcare and dental • enhanced parental leave • family-friendly flexibility & flexible working • sanctus coaching • hybrid working (2 days in our old street office, london) if you don’t feel you meet all the requirements, but are excited by the role and know you bring some key strengths, please do apply or reach out to our talent acquisition team for a confidential chat - talent@faculty.ai please know we are open to conversations about part-time roles or condensed hours.",bournemouth,Machine Learning Engineer,"['aws', 'azure', 'cloud', 'excel', 'gcp', 'machine learning', 'python', 'pytorch', 'r', 'scala', 'scikit-learn', 'statistics', 'tensorflow']","['aws', 'azure', 'cloud', 'excel', 'gcp', 'machine learning', 'python', 'pytorch', 'r', 'scala', 'scikit-learn', 'statistics', 'tensorflow']",
machine learning engineer ii,hudl,"at hudl, we build great teams. we hire the best of the best to ensure you’re working with people you can constantly learn from. you’re trusted to get your work done your way while testing the limits of what’s possible and what’s next. we work hard to provide a culture where everyone feels supported, and our employees feel it—their votes helped us become one of newsweek’s top 100 global most loved workplaces. we think of ourselves as the team behind the team, supporting the lifelong impact sports can have: the lessons in teamwork and dedication; the influence of inspiring coaches; and the opportunities to reach new heights. that’s why we help teams from all over the world see their game differently. our products make it easier for coaches and athletes at any level to capture video, analyze data, share highlights and more. ready to join us? your role we’re looking for a machine learning engineer ii to join our applied machine learning team and deliver new experiences and valuable insights to our coaches, athletes and fans across hudl. you’ll drive game-changing initiatives that use cutting‑edge computer vision and deep learning at scale to shape the future of sports, from professional teams to local high schools. • deliver for customers at scale. you’ll contribute to ml models and systems on both cloud and edge environments, scaling to thousands of simultaneous sports matches. • collaborate. you’ll work in a cross‑functional team with data scientists and engineers to deliver end‑to‑end for our customers. for this role, we’re currently considering candidates who live within a commuting distance of our office in london. but with our flexible work policy, there aren’t any current requirements for the number of days you come to the office. must‑haves • technical expertise. you have hands‑on experience in c++, python, and several of the following areas: kubernetes, pytorch, mlops (automated re‑training, drift monitoring) tensorrt, nvidia deepstream/gstreamer, and aws. • a proven track record. you know how to focus on products, delivering impactful ai/ml products through close collaboration with partners. • strong communicator. you can easily and clearly express yourself. you’re able to convey technical concepts and trade‑offs to cross‑functional stakeholders. • growth mindset. you’ve picked up new technologies and domains on the job. you appreciate ambiguous work that has many possible implementation options because it gives you the chance to identify the best solution while balancing quality, consistency and value to customers. nice‑to‑haves • sports industry experience. if you’ve used ai/ml in sports to generate data and/or create insights, that’s a plus. • video experience. you know how to run video encoding, decoding, and transmission at scale (e.g. hls, webrtc, and ffmpeg). • accelerator experience. you’ve developed gpu kernels and/or ml compilers (e.g., cuda, opencl, tensorrt plugins, mlir, tvm, etc). • real‑time experience. you’ve optimized systems to meet strict utilization and latency requirements with tools such as nvidia nsight. • embedded experience. you’ve used embedded socs, e.g., nvidia jetson, qualcomm, etc. • foundational models experience. you’ve fine‑tuned visual language models or large language models for new domains and know how to apply them to novel genai applications. • embedded experience. when it comes to optimizing, deploying and monitoring ml models for socs e.g. nvidia, qualcomm, etc., you know how it all works. our role • champion work‑life harmony. we’ll give you the flexibility you need in your work life (e.g., flexible vacation time above any required statutory leave, company‑wide holidays and timeout (meeting‑free) days, remote work options and more) so you can enjoy your personal life too. • guarantee autonomy. we have an open, honest culture and we trust our people from day one. your team will support you, but you’ll own your work and have the agency to try new ideas. • encourage career growth. we’re lifelong learners who encourage professional development. we’ll give you tons of resources and opportunities to keep growing. • provide an environment to help you succeed. we’ve invested in our offices, designing incredible spaces with our employees in mind. but whether you’re at the office or working remotely, we’ll provide you the tech you need to do your best work. • support your wellbeing. depending on location, we offer medical and retirement benefits for employees—but no matter where you’re located, we have resources like our employee assistance program and employee resource groups to support your mental health. compensation the base salary range for this role is displayed below—starting salaries will typically fall near the middle of this range. we make compensation decisions based on an individual’s experience, skills and education in line with our internal pay equity practices. base salary range inclusion at hudl hudl is an equal opportunity employer. through our actions, behaviors and attitude, we’ll create an environment where everyone, no matter their differences, feels like they belong. we offer resources to ensure our employees feel safe bringing their authentic selves to work, including employee resource groups and communities. but we recognize there’s ongoing work to be done, which is why we track our efforts and commitments in annual inclusion reports. we also know imposter syndrome is real and the confidence gap can get in the way of meeting spectacular candidates. please don’t hesitate to apply—we’d love to hear from you. voluntary self‑identification, gender and race we’d like to know how well we’re doing to ensure diversity, equity and inclusion in our recruitment processes. to help us reach this goal, we’d appreciate if you’d take our diversity questionnaire. your voluntary choice to complete it will create a separate, confidential record of your name, where you learned about the role you applied for, and information about protected characteristics (uk: the equity act 2010). this information will be automatically removed from your profile prior to reviewing your application. your data is kept strictly confidential and won’t be used as a part of the selection process. we’ll only use it to measure our recruitment activity and to make reasonable adjustments for disabled employees. u.s. equal opportunity employment information individuals seeking employment at hudl are considered without regards to race, color, religion, national origin, age, sex, marital status, ancestry, physical or mental disability, veteran status, gender identity, or sexual orientation. you are being given the opportunity to provide the following information in order to help us comply with federal and state equal employment opportunity/affirmative action record keeping, reporting, and other legal requirements. completion of the form is entirely voluntary . whatever your decision, it will not be considered in the hiring process or thereafter. any information that you do provide will be recorded and maintained in a confidential file. the regulation (eu) 2016/679 and the applicable national data processing regulations (together, the “applicable legislation”) for candidates applying to the positions in the eu give rights to individuals in respect of personal data held about them by others and as directed by the above applicable legislation hudl provides you the following information: • hudl and its affiliates act as data controllers in relation to the personal data hudl collects in connection with its recruiting and hiring processes, as well as with data subprocessors engaged by hudl to help manage those processes. hudl’s headquarters and its sub processes are located in the usa. if you are located outside of the usa, your personal data will be transferred to the usa once you submit it through our careers site. • diversity information is defined by the dpa as “sensitive”. the information you give will be held on both manual and electronic systems by hudl.",feltham,Machine Learning Engineer,"['aws', 'c++', 'cloud', 'computer vision', 'deep learning', 'machine learning', 'python', 'pytorch', 'r']","['aws', 'c++', 'cloud', 'computer vision', 'deep learning', 'machine learning', 'python', 'pytorch', 'r']",
"machine learning engineer | york, uk",hiscox ltd,"machine learning engineer job type: permanent build a brilliant future with hiscox position: machine learning engineer reporting to: lead data scientist location: york type: permanent machine learning engineer as a machine learning engineer at hiscox, you will play a key role in building and maintaining the infrastructure that supports the deployment of machine learning models across the london market business unit. you'll work closely with data scientists, platform engineers, and developers to ensure seamless integration and scalable, production-grade machine learning solutions. you'll be joining an award-winning team, recognised for its pioneering collaboration with google to deliver the market's first ai-enhanced lead underwriting solution, a milestone that reflects our commitment to innovation, impact, and excellence in applying machine learning to real-world insurance challenges. this is a hands-on engineering role focused on developing apis, infrastructure, and deployment pipelines for machine learning models. you'll be expected to write clean, reusable code, follow best practices in cloud and software engineering, and contribute to the operational excellence of our machine learning systems. in addition to strong engineering skills, you'll bring a solid understanding of data science principles. you should be comfortable reading, questioning, and interpreting machine learning models to ensure they are deployed appropriately and effectively. your ability to bridge the gap between model development and production deployment will be key to delivering robust, high-impact machine learning solutions. you'll be expected to understand and implement methodologies from the ml ops lifecycle. you'll also be expected to work in an agile environment, contributing to iterative development cycles, collaborating across disciplines, and adapting quickly to changing requirements. key responsibilities • develop and maintain infrastructure for deploying ml models in both real-time and batch environments. • build and maintain python apis (flask/fastapi) to serve ml models. • collaborate with cross discipline engineers to integrate ml services into user-facing applications. • work with platform engineers to align with infrastructure best practices and ensure scalable deployments. • review pull requests and contribute to code quality across the mle team. • monitor and maintain cloud-based ml services, ensuring reliability and performance. • design and implement ci/cd pipelines for ml model deployment. • write unit tests and follow object-oriented programming principles to ensure maintainable code. • support data modelling and cloud networking tasks as needed. • contribute to the development and improvement to our model registry, including tracking and implementation of model discontinuation upgrades and model monitoring. person specification to succeed in this role, you'll typically have: • bachelor's/master's degree in a quantitative field (e.g., computer science, statistics, mathematics, physics, engineering) or equivalent. • hands on experience in machine learning engineering, including deploying, monitoring, and maintaining ml models in production environments. • experience in finance or insurance is an advantage but not required. • solid experience as a python developer, ideally in a machine learning engineering context. • strong understanding of software engineering best practice. • experience with tdd. • experience with infrastructure as code tools like terraform. • hands on experience with cloud platforms (gcp, aws, or azure). • familiarity with containerization using docker and orchestration of deployments. • experience with ci/cd tools and git-based development workflows. • understanding of api operations monitoring and logging. • strong problem-solving skills and ability to work independently on technical tasks. • familiarity with agile methodologies and experience working in agile teams. key technical skills • python (flask/fastapi, oop, unit testing). • machine learning model experience (neural networks, random forests etc.). • terraform or similar infrastructure as code (iac) tools. • gcp, aws, or azure. • docker and containerised deployments. • ci/cd pipelines. • git based development. • sql. • cloud/api operations monitoring. • cloud networking is an advantage but not required. work with amazing people and be part of a unique culture",york,Machine Learning Engineer,"['aws', 'azure', 'cloud', 'excel', 'gcp', 'machine learning', 'python', 'r', 'scala', 'sql', 'statistics']","['aws', 'azure', 'cloud', 'excel', 'gcp', 'machine learning', 'python', 'r', 'scala', 'sql', 'statistics']",
senior machine learning engineer,takeaway,"ready for a challenge? then just eat takeaway.com might be the place for you. we’re a leading global online food delivery platform, and our vision is to empower everyday convenience. whether it’s a friday-night feast, a post-gym poke bowl, or grabbing some groceries, our tech platform connects tens of millions of customers with hundreds of thousands of restaurant, grocery and convenience partners across the globe. about this role we are looking for a senior machine learning engineer to join a cross functional team, focussing on growing our product algorithmic recommendations within just eat takeaway.com. your team will focus on evolving existing machine learning and ai capabilities across the platform, improving those capabilities, and innovating new ones for the future. as a senior engineer you will drive our architecture, write highly scalable and testable code, mentor engineers and challenge our teams to strive for excellence. you will work closely with a large number of teams, both internal and external, with inner-sourced development our standard way of working. ownership is one of the core engineering principles in our organisation - we write it and we own it. engineers are expected to take responsibility for their work from discovery to production, ensuring the ongoing reliability and stability of our systems. location: hybrid - 3 days a week from our office & 2 days working from home reporting to: technology manager these are some of the key ingredients to the role: • collaborate extensively with data scientists, product managers, and backend engineers to bridge the gap between model development and production systems. • lead the architectural design of end-to-end ml systems, from data ingestion and training pipelines to real-time inference and monitoring infrastructure. • transform innovative data science prototypes into robust, scalable, and secure production software, taking ownership of the ""path to production."" • drive the adoption of mlops best practices (ci/cd for ml, model versioning, feature stores) to accelerate the feedback loop for data scientists. • effectively communicate the complexities of ml systems (e.g., latency vs. accuracy trade-offs) to technical and non-technical stakeholders. • build and maintain a strong network across the data and engineering organizations to ensure ml systems integrate seamlessly with the wider platform. • lead projects, mentor peers, and advocate for engineering excellence within the data science domain. what will you bring to the table? • proficiency in python and a strong understanding of software engineering principles (oo design, patterns, testing) applied to machine learning. • demonstrable experience designing and operating ml systems in production (not just training models in notebooks), including familiarity with serving patterns (e.g., rest apis, batch inference, event-driven). • experience with orchestration tools (e.g., airflow, dagster) and cloud-native ml platforms (e.g., aws sagemaker, gcp vertex ai). • ability to influence decision-making regarding infrastructure and tooling, balancing ""build vs. buy"" discussions. • strong knowledge of infrastructure as code (terraform) and containerization (docker/kubernetes). • familiarity with data engineering fundamentals (sql, distributed data processing) to debug and optimize data flows. • a proactive mindset to automate manual processes and a passion for improving the developer experience for data scientists. at jet, this is on the menu: our teams forge connections internally and work with some of the best-known brands on the planet, giving us truly international impact in a dynamic environment. fun, fast-paced and supportive, the jet culture is about movement, growth and about celebrating every aspect of our jeters. thanks to them we stay one step ahead of the competition. inclusion, diversity & belonging no matter who you are, what you look like, who you love, or where you are from, you can find your place at just eat takeaway.com. we’re committed to creating an inclusive culture, encouraging diversity of people and thinking, in which all employees feel they truly belong and can bring their most colourful selves to work every day. what else is cooking? want to know more about our jeters, culture or company? have a look at our career site where you can find people's stories, blogs, podcasts and more jet morsels. #li-cb2",bristol (+1 other),Machine Learning Engineer,"['airflow', 'aws', 'cloud', 'excel', 'gcp', 'machine learning', 'python', 'r', 'recommendation', 'scala', 'sql']","['airflow', 'aws', 'cloud', 'excel', 'gcp', 'machine learning', 'python', 'r', 'recommendation', 'scala', 'sql']",
"machine learning engineer, inference optimisation",wayve,"at wayve we're committed to creating a diverse, fair and respectful culture that is inclusive of everyone based on their unique skills and perspectives, and regardless of sex, race, religion or belief, ethnic or national origin, disability, age, citizenship, marital, domestic or civil partnership status, sexual orientation, gender identity, veteran status, pregnancy or related condition (including breastfeeding) or any other basis as protected by applicable law. about us founded in 2017, wayve is the leading developer of embodied ai technology. our advanced ai software and foundation models enable vehicles to perceive, understand, and navigate any complex environment, enhancing the usability and safety of automated driving systems. our vision is to create autonomy that propels the world forward. our intelligent, mapless, and hardware-agnostic ai products are designed for automakers, accelerating the transition from assisted to automated driving. in our fast-paced environment big problems ignite us—we embrace uncertainty, leaning into complex challenges to unlock groundbreaking solutions. we aim high and stay humble in our pursuit of excellence, constantly learning and evolving as we pave the way for a smarter, safer future. at wayve, your contributions matter. we value diversity, embrace new perspectives, and foster an inclusive work environment; we back each other to deliver impact. make wayve the experience that defines your career! the role as a machine learning engineer for the ml optimisation team, you’ll optimise our driving model to run efficiently in millions of vehicles with consumer-grade gpus and accelerators. the focus of this team is to run large transformer-based models efficiently in low-cost, low-power edge devices to enable wayve’s first driving product - e.g. via methods such as distillation, efficient architectures, pruning, quantisation and others. this is an exciting opportunity to own and lead high impact, early stage projects at wayve with the ultimate goal of enabling product deployments onto millions of customer vehicles around the world. key responsibilities: • develop state of the art techniques in distillation, quantisation, pruning and other ml compression methods to achieve latency, bandwidth and compute targets • understand how ml compression methods affect driving behaviour • stay up to date with latest papers, conferences etc • you’ll collaborate closely with other model developers and scientists across the business to drive innovation and delivery • you’ll have the opportunity to develop new skills and experience about you essential • 2+ years working as an mle • experience working on optimisation problems with hard latency and/or resource constraints • strong engineering background • proficiency with pytorch • excellent interpersonal and communication skills desirable • experience with ml on edge, e.g. automotive, drones, ar/vr, iot, etc • experience with any of the following: quantisation, distillation, pruning, sparsity methods, nas, efficient architectures, etc • experience with nvidia and qualcomm socs and frameworks are valuable, but not required we understand that everyone has a unique set of skills and experiences and that not everyone will meet all of the requirements listed above. if you’re passionate about self-driving cars and think you have what it takes to make a positive impact on the world, we encourage you to apply. for more information visit careers at wayve. to learn more about what drives us, visit values at wayve disclaimer: we will not ask about marriage or pregnancy, care responsibilities or disabilities in any of our job adverts or interviews. however, we do look to capture information about care responsibilities, and disabilities among other diversity information as part of an optional dei monitoring form to help us identify areas of improvement in our hiring process and ensure that the process is inclusive and non-discriminatory.",london,Machine Learning Engineer,"['excel', 'machine learning', 'pytorch', 'r']","['excel', 'machine learning', 'pytorch', 'r']",
"machine learning engineer | python | pytorch | natural language processing | llm | large language models | remote, europe",enigma,"machine learning engineer | python | pytorch | natural language processing | llm | large language models | remote, europe about us we build ai assistants that make technical knowledge instantly accessible. as a research engineer, you will work on advancing our system's ability to answer increasingly complex technical questions. our technology is already deployed on real-world developer documentation, where users can query information directly through an integrated ai assistant. the challenges you'll work on • evaluating a retrieval-augmented-generation (rag) system in production without labelled data • designing your own benchmarks from scratch • building an agentic retrieval pipeline that adapts between fast and more thorough query strategies • fine-tuning embeddings or reranking models what you'll do • collaborate closely with the core team and software engineers • stay up-to-date with the latest research and apply new ideas to real product challenges • design, run and analyse experiments to push system performance you might be a great fit if you have • a master's or phd in computer science, machine learning, mathematics, statistics, or a related field • strong knowledge of machine learning, deep learning (including llms), and natural language processing • hands-on experience training, fine-tuning, and deploying llms • experience working with vector databases, search indices, or data stores for retrieval use cases • significant experience building evaluation systems for search or language models • familiarity with information retrieval techniques (e.g., lexical search, dense vector search) • comfort working in a fast-moving environment with ambiguous problem spaces • a desire to learn more about ml research please note that this position is fully remote in europe but you must have the right to work in your country of residence. machine learning engineer | python | pytorch | natural language processing | llm | large language models | remote, europe",doncaster,Machine Learning Engineer,"['deep learning', 'machine learning', 'natural language processing', 'python', 'pytorch', 'r', 'statistics']","['deep learning', 'machine learning', 'natural language processing', 'python', 'pytorch', 'r', 'statistics']",
machine learning engineer,job spark,"role: machine learning engineer location: remote we are hiring machine learning engineers to support advanced benchmarking and performance-improvement projects across real-world ml workloads. this part-time, remote opportunity is ideal for early-career ml engineers or ml-focused phd candidates who want to work on high-impact, structured tasks with measurable outcomes. what you’ll do • draft detailed natural-language plans and code implementations for ml tasks • convert novel ml problems into agent-executable tasks for rl environments • identify failure modes and apply golden patches to llm-generated trajectories what you’ll bring • 0–2 years of ml engineering experience or a phd with ml coursework • strong skills in python and ml libraries (tensorflow, xgboost, scikit-learn, etc.) • experience with data prep, training, and model evaluation • bonus: contributions to ml benchmarks • must be u.s.-based opportunity details • ~20 hours/week • remote & asynchronous • duration through dec 22, with potential extension into 2026 compensation • $80–$120/hour • weekly payments via stripe connect • independent contractor role how to apply submit your resume, complete a short system design session, and finish the ml screening form. apply now!",hempstead,Machine Learning Engineer,"['machine learning', 'python', 'r', 'scikit-learn', 'tensorflow', 'xgboost']","['machine learning', 'python', 'r', 'scikit-learn', 'tensorflow', 'xgboost']",
machine learning engineer,electus recruitment solutions,"machine learning engineerwe are working in partnership with a leading technology organisation to recruit an experienced machine learning engineer. the successful candidate will design, train, and optimise high-performance machine learning models, build and manage datasets for real-world sensing systems, and clearly communicate technical work to stakeholders. based in north somerset, you'll be part of a collaborative and forward-thinking environment that encourages rapid prototyping and experimentation. you’ll work within multidisciplinary teams to develop, validate and deploy machine learning models to meet challenging customer requirements.key responsibilities develop and train neural network models using frameworks such as pytorch and tensorflowselect and adapt model architectures to meet specific project requirementsbuild, curate, and manage training datasets, including data augmentation, feature extraction, and labellingconduct model training, validation, and performance optimisationcollaborate with software engineers to integrate models into embedded or application environmentsproduce clear technical documentation and communicate findings to technical and non-technical audiences requirements degree in computer science, engineering, mathematics, or related fieldstrong development skills in python and c/c++experience with neural network architectures including rnns, transformers, and vector quantisationin-depth knowledge of machine learning architectures and training algorithmsexperience in model training, quantisation, and conversion for inferencehands-on experience with data preparation, augmentation, and feature extractionexcellent communication and technical writing skillsuk national, eligible for security clearanc",nan,Machine Learning Engineer,"['c++', 'excel', 'experimentation', 'machine learning', 'python', 'pytorch', 'r', 'tensorflow']","['c++', 'excel', 'experimentation', 'machine learning', 'python', 'pytorch', 'r', 'tensorflow']",£38k–£70k a year
manager - lead ai engineer,kpmg,"job description base location: london plus network of 20 offices nationally: the kpmg audit technology team is dedicated to building cutting-edge solutions in close collaboration with the audit function. we blend audit expertise with the latest technology, enabling us to understand the challenges our customers face daily and develop indispensable products that simplify their lives while promoting audit quality. as a crucial member of the team, you will collaborate with a talented mix of cloud & devops engineers, product owners/managers, solution architects, data engineers, business analysts, and testing specialists. together, we build, deliver, and manage a portfolio of truly exciting products. in recent years, our products' size and scale have rapidly expanded, leading to significant growth in our technology capability. there's never been a better time to join us. with our ambitious growth plans, your future here is something to get excited about. as a valued team member, you'll be expected to stay current with the tech field and the latest trends in audit delivery. why join kpmg as a lead ai engineer? the audit technology team at kpmg is driving innovation at the intersection of auditing and advanced technological solutions, reshaping the future of audit delivery. by combining expertise in artificial intelligence, data engineering, data analytics, and software development, the team is revolutionising the auditing process to deliver smarter, faster, and more reliable outcomes. our mission is to design and implement robust, intelligent, and scalable technologies that not only streamline workflows but also enhance audit quality and generate actionable insights for auditors and clients. through harnessing the power of cutting-edge tools, we aim to transform traditional audit practices into dynamic, forward-thinking processes that are built for the complexities of the modern business environment. our team, supported by kpmg’s global network, serves as the driving force behind this transformative journey. focused on innovation, this team is dedicated to engineering solutions today that anticipate the challenges and opportunities of tomorrow, ensuring that audit services remain at the forefront of technological progression. what will you be doing? as a lead ai engineer, you will report to ai engineering squad lead and lead the technical delivery of ai projects within your squad, collaborating with data scientists, engineers, developers, cloud architects, and audit professionals to create scalable ai systems that improve audit quality, efficiency, and insights. from developing robust proof-of-concepts to deploying enterprise-grade solutions, you will apply your expertise in ai engineering, cloud platforms, and technologies such as generative ai, azure, databricks to embed intelligence into critical audit workflows and products. you will mentor junior engineers, promote best practices, and foster a culture of collaboration, innovation, and continuous improvement. you will stay at the forefront of ai engineering trends, advocate for modern development methodologies, and drive knowledge-sharing across both the technology and audit domains. due to the nature of the role significant time may be spent at client sites/kpmg offices. responsibilities lead by example: lead by example in a hands-on capacity, actively contributing to codebases and technical decisions while mentoring junior engineers. scalable ai engineering: develop and deploy production-grade ai systems tailored to audit applications. contribute to architectural decisions and solution governance. write clean, efficient, and scalable code that adheres to software engineering principles, mlops practices, and cloud-native development. ai solution delivery: own the implementation of ml pipelines, apis, and data integration workflows. operational excellence: contribute to defining reusable development patterns, enforcing coding standards, and implementing mlops best practices that support version control, performance optimisation, and maintainability. cross-disciplinary collaboration: work side-by-side with data scientists, product managers, platform engineers, and qa teams to align on technical requirements, delivery timelines, and integration plans. provide hands-on contributions to ensure ai capabilities are smoothly embedded within core audit platforms and services. capability building & knowledge sharing : contribute to internal capability-building initiatives and empower team members and the broader audit technology function by sharing practical skills and innovative techniques for adopting and adapting ai effectively. what will you need to do it? minimum requirements experience: significant professional experience in backend development as a senior role. languages/frameworks: strong proficiency in python with hands-on experience in asynchronous programming, concurrency and multithreading. api expertise: proven experience in building and integrating apis, with expertise in api documentation and schema definition using openapi/swagger. strong understanding of restful api design, authentication/authorization standards, and api lifecycle best practices; familiarity with microsoft graph api is a plus. experience & knowledge requirements good knowledge of generative ai, machine learning, deep learning, natural language processing or other relevant ai fields. proven track record of designing, developing, and deploying ai systems in production environments. proficient in python and key ml libraries (e.g. pytorch, pyspark, scikit-learn, hugging face transformers). hands-on experience with modern data platforms and ai tooling such as azure ml, databricks, mlflow, langchain, langgraph. proven experience with modern engineering practices git, version control, unit testing and containerisation. familiarity with agile work methodologies and tools like jira and confluence. behavioural attributes and skills: strong communication skills, with the ability to explain technical concepts to varied audiences in a clear and accessible manner. qualifications: bachelor (preferably master or phd) in computer science, artificial intelligence, data science, statistics, engineering, or a related technical field. advanced certifications in ai, machine learning, cloud computing or data engineering are highly advantageous - desirable professional accounting qualification preferred, however not a requirement - desirable to discuss this or wider technology roles with our recruitment team, all you need to do is apply, create a profile, upload your cv and begin to make your mark with kpmg. our locations: we are open to talk to talent across the country but our core tech hubs for this role are: glasgow leeds london canary wharf manchester with 20 sites across the uk, we can potentially facilitate office work, working from home, flexible hours, and part-time options. if you have a need for flexibility, please register and discuss this with our team. find out more: within tech and engineering we have a range of divisions and specialisms. click the links to find out more below: technology and engineering at kpmg : its her future women in tech programme: kpmg workability and disability confidence: for any additional support in applying, please click the links to find out more: applying to kpmg: tips for interview: kpmg values: kpmg competencies: kpmg locations and faq:",southampton,Machine Learning Engineer,"['azure', 'cloud', 'data analytics', 'databricks', 'deep learning', 'excel', 'machine learning', 'natural language processing', 'pyspark', 'python', 'pytorch', 'r', 'scala', 'scikit-learn', 'spark', 'statistics']","['azure', 'cloud', 'data analytics', 'databricks', 'deep learning', 'excel', 'machine learning', 'natural language processing', 'pyspark', 'python', 'pytorch', 'r', 'scala', 'scikit-learn', 'spark', 'statistics']",
machine learning robotics engineer,iconsultera,"job description • the machine learning robotics engineer is responsible for developing ai-driven algorithms and software that enable intelligent robotic systems to perceive, learn, and make autonomous decisions. • this role combines robotics, computer vision, machine learning, and control systems to enhance automation capabilities across industries such as manufacturing, logistics, healthcare, automotive, and aerospace. • this is a remote uk-based role, with occasional travel for on-site integration, testing, or customer support. key responsibilities ai & machine learning development • design, develop, and deploy machine learning models for robotic perception, navigation, manipulation, and decision-making. • build deep learning models for object detection, classification, segmentation, tracking, and scene understanding. • implement reinforcement learning and behaviour-planning algorithms to improve autonomous performance. robotics & autonomous systems • develop core robotics algorithms including slam (simultaneous localization and mapping), motion planning, obstacle avoidance, sensor fusion, and control strategies. • integrate ml-based models into robotics platforms such as amrs, agvs, robotic arms, drones, and autonomous vehicles. • collaborate with robotics hardware teams to optimize ai models for real-world robotic constraints. software engineering • write production-quality code in python and/or c++, using frameworks such as ros/ros2, opencv, pcl, and robotics middleware. • develop simulation environments using gazebo, isaac sim, webots, unity, carla, or airsim to test ml and robotics algorithms. • optimise models for real-time performance on edge computing platforms (nvidia jetson, arm devices, embedded gpu systems). testing, validation & deployment • create test plans and evaluate system performance using both simulated and real-world data. • conduct experiments to validate algorithm accuracy, robustness, and scalability. • document findings, performance metrics, and recommendations for system improvements. cross-functional collaboration • work closely with ai researchers, robotics engineers, product managers, and software teams across the uk and eu. • participate in design reviews, technical discussions, and collaborative problem-solving. • support integration and deployment of robot autonomy solutions for clients and internal teams. required qualifications • bachelor's/master's degree in robotics, computer science, ai, machine learning, electrical engineering, or related fields. • 2–5+ years hands-on experience in: • robotics software development • machine learning or computer vision • autonomous systems engineering • strong expertise in: python, c++, ros/ros2, ml frameworks (tensorflow, pytorch), slam, path planning & control systems • deep learning for perception tasks • experience working with sensors: lidar, rgb/depth cameras, imus, radar, gps.",colchester,Machine Learning Engineer,"['c++', 'classification', 'computer vision', 'deep learning', 'machine learning', 'python', 'pytorch', 'r', 'recommendation', 'scala', 'tensorflow']","['c++', 'classification', 'computer vision', 'deep learning', 'machine learning', 'python', 'pytorch', 'r', 'recommendation', 'scala', 'tensorflow']",
remote machine learning engineer - high compensation contract,crossing hurdles,"crossing hurdles is a dynamic recruitment firm connecting experienced backend engineers with leading ai research labs to enhance intelligent systems through innovative coding benchmark development. organization: mercor position: backend software engineer (go) type: hourly contract compensation: $200/hour median average pay (base $90/hour plus lucrative bonuses per approved task) location: remote duration: 1 month commitment: part-time, 10-40 hours/week, fully remote and asynchronous role responsibilities (training support will be provided) • develop and validate coding benchmarks in go by carefully curating issues, solutions, and test suites from real-world repositories. • ensure benchmark tasks include comprehensive unit and integration tests for effective solution verification. • maintain consistency and scalability of benchmark task distribution to facilitate smooth processes. • provide structured feedback on solution quality and clarity to improve project outcomes. • debug, optimize, and document benchmark code for enhanced reliability and reproducibility of results. ideal qualifications • strong experience as a backend software engineer, machine learning engineer, or applied data scientist. • degree in software engineering, computer science, or a related field. • strong proficiency in the go programming language. • experience with debugging, testing, and validating code to ensure high standards. • comfortable with technical writing and possess keen attention to detail. application process: (takes 20 min) • upload your resume. • participate in an ai interview based on your resume (15 min). • submit the application form.",sheffield,Machine Learning Engineer,"['machine learning', 'r', 'scala']","['machine learning', 'r', 'scala']",
"lead data scientist, machine learning engineer 2025",aimpoint digital,"aimpoint digital is a premier analytics consulting firm with a mission to drive business value for clients through expertise in data strategy, data analytics, decision sciences, and data engineering and infrastructure. this position is within our decision sciences practice which focuses on delivering solutions via machine learning and statistical modelling. what you will do as a part of aimpoint digital, you will focus on enabling clients to get the most out of their data. you will work with all levels of the client organization to build value driving solutions that extract insights and then train them on how to manage and maintain these solutions. typical solutions will utilize machine learning, artificial intelligence, statistical analysis, automation, optimization, and/or data visualizations. as a lead data scientist, you will be expected to work independently on client engagements, take part in the development of our practice, aid in business development, and contribute innovative ideas and initiatives to our company. as a lead data scientist you will: • become a trusted advisor working with clients to design end-to-end analytical solutions • work independently to solve complex data science use-cases across various industries • design and develop feature engineering pipelines, build ml & ai infrastructure, deploy models, and orchestrate advanced analytical insights • write code in sql, python, and spark following software engineering best practices • collaborate with stakeholders and customers to ensure successful project delivery who we are looking for we are looking for collaborative individuals who want to drive value, work in a fast-paced environment, and solve real business problems. you are a coder who writes efficient and optimized code leveraging key databricks features. you are a problem-solver who can deliver simple, elegant solutions as well as cutting-edge solutions that, regardless of complexity, your clients can understand, implement, and maintain. you genuinely think about the end-to-end machine learning pipeline as you generate robust solutions. you are both a teacher and a student as we enable our clients, upskill our teammates, and learn from one another. you want to drive impact for your clients and do so through thoughtfulness, prioritization, and seeing a solution through from brainstorming to deployment. in particular you have these traits: • degree in computer science, engineering, mathematics, or equivalent experience. • experience with building high quality data science models to solve a client's business problems • experience with managing stakeholders and collaborating with customers • strong written and verbal communication skills required • ability to manage an individual workstream independently • 3+ years of experience developing and deploying ml models in any platform (azure, aws, gcp, databricks etc.) • ability to apply data science methodologies and principles to real life projects • expertise in software engineering concepts and best practices • self-starter with excellent communication skills, able to work independently, and lead projects, initiatives, and/or people • willingness to travel want to stand out? • consulting experience • databricks machine learning associate or machine learning professional certification • familiarity with traditional machine learning tools such as python, sklearn, xgboost, sparkml, etc • experience with deep learning frameworks like tensorflow or pytorch • knowledge of ml model deployment options (e.g., azure functions, fastapi, kubernetes) for real-time and batch processing. • experience with ci/cd pipelines (e.g., devops pipelines, github actions) • knowledge of infrastructure as code (e.g., terraform, arm template, databricks asset bundles • understanding of advanced machine learning techniques, including graph-based processing, computer vision, natural language processing, and simulation modeling • experience with generative ai and llms, such as llamaindex and langchain • understanding of mlops or llmops • familiarity with agile methodologies, preferably scrum we are actively seeking candidates for full-time, remote work within the us, uk or colombia.",anywhere,Machine Learning Engineer,"['aws', 'azure', 'computer vision', 'data analytics', 'databricks', 'deep learning', 'excel', 'feature engineering', 'gcp', 'machine learning', 'natural language processing', 'python', 'pytorch', 'r', 'sklearn', 'spark', 'sql', 'tensorflow', 'xgboost']","['aws', 'azure', 'computer vision', 'data analytics', 'databricks', 'deep learning', 'excel', 'feature engineering', 'gcp', 'machine learning', 'natural language processing', 'python', 'pytorch', 'r', 'sklearn', 'spark', 'sql', 'tensorflow', 'xgboost']",
senior software/machine learning engineer - apple music,apple,"here at apple new ideas have a way of becoming great products very quickly, and innovation never stops. bring passion and dedication to your job and there's no telling what you could accomplish. the music ml team at apple media products is responsible for personalisation and recommendation in apple music. we are looking for an experienced software engineer to help design and run our customer-facing recommendation services reliably, efficiently, and with dedication to delivering relevant and diverse music to our users. music is our passion, and our aim is to connect artists to music lovers like ourselves. we build amazing experiences for our users while respecting their privacy. our team is a friendly bunch of people from more than 10 countries. we help each other grow and realise the best work for our users. we’re also part of a larger team at apple services engineering and beyond. we work together to realise a single unified vision, making use of apple’s unique integration of hardware, software, and services. and although services are a bigger part of apple’s business than ever before, these teams remain small, nimble, and cross-functional, offering great opportunities to collaborate and grow. description the music ml team within apple services engineering is looking for a great software engineer to build and improve the features and services driving apple music personalisation. our team is responsible for providing personalised features for apple music including home, new, radio, and personal mixes. our work includes data analysis, large-scale offline pipelines, machine-learned model training and inference, and online services to provide real-time personalised experiences. our growing london-based team builds and evolves global-scale, leading-edge dynamic data systems. we are responsible for the full lifecycle: collaboration with the product team, system design, implementation, continuous optimisation and improvement. "",""responsibilities"":"" building products and services for millions of users with a focus on great customer experience and privacy developing complex systems that integrate data from many sources to deliver on-the-fly personalisation with low latency tuning performance considering both latency and throughput deploying our systems globally for improved resiliency and end-user experience collaborating across teams to take new user-facing features from conception to production working within our team to develop and deploy massive datasets to improve personsalised features prototyping algorithm changes and launching a/b tests to measure changes to personalised products if this sounds exciting to you, we’d love to hear from you. adding a cover letter to explain your passion for this particular job is greatly appreciated. preferred qualifications a vision of how to engineer modern ml-driven pipelines, apis and services at scale extensive experience with object-oriented languages such as java, c++, and python minimum qualifications hands-on experience crafting highly scalable recommendations systems understanding of concurrency, algorithms and object oriented programming a vision of how to engineer modern ml-driven systems that allow for fast iteration cycles effective collaboration with researchers to improve recommendation algorithms at apple, we’re not all the same. and that’s our greatest strength. we draw on the differences in who we are, what we’ve experienced and how we think. because to create products that serve everyone, we believe in including everyone. therefore, we are committed to treating all applicants fairly and equally. as a registered disability confident employer, we will work with applicants to make any reasonable accommodations. apple will consider for employment all qualified applicants with criminal backgrounds in a manner consistent with applicable law. learn more",london,Machine Learning Engineer,"['c++', 'data analysis', 'java', 'python', 'r', 'recommendation', 'scala']","['c++', 'data analysis', 'java', 'python', 'r', 'recommendation', 'scala']",
machine learning/data engineer,sanderson,"machine learning/data engineer £700-750/day overall assignment rate to umbrella fully remote 3-6 month initial apply today to join a forward-thinking, tech-driven ftse 100 organisation using data science and ai to enhance customer experience, optimise supply chains and drive sustainable growth. with 40% of sales from sustainable products, this is a company that combines scale, innovation and purpose. as a machine learning engineer, you'll help maintain the stability and performance of core data and ml systems across europe. this technical engineering role focuses on reliability, optimisation and critical fixes, ideal if you excel at investigating and debugging complex data flows and ml issues in live production environments. we're looking for individuals with: experience: proven background as a machine learning engineer.technical skills: strong in sql and python (pandas, scikit-learn, jupyter, matplotlib).data transformation & manipulation: experience with airflow, dbt and kubeflowcloud: experience with gcp and vertex ai (developing ml services).expertise: solid understanding of computer science fundamentals and time-series forecasting.machine learning: strong grasp of ml and deep learning algorithms (e.g. logistic regression, random forest, xgboost, bert, lstm, nlp, transfer learning). reasonable adjustments: respect and equality are core values to us. we are proud of the diverse and inclusive community we have built, and we welcome applications from people of all backgrounds and perspectives. our success is driven by our people, united by the spirit of partnership to deliver the best resourcing solutions for our clients. if you need an",london,Machine Learning Engineer,"['airflow', 'cloud', 'dbt', 'deep learning', 'excel', 'gcp', 'machine learning', 'matplotlib', 'nlp', 'pandas', 'python', 'r', 'regression', 'scikit-learn', 'sql', 'xgboost']","['airflow', 'cloud', 'dbt', 'deep learning', 'excel', 'gcp', 'machine learning', 'matplotlib', 'nlp', 'pandas', 'python', 'r', 'regression', 'scikit-learn', 'sql', 'xgboost']",£700–£750 a day
"machine learning engineering lead, london",isomorphic labs,"machine learning engineering lead, london isomorphic labs job overviewisomorphic labsbiotechnology united kingdom , londonapply now machine learning engineering lead, london this is an extraordinary opportunity to join a new alphabet company that will reimagine drug discovery through a computational and ai-first approach. we are assembling a world-class, multi-disciplinary team who want to drive forward groundbreaking innovations. as one of the first members of this pioneering organisation, you will play a meaningful role in building this team, embodying an inspiring, collaborative and entrepreneurial culture. this early-stage venture is on a mission to accelerate the speed, increase the efficacy and lower the cost of drug discovery. you’ll be working at the cutting edge of the new era of ‘digital biology’ and advancing a new type of biotech that will deliver transformative social impact for the benefit of millions of people. your impact as an engineering lead, you will build, grow and lead a talented team of platform engineers and software engineers. working closely with the ml research team, you and your team will develop an infrastructure platform for deploying and scaling cutting edge machine learning models and algorithms. you’ll be developing these through all stages from research grade to real world production use in pursuit of groundbreaking bio-pharmaceutical discoveries. this newly created role will require you to draw on your extensive experience and offers an exciting opportunity to carve out your contribution in this entrepreneurial environment. this will include working with other software engineering leads to develop a new platform that underpins the company technology and business strategy. what you will do • create a platform for the ml research team to conduct and accelerate ground-breaking research • ensure the models meet a high engineering standard with respect to architecture, scalability, maintainability and other operational characteristics • partner and collaborate with a diverse set of teams incl. science, research, product, business development and operations • build a high performing, nimble team of ml software engineers and platform engineers • provide technical leadership to the organisation and own core technical decisions (e.g. choice of tooling, infrastructure, and architectural design) skills and qualifications essential • strong foundations in software engineering with previous experience operating as a senior individual contributor with software architecture skills • strong experience with platform engineering • experience with a variety of infrastructure frameworks • experience with the full ml development lifecycle • experience working with and leading cross functional teams • experience partnering with research and product teams • experience building secure/scalable platforms/products on cloud • experience building, leading and coaching high performing, diverse engineering teams of ideally 5+ people • exposure to modern devops and sre best practices nice to have • pharma and/or biotech industry experience, ideally with a focus on drug discovery • familiarity with ml accelerator hardware • bachelor’s degree in computer science, a related technical field, or equivalent experience culture and values what does it take to be successful at isolabs? it's not about finding people who think and act in the same way, but we do have some shared values: thoughtfulthoughtful at iso is about curiosity, creativity and care. it is about good people doing good, rigorous and future-making science every single day. bravebrave at iso is about fearlessness, but it’s also about initiative and integrity. the scale of the challenge demands nothing less. determineddetermined at iso is the way we pursue our goal. it’s a confidence in our hypothesis, as well as the urgency and agility needed to deliver on it. because disease won’t wait, so neither should we. togethertogether at iso is about connection, collaboration across fields and catalytic relationships. it’s knowing that transformation is a group project, and remembering that what we’re doing will have a real impact on real people everywhere. creating an inclusive company we realise that to be successful we need our teams to reflect and represent the populations we are striving to serve. we’re working to build a supportive and inclusive environment where collaboration is encouraged and learning is shared. we value diversity of experience, knowledge, backgrounds and perspectives and harness these qualities to create extraordinary impact. we are committed to equal employment opportunities regardless of sex, race, religion or belief, ethnic or national origin, disability, age, citizenship, marital, domestic or civil partnership status, sexual orientation, gender identity, pregnancy or related condition (including breastfeeding) or any other basis protected by applicable law. if you have a disability or additional need that requires accommodation, please do not hesitate to let us know. hybrid working it’s hugely important for us to be able to share knowledge and establish relationships with each other, and we find it easier to do this if we spend time together in person. this is why we’ve decided to follow a hybrid model, and for full time positions we would require you to be able to come into the office 3 days a week (currently tue, wed, and one other day depending on which team you’re in). for part time positions this may vary. as an equal opportunities employer we are committed to building an equal and inclusive team. if you have additional needs that would prevent you from following this hybrid approach, we’d be happy to talk through these if you’re selected for an initial screening call. please note that when you submit an application, your data will be processed in line with our privacy policy. >> click to view other open roles at isomorphic labs",united kingdom,Machine Learning Engineer,"['cloud', 'machine learning', 'r', 'scala']","['cloud', 'machine learning', 'r', 'scala']",
machine learning engineer - bournemouth (hybrid),faculty,"about faculty faculty transforms organisational performance through safe, impactful and human-led ai. we are europe’s leading applied ai company, and saw its potential a decade ago - long before the current hype cycle. we founded in 2014 with our fellowship programme, training academics to become commercial data scientists. today, we provide over 300 global customers with industry-leading software, and bespoke ai consultancy for retail, healthcare, energy, and governmental organisations, as well as our award winning fellowship. our expertise and safety credentials are such that openai asked us to be their first technical partner, helping customers deploy cutting-edge generative ai safely. our high-impact work has saved lives through forecasting nhs demand during covid, produced green energy by routing boats towards the wind, slashed marketing spend by predicting customer spending habits, and kept children safe online. ai is an epoch-defining technology. we want people to join us who can help our customers reap its enormous benefits safely. what you'll be doing working in our defence business unit you will design, build, and deploy production-grade software, infrastructure, and mlops systems that leverage machine learning. the work you do will help our customers solve a broad range of high-impact problems in the defence and national security space - examples of which can be found here you are engineering-focused, with a keen interest and working knowledge of operationalised machine learning. you have a desire to take cutting-edge ml applications into the real world. you will develop new methodologies and champion best practices for managing ai systems deployed at scale, with regard to technical, ethical and practical requirements. you will support both technical and non-technical stakeholders to deploy ml to solve real-world problems. to enable this, we work in cross-functional teams with representation from commercial, data science, product management and design specialities to cover all aspects of ai product delivery. the machine learning engineering team is responsible for the engineering aspects of our customer delivery projects. as a machine learning engineer, you’ll be essential to helping us achieve that goal by: • building software and infrastructure that leverages machine learning; • creating reusable, scalable tools to enable better delivery of ml systems • working with our customers to help understand their needs • working with data scientists and engineers to develop best practices and new technologies; and • implementing and developing faculty’s view on what it means to operationalise ml software. we’re a rapidly growing organisation, so roles are dynamic and subject to change. your role will evolve alongside business needs, but you can expect your key responsibilities to include: • working in cross-functional teams of engineers, data scientists, designers and managers to deliver technically sophisticated, high-impact systems. • working with senior engineers to scope projects and design systems • providing technical expertise to our customers • technical delivery who we're looking for at faculty, your attitude and behaviour are just as important as your technical skill. we look for individuals who can support our values, foster our culture, and deliver for our organisation. we like people who combine expertise and ambition with optimism -- who are interested in changing the world for the better -- and have the drive and intelligence to make it happen. if you’re the right candidate for us, you probably: • think scientifically, even if you’re not a scientist - you test assumptions, seek evidence and are always looking for opportunities to improve the way we do things. • love finding new ways to solve old problems - when it comes to your work and professional development, you don’t believe in ‘good enough’. you always seek new ways to solve old challenges. • are pragmatic and outcome-focused - you know how to balance the big picture with the little details and know a great idea is useless if it can’t be executed in the real world. to succeed in this role, you’ll need the following - these are illustrative requirements and we don’t expect all applicants to have experience in everything (70% is a rough guide): • understanding of and interest in the full machine learning lifecycle, including deploying trained machine learning models developed using common frameworks such as scikit-learn, tensorflow, or pytorch • demonstrable experience of managing/ mentoring more junior members of the team • understanding of the core concepts of probability and statistics and familiarity with common supervised and unsupervised learning techniques • experience in software engineering including programming in python. • technical experience of cloud architecture, security, deployment, and open-source tools. hands-on experience required of at least one major cloud platform • demonstrable experience with containers and specifically docker and kubernetes • comfortable in a high-growth startup environment. • outstanding verbal and written communication. • excitement about working in a dynamic role with the autonomy and freedom you need to take ownership of problems and see them through to execution",blandford forum,Machine Learning Engineer,"['cloud', 'machine learning', 'python', 'pytorch', 'r', 'scala', 'scikit-learn', 'statistics', 'tensorflow']","['cloud', 'machine learning', 'python', 'pytorch', 'r', 'scala', 'scikit-learn', 'statistics', 'tensorflow']",
"applied ai, senior/staff forward deployed machine learning engineer - emea",mistral ai,"about mistral at mistral ai, we believe in the power of ai to simplify tasks, save time, and enhance learning and creativity. our technology is designed to integrate seamlessly into daily working life. we democratize ai through high-performance, optimized, open-source and cutting-edge models, products and solutions. our comprehensive ai platform is designed to meet enterprise needs, whether on-premises or in cloud environments. our offerings include le chat, the ai assistant for life and work. we are a dynamic, collaborative team passionate about ai and its potential to transform society. our diverse workforce thrives in competitive environments and is committed to driving innovation. our teams are distributed between france, usa, uk, germany and singapore. we are creative, low-ego and team-spirited. join us to be part of a pioneering company shaping the future of ai. together, we can make a meaningful impact. see more about our culture on https://mistral.ai/careers. about the job mistral ai is seeking a applied ai engineer to facilitate the adoption of its products among customers and collaborate with them to address complex technical challenges. the applied ai engineer is an integral part of our applied ai engineering team, which is dedicated to driving the successful deployment of mistral ai products and building complex enterprises use-cases. they work hand-in-hand with customers from the pre-sale stage to post-implementation, ensuring our solutions meet and exceed client expectations. in this role, you’ll manage customer relations involving multiple stakeholders (ceo/cto, data scientists, and software engineers) and function as a key resource in externalizing our research in production settings. what you will do - you’ll individually help deploy into production use cases with a considerable business impact across various industries. - you’ll work on state-of-the-art genai applications from consumer products to industrial use cases, driving with our customers a crucial technological transformation. - you’ll work in collaboration with our researchers, other ai engineers, product engineers on our most complex customer projects involving complex fine-tuning, state-of-the-art llm applications, and contributing to our open-source codebases our open source codebases for tasks such as inference and fine-tuning. - you’ll be involved in pre-sales calls to understand potential clients' needs, challenges, and aspirations. you will provide technical guidance on our products and explain mistral technologies to various stakeholders. - your collaboration with our product and science team to improve continuously our product and model capabilities based on customers’ feedback about you - you are fluent in english - you hold a phd / master in ai / data science or you’re self-made - you have 7-10+ years as a technical individual contributor (data scientist or software engineer) on ai-based products - you have experience in fine tuning llms, tackling advanced rag or agentic use cases - you have deep understanding of concepts and algorithms underlying machine learning and llms - you have a deep understanding of cloud infrastructure and how to deploy ai based products - you have deployed or built products with large scale user based - you hold strong communication skills with an ability to explain complex technical concepts in simple terms with technical and non-technical audiences ideally you have: - contributed to open-source projects in particular in the space of llm - experience as a forward deployed engineer, staff engineer machine learning engineer, staff data scientist. benefits we have local offices in paris, london, marseille, amsterdam, lausanne and singapore. france 💰 competitive cash salary and equity 🥕 food : daily lunch vouchers 🥎 sport : monthly contribution to a gympass subscription 🚴 transportation : monthly contribution to a mobility pass 🧑‍⚕️ health : full health insurance for you and your family 🍼 parental : generous parental leave policy 🌎 visa sponsorship uk 💰 competitive cash salary and equity 🚑 insurance 🚴 transportation: reimburse office parking charges, or 90gbp/month for public transport 🥎 sport: 90gbp/month reimbursement for gym membership 🥕 meal voucher: £200 monthly allowance for its meals 💰 pension plan: smartpension (percentages are 5% employee & 3% employer) about the team at mistral ai, we are a tight-knit, nimble team dedicated to bringing our cutting-edge ai technology to the world. our mission is to make ai ubiquitous and open. our team values are reflected in our product values: - cool: we have a tongue-in-cheek way of looking at things, it’s hard to describe but you know it when you see it - precision: our designs mirror the rigor and excellence that underpin our technology, reflecting our commitment to quality and reliability - human-centric: we strive to make our technology open, approachable, and accessible - captivating: our designs reflect the magic of our technology and our playful, exploratory approach to innovation - ambitious: we push the boundaries of what is possible, reflecting our bold vision for the future",london (+5 others),Machine Learning Engineer,"['cloud', 'excel', 'machine learning', 'r']","['cloud', 'excel', 'machine learning', 'r']",
"machine learning engineer | manchester, uk",hiscox ltd,"machine learning engineer job type: permanent build a brilliant future with hiscox about hiscox hiscox uk is a leading brand in the insurance market, recognised as setting the standards others try to emulate. we consistently deliver strong growth and exceptional returns, recruiting only the very best and empowering them to deliver. we are known for insuring the homes of the rich and famous through to the most innovative technology companies. our customers are diverse and unique and are only united by our ability to provide specialist insurance tailored to their needs. the team the hiscox uk data science team operates across the uk business unit, providing data-driven insights that inform strategic decision-making and operational improvements. we specialise in machine learning and generative ai solutions to address complex business challenges in collaboration with stakeholders across the business. we deliver robust, scalable models and analytical solutions that drive innovation and support evidence-based decisions. the role we're looking for a talented and pragmatic machine learning engineer to join our growing data science team. we're working on a wide range of greenfield projects, from fraud detection to generative ai, giving you the chance to help shape solutions from the ground up. you'll be shaping the full machine learning lifecycle, collaborating closely with data scientists and engineers in a cross-functional environment to define how we solve problems with data science. this role is key to ensuring that models developed in research are successfully transitioned into scalable, production-ready solutions. this role is suited to individuals who are passionate about data and committed to software engineering best practices, with a drive to innovate and advance organisational capabilities. key responsibilities • contribute to the design and evolution of our data science platform, helping define best practices, tooling and the ml engineering function as the team and project portfolio grow. • have a strong voice in the automation of the end-to-end data science lifecycle, leveraging ci/cd and infrastructure as code to support scalable, enterprise-grade production workflows. • work closely as a team, collaborating on all aspects of the data science and deployment lifecycle across traditional ml and generative solutions. • work collaboratively with dependency teams including data engineers, software engineers and business stakeholders. • write high quality python code following industry best practice for model development, deployment and maintainability. • contribute technically to the data science modelling and project workflows, helping select modelling approaches, participating in architecture discussions, and deployment strategies. candidate profile skills and experience: • proven track record in data science or ml engineering roles within a business setting • strong python programming skills and wider software engineering best practice • strong communication skills including translation of technical concepts for non-technical stakeholders • good understanding of core data science principles • experience with production-level cloud-native deployment of machine learning services, using containerisation, kubernetes or equivalent. we work across an azure and databricks estate, therefore experience with these platforms would be particularly beneficial • utilisation of an industry-standard software stack for data and software, including vcs (git), ci/cd (azure devops desirable) and project management (jira) • experience deploying data science models to solve real-world business problems in production, ideally within a regulated industry such as finance or insurance • experience utilising llms, generative or agentic ai in a commercial setting is beneficial recruitment process our hiring process is designed to be thorough yet transparent, giving you the opportunity to showcase your skills and learn more about us. here's what you can expect: • initial screening call - an initial conversation with a member of our talent acquisition team to discuss your skills and experience and interest in the role. • informal call with the hiring manager - an opportunity to talk through your cv and learn more about the position. • technical take-home task - a technical exercise (approx. 2-3 hours to complete) to demonstrate your ability. we'll review this ahead of the subsequent stages and provide feedback. • technical interview - a deeper discussion of your technical expertise & your solution to the task. • business stakeholder interview - a final conversation with key stakeholders to discuss the role's requirements, how your skills and experience align with business objectives, and how you embody our values. this is also an opportunity for you to ask broader questions about the team, culture, and the company's direction. why join us? a career at hiscox is more than just a job-it's an opportunity to grow, thrive, and be rewarded for your contribution. beyond a competitive salary, we offer a comprehensive benefits package designed to support your financial, physical, and personal wellbeing. from retirement plans and healthcare coverage to flexible working options and professional development support, we aim to create an environment where you can succeed both inside and outside of work. to explore the full range of benefits available in your location, visit: benefits | hiscox group we also know that none of us ever stops learning. whether you're just starting out or have decades of experience, we'll give you the tools and opportunities to nurture your talent and fulfil your potential. our learning and development programmes include financial support for professional qualifications, world-class technical training, and a wide range of courses focused on personal growth, career progression, and leadership skills. diversity and hybrid working at hiscox we care about our people. we hire the best people for the job and we're committed to diversity and creating a truly inclusive culture, which we believe drives success. we operate a hybrid working model, set by the team rather than the business, to enable you to manage your own personal work-life balance. we see it as the best of both worlds; structure and sociability on one hand, and independence and flexibility on the other work with amazing people and be part of a unique culture",manchester,Machine Learning Engineer,"['azure', 'cloud', 'databricks', 'machine learning', 'python', 'r', 'scala']","['azure', 'cloud', 'databricks', 'machine learning', 'python', 'r', 'scala']",
on senior machine learning engineer,on,"as a senior machine learning engineer, you'll build and automate production-level machine learning pipelines to drive our marketing programs at on. you'll partner with business, marketing, and data science teams to design, test, and deploy robust and scalable solutions. you will ensure our models are reliable and effectively integrated into our marketing technology stack. in the dynamic landscape of on data, machine learning and ai play a crucial role in accelerating our business growth and operations. we are enhancing our technology landscape to fuel the growth of on, helping to ignite the human spirit through movement. your mission • drive impact through ai: collaborate with data scientists to translate models into production-grade machine learning services that drive strong business impact through our marketing technology stack (e.g., email service providers, ad platforms, app platforms). • platform excellence: design, build, and maintain scalable and reliable data and model pipelines. build an ml ops infrastructure that monitors model performance and implement alerting to ensure high availability and accuracy. • data culture: work with cross-functional teams to understand business requirements and needs, and translate these into technical plans. ai/ml best practices: contribute to the development of our mlops best practices and infrastructure. your story • technical acumen: 6+ years of experience in implementing complex machine learning initiatives and independently designing production grade end to end ml/ai pipelines (e.g. kubeflow, mlflow, airflow). you have strong programming skills in python. • deep machine learning expertise: you have a strong theoretical foundation and practical expertise in areas such as deep learning, embeddings, clustering models, and prediction. • ai platform experience: you are familiar with the core components of ai platforms and have experience in working with production grade ai platforms and components (e.g. vertex ai, docker, kubernetes). • cloud and platform expertise: you are have experience with cloud-based machine learning platforms (e.g. gcp, aws). • team player: to be successful you will need to partner with a range of data science and engineering team members. you have strong communication and interpersonal skills, allowing you to effectively convey complex technical information to diverse audiences. • gen ai: experience deploying generative ai a plus meet the team you will be part of a growing and diverse team of data engineers, data scientists and product managers passionate about revolutionizing how we leverage ai/ml to solve complex challenges across on. we are building innovative machine learning solutions to optimize internal processes, enhance customer experiences, and drive business growth in areas ranging from e-commerce to supply chain optimisation. what we offer on is a place that is centered around growth and progress. we offer an environment designed to give people the tools to develop holistically - to stay active, to learn, explore and innovate. our distinctive approach combines a supportive, team-oriented atmosphere, with access to personal self-care for both physical and mental well-being, so each person is led by purpose. on is an equal opportunity employer. we are committed to creating a work environment that is fair and inclusive, where all decisions related to recruitment, advancement, and retention are free of discrimination.",greater london,Machine Learning Engineer,"['airflow', 'aws', 'cloud', 'clustering', 'deep learning', 'excel', 'gcp', 'machine learning', 'python', 'r', 'scala']","['airflow', 'aws', 'cloud', 'clustering', 'deep learning', 'excel', 'gcp', 'machine learning', 'python', 'r', 'scala']",
"machine learning engineer | python | pytorch | natural language processing | llm | large language models | remote, europe",enigma,"machine learning engineer | python | pytorch | natural language processing | llm | large language models | remote, europe about us we build ai assistants that make technical knowledge instantly accessible. as a research engineer, you will work on advancing our system's ability to answer increasingly complex technical questions. our technology is already deployed on real-world developer documentation, where users can query information directly through an integrated ai assistant. the challenges you'll work on • evaluating a retrieval-augmented-generation (rag) system in production without labelled data • designing your own benchmarks from scratch • building an agentic retrieval pipeline that adapts between fast and more thorough query strategies • fine-tuning embeddings or reranking models what you'll do • collaborate closely with the core team and software engineers • stay up-to-date with the latest research and apply new ideas to real product challenges • design, run and analyse experiments to push system performance you might be a great fit if you have • a master's or phd in computer science, machine learning, mathematics, statistics, or a related field • strong knowledge of machine learning, deep learning (including llms), and natural language processing • hands-on experience training, fine-tuning, and deploying llms • experience working with vector databases, search indices, or data stores for retrieval use cases • significant experience building evaluation systems for search or language models • familiarity with information retrieval techniques (e.g., lexical search, dense vector search) • comfort working in a fast-moving environment with ambiguous problem spaces • a desire to learn more about ml research please note that this position is fully remote in europe but you must have the right to work in your country of residence. machine learning engineer | python | pytorch | natural language processing | llm | large language models | remote, europe",milton keynes,Machine Learning Engineer,"['deep learning', 'machine learning', 'natural language processing', 'python', 'pytorch', 'r', 'statistics']","['deep learning', 'machine learning', 'natural language processing', 'python', 'pytorch', 'r', 'statistics']",
junior machine learning engineer – ai startup,founding teams,"job description founding teams is a stealth ai tech incubator & talent platform. we are supporting the next generation of ai startup founders with the resources they need including engineering, product, sales, marketing and operations staff to create and launch their product. the ideal candidate will have a passion for next generation ai tech startups and working with great global startup talent. about the role: we are looking for an experienced and highly motivated lead machine learning engineer to drive the development, deployment, and optimization of machine learning solutions. as a technical leader, you will collaborate closely with data scientists, software engineers, and product managers to bring cutting-edge ml models into production at scale. you'll play a key role in shaping the ai strategy and mentoring the machine learning team. responsibilities: • lead the end-to-end development of machine learning models, from prototyping to production deployment. • architect scalable ml pipelines and infrastructure. • work closely with data scientists to transition research models into robust production systems. • collaborate with engineering teams to integrate ml models into applications and services. • manage and mentor a team of machine learning and data engineers. • establish best practices for model development, evaluation, monitoring, and retraining. • design experiments, analyze results, and iterate rapidly to improve model performance. • stay current with the latest research and developments in machine learning and ai. • define and enforce ml model governance, versioning, and documentation standards. required skills & qualifications: • bachelor's or master’s degree in computer science, machine learning, data science, statistics, or a related field (phd preferred but not required). • 3+ years of professional experience in machine learning engineering. • 2+ years of leadership or technical mentoring experience. • strong expertise in python for machine learning (pandas, numpy, scikit-learn, etc.). • experience with deep learning frameworks such as tensorflow, pytorch, or jax. • strong understanding of machine learning algorithms (supervised, unsupervised, reinforcement learning). • experience building and maintaining ml pipelines and data pipelines. • proficiency in model deployment techniques (e.g., serving models with rest apis, grpc, or via cloud services). • hands-on experience with cloud platforms (aws, gcp, azure) for model training and deployment. • deep understanding of mlops concepts: monitoring, logging, ci/cd for ml, reproducibility. • experience with docker and container orchestration (e.g., kubernetes). preferred skills: • experience with feature stores (e.g., feast, tecton). • knowledge of distributed training (e.g., horovod, distributed pytorch). • familiarity with big data tools (e.g., spark, hadoop, beam). • understanding of nlp, computer vision, or time series analysis techniques. • knowledge of experiment tracking tools (e.g., mlflow, weights & biases). • experience with model explainability techniques (e.g., shap, lime). • familiarity with reinforcement learning or generative ai models. tools & technologies: • languages: python, sql (optionally: scala, java for large-scale systems) • ml frameworks: tensorflow, pytorch, scikit-learn, xgboost, lightgbm • mlops: mlflow, weights & biases, kubeflow, seldon core • data processing: pandas, numpy, apache spark, beam • model serving: tensorflow serving, torchserve, fastapi, flask • cloud platforms: aws (sagemaker, s3, ec2), google cloud ai platform, azure ml • orchestration: docker, kubernetes, airflow • databases: postgresql, bigquery, mongodb, redis • experiment tracking & monitoring: mlflow, neptune.ai, weights & biases • version control: git (github, gitlab) • communication: slack, zoom • project management: jira, confluence",reading,Machine Learning Engineer,"['airflow', 'aws', 'azure', 'bigquery', 'cloud', 'computer vision', 'data pipeline', 'deep learning', 'gcp', 'google cloud', 'hadoop', 'java', 'lightgbm', 'machine learning', 'nlp', 'numpy', 'pandas', 'python', 'pytorch', 'r', 'scala', 'scikit-learn', 'spark', 'sql', 'statistics', 'tensorflow', 'time series', 'xgboost']","['airflow', 'aws', 'azure', 'bigquery', 'cloud', 'computer vision', 'data pipeline', 'deep learning', 'gcp', 'google cloud', 'hadoop', 'java', 'lightgbm', 'machine learning', 'nlp', 'numpy', 'pandas', 'python', 'pytorch', 'r', 'scala', 'scikit-learn', 'spark', 'sql', 'statistics', 'tensorflow', 'time series', 'xgboost']",£36k–£60k a year
data scientist / machine learning engineer – parental leave cover,lego,"#li-ko1 can you drive measurable value through the applied use of data science? are you passionate about helping us innovate and test out new paths? this is a temporary hire for approximately one year, and we do not offer financial support for relocation for this role. core responsibilities the utilization of data science and ai is rapidly growing within the lego group. we use the methodologies to discover deeper insights, make predictions, or generate recommendations to continue delivering world class learning-through play experiences to children. you will be joining the ai + data science organisation, which is the home of 50 data scientists and engineering colleagues, distributed across our denmark and uk offices. you will be reporting into the trade promotion effectiveness which has the purpose of providing insights and guidance to our key account managers and b2b customers on trade promotions. we are looking for a data scientist who can help build solutions in a production ready setup with a focus on scalability and maintainability. your tasks will include: • develop working and robust machine learning models to solve complex problems. • build end-to-end ml and ai solutions that can are easy to maintain and scale • collaborating with partners to ensure that data science work is delivering real value. • contribute to our ways of working and help establish standard methodologies. • staying up to date with machine learning developments across the industry. do you have what it takes? we are looking for a candidate with emphasis on one or more of the following: data science, computer science, engineering, mathematics, or an equivalent quantitative subject area. you have previous experience creating working ml and ai solutions in a large organization and are used to working in an agile cross functional setup and with a broad range of partners. • proficiency with python (or similar languages) and its libraries for machine learning such as pyspark, scikit-learn, pandas and pytorch. • prior experience with general software development practices including code management (git) and testing. • collaborative spirit and a playful can-do attitude. • persistent with a well-rounded view on creating solutions. • experience with tools and platforms such as databricks and mlflow for ml training and experimenting is highly desirable. applications are reviewed on an ongoing basis. however, please note we do amend or withdraw our jobs and reserve the right to do so at any time, including prior to any advertised closing date. so, if you're interested in this role we encourage you to apply as soon as possible. what’s in it for you? here is what you can expect: family care leave - we offer enhanced paid leave options for those important times. insurances – all colleagues are covered by our life and disability insurance which provides protection and peace of mind. wellbeing - we want our people to feel well and thrive. we offer resources and benefits to nurture physical and mental wellbeing along with opportunities to build community and inspire creativity. colleague discount – we know you'll love to build, so from day 1 you will qualify for our generous colleague discount. bonus - we do our best work to succeed together. when goals are reached and if eligible, you'll be rewarded through our bonus scheme. workplace - when you join the team you'll be assigned a primary workplace location i.e. one of our offices, stores or factories. our hybrid work policy means an average of 3 days per week in the office. the hiring team will discuss the policy and role eligibility with you during the recruitment process. children are our role models. their curiosity, creativity and imagination inspire everything we do. we strive to create a diverse, dynamic and inclusive culture of play at the lego group, where everyone feels safe, valued and they belong. the lego group is highly committed to equal employment opportunity and equal pay and seeks to encourage applicants from all backgrounds (eg. sex, gender identity or expression, race/ethnicity, national origin, sexual orientation, disability, age and religion) to apply for roles in our team. the lego group is fully committed to children’s rights and child wellbeing across the globe. candidates offered positions with high engagement with children are required to take part in child safeguarding background screening, as a condition of the offer. thank you for sharing our global commitment to children’s rights. just imagine building your dream career. then make it real. join the lego® team today.",london,Machine Learning Engineer,"['databricks', 'machine learning', 'pandas', 'pyspark', 'python', 'pytorch', 'r', 'recommendation', 'scala', 'scikit-learn', 'spark']","['databricks', 'machine learning', 'pandas', 'pyspark', 'python', 'pytorch', 'r', 'recommendation', 'scala', 'scikit-learn', 'spark']",
"machine learning engineer | python | pytorch | natural language processing | llm | large language models | remote, europe",enigma,"machine learning engineer | python | pytorch | natural language processing | llm | large language models | remote, europe about us we build ai assistants that make technical knowledge instantly accessible. as a research engineer, you will work on advancing our system's ability to answer increasingly complex technical questions. our technology is already deployed on real-world developer documentation, where users can query information directly through an integrated ai assistant. the challenges you'll work on • evaluating a retrieval-augmented-generation (rag) system in production without labelled data • designing your own benchmarks from scratch • building an agentic retrieval pipeline that adapts between fast and more thorough query strategies • fine-tuning embeddings or reranking models what you'll do • collaborate closely with the core team and software engineers • stay up-to-date with the latest research and apply new ideas to real product challenges • design, run and analyse experiments to push system performance you might be a great fit if you have • a master's or phd in computer science, machine learning, mathematics, statistics, or a related field • strong knowledge of machine learning, deep learning (including llms), and natural language processing • hands-on experience training, fine-tuning, and deploying llms • experience working with vector databases, search indices, or data stores for retrieval use cases • significant experience building evaluation systems for search or language models • familiarity with information retrieval techniques (e.g., lexical search, dense vector search) • comfort working in a fast-moving environment with ambiguous problem spaces • a desire to learn more about ml research please note that this position is fully remote in europe but you must have the right to work in your country of residence. machine learning engineer | python | pytorch | natural language processing | llm | large language models | remote, europe",dartford,Machine Learning Engineer,"['deep learning', 'machine learning', 'natural language processing', 'python', 'pytorch', 'r', 'statistics']","['deep learning', 'machine learning', 'natural language processing', 'python', 'pytorch', 'r', 'statistics']",
junior machine learning engineer – ai startup,founding teams,"job description founding teams is a stealth ai tech incubator & talent platform. we are supporting the next generation of ai startup founders with the resources they need including engineering, product, sales, marketing and operations staff to create and launch their product. the ideal candidate will have a passion for next generation ai tech startups and working with great global startup talent. about the role: we are looking for an experienced and highly motivated lead machine learning engineer to drive the development, deployment, and optimization of machine learning solutions. as a technical leader, you will collaborate closely with data scientists, software engineers, and product managers to bring cutting-edge ml models into production at scale. you'll play a key role in shaping the ai strategy and mentoring the machine learning team. responsibilities: • lead the end-to-end development of machine learning models, from prototyping to production deployment. • architect scalable ml pipelines and infrastructure. • work closely with data scientists to transition research models into robust production systems. • collaborate with engineering teams to integrate ml models into applications and services. • manage and mentor a team of machine learning and data engineers. • establish best practices for model development, evaluation, monitoring, and retraining. • design experiments, analyze results, and iterate rapidly to improve model performance. • stay current with the latest research and developments in machine learning and ai. • define and enforce ml model governance, versioning, and documentation standards. required skills & qualifications: • bachelor's or master’s degree in computer science, machine learning, data science, statistics, or a related field (phd preferred but not required). • 3+ years of professional experience in machine learning engineering. • 2+ years of leadership or technical mentoring experience. • strong expertise in python for machine learning (pandas, numpy, scikit-learn, etc.). • experience with deep learning frameworks such as tensorflow, pytorch, or jax. • strong understanding of machine learning algorithms (supervised, unsupervised, reinforcement learning). • experience building and maintaining ml pipelines and data pipelines. • proficiency in model deployment techniques (e.g., serving models with rest apis, grpc, or via cloud services). • hands-on experience with cloud platforms (aws, gcp, azure) for model training and deployment. • deep understanding of mlops concepts: monitoring, logging, ci/cd for ml, reproducibility. • experience with docker and container orchestration (e.g., kubernetes). preferred skills: • experience with feature stores (e.g., feast, tecton). • knowledge of distributed training (e.g., horovod, distributed pytorch). • familiarity with big data tools (e.g., spark, hadoop, beam). • understanding of nlp, computer vision, or time series analysis techniques. • knowledge of experiment tracking tools (e.g., mlflow, weights & biases). • experience with model explainability techniques (e.g., shap, lime). • familiarity with reinforcement learning or generative ai models. tools & technologies: • languages: python, sql (optionally: scala, java for large-scale systems) • ml frameworks: tensorflow, pytorch, scikit-learn, xgboost, lightgbm • mlops: mlflow, weights & biases, kubeflow, seldon core • data processing: pandas, numpy, apache spark, beam • model serving: tensorflow serving, torchserve, fastapi, flask • cloud platforms: aws (sagemaker, s3, ec2), google cloud ai platform, azure ml • orchestration: docker, kubernetes, airflow • databases: postgresql, bigquery, mongodb, redis • experiment tracking & monitoring: mlflow, neptune.ai, weights & biases • version control: git (github, gitlab) • communication: slack, zoom • project management: jira, confluence",high wycombe,Machine Learning Engineer,"['airflow', 'aws', 'azure', 'bigquery', 'cloud', 'computer vision', 'data pipeline', 'deep learning', 'gcp', 'google cloud', 'hadoop', 'java', 'lightgbm', 'machine learning', 'nlp', 'numpy', 'pandas', 'python', 'pytorch', 'r', 'scala', 'scikit-learn', 'spark', 'sql', 'statistics', 'tensorflow', 'time series', 'xgboost']","['airflow', 'aws', 'azure', 'bigquery', 'cloud', 'computer vision', 'data pipeline', 'deep learning', 'gcp', 'google cloud', 'hadoop', 'java', 'lightgbm', 'machine learning', 'nlp', 'numpy', 'pandas', 'python', 'pytorch', 'r', 'scala', 'scikit-learn', 'spark', 'sql', 'statistics', 'tensorflow', 'time series', 'xgboost']",£36k–£60k a year
machine learning specialist (computer vision & object detection),safe intelligence,"machine learning specialist (computer vision) profile: safe intelligence is on a mission to make ai safe and reliable for anyone to use. to help us succeed, our team is looking for machine learning specialists, and we’re hoping it’s you! in this role, you’ll play a leading role in helping both customers with their ml validation challenges and in helping drive our product forward with insights on how to build the best validation solutions for high-stakes machine vision problems. the specific focus of this position is on ml-based computer vision in high-stakes applications in aviation, mobility, robotics and edge devices, with particular emphasis on object detection and tracking. the role has customer & user-facing elements working on real world problems to help ml teams (r&d and product within other organizations) improve the quality of their models, but also requires deep foundational knowledge on ml-based computer vision. in addition you will also work closely with the safe intelligence r&d team to help improve the company’s tools based on the challenges you see in your domains of expertise. this can range from inputs to product to working on the product itself. previous contributions to state-of-the-art machine vision models including object detectors as used in applications is required. knowledge of machine learning verification is not required, but a solid knowledge of existing testing practices, metrics, state-of-the-art training and validation methods is essential. we’re looking forward to having you on board! responsibilities. as a safe intelligence machine learning specialist - computer vision, you will: • work closely with customers and end-users to understand their machine vision models and help them assess performance. generally these will be r&d and product teams at customer organisations including leading teams in major companies in aviation, mobility, robotics, and edge devices. • implement prototypes, use cases, and solutions that apply the methods and tools developed at safe intelligence to address user-specific problems in deep validation and robustness, particularly in the field of machine vision for aviation, mobility, robotics and edge devices. • conduct experiments to evaluate various approaches and weigh their respective trade-offs. • coordinate with the research and platform teams to guide future development based on use-case specific challenges. • contribute to the development of an efficient and scalable package for performing verification and robust learning. requirements. the technical requirements for the role are: • previous scientific and engineering contributions to key problems in machine vision, such as object detection or tracking, typically evidenced via first-author papers in top computer vision conferences such as cvpr, iccv, and eccv. • in-depth experience in training, evaluating and deploying state-of-the-art machine vision models, including standard architectures such as yolo, vision transformer architectures, and eva. • experience talking to stakeholders in these models to understand their requirements and guiding them through what is and is not possible or desirable in a model. • familiarity with python and the packages widely used in data science and machine learning. developers should be familiar with libraries like numpy, pandas, scikit-learn, tensorflow, and pytorch. • familiarity with best practice in machine learning workflows and mlops tools. • fluency in validation and evaluation framework and metrics frameworks for machine learning such as accuracy, recall, f1, intersection over union, and others. additional beneficial experience includes: • experience in the aviation, mobility, edge devices, and/or robotics industries. • experience on industrial real world deployment of machine learning solutions. at a personal level we’re also looking for some who is: • passionate about helping engineering teams achieve their ai and ml goals. • comfortable and energized in a fast-paced environment. • excited about interacting with others and digging in to help solve their problems collaboratively. • technical and constantly in a state of learning. • able to communicate clearly and efficiently with a variety of audiences including developers, customers, researchers, partners and executives. • fearless in getting ""hands-on"" with technology and execution. • knowledgeable about modern software engineering processes. • comfortable with ambiguity with a drive for clarity. • collaborative with, and respectful of others on the team. • honest, straightforward and caring about each other’s well being. why safe intelligence is for you: we strongly believe ai can bring great benefits to individuals and society, but these will only be achieved if the systems we build are safe to use. to meet this need, we are developing advanced deep validation techniques and tools that allow ai/ml engineers world-wide to validate the robustness of their models, as well as repair the fragilities that they discover. by joining us, you’ll be able to help advance the techniques, bring advanced technologies to ai/ml engineers worldwide and contribute to our shared mission to realise successful and reliable ai. grow with us! if you think you can bring something special to this role, please apply even if you do not meet all listed criteria. safe intelligence is exploring uncharted waters, and finding the right crewmates is important to us. we support ongoing learning for the whole team, ranging from individual mentorship to internal seminars and support for sector and technology-specific upskilling. compensation & benefits: safe intelligence provides competitive compensation based on role and candidate experience. we aim to be competitive with pay rates. company benefits for all roles include: • stock option benefits • mentoring, learning, and development allowance • regular team social and work events • flexible and generous holidays. we work hard and encourage everyone to take time off to recharge and enjoy other aspects of our lives. equality and inclusion: we are proud to be an equal-opportunity employer and work hard to create an environment where people of diverse backgrounds and life experiences can thrive. the team is highly collaborative and meritocratic. great ideas come from everywhere, and we strive to make it easy for people to express themselves and be heard. location & office culture: safe intelligence is based in london, uk, and we’re focused on building the initial team here. we highly value the ability to work flexibly and remotely at times, but we also have a strong belief that regular in-office interactions make for a much more fulfilling and productive work experience. our company culture combines optimism for the future (hard problems can be solved with the right effort), speed of iteration (the best ideas come from many ideas tested), and rigour in what matters (correctness and precision are critical for safety). come and join us to add your skills and passion to the future of safe artificial intelligence! how to apply: find us on linkedin and submit for this role. if you have any questions, please feel free to email join@safeintelligence.ai. not ticking every box on our list? if you don’t meet all the criteria but feel you have something special to bring to the table, we encourage you to apply anyway.",united kingdom,Machine Learning Engineer,"['computer vision', 'machine learning', 'numpy', 'pandas', 'python', 'pytorch', 'r', 'scala', 'scikit-learn', 'tensorflow']","['computer vision', 'machine learning', 'numpy', 'pandas', 'python', 'pytorch', 'r', 'scala', 'scikit-learn', 'tensorflow']",
senior manufacturing machine learning engineer,ge vernova,"job description summary we are looking for a passionate, creative, and results-driven senior machine learning (ml) engineer with min 5+ years of experience in multinational manufacturing environments, ideally within the energy, smart infrastructure, or industrial automation sectors. the ideal candidate has a proven track record of independently leading and delivering ml projects in complex, data-intensive environments. job description essential responsibilities: • lead the design, development, and deployment of scalable ai/ml models for grid innovation applications in the energy, smart infrastructure, or industrial automation sectors. • create innovative analytics to optimize grid system performance and product differentiation. • develop ai/ml applications for customer-driven use cases, including predictive maintenance and load forecasting. • validate and verify ai/ml proof-of-concepts in real-world environments, ensuring they meet the diverse needs of our customers. • monitor, maintain, and optimize deployed ai/ml models to continuously enhance their accuracy and performance. • manage the collection, structuring, and analysis of data to enable seamless ai/ml applications. • ensure that models are production-ready and continuously improve in line with emerging needs and technologies. • embrace mlops principles to streamline the deployment and updating of ml models in production. • collaborate closely with cross-functional teams to identify business challenges and deliver ai-driven solutions that are efficient, equitable, and scalable. • integrate ai/ml solutions effortlessly into grid automation systems, whether in the cloud or at the edge. must-have requirements: • main requirement: minimum of 5 years of experience in the role in a multinational manufacturing company, with proven expertise in the energy, smart infrastructure, or industrial automation sectors, including system protection, automation, monitoring, and diagnostics. • master’s or phd in computer science, information technology, electrical engineering, or a related field. • solid foundation in ai/ml techniques, including supervised, unsupervised, and reinforcement learning, deep learning, and large language models (llms). • experience with ml frameworks such as tensorflow, pytorch, and scikit-learn. • hands-on experience deploying ml models in production environments using mlops principles. • expertise in relevant ai/ml applications, such as predictive maintenance, load forecasting, or optimization. • proficiency in programming languages such as python, r, matlab, or c++. • familiarity with cloud platforms (aws, azure, google cloud) and microservices architecture. nice-to-have requirements: • experience with data modeling, containerization (docker, kubernetes), and distributed computing (spark, scala). • familiarity with graphdb, mongodb, sql/nosql, and other dbms technologies. • understanding of system automation, protection, and diagnostics in relevant sectors. • experience with deep learning algorithms, reinforcement learning, nlp, and computer vision in applicable domains. • excellent communication, organizational, and problem-solving skills, with a strong emphasis on teamwork, collaboration, and fostering inclusive environments. at ge vernova - grid automation, you will have the opportunity to work on cutting-edge projects that shape the future of energy. we offer a collaborative environment where your expertise will be valued, and your contributions will make a tangible impact. join us and be part of a team that is driving innovation and excellence in control systems. about gev grid solutions: at gev grid solutions we are electrifying the world with advanced grid technologies. as leaders in the energy space our goal is to accelerate the transition for a more energy efficient grid to full fill the needs of tomorrow. with a focus on growth and sustainability ge grid solutions plays a pivotable role in integrating renewables onto the grid to drive to carbon neutral. in grid solutions we help enable the transition for a greener more reliable grid. ge grid solutions has the most advanced and comprehensive product and solutions portfolio within the energy sector. why we come to work: at gev, our engineers are always up for the challenge - and we’re always driven to find the best solution. our projects are unique and interesting, and you’ll need to bring a solution-focused, positive approach to each one to do your best. surrounded by committed, loyal colleagues, if you can dare to bring your ingenuity and desire to make an impact, you’ll be exposed to game-changing, diverse projects that truly allow you to play your part in the energy transition. what we offer: a key role in a dynamic, international working environment with a large degree of flexibility of work agreements competitive benefits, and great development opportunities - including private health insurance. additional information relocation assistance provided: no",stafford,Machine Learning Engineer,"['aws', 'azure', 'c++', 'cloud', 'computer vision', 'deep learning', 'excel', 'google cloud', 'machine learning', 'matlab', 'nlp', 'python', 'pytorch', 'r', 'scala', 'scikit-learn', 'spark', 'sql', 'tensorflow']","['aws', 'azure', 'c++', 'cloud', 'computer vision', 'deep learning', 'excel', 'google cloud', 'machine learning', 'matlab', 'nlp', 'python', 'pytorch', 'r', 'scala', 'scikit-learn', 'spark', 'sql', 'tensorflow']",
"senior machine learning engineer, simulation",waymo,"waymo is an autonomous driving technology company with the mission to be the world's most trusted driver. since its start as the google self-driving car project in 2009, waymo has focused on building the waymo driver—the world's most experienced driver™—to improve access to mobility while saving thousands of lives now lost to traffic crashes. the waymo driver powers waymo’s fully autonomous ride-hail service and can also be applied to a range of vehicle platforms and product use cases. the waymo driver has provided over ten million rider-only trips, enabled by its experience autonomously driving over 100 million miles on public roads and tens of billions in simulation across 15+ u.s. states. the simulator team builds state-of-the-art simulations of realistic environments for the testing and training of the waymo driver. we use machine learning to model the real world, including realistic agents (vehicles, pedestrians, cyclists, motorcyclists etc.), roads, traffic control systems, and weather. to increase the fidelity and steerability of the simulations, we employ large foundation models, trained on our massive datasets that allow us to quickly setup and rollout multiple scenarios to subject our driver to. we have set up a team in london uk to work with the teams in mtv and oxford to build these foundation models out and to integrate them into several evaluation and training products. we are looking for research engineers to work on these exciting problems. in this hybrid role, you will report to an engineering manager. you will: • be part of a world class research engineering team to grow the state-of-the-art of ultra realistic av simulations using foundation models • collaborate with teams in waymo oxford to use large models to improve sim realism • design experiments that push the frontiers of av simulations • develop metrics that measure the realism of simulated worlds • train and evaluate large models and integrate them into the simulator and its downstream applications • help hire outstanding research engineers from diverse backgrounds • be a part of a collaborative research engineering team that takes research ideas and productionizes them we prefer: • 4+ years experience in applied deep learning • 4+ years coding and design skills • experience solving complex production problems using state-of-the-art ml techniques • experience taking research to production • expertise in data analysis or data science the expected base salary range for this full-time position is listed below. actual starting pay will be based on job-related factors, including exact work location, experience, relevant training and education, and skill level. waymo employees are also eligible to participate in waymo’s discretionary annual bonus program, equity incentive plan, and generous company benefits program, subject to eligibility requirements. salary range £120,000—£130,000 gbp",london,Machine Learning Engineer,"['data analysis', 'deep learning', 'machine learning', 'r']","['data analysis', 'deep learning', 'machine learning', 'r']",£120k–£130k a year
machine learning engineer | £50k–£70k + equity | remote (uk),tellme,"about tellme tellme is an ai-driven platform that enriches visitor experiences at attractions (museums, aquariums, heritage sites etc.). we use computer vision and generative ai to deliver interactive and personalised experiences straight to visitors' phones. after successful pilots with madame tussauds and national museums wales, we have closed our pre-seed round and are now building a scalable platform for wider rollout. as we establish our founding engineering team, we’re hiring a machine learning engineer to lead the development of our core ai systems. what you’ll work on • computer vision: you might come from a background in image embeddings and similarity-based approaches (e.g. clip, vector search), or from more traditional computer vision techniques like classification and object detection (e.g. mobilenet, yolo). either way, we’re looking for someone who can help our app understand what the visitor is looking at – reliably and at scale. • rag systems, data pipelines & internal agents: you'll design the data pipelines that power our ai features, including retrieval-augmented generation (rag), internal llm-based tools, and content delivery workflows. experience with vector stores, agent frameworks, or scalable data workflows is all relevant here. • mlops & deployment: build and maintain the infrastructure behind our ai stack – including serving models via apis, monitoring performance, and deploying systems to the cloud. aws is a plus, but experience with any major cloud platform is welcome. experience working offline or on edge devices is also highly valuable. • collaboration & growth: you’ll work closely with the founder on shaping the product roadmap, defining priorities, and making key technical decisions. as an early team member, there’s real opportunity to grow into a broader leadership role as the company scales. role details • remote-first (uk-based), with regular travel to partner sites. london or belfast-based is a bonus, but not essential. • compensation: £50,000–£70,000 salary + share options (0–2%), depending on experience and preferred balance. • start: asap we're early-stage, with plenty of room to grow — in scope, impact, and salary. interested? if you’re passionate about ai and want to grow with an ambitious startup, get in touch. even if you don’t tick every box, we’d still love to hear from you! 📩 apply by directly emailing eamon byrne (founder and ceo) at eamon.byrne@tell-me.ai",edinburgh,Machine Learning Engineer,"['aws', 'classification', 'cloud', 'computer vision', 'data pipeline', 'machine learning', 'r', 'scala']","['aws', 'classification', 'cloud', 'computer vision', 'data pipeline', 'machine learning', 'r', 'scala']",£40k–£60k a year
machine learning engineer | £50k–£70k + equity | remote (uk),tellme,"about tellme tellme is an ai-driven platform that enriches visitor experiences at attractions (museums, aquariums, heritage sites etc.). we use computer vision and generative ai to deliver interactive and personalised experiences straight to visitors' phones. after successful pilots with madame tussauds and national museums wales, we have closed our pre-seed round and are now building a scalable platform for wider rollout. as we establish our founding engineering team, we’re hiring a machine learning engineer to lead the development of our core ai systems. what you’ll work on • computer vision: you might come from a background in image embeddings and similarity-based approaches (e.g. clip, vector search), or from more traditional computer vision techniques like classification and object detection (e.g. mobilenet, yolo). either way, we’re looking for someone who can help our app understand what the visitor is looking at – reliably and at scale. • rag systems, data pipelines & internal agents: you'll design the data pipelines that power our ai features, including retrieval-augmented generation (rag), internal llm-based tools, and content delivery workflows. experience with vector stores, agent frameworks, or scalable data workflows is all relevant here. • mlops & deployment: build and maintain the infrastructure behind our ai stack – including serving models via apis, monitoring performance, and deploying systems to the cloud. aws is a plus, but experience with any major cloud platform is welcome. experience working offline or on edge devices is also highly valuable. • collaboration & growth: you’ll work closely with the founder on shaping the product roadmap, defining priorities, and making key technical decisions. as an early team member, there’s real opportunity to grow into a broader leadership role as the company scales. role details • remote-first (uk-based), with regular travel to partner sites. london or belfast-based is a bonus, but not essential. • compensation: £50,000–£70,000 salary + share options (0–2%), depending on experience and preferred balance. • start: asap we're early-stage, with plenty of room to grow — in scope, impact, and salary. interested? if you’re passionate about ai and want to grow with an ambitious startup, get in touch. even if you don’t tick every box, we’d still love to hear from you! 📩 apply by directly emailing eamon byrne (founder and ceo) at eamon.byrne@tell-me.ai",gloucester,Machine Learning Engineer,"['aws', 'classification', 'cloud', 'computer vision', 'data pipeline', 'machine learning', 'r', 'scala']","['aws', 'classification', 'cloud', 'computer vision', 'data pipeline', 'machine learning', 'r', 'scala']",£40k–£60k a year
head of machine learning,williams lea,"lead machine learning engineer salary: £97,500 per annum, plus company benefits contract: full time, permanent shifts: 37.5 hours per week mon-fri, 8:30am-5pm with a 1-hour unpaid break work model: fully remote williams lea seeks a lead machine learning engineer to join our team! williams lea is the leading global provider of skilled, technology-enabled, business-critical support services, with long-term trusted relationships with blue-chip clients across investment banks, law firms and professional services firms. williams lea employees, nearly 7,000 people worldwide provide efficient business services at client sites in often complex and highly regulated environments, from centralised williams lea onshore facilities, and through best cost company offshore locations. purpose of the role this role is responsible for maintaining and expanding the enterprise data model, and for developing, publishing, and maintaining business-critical reports for both internal and external stakeholders. you will collaborate closely with the data & analytics team, business stakeholders, and subject matter experts to solve organizational challenges through reporting, analysis, and data visualization. key responsibilities • provide enterprise-wide expertise in data modelling, data quality management, report design, environment management, and automated data ingestion/refresh • act as a creative problem-solver, contributing to the full product lifecycle and maintaining an organized, scalable reporting environment • produce reports that inform high-level decision-making and drive revenue growth about you the ideal candidate is a self-starter and individual contributor who thrives in a global, fast-paced environment. you will be part of a team delivering market-changing online services, contributing your technical expertise and strong work ethic. working environment • operate within an agile/scrum framework to meet the needs of a dynamic customer service and operations environment • be a hands-on technologist, driving best practices and helping shape the strategic direction of the it function • lead a small, distributed team of engineers across the us, uk, and india, ensuring alignment with business goals and service delivery expectations • manage cloud-based platforms, leveraging tools like auto-scaling, infrastructure as code, and continuous delivery methodologies to optimize performance and accelerate delivery key responsibilities: • machine learning solution development: design, develop and deploy ml models, algorithms and agentic ai systems to address complex business challenges across a range of sectors • cloud & mlops management: lead the implementation of ml solutions on aws cloud (with heavy use of amazon sagemaker and related aws services). develop and maintain end-to-end ci/cd pipelines for ml projects, using infrastructure-as-code tools like aws cloudformation and terraform to automate model deployment and system setup • project leadership: oversee the ml lifecycle from data preparation to model training, validation, and deployment. make high-level design decisions on model architecture and data pipelines. mentor junior engineers and collaborate with data scientists, ml engineers, and software engineering teams to ensure successful delivery of ml projects • client & stakeholder collaboration: collaborate with project managers and stakeholders across a range of sectors to gather requirements and translate business needs into technical solutions. present findings and ml model results to non-technical audiences in a clear manner, and refine solutions based on their feedback • quality, security & compliance: ensure that ml solutions meet quality and performance standards. implement monitoring and logging for models in production, and proactively improve model accuracy and efficiency. given the sensitive nature of our data, enforce data security best practices and compliance with relevant regulations (e.g. data privacy and confidentiality) in all ml workflows required qualifications & experience: • education: bachelor's or master's degree in computer science, data science, machine learning, or related field. strong foundation in statistics and algorithms is expected • experience: 5+ years of hands-on experience in machine learning or data science roles, with a track record of building and deploying ml models into production. prior experience leading projects or teams is a plus for a lead role • programming & ml skills: advanced programming skills in python (including libraries such as pandas, scikit-learn, tensorflow/pytorch). solid understanding of ml algorithms, model evaluation techniques, and optimisation. experience with nlp techniques, generative ai or financial data modelling is advantageous • cloud & devops: proven experience with aws cloud services relevant to data science – particularly amazon sagemaker for model development and deployment. familiarity with data storage and processing on aws (s3, aws lambda, athena/redshift, etc.) is expected. strong knowledge of devops/mlops practices – candidates should have built or worked with ci/cd pipelines for ml, using tools like docker and jenkins, and infrastructure-as-code tools like cloudformation or terraform to automate deployments • soft skills: excellent problem-solving and analytical thinking. strong communication skills to explain complex ml concepts to clients or management. ability to work under tight deadlines and multitask across projects for different clients. a client- focused mindset is essential, as the role involves understanding and addressing the needs of large clients who come to us because they trust us preferred experience: • domain knowledge: familiarity with use-cases like document classification, contract analytics, fraud/risk modelling, or nlp on legal texts will help the engineer design better domain-tailored solutions • certifications: relevant certifications such as aws certified machine learning – specialty or aws solutions architect, and any machine learning/deep learning specialisations, will be a plus (demonstrating validated expertise) • tools & frameworks: experience with collaborative software development tools and practices (git version control, code review), and with project management tools (jira or similar) in an agile environment. familiarity with other ml ops tools (kubeflow, mlflow, etc.) or big data processing frameworks (spark) can be an added advantage rewards and benefits we believe in supporting our employees in both their professional and personal lives. as part of our commitment to your well-being, we offer a comprehensive benefits package, including but not limited to: • 25 days holiday, plus bank holidays(pro-rata for part time roles) • salary sacrifice schemes, retail vouchers – including our techscheme which can be used on a range of gadgets such as smart tv's, laptops and computers or household appliances. • life assurance • private medical insurance • dental insurance • health assessments • cycle-to-work scheme • discounted gym memberships • referral scheme you will also have the opportunity to work for a global employer who is dedicated to offering each and every employee an enjoyable, challenging and rewarding career with future career development prospects! equality and diversity the company values the differences that a diverse workforce brings to the organisation and will not discriminate because of age, disability, gender reassignment, marriage and civil partnership, pregnancy and maternity, race (which includes colour, nationality and ethnic or national origins), religion or belief, sex or sexual orientation (each of these being a ""protected characteristic"" in discrimination law). it will not discriminate because of any other irrelevant factor and will build a culture that values openness, fairness and transparency. if you have a disability and would prefer to apply in a different format or would like to make a reasonable adjustment to enable you to make an interview please contact us at careersatwl@williamslea.com(we do not accept applications to this email address). view our privacy notice",dartford,Machine Learning Engineer,"['aws', 'classification', 'cloud', 'data pipeline', 'deep learning', 'excel', 'machine learning', 'nlp', 'pandas', 'python', 'pytorch', 'r', 'redshift', 'scala', 'scikit-learn', 'spark', 'statistics', 'tensorflow']","['aws', 'classification', 'cloud', 'data pipeline', 'deep learning', 'excel', 'machine learning', 'nlp', 'pandas', 'python', 'pytorch', 'r', 'redshift', 'scala', 'scikit-learn', 'spark', 'statistics', 'tensorflow']",
ai&ml engineer (artificial intelligence & machine learning engineer),a-safe group,"ai&ml engineer (artificial intelligence & machine learning engineer) join to apply for the ai&ml engineer (artificial intelligence & machine learning engineer) role at a-safe group about a-safe and the role as the engineer for artificial intelligence & machine learning at a-safe, you will collaborate across multiple teams to lead ai & ml model training, validation, experimentation, and monitoring, ensuring reliable performance in production. this full-stack role involves working with data teams to ensure system compatibility and efficient handling of workloads, as well as with software development teams for poc experiments and deployment. you will design and implement scalable ai and ml solutions, fostering innovation and experimentation. your work will significantly impact operational efficiency, safety, and data-driven decision-making, supporting a-safe’s mission to prevent workplace injuries and safeguard lives globally. in this role, you will proven experience with generative ai, llms, rag, transformers, diffusion models, and recommender systems. developing and managing data pipelines, including data gathering, cleaning, augmentation, labelling, and managing vector databases for rag workflows. model deployment, monitoring, versioning, and continuous improvement using frameworks like mlflow and aws sagemaker. experience with deep learning frameworks (tensorflow, pytorch), aws sagemaker, bedrock, lambda; familiarity with azure ai foundry is a plus. knowledge of software engineering best practices (version control, ci/cd, code reviews); exposure to java/scala for databricks is advantageous. experience integrating models with front-end/back-end systems, apis, microservices, and serverless architectures using python and typescript/node.js. the ideal candidate will have experience with generative ai, llms, rag, transformers, diffusion models, and recommender systems. skills in developing data pipelines and managing vector databases for large-scale workflows. proficiency in model deployment, monitoring, and continuous improvement frameworks. deep learning framework experience (tensorflow, pytorch), aws services, and familiarity with azure is a plus. strong software engineering practices and experience with api/microservice integration. why a-safe? since 1984, a-safe has pioneered industrial safety solutions, including the first polymer safety barrier and iot systems like rackeye. we partner with global companies such as amazon, coca-cola, and bmw to enhance workplace safety. our values include pioneering spirit, teamwork, and integrity. our commitment to diversity we are dedicated to building an inclusive workplace where everyone belongs, believing diversity drives innovation. ready to set new standards in workplace safety? apply today and explore more opportunities on our careers page. additional details seniority level: entry level employment type: full-time job function: engineering and it industries: industrial machinery manufacturing #j-18808-ljbffr",halifax,Machine Learning Engineer,"['aws', 'azure', 'data pipeline', 'databricks', 'deep learning', 'experimentation', 'java', 'machine learning', 'python', 'pytorch', 'r', 'recommender', 'scala', 'tensorflow']","['aws', 'azure', 'data pipeline', 'databricks', 'deep learning', 'experimentation', 'java', 'machine learning', 'python', 'pytorch', 'r', 'recommender', 'scala', 'tensorflow']",£28.8k–£48k a year
ai / ml engineers,awtg ltd,"we are looking for a goal oriented and driven ai/ml engineers with relevant experiences. key responsibilities: • neural network training: develop and train advanced neural network models to meet specific functional requirements. • deployment and integration: efficiently deploy these neural networks on servers, ensuring optimal performance and scalability. • api development: create robust api endpoints for model access, enabling seamless integration with external systems or applications. • platform development: architect and develop a user-friendly platform where multiple ai models are accessible and can be utilised through api calls, facilitating a broad range of ai capabilities. • continuous improvement: continuously monitor, update, and improve the models and the platform based on user feedback and evolving technological trends. skills and qualifications: • minimum 3 years of experience in building ai/ml software. • strong expertise in machine learning, neural network algorithms and generative ai applications. • proficient in programming languages like python, and libraries such as tensorflow, pytorch, numpy, langchain, fastapi and langgraph • experience with rag applications, server deployment, cloud computing environments, and api development. • good understanding of software engineering best practices. • excellent problem-solving abilities and a keen eye for detail. • strong communication skills to collaborate effectively in a team-oriented environment. educational requirements: • bachelor's or master's degree in computer science, engineering, or a related field, with a focus on ai/ml. working hours: • candidates must be available to work core uk business hours, 9:00–17:30 (gmt/bst), monday–friday.",london,Machine Learning Engineer,"['cloud', 'excel', 'machine learning', 'numpy', 'python', 'pytorch', 'r', 'scala', 'tensorflow']","['cloud', 'excel', 'machine learning', 'numpy', 'python', 'pytorch', 'r', 'scala', 'tensorflow']",
machine learning engineer – genai,experian ltd,"company description experian is a global data and technology company, powering opportunities for people and businesses around the world. we help to redefine lending practices, uncover and prevent fraud, simplify healthcare, create marketing solutions, and gain deeper insights into the automotive market, all using our unique combination of data, analytics and software. we also assist millions of people to realize their financial goals and help them save time and money. like the look of this opportunity make sure to apply fast, as a high volume of applications is expected scroll down to read the complete job description. we operate across a range of markets, from financial services to healthcare, automotive, agribusiness, insurance, and many more industry segments. we invest in people and new advanced technologies to unlock the power of data. as a ftse 100 index company listed on the london stock exchange (expn), we have a team of 22,500 people across 32 countries. our corporate headquarters are in dublin, ireland. learn more at experianplc.com. job description the generative ai centre of expertise (genai coe) at experian exists to improve our products, our internal processes and our daily work through genai and process automation. the team is a mix of ml engineers, data scientists and product owners, who are dedicated to the next wave of innovation using genai. reporting into our head of machine learning product engineering you will drive the delivery of concepts and ideas into products and services that experian can take to their customers, whether that be businesses or direct to consumers. to do this, we build upon the outcomes of our experiments to meet the product requirements - considering performance, maintainability, and scalability. we, alongside the data scientists in the team, collaborate with a range of stakeholders. you will: partner with teams across the organisation to develop genai solutionsfrom early experimentation to full-scale productionpotentially including devops work where needed. architect and build high-performant solutions, which may involve traditional ml modelling or large datasets, as well as genai. discover and introduce new technologies to the team, staying up to date with the latest approaches that enable the next generation of experian's products with genai and ml. spend 10% of your work time on learning and sharing expertise on generative-ai technologies. qualifications have a degree or equivalent qualification in a stem subject. familiar with unix environments. exposure to at least one other programming language besides python. proficiency in object-oriented programming (oop), solid principles, and test-driven development (tdd). proficiency with docker and experience working with container orchestration tools such as kubernetes, docker swarm, or cloud-based alternatives. comfort working across the full development stack, especially for prototyping. passion for applying genai and machine learning across diverse domains and throughout the full project lifecycle. experience with common ml approaches (e.g., llms, gbms, deep learning) and typical software architectures. experience as a lead developer solving complex problems at scale. experience mentoring junior engineers. familiarity with various machine learning frameworks and toolkits (e.g., scikit-learn, xgboost, tensorflow). hands-on experience building genai solutions using patterns such as retrieval-augmented generation (rag) or fine-tuning large language models (llms). greater familiarity with aws compared to other cloud computing platforms. have experience developing rest apis. additional information benefits package includes: hybrid working great compensation package and discretionary bonus core benefits include pension, bupa healthcare, sharesave scheme and more 25 days annual leave with 8 bank holidays and 3 volunteering days. you can purchase additional annual leave. we take our people agenda very seriously and focus on what matters; dei, work/life balance, development, authenticity, collaboration, wellness, reward & recognition, volunteering... the list goes on. experian's people first approach is award-winning; world's best workplaces 2024 (fortune top 25), great place to work in 24 countries, and glassdoor best places to work 2024 to name a few. check out experian life on social or our careers site to understand why. experian is proud to be an equal opportunity and affirmative action employer. innovation is an important part of experian's dna and practices, and our diverse workforce drives our success. everyone can succeed at experian and bring their whole self to work, irrespective of their gender, ethnicity, religion, colour, sexuality, physical ability or age. if you have a disability or special need that requires accommodation, please let us know at the earliest opportunity. experian careers - creating a better tomorrow together find out what its like to work for experian by clicking here experian careers - creating a better tomorrow together find out what its like to work for experian by clicking here tpbn1_uktj",united kingdom,Machine Learning Engineer,"['aws', 'cloud', 'deep learning', 'experimentation', 'machine learning', 'python', 'r', 'scala', 'scikit-learn', 'tensorflow', 'xgboost']","['aws', 'cloud', 'deep learning', 'experimentation', 'machine learning', 'python', 'r', 'scala', 'scikit-learn', 'tensorflow', 'xgboost']",£36k–£60k a year
machine learning engineer – genai,experian,"company description experian is a global data and technology company, powering opportunities for people and businesses around the world. we help to redefine lending practices, uncover and prevent fraud, simplify healthcare, create marketing solutions, and gain deeper insights into the automotive market, all using our unique combination of data, analytics and software. we also assist millions of people to realize their financial goals and help them save time and money. we operate across a range of markets, from financial services to healthcare, automotive, agribusiness, insurance, and many more industry segments. we invest in people and new advanced technologies to unlock the power of data. as a ftse 100 index company listed on the london stock exchange (expn), we have a team of 22,500 people across 32 countries. our corporate headquarters are in dublin, ireland. learn more at experianplc.com. job description the generative ai centre of expertise (genai coe) at experian exists to improve our products, our internal processes and our day-to-day work through genai and process automation. the team is a mix of ml engineers, data scientists and product owners, who are dedicated to the next wave of innovation using genai. who is a ml engineer? in the genai coe, ml engineers drive the delivery of concepts and proven ideas into products and services that experian can take to their customers, whether that be businesses or direct to consumers. to do this, we build upon the outcomes of our experiments to meet the product requirements - considering performance, maintainability, and scalability. we, alongside the data scientists in the team, collaborate with a range of stakeholders. you will: • partner with teams across the organisation to design and build genai solutions-from early experimentation to full-scale production-potentially including devops work where needed. • architect and build high-performant solutions, which may involve traditional ml modelling and/or large datasets, as well as genai. • discover and introduce new technologies to the team, staying up to date with the latest approaches that enable the next generation of experian's products with genai and ml. • learn quickly and be able to put new genai concepts into practice. • spend 10% of your work time on continuous learning and sharing expertise on generative-ai technologies. you • have experience in python. • are a self-starter with strong troubleshooting skills. it would be fantastic if you also have: • have a degree or equivalent qualification in a stem subject. • are familiar with unix environments. • exposure to at least one other programming language besides python. • proficiency in object-oriented programming (oop), solid principles, and test-driven development (tdd). • proficiency with docker and experience working with container orchestration tools such as kubernetes, docker swarm, or cloud-based alternatives. • comfort working across the full development stack, especially for prototyping. • passion for applying genai and machine learning across diverse domains and throughout the full project lifecycle. • conceptual understanding of common ml approaches (e.g., llms, gbms, deep learning) and typical software architectures. • detailed-oriented, pragmatic, and collaborative team player. • experience as a lead developer tackling complex problems at scale. • experience mentoring junior engineers. • familiarity with various machine learning frameworks and toolkits (e.g., scikit-learn, xgboost, tensorflow). • hands-on experience building genai solutions using patterns such as retrieval-augmented generation (rag) or fine-tuning large language models (llms). • have experience with cloud computing platforms. • greater familiarity with aws compared to other cloud computing platforms. • have experience developing rest apis. qualifications • python • self-starter with strong troubleshooting skills • machine learning additional information our uniqueness is that we celebrate yours. experian's culture and people are important differentiators. we take our people agenda very seriously and focus on what matters; dei, work/life balance, development, authenticity, collaboration, wellness, reward & recognition, volunteering... the list goes on. experian's people first approach is award-winning; world's best workplaces 2024 (fortune top 25), great place to work in 24 countries, and glassdoor best places to work 2024 to name a few. check out experian life on social or our careers site to understand why. experian is proud to be an equal opportunity and affirmative action employer. innovation is an important part of experian's dna and practices, and our diverse workforce drives our success. everyone can succeed at experian and bring their whole self to work, irrespective of their gender, ethnicity, religion, colour, sexuality, physical ability or age. if you have a disability or special need that requires accommodation, please let us know at the earliest opportunity. experian careers - creating a better tomorrow together find out what its like to work for experian by clicking here #j-18808-ljbffr",nottingham,Machine Learning Engineer,"['aws', 'cloud', 'deep learning', 'experimentation', 'machine learning', 'python', 'r', 'scala', 'scikit-learn', 'tensorflow', 'xgboost']","['aws', 'cloud', 'deep learning', 'experimentation', 'machine learning', 'python', 'r', 'scala', 'scikit-learn', 'tensorflow', 'xgboost']",£36k–£60k a year
junior machine learning engineer – ai startup,founding teams,"job description founding teams is a stealth ai tech incubator & talent platform. we are supporting the next generation of ai startup founders with the resources they need including engineering, product, sales, marketing and operations staff to create and launch their product. the ideal candidate will have a passion for next generation ai tech startups and working with great global startup talent. about the role: we are looking for an experienced and highly motivated lead machine learning engineer to drive the development, deployment, and optimization of machine learning solutions. as a technical leader, you will collaborate closely with data scientists, software engineers, and product managers to bring cutting-edge ml models into production at scale. you'll play a key role in shaping the ai strategy and mentoring the machine learning team. responsibilities: • lead the end-to-end development of machine learning models, from prototyping to production deployment. • architect scalable ml pipelines and infrastructure. • work closely with data scientists to transition research models into robust production systems. • collaborate with engineering teams to integrate ml models into applications and services. • manage and mentor a team of machine learning and data engineers. • establish best practices for model development, evaluation, monitoring, and retraining. • design experiments, analyze results, and iterate rapidly to improve model performance. • stay current with the latest research and developments in machine learning and ai. • define and enforce ml model governance, versioning, and documentation standards. required skills & qualifications: • bachelor's or master’s degree in computer science, machine learning, data science, statistics, or a related field (phd preferred but not required). • 3+ years of professional experience in machine learning engineering. • 2+ years of leadership or technical mentoring experience. • strong expertise in python for machine learning (pandas, numpy, scikit-learn, etc.). • experience with deep learning frameworks such as tensorflow, pytorch, or jax. • strong understanding of machine learning algorithms (supervised, unsupervised, reinforcement learning). • experience building and maintaining ml pipelines and data pipelines. • proficiency in model deployment techniques (e.g., serving models with rest apis, grpc, or via cloud services). • hands-on experience with cloud platforms (aws, gcp, azure) for model training and deployment. • deep understanding of mlops concepts: monitoring, logging, ci/cd for ml, reproducibility. • experience with docker and container orchestration (e.g., kubernetes). preferred skills: • experience with feature stores (e.g., feast, tecton). • knowledge of distributed training (e.g., horovod, distributed pytorch). • familiarity with big data tools (e.g., spark, hadoop, beam). • understanding of nlp, computer vision, or time series analysis techniques. • knowledge of experiment tracking tools (e.g., mlflow, weights & biases). • experience with model explainability techniques (e.g., shap, lime). • familiarity with reinforcement learning or generative ai models. tools & technologies: • languages: python, sql (optionally: scala, java for large-scale systems) • ml frameworks: tensorflow, pytorch, scikit-learn, xgboost, lightgbm • mlops: mlflow, weights & biases, kubeflow, seldon core • data processing: pandas, numpy, apache spark, beam • model serving: tensorflow serving, torchserve, fastapi, flask • cloud platforms: aws (sagemaker, s3, ec2), google cloud ai platform, azure ml • orchestration: docker, kubernetes, airflow • databases: postgresql, bigquery, mongodb, redis • experiment tracking & monitoring: mlflow, neptune.ai, weights & biases • version control: git (github, gitlab) • communication: slack, zoom • project management: jira, confluence",milton keynes,Machine Learning Engineer,"['airflow', 'aws', 'azure', 'bigquery', 'cloud', 'computer vision', 'data pipeline', 'deep learning', 'gcp', 'google cloud', 'hadoop', 'java', 'lightgbm', 'machine learning', 'nlp', 'numpy', 'pandas', 'python', 'pytorch', 'r', 'scala', 'scikit-learn', 'spark', 'sql', 'statistics', 'tensorflow', 'time series', 'xgboost']","['airflow', 'aws', 'azure', 'bigquery', 'cloud', 'computer vision', 'data pipeline', 'deep learning', 'gcp', 'google cloud', 'hadoop', 'java', 'lightgbm', 'machine learning', 'nlp', 'numpy', 'pandas', 'python', 'pytorch', 'r', 'scala', 'scikit-learn', 'spark', 'sql', 'statistics', 'tensorflow', 'time series', 'xgboost']",£36k–£60k a year
mlops engineer,tekever,"are you ready to revolutionise the world with tekever? join us, the european leader in unmanned technology, where cutting-edge advancements meet unparalleled innovation. we offer a unique surveillance-as-a-service solution that provides real-time intelligence, enhancing maritime safety and saving lives. tekever is setting new standards in intelligence services, data and ai technologies. become part of a dynamic team transforming maritime surveillance and making a significant impact on global safety. at tekever, our mission is to provide limitless support through mission-oriented game-changers, delivering the right information at the right time to facilitate critical decisions. if you’re passionate about technology and eager to shape the future, tekever is the place for you! job overview: as an mlops engineer, you will be responsible for managing and optimizing the machine learning lifecycle, from model development to deployment and monitoring. you will work closely with data scientists, software engineers and it operations teams to ensure seamless integration, scalability and reliability of machine learning models in production environments. the ideal candidate will have a strong background in both machine learning and devops, with experience in building and maintaining robust mlops pipelines. what will be your responsibilities: • pipeline development: design, implement and maintain scalable and efficient machine learning pipelines that automate the process of model training, testing, deployment and monitoring. • model deployment: collaborate with data scientists to deploy machine learning models to production environments, ensuring they are scalable, reliable and secure. • ci/cd integration: develop and maintain continuous integration and continuous deployment (ci/cd) processes for machine learning models, ensuring seamless updates and version control. • infrastructure management: set up and manage cloud-based and on-premise infrastructure for machine learning workflows, including data storage, computing resources and model serving platforms. • monitoring and maintenance: monitor the performance and health of deployed models, implementing automated systems for anomaly detection, logging and alerting to ensure high availability and performance. • collaboration: work closely with cross-functional teams, including data scientists, software developers and it operations, to define requirements and deliver solutions that meet business and technical needs. • security: implement best practices for data security, model governance and compliance, ensuring that machine learning workflows adhere to industry standards and regulations. • documentation: maintain comprehensive documentation of mlops processes, infrastructure and best practices for future reference and reproducibility. • innovation: stay current with the latest advancements in mlops tools and technologies, continuously improving and evolving the mlops processes and infrastructure. profile and requirements: • education: bachelors or masters degree in computer science, data science, engineering, or a related field • experience: 3+ years of experience in machine learning, devops, or a related field, with specific experience in mlops. • technical skills: • proficiency in programming languages such as python, go, rust, or a similar language. • experience with machine learning and deep learning frameworks such as tensorflow, tensorrt, pytorch, or scikit-learn. • strong knowledge of devops practices, including ci/cd, infrastructure as code (iac) and containerization (docker, kubernetes). • experience with version control systems (e.g., git) and collaborative development tools. • understanding of data engineering concepts and tools for data preprocessing and etl. • knowledge of monitoring and logging tools (e.g., prometheus, grafana, elk stack). • experience with relevant tooling such as clearml for ml lifecycle management. • experience in getting machine learning products to production. • experience with cloud platforms such as aws, azure, or google cloud, with focus on google cloud. • analytical skills: excellent analytical and problem-solving skills with the ability to design innovative solutions to complex problems. • communication: strong verbal and written communication skills, with the ability to effectively collaborate with technical and non-technical stakeholders. • attention to detail: high attention to detail and a commitment to ensuring the accuracy and quality of work. • adaptability: ability to thrive in a fast-paced, dynamic environment and manage multiple projects simultaneously. what we have to offer you: • an excellent work environment and an opportunity to create a real impact in the world; • a truly high-tech, state-of-the-art engineering company with flat structure and no politics; • working with the very latest technologies in data & ai, including edge ai, swarming - both within our software platforms and within our embedded on-board systems; • flexible work arrangements; • professional development opportunities; • collaborative and inclusive work environment; • salary compatible with the level of proven experience.",bristol (+1 other),Machine Learning Engineer,"['aws', 'azure', 'cloud', 'deep learning', 'etl', 'excel', 'google cloud', 'machine learning', 'python', 'pytorch', 'r', 'scala', 'scikit-learn', 'tensorflow']","['aws', 'azure', 'cloud', 'deep learning', 'etl', 'excel', 'google cloud', 'machine learning', 'python', 'pytorch', 'r', 'scala', 'scikit-learn', 'tensorflow']",
machine learning engineer - ai and automation,ocho,"machine learning engineer - intelligent automation location: belfast (hybrid) eligibility: must have the right to work in the uk (no sponsorship available) i am working with a high-growth ai automation company that is launching a brand-new engineering hub in belfast, and we're searching for machine learning engineers to help build the ai systems that power intelligent, agent-driven software testing. you'll work at the intersection of data engineering, machine learning, and platform development - turning research ideas into production-ready models and shaping how ai is used across a global automation platform. if you enjoy building real-world ml systems, working with large datasets, and solving complex technical challenges, this is an opportunity to join early and make a real impact. why join? • join the founding ml team of a brand-new belfast hub • work with cutting-edge ai powering next-generation test automation • build production ml systems used by global enterprise customers • collaborate with data engineers, software engineers, and product teams • £65k - £85k what you'll be doing: • build, deploy, and optimise machine learning models for intelligent testing and automation • work closely with data engineering to design and maintain robust feature pipelines • experiment with algorithms across nlp, anomaly detection, sequence models, and deep learning • monitor model performance and create tools for observability, diagnostics, and evaluation • collaborate with software engineering teams to integrate ml into production services • contribute to improving mlops practices, tooling, documentation, and automation workflows • help shape the technical direction for ml within the new belfast hub what you'll bring: • strong experience developing ml models using python and modern frameworks (e.g., pytorch, tensorflow, scikit-learn) • experience deploying ml models into production environments • solid understanding of data structures, feature engineering, and model lifecycle management • comfort working with cloud platforms (gcp ideal, aws/azure welcome) • hands-on experience with containerisation (docker/kubernetes) • strong collaboration skills and the ability to explain ml concepts to non-experts • a pragmatic, experiment-driven mindset focused on shipping reliable solutions interested? if you'd like to join a growing ai team building genuinely impactful systems, reach out to justin donaldson for a confidential chat or send your cv to learn more.",belfast,Machine Learning Engineer,"['aws', 'azure', 'cloud', 'deep learning', 'feature engineering', 'gcp', 'machine learning', 'nlp', 'python', 'pytorch', 'r', 'scikit-learn', 'tensorflow']","['aws', 'azure', 'cloud', 'deep learning', 'feature engineering', 'gcp', 'machine learning', 'nlp', 'python', 'pytorch', 'r', 'scikit-learn', 'tensorflow']",
remote product data analyst,scale ai,"join a global community of talented professionals to shape the future of ai. earn up to $15 usd/hr and additional rewards based on quality of submission. outlier is committed to improving the intelligence & safety of ai models. owned and operated by scale ai, we've recently been featured in forbes for partnering experts with top ai labs to provide the high quality data for llms. we believe ai can only perform as well as the data it's trained on. that's why we work with contributors from all over the world, who help improve ai models by providing expert human feedback. this data has led to ai advancements for the world's leading ai labs and large language model builders. we've built a best-in-class remote work platform for our freelance contributors to provide valuable, specialized skills, and we in turn strive to provide them with a positive experience based on our core pillars of reliability, transparency, and flexibility. what you will be doing we are looking for someone who speaks fluent english to contribute their expertise toward training and refining cutting-edge ai systems. • adopt a user mindset to produce natural data to meet the realistic needs you have or would use ai for. • use the tool of rubrics to address user needs in a structured way. • evaluate ai outputs by reviewing and ranking reasoning and problem-solving responses from large language models. • contribute across projects depending on your specific skillset and experience. what we're looking for • education: bachelor's degree or higher (or currently enrolled). • analytical and problem-solving skills: ability to develop complex, professional-level prompts and evaluate nuanced ai reasoning. • strong writing: clear, concise, and engaging writing to explain decisions or critique responses. • attention to detail: commitment to accuracy and ability to assess technical aspects of model outputs. nice to haves: • experience in fields like literature, creative writing, history, philosophy, theology, etc. • prior writing or editorial experience (content strategist, technical writer, editor, etc.). • interest or background in ai, machine learning, or creative tech tools. compensation and benefits earn up to $15 usd/hr, paid out weekly rates vary based on quality, accuracy, and time spent. paid via paypal & airtm free access to model playground interact, experiment and engage with leading large language models free of cost flexible schedule and time commitment no contracts, no 9-to-5. you control your schedule. (most experts spend 5-10 hours/week, up to 40 hours working from home join a global community of coding experts join a global network of experts contributing to advanced ai tools disclaimer: for non-core work, such as during initial project onboarding or project overtime phases, lower rates may apply. certain projects offer incentive payments. please review the payment terms for each project. equal opportunity employer: outlier is committed to fostering a diverse and inclusive work environment. we welcome applicants from all backgrounds and celebrate diversity in our workforce.","denver, co",Data Analyst,"['machine learning', 'r']","['machine learning', 'r']",
data intelligence analyst,prove,"denver, co about prove as the world moves to a mobile-first economy, businesses need to modernize how they acquire, engage with and enable consumers. prove’s phone-centric identity tokenization and passive cryptographic authentication solutions reduce friction, enhance security and privacy across all digital channels, and accelerate revenues while reducing operating expenses and fraud losses. over 1,000 enterprise customers use prove’s platform to process 20 billion customer requests annually across industries, including banking, lending, healthcare, gaming, crypto, e-commerce, marketplaces, and payments. for the latest updates from prove, follow us on linkedin. prove is driving the future of digital identity. we are looking for provers who know how to make an impact. we’re talking self-starting professionals who thrive in a fast-paced environment, process information quickly, and make intelligent decisions. the work is challenging and requires not only smart but natural curiosity and tenacity. teamwork is also important to us – we work together and play together. prove has big plans, and we’re excited about the future. if this sounds like the place for you – come join our team! department: data science reports to: manager, data scientist engineering flsa status: exempt location: denver, co or chicago, il job summary as a prove data intelligence analyst you’ll be responsible for managing the data infrastructure and reporting systems that directly inform business strategy. you’ll partner with stakeholders across sales, customer success, finance, and product to transform complex raw data into actionable business insights and manage enterprise-level reporting to our customers. you will join a fast-paced data science team that enjoys solving mysteries and has a desire to continuously grow and learn. the right candidate is highly collaborative, committed to giving and receiving constructive feedback, and effective in presenting analysis to both technical and non-technical stakeholders. responsibilities • develop and maintain lookml models to standardize key performance indicators (kpis). • optimize dashboards for usability and performance, adhering to snowflake limitations. • automate recurring reporting tasks, such as scheduling looker email deliveries. • incorporate observability metrics into business intelligence (bi) reports to enhance analytical depth. • partner with data engineering and data science teams to champion data requirements. • collaborate with the finance and legal teams to provide reporting support for contracts. • synthesize data learnings into compelling stories and communicate to stakeholders. • monitor and help refine metrics for product efficacy and customer success. • assist in prototyping new analytics & machine learning models that improve both our insights and the product directly. • work with product and engineering teams to aid testing of new product ideas and analyze results to provide actionable recommendations. • perform deep analysis and build models to understand customer and product behavior, and extract key insights that impact product decisions. • work with the broader data science team to find ways to scale our insights through better systems and automation. • monitor and troubleshoot bi system performance to ensure data accuracy and availability • implement governance practices, including row-level security, to ensure secure and compliant access to data. • lead customer data studies, providing valuable insight to sales & customer success teams both during pre & post sale. experience • 3+ years of experience with looker as a data visualization tool. • strong expertise with sql and preferred knowledge of lookml • strong understanding of looker performance tuning, caching behavior, and dashboard optimizations. • advanced investigation using observability tools (splunk/honeycomb). • knowledge of data warehousing and modeling concepts • familiar with gitlab or similar tool for code management • experience training non-technical & technical users to use and/or build bi reports. • proficiency with apis connections for automation and integration. • proficient in etl and alerting functions within snowflake/looker. • experience in r or python. • experience with applying high level statistical concepts to solve analytics problems. • degree in computer science, information systems, data analytics or equivalent experience. • excellent communication and presentation skills nice to have: • experience with supervised and unsupervised modeling • deep experience (5+ years) working as a looker developer building views, explores, models, and final dashboards / reports this position description should not be considered the final description of the position. the position description is not intended to be an all-inclusive list of duties and standards of the positions. it should be assumed that we would, to some extent, structure responsibilities in accordance with the successful candidate’s capabilities and changing business conditions. incumbents will follow any other instructions, and perform any other related duties, as assigned by their supervisor. the anticipated salary range for this role is $95,000 - $110,000 plus variable commission / company bonus. offered salary will be determined by the applicant’s education, experience, knowledge, skills, geo-location and abilities, as well as internal equity and alignment with market data. benefits & perks for fte provers: • competitive salaries & bonus plan (for eligible roles) and equity plan • modern health for financial, mental, and physical wellness • 401(k) retirement plan & match (us offices) and local country pension (international offices) • unlimited vacation and flexible hours • comprehensive medical benefits for you and your family ❤️ • emotional & physical wellness – access to wellness services (eap & prove well-being reimbursement) • bottomless snacks & beverages for certain office locations • daily grubhub stipend for lunch if coming into the office (us offices) • a great place to work and connect with other talented provers like yourself! don’t meet every single requirement? studies have shown that women and people of color are less likely to apply to jobs unless they meet every single qualification. at prove we are dedicated to building a diverse, inclusive and authentic workplace, so if you’re excited about this role but your past experience doesn’t align perfectly with every qualification in the job description, we encourage you to apply anyways. you may be just the right candidate for this or other roles. equal opportunity employment: prove is an equal opportunity employer committed to providing equal employment opportunity for all people regardless of race, color, religion, gender or sexual orientation, age, marital status, national origin, citizenship status, disability, veteran status or other personal characteristics privacy & data protection: when you are applying for a job at prove, we collect and use your personal information in the job application process. to understand more about how prove uses your personal information, please see our recruitment privacy policy on our website. create a job alert interested in building your career at prove? get future opportunities sent straight to your email. create alert","denver, co",Data Analyst,"['business intelligence', 'dashboard', 'data analytics', 'etl', 'excel', 'looker', 'machine learning', 'python', 'r', 'recommendation', 'snowflake', 'sql']","['business intelligence', 'dashboard', 'data analytics', 'etl', 'excel', 'looker', 'machine learning', 'python', 'r', 'recommendation', 'snowflake', 'sql']",95K–110K a year
[remote] data analyst remote,liviniti,"note: the job is a remote job and is open to candidates in usa. southern scripts is a company that focuses on pharmacy benefit management. the pbm data analyst is responsible for summarizing, analyzing, and interpreting pharmaceutical data to support decision-making processes, with the goal of enhancing pharmacy benefit programs and ensuring high-quality patient care. responsibilities • gather and compile pharmacy and healthcare-related data from various sources, including claims, prescriptions, and pharmacy networks, ensuring comprehensive data capture • ensure data accuracy, completeness, and consistency through rigorous validation and cleaning processes to maintain high data quality standards • apply statistical and analytical techniques to identify trends, patterns, and anomalies in pharmaceutical claim data, providing actionable insights to support decision-making • perform cost-benefit analysis and cost-effectiveness studies for different medication and treatment options, helping to optimize financial and clinical outcomes • create reports, dashboards, and visualizations using tools like domo, tableau, or power bi, effectively conveying insights and findings to various stakeholders, including senior leadership and cross-functional teams • compare data from different time periods and between different types of customers to better assess which plan actions are working and to what degree • utilize predictive modeling and forecasting techniques to project future pharmacy benefit program costs and utilization patterns, aiding in strategic planning and budgeting • assess the impact of different benefit design changes on costs and patient outcomes, providing data-driven recommendations for program adjustments • stay updated with healthcare regulations and ensure that pharmacy benefit programs comply with relevant laws and regulations, including those related to data privacy and security • assist in audits and compliance reporting as necessary, ensuring transparency and adherence to regulatory requirements • provide data-driven recommendations for optimizing pharmacy benefit programs, including formulary management, prior authorization criteria, and cost-saving strategies • collaborate with cross-functional teams including pharmacists, account managers, and it professionals to implement recommended changes and enhance program performance • implement data validation processes to ensure data integrity and accuracy, troubleshooting data-related issues and discrepancies promptly • communicate complex data analyses clearly and understandably, tailoring messages to diverse audiences, including technical and non-technical stakeholders • develop and disseminate best practices for researching and answering our clients' questions about their benefit experiences and efficacies • proficiency in data analysis tools such as sql, python, or r, and familiarity with data visualization tools like domo or power bi, enabling robust and efficient data handling • familiarity with pbm software and pharmacy claims processing systems, ensuring seamless integration and analysis of diverse data sources • ability to maintain the confidentiality of sensitive medical information and knowledge of hipaa privacy and security rules • abide by all obligations under hipaa related to protected health information (phi) • if a hipaa violation is discovered, whether individually or by another, you must report the violation to the compliance officer and/or human resources • attend, complete, and demonstrate competency in all required hipaa training offered by the company • flexibility to understand, appreciate, and embrace that this job description is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee. duties, responsibilities, and activities may change, or new ones may be assigned at any time with or without notice skills • strong analytical skills and proficiency in data analysis tools and programming languages, including sql, python, or r • understanding of pharmacy benefit terminology, formulary management, and medication pricing, providing a solid foundation for effective program analysis • previous experience in healthcare analytics, pharmacy benefit management, or a related field, with a preference for experience in healthcare data analysis • excellent communication and presentation skills, capable of conveying complex analyses clearly and persuasively • attention to detail and the ability to work independently and as part of a team, fostering a collaborative and precise work environment • bachelor's degree in data science, statistics, healthcare management, or a related field; advanced degree preferred • 2 years experience in data reporting or analysis role • familiarity with pharmacy claims coding terminology preferred • degree or equivalent experience; preferably in a technology or technical business discipline • intermediate level knowledge with any of the following: sql server or other rdms, ssrs, or power bi benefits • medical, dental, vision insurance • disability and life insurance • employee assistance program • remote work options • generous paid-time off • annual reviews and development plans • retirement plan with company match immediately 100% vested company overview • liviniti specializes in pharmacy benefit management, offering services like prior authorization and drug price look-up. it was founded in 2011, and is headquartered in natchitoches, louisiana, usa, with a workforce of 501-1000 employees. its website is https://liviniti.com.",anywhere,Data Analyst,"['aws', 'dashboard', 'data analysis', 'excel', 'power bi', 'python', 'r', 'recommendation', 'sql', 'sql server', 'statistics', 'tableau']","['aws', 'dashboard', 'data analysis', 'excel', 'power bi', 'python', 'r', 'recommendation', 'sql', 'sql server', 'statistics', 'tableau']",
project manager & sales data analyst,wutznxt,"join wutznxt, a strategy and innovation consulting firm, as a project manager & sales data analyst. in this role, you’ll partner with clients to lead strategic initiatives, analyze sales performance, and create impactful communication that influences decision-making. if you thrive in dynamic environments, love leveraging ai tools, and want to work with cutting-edge technologies, this is your opportunity. employment type: independent contractor, full-time/40+ hours/week (not a w2 position) location: remote (must be within the united states) what you’ll do • lead projects: drive multiple strategic programs from planning to execution. • deliver insights: analyze sales and margin data, build dashboards, and uncover trends using powerbi, excel, and ai-driven tools. • communicate impact: craft executive-ready presentations, reports, and updates that influence leadership decisions. • optimize operations: manage team rhythms, meetings, and reporting to keep projects on track. • ensure compliance: submit monthly proof-of-execution reports on all deliverables per client vendor standards. what you bring • proven experience in project management and data analytics. • advanced skills in powerbi, excel, and powerpoint. • strong communication and storytelling abilities. • familiarity with microsoft cloud and sales processes. • ability to adapt quickly and deliver under tight deadlines. • strong time management and ability to work well both independently and in virtual meetings. • certification: microsoft 365 fundamentals (required within 90 days). why you’ll love this role • work with client stakeholders on high-impact projects. • gain exposure to emerging technologies and ai tools. • enjoy flexible remote work across global teams. • be part of a 15+ year trusted strategic client partnership driving innovation. about wutznxt wutznxt specializes in bringing creative ideas from concept to market, combining creativity, technology, and executive business expertise. for 15+ years, wutznxt has partnered with companies to lead initiatives for growth strategies, ai utilization, customer-experience reinvention, and leadership empowerment. additional information employment is contingent upon the successful completion of a background check in accordance with applicable laws and regulations. wutznxt is an equal opportunity employer. we celebrate diversity and are committed to creating an inclusive environment for all contractors and employees. we welcome individuals from all backgrounds and strive to build a workforce that reflects the communities we serve. job type: contract pay: $80,000.00 - $200,000.00 per year location: • united states (required) work location: remote",anywhere,Data Analyst,"['aws', 'cloud', 'dashboard', 'data analytics', 'excel', 'r']","['aws', 'cloud', 'dashboard', 'data analytics', 'excel', 'r']",80K–200K a year
senior data analyst contract,noredink,"teaching kids to think critically and communicate effectively has never been more important, and noredink helps students become better writers in more than 60% of u.s. school districts. we are looking for a highly analytical and detail-oriented data analyst to join our team as a full-time contractor. in this role, you will partner closely with departments across the company including sales, customer success, product, and leadership to deliver high-quality analytics, reporting, and insights that drive strategic and operational decision-making across the company. you’ll work within our modern data stack including snowflake, dbt, and sigma, to transform raw data into trusted dashboards, ad-hoc analyses, and actionable recommendations. this is a hands-on role that requires strong sql skills, excellent communication, and the ability to juggle multiple requests at once. responsibilities • serve as the primary analytics partner for cross-functional stakeholders (sales, cs, product, operations, leadership). • create, maintain, and improve sigma dashboards, reports, and deep-dive analyses. • develop and optimize sql queries and transformations in snowflake. • translate ambiguous business questions into clear analytical frameworks and data requirements. • conduct exploratory data analysis and root-cause investigations. • build and maintain metric definitions and data documentation. • ensure data integrity, accuracy, and consistency across dashboards and analytical workflows. • prioritize incoming data requests and manage expectations with stakeholders. • identify trends, patterns, and opportunities to improve business performance. • support experimentation and product analytics (e.g., usage patterns, cohort analysis). requirements • 3–5+ years of experience as a data analyst, bi analyst, product analyst, or similar role. • strong proficiency in sql (snowflake experience strongly preferred). • hands-on experience with dbt and modern elt/analytics engineering best practices. • strong experience with bi tools; sigma experience a major plus. • ability to translate business questions into technical requirements and analytical outputs. • experience supporting cross-functional stakeholders (sales, cs, product highly preferred). • high attention to detail and strong data quality mindset. • excellent written and verbal communication skills. • strong prioritization skills and ability to manage multiple workstreams independently. • comfort working in a fast-paced, high-growth b2b saas environment. nice-to-have • experience with product analytics tools (e.g., amplitude, mixpanel, heap). • experience with forecasting, propensity modeling, or other statistical techniques. • prior work in early-stage or mid-stage saas startups. • basic familiarity with python or r for advanced analysis. about noredink: noredink helps students in over 60% of us school districts become better writers. our deeply engaging, adaptive curriculum personalizes exercises to kids' interests, guides them through the writing process with instructional support, and boosts their skills through targeted practice. students have completed over 10 billion exercises on our site, and our mission is to help all students harness the power of the written word. check out our press page, including our 2-minute pitch on nbc and articles in the washington post, wall street journal, and forbes. noredink believes that diversity and inclusion among our teammates is critical to our success, and we seek to recruit, develop, and retain the most talented people from a diverse candidate pool. at noredink, we are committed to providing an environment of mutual respect where equal employment opportunities are available to all applicants and teammates without regard to race, color, religion, sex, pregnancy, national origin, age, physical and mental disability, marital status, sexual orientation, gender identity, gender expression, genetic information, military and veteran status, and any other characteristic protected by applicable law. the hourly compensation for this role is $75/hour and accounts for a variety of locations and skills/experience. please know that if you are invited to speak with a recruiter at noredink, they will have an open discussion about compensation during your first call to ensure alignment. • note: agencies or other third-party recruiters may not submit unsolicited candidate resumes or their information to any noredink employee, including a noredink recruiter, unless a contract is signed and you are given permission by the talent acquisition team to work on a job opening.",anywhere,Data Analyst,"['bi tools', 'dashboard', 'data analysis', 'dbt', 'elt', 'excel', 'experimentation', 'python', 'r', 'recommendation', 'snowflake', 'sql']","['bi tools', 'dashboard', 'data analysis', 'dbt', 'elt', 'excel', 'experimentation', 'python', 'r', 'recommendation', 'snowflake', 'sql']",75 an hour
data analyst i,developmental pathways inc,"title: data analyst i location: candidates must reside in colorado. hybrid schedule: this position follows a flexible hybrid schedule, with an equal balance of remote work and on-site presence at our headquarters in aurora, co. must have: professional field experience in sql and power bi. we are ideally seeking an individual with at least 3 years of experience as a data analyst. please note: this position requires workers to be authorized to work in the u.s without requiring visa sponsorship by an employer now or in the future. developmental pathways (dp) has an opportunity for a data analyst i in our information technology (it) department. the data analyst i role is a valuable opportunity to contribute to impactful, data-driven initiatives across the organization. this role will work with diverse datasets to uncover insights, support reporting needs, and help create visualizations that inform decision-making. as a key member of the data and development team, the data analyst i will collaborate with cross-functional partners to support business goals through data analysis, visualization, and reporting. this role will apply skills in sql, power bi, and data modeling to help turn raw data into meaningful information, while continuing to develop technical and analytical expertise. this role offers a 4-day, 40-hour work week, monday-thursday. dp operates a primarily hybrid work environment with in-person requirements at our colorado office. the frequency of on-site work varies by department and is subject to change based on operational needs. application deadline: sunday, december 21st at 11:59 pm (mst) compensation and benefits $76,839 - $86,444 annually • health/dental/vision coverage • employer-paid and supplemental life insurance • short- and long-term disability insurance • generous paid time off and holiday pay • flexible work schedule • monthly remote work stipend • 401(k) investment plan, with an employer match of up to 4% • mileage reimbursement • tuition reimbursement program • certified employer for public service loan forgiveness program • healthcare reimbursement and flexible spending plan • discounts on auto and homeowners’ insurance • employee assistance program • credit union membership • employer-paid training you belong! to learn more about our commitment to inclusion and diversity, visit our website: https://www.dpcolo.org/about-us/inclusion-and-diversity/ required • bachelor's degree in a computer-related or business studies field and two (2) years of related experience, or associate's degree and three (3) to five (5) years of related experience, or six (6) years of related work experience • knowledge and field experience in t-sql, including experience in writing, maintaining, and optimizing sql queries for efficient data transformation and complex business logic • assist in monitoring and interpreting quality metrics • understanding of data visualization principles and best practices, with a proven ability to design clear, engaging, and effective visual representations of data that communicate complex insights to a wide range of stakeholders • knowledge in selecting appropriate chart types, designing intuitive dashboards, and ensuring that visualizations are both functional and easy to interpret. familiarity with data visualization best practices and principles • excellent communication and presentation skills, with the ability to effectively convey technical concepts to non-technical stakeholders • ability to work effectively in a fast-paced environment and manage multiple priorities • adaptability and willingness to learn new technologies and methodologies • personal vehicle in good operating condition for use during work as appropriate • valid colorado motor vehicle license and proof of motor vehicle insurance • ability to meet our driving requirements - no suspensions/ no more than two tickets in the past three years upon request, qualifying need, and approval, driving requirements may be waived in lieu of reliable transportation. preferred • experience with azure devops boards: proficiency in using azure boards to track, manage, and analyze work items, sprints, and project progress. familiarity with creating dashboards, generating insights from board data, and using custom queries to support team reporting and agile processes • basic understanding of cms quality measures and regulatory reporting frameworks • certifications: (microsoft certified: power bi data analyst associate, microsoft certified: azure data engineer associate, microsoft certified: azure fundamentals, certified analytics professional (cap)) additional responsibilities • data analysis: analyze large volumes of structured and unstructured data from a variety of internal and external sources. use data analysis techniques to identify key trends, patterns, and anomalies that support business objectives and strategic priorities • t-sql: write, maintain, and optimize stored procedures, functions, and dynamic sql for efficient data transformation and complex business logic. craft t-sql queries, utilizing window functions, and implementing best practices for error handling • power bi development: design and develop interactive dashboards, reports, and data visualizations using power bi to translate complex data into easily understandable insights. utilize power bi features, including dax calculations, power query, and data modeling to ensure reports meet the needs of business stakeholders • collaboration: collaborate with cross-functional teams to understand business requirements and translate them into well-designed data-driven solutions • performance monitoring: monitor the performance of data models, dashboards, and reports to ensure they are delivering the expected value to stakeholders. proactively recommend and implement optimizations to improve speed, efficiency, and scalability • continuous improvement: stay up to date with the latest trends, emerging technologies, and best practices in data analytics, power bi, and data visualization. continuously refine skills and knowledgebase to adopt cutting-edge tools and techniques for optimizing data analysis and reporting processes about us started in 1964, developmental pathways (dp) is a nonprofit agency serving more than 15,000 individuals with disabilities and their families in arapahoe, douglas, and elbert counties. we are a designated case management agency (cma) for long-term care services and are one of colorado’s community centered boards (ccb), connecting people to federal, state, county, and private funding. we are also colorado’s largest early intervention (ei) provider and help connect young children to essential resources, serving the city of aurora in addition. our mission is to enrich the lives of people with disabilities/delays by partnering to provide expertise, support, and advocacy in their pursuit of a meaningful life. at dp, we believe that our people are our greatest asset. we want to give you the ability to grow and do what you love. we are committed to creating an inclusive and dynamic work environment where employees can thrive and make a meaningful impact. if you are looking for a challenging and rewarding career with an organization that value. if you need an accommodation with this application process, please contact support-hr@dpcolo.org","aurora, co",Data Analyst,"['azure', 'dashboard', 'data analysis', 'data analytics', 'excel', 'power bi', 'r', 'scala', 'sql']","['azure', 'dashboard', 'data analysis', 'data analytics', 'excel', 'power bi', 'r', 'scala', 'sql']","76,839–86,444 a year"
"data analyst, clinical analytics and reporting",davita,"posting date 11/19/2025 2000 16th st, denver, colorado, 80202-5117, united states of america the clinical analytics & reporting team is responsible for designing, developing and overseeing analytical solutions for our clinical and operations executives in support of davita’s clinical strategy. our team’s mission is to improve patient quality of life in terms of care safety, quality, and efficiency by transforming data into actionable information that guides strategic decisions, supports clinical innovation, and informs operating tactics. this position will primarily support and develop business intelligence tools to facilitate and guide collaboration between the integrated kidney care and davita kidney care teams. the senior clinical data analyst’s primary objective is to partner with diverse teams from around the enterprise to produce analytical insights that support analytical products across a broad range of clinical initiatives. a successful teammate exemplifies personal behaviors in line with davita’s core values: service excellence, integrity, team, continuous improvement, accountability, fulfillment, and fun. sound like you? then you might be a great fit for the role of senior clinical data analyst on davita’s clinical analytics & reporting team. responsibilities • create, maintain, and distribute reports related to clinical outcomes, surveillance, and risk mitigation on a regular basis • utilize multiple computational languages, platforms and environments to mine and manage data in preparation for analysis and/or reporting • provide clinical insights and data trends to support and inform a multitude of projects and initiatives across the organization • present data to and work closely with a variety of stakeholders including senior-level operators and teammates in the clinics • synthesize results in a non-technical manner to inform recommendations and next steps for leadership • create cohesive presentations based on findings, summarizing insights from analysis • act as a collaborative partner during strategy meetings • become a subject matter expert and thought partner in clinical and methodologic areas • willing to travel (up to 10% of time) qualifications • bachelor’s degree required; quantitative fields such as biostatistics, statistics, informatics, computer science, or math strongly recommended • 3-5 years of prior relevant work experience preferred • proficiency in any of the following preferred: sql, tableau, anaplan, python, r, etc. • demonstrated ability to learn and develop proficiency in any or all of the following as needed: sql, tableau, anaplan, python, r, etc. • microsoft excel and powerpoint proficiency required • advanced critical thinking, strong work ethic, creative mindset, adept communication skills, attention to detail, design-mindedness, flexibility with change, comfort with ambiguity, efficient time management, technical aptitude, and business acumen what we’ll provide: more than just pay, our davita rewards package connects teammates to what matters most. teammates are eligible to begin receiving benefits on the first day of the month following or coinciding with one month of continuous employment. below are some of our benefit offerings. • comprehensive benefits: medical, dental, vision, 401(k) match, paid time off, pto cash out • support for you and your family: family resources, eap counseling sessions, access headspace®, backup child and elder care, maternity/paternity leave and more • professional development programs: davita offers a variety of programs to help strong performers grow within their career and also offers on-demand virtual leadership and development courses through davita’s online training platform starlearning. #li-cm5 at davita, we strive to be a community first and a company second. we want all teammates to experience davita as ""a place where i belong."" our goal is to embed belonging into everything we do in our village, so that it becomes part of who we are. we are proud to be an equal opportunity workplace and comply with state and federal affirmative action requirements. individuals are recruited, hired, assigned and promoted without regard to race, national origin, religion, age, color, sex, sexual orientation, gender identity, disability, protected veteran status, or any other protected characteristic. this position will be open for a minimum of three days. the salary range for the role is $56,500.00 - $84,000.00 per year. if a candidate is hired, they will be paid at least the minimum wage according to their geographical jurisdiction and the exemption status for the position. new york exempt: new york city and long island: $64,350.00/year, nassau, suffolk, and westchester counties: $64,350.00/year, remainder of new york state: $60,405.80/year new york non-exempt: new york city and long island: $16.50/hour, nassau, suffolk, and westchester counties: $16.50/hour, remainder of new york state: $15.50/hour washington exempt: $77,968.80/year washington non-exempt: bellingham: $17.66/hour, burien: $21.16/hour, unincorporated king county: $20.29/hour, renton: $20.90/hour, seattle: $20.76/hour, tukwila: $21.10/hour, remainder of washington state: $16.66/hour for location-specific minimum wage details, see the following link: davita.jobs/wagerates compensation for the role will depend on a number of factors, including a candidate’s qualifications, skills, competencies and experience. davita offers a competitive total rewards package, which includes a 401k match, healthcare coverage and a broad range of other benefits. learn more at https://careers.davita.com/benefits colorado residents: please do not respond to any questions in this initial application that may seek age-identifying information such as age, date of birth, or dates of school attendance or graduation. you may also redact this information from any materials you submit during the application process. you will not be penalized for redacting or removing this information.","denver, co",Data Analyst,"['business intelligence', 'excel', 'python', 'r', 'recommendation', 'sql', 'statistics', 'tableau']","['business intelligence', 'excel', 'python', 'r', 'recommendation', 'sql', 'statistics', 'tableau']",
data analyst (enterprise customer focus),"servicecore, inc.","data analyst (enterprise customer focus) internal applicants only company overview: servicecore and docket are rapidly-growing field-service software as a service platforms for the portable sanitation and dumpster industries, being named as the #80 fastest growing software companies in america by inc. 5,000. the customers we serve have been underserved by software, making us the leading players in a huge industry with very little competition. our software helps our incredibly hard-working business owners get more done and stress less. how? by supercharging their businesses with software that cuts wasted time, manages jobs, optimizes routes, tracks inventory, and automates billing. we are proud to offer a one-stop solution that allows our hard-working customers to be more productive and successful! we live by our core values of love our customers, be real, give a shit, deliver results and of course keep it fun. servicecore provides hard-working individuals the opportunity to work and grow within an agile, fast-paced start-up environment. we are proud of our accomplishments and take our jobs seriously while not taking ourselves too seriously. role overview servicecore is seeking a data analyst (enterprise customer focus) -a customer-obsessed, product-savvy teammate who thrives at the intersection of data, customer support, and enterprise account enablement. this role is ideal for someone who already understands servicecore's product lines deeply and has strong experience working directly with servicecore or docket customers-especially large, multi-location, or enterprise-level clients. while prior exposure to data, reporting, or analytics tools is a plus, technical skills can be taught. we are primarily looking for someone who knows how our customers operate within servicecore's software, can navigate complex client needs, and is excited to grow into more advanced analytical skills and data workflows over time. you will work closely with our data & analytics team, customer success, product, and support to ensure enterprise customers are successful with their reporting, data exports, and analytics-related needs. this role can flex from junior to mid-level, depending on the technical experience the candidate brings. what you'll do: • serve as a primary point of contact for enterprise and high-value customers who need support with reporting, analytics, data exports, and servicecore's data-powered features • troubleshoot data or reporting questions by blending your product knowledge with emerging technical skills • translate customer questions into clear problem statements, requirements, or tickets for the data & analytics team • partner closely with customer success managers to support onboarding and ongoing data needs for enterprise accounts • review, validate, and troubleshoot customer data sets, ensuring accuracy and consistency across servicecore's reports and dashboards • assist in building or modifying customer-facing dashboards (e.g., in sigma) under the guidance of the data & analytics team • over time, you will own the development of customer facing dashboards as a primary builder • document repeatable workflows, common faqs, and best practices for internal teams and customers • act as a bridge between customers and internal teams-helping to communicate technical concepts in a customer-friendly way • support the rollout of new reporting tools, data features, and analytics enhancements by gathering feedback and guiding customers through adoption • grow your technical skills through hands-on training in sql, snowflake, sigma, and more (depending on interest and level) what we're looking for: required • deep working knowledge of servicecore's product lines, features, customer workflows, and common data/reporting challenges • experience working directly with customers, ideally including enterprise or multi-location accounts • strong communication and client-consultation skills; able to explain complex topics clearly and diplomatically • ability to navigate ambiguity, ask clarifying questions, and stay organized when managing multiple customer needs • curiosity and willingness to learn technical concepts such as sql, data pipelines, dashboard tools, and schemas • a team-first mindset and a strong alignment to our core values • outstanding written and verbal communication is required to be successful in this role nice-to-have (but not required) • experience building or supporting dashboards, reports, or data exports (in sigma, powerbi, tableau, looker, excel, etc.) • exposure to sql or any data analysis workflow • familiarity with data warehouse concepts (star schema, fact vs dimension tables, etc.) • experience supporting saas enterprise clients or data integrations • prior experience collaborating with product, engineering, or data teams we value potential and product expertise. if you're passionate about helping customers succeed and excited to grow your technical skill set, we want to talk to you. work environment: • casual, open-office environment • tuesdays and wednesday in office for local employees (free parking!) • fully stocked kitchen with fresh food and coffee at our denver hq • hybrid and fully-remote work • regular company events / outings salary & benefits: • base salary:$75,000 - $110,000 (dependent on experience) • 14 company holidays in addition to unlimited pto • healthcare, dental and vision insurance with generous employer contributions • 401k w/ match • equity appreciation plan (units granted upon hire) • regular lunches and a fully-stocked kitchen (if in denver) • bi-weekly grubhub lunch stipend for remote folks • company-provided hardware of your choice/configuration • a strong company culture that lives by our core values - love our customers, be real, give a shit, deliver results, and keep it fun. if you believe you match the above criteria and are desiring a dynamic start-up environment with a management team that is dedicated to your success, then please apply! in addition to our commitment to equal pay for equal work, servicecore is also committed to equal opportunity regardless of race, color, age, ancestry, religion, gender, gender identity, genetic information, parental or pregnancy status, sexual orientation, marital status, citizenship, national origin, disability, or veteran status. location denver, co (hybrid) or remote within the usa department engineering employment type full-time minimum experience experienced in customer facing roles; beginner to experienced data handling roles. compensation $75,000 - $110,000","denver, co",Data Analyst,"['dashboard', 'data analysis', 'data pipeline', 'data warehouse', 'excel', 'looker', 'r', 'snowflake', 'sql', 'tableau']","['dashboard', 'data analysis', 'data pipeline', 'data warehouse', 'excel', 'looker', 'r', 'snowflake', 'sql', 'tableau']",75K–110K a year
"avp, actuary (#57994)",dw simpson global actuarial & analytics recruitment,"avp, actuary join an innovative company is looking to drive change in the marketplace. p&c insurance company is seeking an avp & actuary, who will lead and grow a team of actuarial analysts. in this role, you will execute the quarterly & annual reserving process, lead all pricing work for the company, and work closely with erm & data science on catastrophe modeling & interpretation. (#57994) requirements: • bachelor's degree. • acas or fcas designation. • 5+ years of actuarial experience. • previous leadership experience. • strong interpersonal, collaborative, & presentation skills. • familiarity with next-gen actuarial reserving & pricing tools preferred. locations: • melbourne, fl • dallas, tx • remote with 1 week a month in melbourne, fl • no remote from california",anywhere,Data Analyst,['r'],['r'],
data analyst - remote,zywave inc.,"position purpose: • assist zywave partners with loading critical business data to zywave insurance applications to provide a seamless client experience, foster product adoption, and assist with data reports. • manage data conversion and formatting processes to ensure quality data is loaded into zywave software products. • consult on initiatives by transferring data knowledge to internal and external stakeholders. • make zywave the best in our business. responsibilities: • partner with internal and external business associates to import data into zywave applications. • map, transform, and format data using a variety of third-party data mapping software platforms. • interpret data and provide recommendations based on findings to ensure quality data is successfully loaded or consumed. • generate and inspect routine and ad hoc reports to assist with zywave applications, both internally and for our customers. • maintain data standards and structure by producing validation and quality assurance reports to ensure data quality and consistency. • interface with zywave clients and third parties as needed to obtain and interpret data. • assist with data services team training and cross-departmental training efforts. • follow through on project assignments and keep stakeholders apprised of progress or any barriers to completion. • steer data related activities to help drive projects forward and to completion both internally and for our customers. • perform other tasks on projects as assigned. qualifications: • bs/ba in information systems, business, economics or other related discipline, or equivalent combination of education and experience. • minimum 2 years of experience in data or database related field required. • minimum 2 years of customer-facing or service experience required. • technical skills and experience with relational databases, ms sql server, and ms excel or similar data related software tools. • detail-orientation, strong sense of project ownership, and the ability to manage high project volume effectively. • strong customer service skills and the ability to work both with others and alone. • solid business writing and verbal communication skills. • passion for continuous learning, problem solving, and embracing change. • familiarity with medical or property and casualty insurance claims and general knowledge of insurance industry a plus. why work at zywave? zywave is a cutting-edge saas company that has led the digital revolution of the insurance industry over the last 25 years. we continue to make waves by launching new product functionality and integrations to deliver a world-class customer experience. since 2020, zywave has acquired several insurtech companies representing over 900 employees globally. with our remote-first environment and core values that encourage employees to grow, belong and transform professionally there are endless opportunities to build new skills and relationships. why wait to hop aboard the rocket ship? get onboard now! #li-mh1",anywhere,Data Analyst,"['excel', 'r', 'recommendation', 'sql', 'sql server']","['excel', 'r', 'recommendation', 'sql', 'sql server']",
data analyst/fm product consultant,us101 guidehouse inc.,"job family: data engineering & architecture consulting travel required: up to 10% clearance required: ability to obtain secret what you will do: guidehouse is currently seeking qualified data analysts & product managers to support guidehouse's defense & security segment. responsibilities will include, but are not limited to: serve as a key contributor on a data-driven team supporting army audit readiness objectives, focusing on financial management and technology enablement. design and develop financial dashboards and data visualizations that translate complex financial and operational data into actionable insights for senior stakeholders. act as a product manager for financial management technology solutions, defining requirements and ensuring alignment with audit readiness and compliance standards. collaborate with cross-functional teams to gather business requirements, prioritize features, and manage delivery of scalable solutions that enhance transparency and efficiency. analyze existing financial processes and systems to identify gaps, recommend improvements, and support modernization initiatives. support integration of advanced analytics and automation tools, leveraging platforms such as power bi, qlik, power apps, and databricks to improve reporting and decision-making. ensure all solutions adhere to federal financial management standards, audit readiness requirements, and applicable compliance frameworks. participate in agile ceremonies, manage product backlogs, and coordinate iterative development cycles to deliver high-quality solutions. what you will need: must be able to obtain and maintain a federal or dod ""secret"" security clearance; candidates must obtain approved adjudication of clearance prior to onboarding with guidehouse. candidates with an active ""secret"" or higher-level clearance are preferred. bachelor’s degree in finance, accounting, data analytics, information systems, or a related field. one (1) year of experience in data analysis, financial management, or technology-enabled transformation projects. strong proficiency in data visualization tools (e.g., power bi, qlik) and familiarity with data platforms (e.g., databricks). understanding of federal financial management processes, audit readiness requirements, and compliance standards. excellent communication and stakeholder engagement skills, with the ability to translate technical concepts into business terms. ability to commute to client and guidehouse offices in respective locations 3 days per week. what would be nice to have: an active and maintained ""secret"" federal or dod security clearance certifications in agile, scrum, or project management (e.g., pmp, csm, safe). advanced degree in finance, accounting, data analytics, information systems, or a related field. prior experience supporting federal audit readiness or cfo functions within dod or similar agencies. experience with army financial systems, such as gfebs and lmp. what we offer: guidehouse offers a comprehensive, total rewards package that includes competitive compensation and a flexible benefits package that reflects our commitment to creating a diverse and supportive workplace. benefits include: medical, rx, dental & vision insurance personal and family sick time & company paid holidays position may be eligible for a discretionary variable incentive bonus parental leave and adoption assistance 401(k) retirement plan basic life & supplemental life health savings account, dental/vision & dependent care flexible spending accounts short-term & long-term disability student loan paydown tuition reimbursement, personal development & learning opportunities skills development & certifications employee referral program corporate sponsored events & community outreach emergency back-up childcare program mobility stipend about guidehouse guidehouse is an equal opportunity employer–protected veterans, individuals with disabilities or any other basis protected by law, ordinance, or regulation. guidehouse will consider for employment qualified applicants with criminal histories in a manner consistent with the requirements of applicable law or ordinance including the fair chance ordinance of los angeles and san francisco. if you have visited our website for information about employment opportunities, or to apply for a position, and you require an accommodation, please contact guidehouse recruiting at 1-571-633-1711 or via email at recruitingaccommodation@guidehouse.com. all information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodation. all communication regarding recruitment for a guidehouse position will be sent from guidehouse email domains including @guidehouse.com or guidehouse@myworkday.com. correspondence received by an applicant from any other domain should be considered unauthorized and will not be honored by guidehouse. note that guidehouse will never charge a fee or require a money transfer at any stage of the recruitment process and does not collect fees from educational institutions for participation in a recruitment event. never provide your banking information to a third party purporting to need that information to proceed in the hiring process. if any person or organization demands money related to a job opportunity with guidehouse, please report the matter to guidehouse’s ethics hotline. if you want to check the validity of correspondence you have received, please contact recruiting@guidehouse.com. guidehouse is not responsible for losses incurred (monetary or otherwise) from an applicant’s dealings with unauthorized third parties. guidehouse does not accept unsolicited resumes through or from search firms or staffing agencies. all unsolicited resumes will be considered the property of guidehouse and guidehouse will not be obligated to pay a placement fee. guidehouse is a global ai-led professional services firm delivering advisory, technology, and managed services to the commercial and government sectors. with an integrated business technology approach, guidehouse drives efficiency and resilience in the healthcare, financial services, energy, infrastructure, and national security markets. built to help clients across industries outwit complexity, the firm brings together approximately 18,000 professionals to achieve lasting impact and shape a meaningful future. guidehouse.com",united states,Data Analyst,"['dashboard', 'data analysis', 'data analytics', 'databricks', 'excel', 'power bi', 'r', 'scala']","['dashboard', 'data analysis', 'data analytics', 'databricks', 'excel', 'power bi', 'r', 'scala']",
sr data analyst,oracle,"we’re on a journey to advance how health happens with technologies that empower patients, support clinicians, inspire innovation, and save lives. our mission? to create a human-centric healthcare experience powered by unified global data. it’s a big challenge, but big challenges are what we do best. we’re already transforming some of the world’s largest health systems—helping them turn data into lifesaving decisions and better patient care. we want people just as dedicated as we are to improving health equity and delivering quality care across the globe. if you’re excited about making healthcare more human, you’ve come to the right place. it's an exciting time to join oracle health! our government services team is hiring a senior data analyst, where you will analyze, prepare, and process data sets to be consumed for direct insights, statistical modeling or other analytical exploration for internal and external clients. in this role, some of your responsibilities will include to compile complex data sets from various sources for exploratory and pre-defined analyses; audit complex data sets for completeness, validity, and other pertinent data health measures; evaluate complex data sets for analytical possibilities and pitfalls with some assistance. you will transform complex data for specific team functions or processes as appropriate; conduct basic statistical testing and exploratory data analysis; create basic data visualizations leveraging design and data presentation guides. consult with internal and external clients on data availability and identification of key performance indicators, while pinpointing trends, correlations and patterns in complex data sets with some assistance. in addition, you will consult with internal and external clients on oracle health reporting capabilities and provide support to end-users on accessing and running reports. apply today to learn more about this role!",united states,Data Analyst,"['data analysis', 'r']","['data analysis', 'r']",
finance data analyst,scale ai,"outlier is a platform owned and operated by scale ai and believes ai can only perform as well as the data its trained on. thats why we work with contributors from all over the world, who help improve ai models by providing expert human feedback. this data has led to ai advancements for the world's leading ai labs and large language model builders. weve built a best-in-class remote work platform for our freelance contributors to provide valuable, specialized skills, and we in turn strive to provide them with a positive experience based on our core pillars of reliability, transparency, and flexibility. what you will be doing • create a grading rubric on what a good answer would be. • write the correct answer that scores 100% on the rubric. • provide clear, constructive feedback to improve ai-generated responses. what were looking for • phd or master's degree in finance or a related field. can be currently enrolled. • deep subject matter expertise with the ability to create complex, graduate-level problems that challenge ai reasoning. • strong analytical and problem-solving skills , with experience in crafting rigorous, high-quality questions and solutions. • attention to detail to accurately assess ai capabilities and evaluate peer submissions. • fluency / high proficiency in english .","denver, co (+1 other)",Data Analyst,['r'],['r'],
data and business operations analyst,echostar,"company summary echostar is reimagining the future of connectivity. our business reach spans satellite television service, live-streaming and on-demand programming, smart home installation services, mobile plans and products.today, our brands include boost mobile, dish tv, gen mobile, hughes and sling tv. department summary the in-home services corporate team is devoted to supporting thousands of talented employees in delivering the industry’s best products and solutions. constantly refining our best-in-class service, ihs corporate manages the behind-the-scenes operations to ensure a seamless customer experience through analytics, project management and training initiatives. job duties and responsibilities this is a position in the command center organization within in home services. the command center is an organization consisting of five different strategic teams, all focused on optimizing efficiency in the field. this team also analyzes forecasts and plans the reverse/forward logistics flow for dish equipment throughout dish’s manufacturing and distribution sites as part of the overall dish supply chain process. this role will have the opportunity to interact with internal and external customers across the country. accuracy, timeliness and effective communication are expectations for success in the command center. key responsibilities: • analyze data and provide actionable insights using sql, python, r, tableau, and microsoft/google suite • solve problems and meet goals with advanced statistical techniques and innovative approaches • develop recommendations for continuous improvement for field management, executives, and partners • conduct short and long term data analyses to enhance customer experience and reduce costs • support ad-hoc projects to drive cost savings, improve quality, and increase efficiency • create and maintain forecasting and resource/headcount planning models supporting budgeting and variance analysis • lead and facilitate forecasting meetings to align stakeholders and leadership on realistic and accurate operational plans • collaborate with supply chain, manufacturing and distribution teams to develop dashboards and track kpis • assess business operations and analyze upstream changes' impacts on logistics and operations • provide reports and updates to executives and partners across sites skills, experience and requirements education and experience: • bachelor's degree in a quantitative field (e.g., mathematics, statistics, computer science, business analytics) • 1-3 years of experience; or equivalent combination of education and experience • experience manipulating large datasets in a business environment required • experience translating data to actionable insights required • tableau experience preferred • sql/r/python experience preferred • prior experience in supply chain is a plus skills and qualifications: • advanced knowledge of microsoft office or google suite, including high level formulas and linking of files • strong analytical and modeling skills • ability to define problems, collect data, establish facts, and draw valid conclusions • excellent verbal and written communication skills and the ability to communicate complex data clearly and concisely to executives • embrace the adventure; willing to take risks, try innovative approaches, be adaptable to change, and learn from failures visa sponsorship not available for this role salary ranges compensation: $63,150.00/year - $90,000.00/year benefits we offer versatile health perks, including flexible spending accounts, hsa, a 401(k) plan with company match, espp, career opportunities, and a flexible time away plan; all benefits can be viewed here: dish benefits the base pay range shown is a guideline. individual total compensation will vary based on factors such as qualifications, skill level, and competencies; compensation is based on the role's location and is subject to change based on work location. candidates need to successfully complete a pre-employment screen, which may include a drug test and dmv check. our company is committed to fostering an inclusive and equitable workplace where every individual has the opportunity to succeed. we are dedicated to providing individuals with criminal or arrest records a fair chance of employment in accordance with local, state, and federal laws. the posting will be active for a minimum of 3 days. the active posting will continue to extend by 3 days until the position is filled.","wheat ridge, co",Data Analyst,"['aws', 'dashboard', 'excel', 'python', 'r', 'recommendation', 'sql', 'statistics', 'tableau']","['aws', 'dashboard', 'excel', 'python', 'r', 'recommendation', 'sql', 'statistics', 'tableau']",72.4K–103K a year
"senior data analyst, bi",denver broncos,"denver broncos is hiring a senior data analyst, bi with 5 - 10 years of experience. based in united states - denver, co and with in-office ways of working. job description and responsibilities: job summary: the sr. bi and data analyst is responsible for planning and creating data-driven initiatives supporting multiple business functions. this role is primarily responsible for collecting, analyzing, and interpreting large amounts of data from various sources, and transforming it into meaningful insights that can be used to improve business performance. duties and responsibilities business intelligence and analysis: • build high-quality data visualizations, dashboards, and reports for organization-wide use, including executive management and all business department, primarily leveraging tableau. • identify new kpis to track metrics against goals and provide actionable business insights. • works cross-functionally to collect data and makes actionable recommendations at all levels. • present findings and recommendations to all stakeholders, including executive management and department heads. • help build and update regular reports and build ad-hoc analyses supporting ticketing, sponsorship, marketing, and other business related functions. • lead meetings regularly with the executive team and department heads to understand their needs and challenges and work to craft and explain data lead solutions. data preparation and model development: • help complete entire data analysis life cycle from procuring and cleaning raw data, identifying key insights, and working with others on the bi team to create interactive dashboards displaying those key insights. • design, engineer and maintain analytical tools and models using statistical or data science approach and develop custom algorithms to create business solutions • collaborate with data and analytics team to ensure operational and business metric health by monitoring key decision points and provide actionable insights • ensure data accuracy, completeness, and consistency by establishing data governance policies and procedures. • ensure compliance with gdpr, ccpa, and any other data regulations. business research and analysis: • review current business operation processes to identify automation opportunities within current data environment. • co llaborate with data and analytics team to ensure operational and business metric health by monitoring key decision points and provide actionable insights • engage and collaborate with multiple departments to find opportunities, understand requirements, and translate into technical data science solutions other • research new technologies and methods across data science, data engineering, and data visualization to improve the technical capabilities of the team • other projects and duties as assigned requirements and qualifications: minimum requirements • bachelor’s degree and minimum 5 years of experience in data-driven industry or job function. • advanced proficiency with sql, bi tools (tableau preferred) • advanced proficiency in data programming skills (r/python) • proficient in microsoft office (microsoft excel, powerpoint and word) • experience working closely and communicating with non-data related stakeholder groups/departments. • skills form completed preferred expectations • advanced proficiency with tableau • experience with eloqua, microsoft dynamics crm, ticketmaster • experience with snowflake • experience with common sports industry/league data sources. • advanced knowledge of machine learning models using either r or python.","denver, co",Data Analyst,"['bi tools', 'business intelligence', 'dashboard', 'data analysis', 'excel', 'machine learning', 'python', 'r', 'recommendation', 'snowflake', 'sql', 'tableau']","['bi tools', 'business intelligence', 'dashboard', 'data analysis', 'excel', 'machine learning', 'python', 'r', 'recommendation', 'snowflake', 'sql', 'tableau']",90K–115K a year
actuarial data analyst,berkley,"about the position responsibilities • to support the cross functional departments by inputting information quickly and accurately from a variety of sources including incoming applications into verus core systems and setting them up in the workflow, providing excellent customer service. • develop introductory – intermediate queries and reports for actuarial department and executive team as requested. • intermediate knowledge of internal data systems in order to be a relied upon resource for data-driven changes or problems across the organization. • help manage transition from legacy systems into more efficient systems and solutions. • identify and analyze information needs and flow for design, development, and maintenance. • construct reports within microsoft excel and power bi that aid underwriting in making correct strategic decisions in reaching profitability and growth goals. • develop and maintain financial planning, budgeting, and forecasting visuals. • analyzes complex business problems and issues using data from internal and external sources to provide insight to analysts and decision makers. requirements • bachelor’s degree in a related field (mathematics, business, finance, technical). • 2+ years of experience in a data-related role with working knowledge of information systems. • required: strong knowledge of ms office tools (excel, outlook, word, powerpoint). • required: strong knowledge of sql. • required: intermediate knowledge of power bi. • ability to adapt. • strong problem solving and communication skills. • strong organization skills and ability to multitask and prioritize multiple tasks at once. • ability to work outside normal work schedule to fulfill responsibilities. • ability to travel on occasional basis. • reasonable, regular, and predictable attendance. nice-to-haves • preferred: knowledge of the insurance industry and it systems. • preferred: intermediate knowledge of r and python. • preferred: strong knowledge of vba. benefits • health • dental • vision • life • disability • wellness • paid time off • 401(k) and profit-sharing plans",colorado,Data Analyst,"['excel', 'power bi', 'python', 'r', 'sql']","['excel', 'power bi', 'python', 'r', 'sql']",
it data analyst,southeast alaska regional health consortium,"pay range: pay range:$34.15 - $47.93 searhc is a non-profit health consortium which serves the health interests of the residents of southeast alaska. we see our employees as our strongest assets. it is our priority to further their development and our organization by aiding in their professional advancement. working at searhc is more than a job, it’s a fulfilling career. we offer generous benefits, including retirement, paid time off, paid parental leave, health, dental, and vision benefits, life insurance and long and short-term disability, and more. under the supervision of the manager of applications and integration, this position is responsible for management and maintenance of searhc’s electronic master patient index (empi) environment. the data analyst works with all searhc electronic systems, clinical and financial, to maintain the integrity of the patient record. this includes all current and historic (legacy) systems as well as any future systems. the position is responsible for reviewing, analyzing, and maintaining patient data integrity within the empi and the electronic medical record (emr). the analyst will have a breadth of experience including data/information systems – extracting, validating, analyzing, reporting data, quality/process improvement, and electronic health records including knowledge of radiology/imaging, ordering, health information management (him), or chart review and registration functions and workflows. key essential functions and accountabilities of the job performs enterprise master patient index (empi) management and maintenance. monitors appropriate queues and systems to identify, resolve, communicate the documentation errors and validates the medical record documentation error has been resolved or remediated. merges/unmerges medical records (patient duplicates and overlays). partners with interface and data engineers to test and troubleshoot issues. performs a variety of tasks to support data integrity for various clinical systems such as vendor neutral archive and pacs. monitors for errors, ensures clean data, corrects status on studies, updates demographics, and/or studies data when necessary. resends imaging reports when necessary and performs initial troubleshooting for studies without reports. assists departments with importing and exporting images. works with imaging and informatics staff to troubleshoot issues. escalates issues to management as needed. performs root cause analysis and auditing on significant integrity incidents and communicates to managers as needed. provides input to managers and end-users to correct and to minimize data integrity issues and improve data quality. develops and maintains policies, procedures, and training material related to empi. has a thorough understanding of policies and procedures with regards to data integrity, chart corrections, identity, and medical record merges. has in-depth knowledge of, or ability to learn, searhc data sources (legacy and current), imaging sources (legacy and current), and understanding of registration process. other duties as assigned. education, certifications, and licenses required associate’s degree in computer science, information management systems, radiology, him, or related field is preferred. him certification preferred. arrt certification preferred. experience required 5 years in developing, supporting, or utilizing information systems or equivalent combination of experience and education required. 1-3 years working with clinical information systems required. experience with cerner and/or meditech systems preferred. experience in diagnostic imaging/radiology, him, healthcare registration, or data integrity role preferred. quality and process improvement experience preferred. knowledge of imaging workflows in various modalities and specialties. specialized knowledge in both radiology department and emr registration processes. medical terminology. skills in broad computer skill set excellent written and verbal communication. intense concentration. attention to detail. ability to self-motivate. continually learn new functions and systems. communicate with a variety of audiences. work independently, when necessary. required certifications: if you like wild growth and working with happy, enthusiastic over-achievers, you'll enjoy your career with us! we want you to join our team southeast alaska is home to some of the world’s most breathtaking landscapes, proudest native cultures, and thriving pioneer spirits. southeast alaska regional health consortium (searhc) helps to keep it healthy. every career with searhc comes with a positive work culture, a team with shared values, and competitive benefits. searhc offers sign-on bonuses for registered nurses, hot jobs and certified nurses assistants. at searhc, we are passionate about enhancing the health and wellbeing of our team as well as the communities we serve. we embody our values: respect cultural identity service professionalism compassion",united states,Data Analyst,"['excel', 'r', 'scala']","['excel', 'r', 'scala']",
"analyst, data analytics","0020 paypal, inc.","the company paypal has been revolutionizing commerce globally for more than 25 years. creating innovative experiences that make moving money, selling, and shopping simple, personalized, and secure, paypal empowers consumers and businesses in approximately 200 markets to join and thrive in the global economy. we operate a global, two-sided network at scale that connects hundreds of millions of merchants and consumers. we help merchants and consumers connect, transact, and complete payments, whether they are online or in person. paypal is more than a connection to third-party payment networks. we provide proprietary payment solutions accepted by merchants that enable the completion of payments on our platform on behalf of our customers. we offer our customers the flexibility to use their accounts to purchase and receive payments for goods and services, as well as the ability to transfer and withdraw funds. we enable consumers to exchange funds more safely with merchants using a variety of funding sources, which may include a bank account, a paypal or venmo account balance, paypal and venmo branded credit products, a credit card, a debit card, certain cryptocurrencies, or other stored value products such as gift cards, and eligible credit card rewards. our paypal, venmo, and xoom products also make it safer and simpler for friends and family to transfer funds to each other. we offer merchants an end-to-end payments solution that provides authorization and settlement capabilities, as well as instant access to funds and payouts. we also help merchants connect with their customers, process exchanges and returns, and manage risk. we enable consumers to engage in cross-border shopping and merchants to extend their global reach while reducing the complexity and friction involved in enabling cross-border trade. our beliefs are the foundation for how we conduct business every day. we live each day guided by our core values of inclusion, innovation, collaboration, and wellness. together, our values ensure that we work together as one global team with our customers at the center of everything we do – and they push us to ensure we take care of ourselves, each other, and our communities. job summary: this job will analyze data to design and build bi dashboards (in react.js and looker) in short timeframes via rapid prototyping and agile development to provide insights that support business decisions. and your role would involve hands on development, and maintenance of all aspects of bi solutions – etl, data quality, visualisation, documentation, and production movement. job description: essential responsibilities: analyze data to provide insights. collaborate with stakeholders to understand their data needs. deliver recommendations based on data analysis. ensure data accuracy and integrity in all analysis processes. present findings to stakeholders. drive continuous improvement in data analysis practices. minimum qualifications: minimum of 2 years of relevant work experience and a bachelor's degree or equivalent experience. additional responsibilities & preferred qualifications: must have skills and experience: 4~6 years of experience with full-stack development supporting both frontend/backend development 3+ years of hands-on experience in react.js development 3+ years of hands-on experience in looker or similar bi tool-based development proficient with sql desired skills and experience: proficient with react.js and other front-end frameworks (e.g. angular, vue) proficient with functional and object-oriented javascript/es6+ coding proficient with node.js and express application design and development proficient with html5 and css3 experience with restful web services experience with bi tools such as looker is an added advantage proficient with webpack and other build and packaging systems proficient in writing automated unit and functional tests proficient with git source control and effective branching and release management practices proficient in creating web user interfaces and resolving cross-browser and backward compatibility issues ability to debug, diagnose and resolve complex javascript bugs throughout the stack exceptional communication and collaboration skills to engage with multiple stakeholders and different teams to deliver bi solutions efficiently strong critical thinking skills; ability to devise innovative solutions bonus skills: experience with sales force crma development experience with looker rest apis and extension framework experience with continuous integration and delivery tools such as jenkins experience in the fintech/banking/credit/payment industry is a strong plus subsidiary: paypal travel percent: 0 - paypal is committed to fair and equitable compensation practices. actual compensation is based on various factors including but not limited to work location, and relevant skills and experience. the total compensation for this practice may include an annual performance bonus (or other incentive compensation, as applicable), equity, and medical, dental, vision, and other benefits. for more information, visit https://www.paypalbenefits.com. the us national annual pay range for this role is $76,500 to $126,500 paypal does not charge candidates any fees for courses, applications, resume reviews, interviews, background checks, or onboarding. any such request is a red flag and likely part of a scam. to learn more about how to identify and avoid recruitment fraud please visit https://careers.pypl.com/contact-us. for the majority of employees, paypal's balanced hybrid work model offers 3 days in the office for effective in-person collaboration and 2 days at your choice of either the paypal office or your home workspace, ensuring that you equally have the benefits and conveniences of both locations. our benefits: at paypal, we’re committed to building an equitable and inclusive global economy. and we can’t do this without our most important asset—you. that’s why we offer benefits to help you thrive in every stage of life. we champion your financial, physical, and mental health by offering valuable benefits and resources to help you care for the whole you. we have great benefits including a flexible work environment, employee shares options, health and life insurance and more. to learn more about our benefits please visit https://www.paypalbenefits.com. who we are: click here to learn more about our culture and community. commitment to diversity and inclusion paypal provides equal employment opportunity (eeo) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state, or local law. in addition, paypal will provide reasonable accommodations for qualified individuals with disabilities. if you are unable to submit an application because of incompatible assistive technology or a disability, please contact us at paypalglobaltalentacquisition@paypal.com. belonging at paypal: our employees are central to advancing our mission, and we strive to create an environment where everyone can do their best work with a sense of purpose and belonging. belonging at paypal means creating a workplace with a sense of acceptance and security where all employees feel included and valued. we are proud to have a diverse workforce reflective of the merchants, consumers, and communities that we serve, and we continue to take tangible actions to cultivate inclusivity and belonging at paypal. any general requests for consideration of your skills, please join our talent community. we know the confidence gap and imposter syndrome can get in the way of meeting spectacular candidates. please don’t hesitate to apply. notice to applicants and employees who reside within new york city. click here to view the notice. paypal is on a mission to revolutionize commerce globally. we’re driving our company, industry, and society forward with vision and velocity. with our commitment to excellence, innovation, and talent, the possibilities are limitless. learn more at paypal.com/jobs learn more about privacy-related questions or data retention. if you would like your profile to be deleted from our system, please let us know please note that this site has updated features that can’t run on older versions of internet explorer. please use ie11 or ms edge for an optimal experience. when applying for a job you are required to create an account, if you have already created an account - click sign in. creating an account will allow you to follow the progress of your applications. our system does have some requirements that will help us process your application, below are some guidelines for creation of your account: provide full legal first name/family name – this is important for us to ensure our future hires have the right system set up. please capitalize first letter of your first and last name. please avoid using fully capitalized text for your first and/or last name. note: if your name is hyphenated or has multiple capitalization, please use the same format as your government id. please note that this site has updated features that can’t run on older versions of internet explorer. please use ie11 or ms edge for an optimal experience.",united states,Data Analyst,"['bi tools', 'dashboard', 'data analysis', 'etl', 'excel', 'java', 'looker', 'r', 'recommendation', 'sql']","['bi tools', 'dashboard', 'data analysis', 'etl', 'excel', 'java', 'looker', 'r', 'recommendation', 'sql']",
senior data analyst,rtl networks inc,"description: position title: senior data analyst location: dahlgren, va status: full-time / on-site required clearance: secret required certification(s): n/a about us: rtl networks, inc. is a rapidly growing company primarily focused on providing information technology (it) support services and personnel to various commercial and government customers for extended-term contracts. by providing a wide array of professional services and products, we help our customers leverage technology and operate confidently in their technology environments to ensure resource predictability, security, and reliability to meet business objectives. summary: rtl networks has an exciting opportunity for a dod secret cleared senior data analyst to join our team supporting the naval surface warfare center dahlgren division (nswcdd). this position will be supporting nswcdd’s mission to modernize, optimize, and simplify its it environment and mature its it service management (itsm) practices to achieve measurable improvements and desired outcomes. primary responsibilities: essential responsibilities of the senior data analyst include, but are not limited to: • develop and refine logical data models to support enterprise-level data integration. • define data interface specifications and on-line query/reporting requirements to meet mission needs. • use sql to extract, transform, and load (etl) data, optimize queries, and validate data integrity. • design and implement data warehouse architectures that support large-scale analytics and reporting. qualifications: • bachelor’s degree in a business-related field from an accredited college or university. • minimum of six (6) years of professional experience in relation to analysis, design, and development related to data warehousing and integration capabilities. • experience with data warehouse development and design, data knowledge acquisition, legacy conversion specifications and design of data structures/load specifications, and knowledge of working capital fund and enterprise resource planning (erp), financial, and human resources systems methods and strategies for data warehousing. • experience with requirements developing definition and application designs in the form of logical data models, data interface specifications, on-line query and report specifications, structured query language (sql), database performance loading specifications, and data validation specifications. • experience with help desk tools, such as servicenow • must be a u.s. citizen with an active dod secret clearance or higher. • this position is contingent upon contract award and/or customer approval. physical demands: • must be able to lift up to 15 pounds. • must be able to stand and walk for prolonged amounts of time. • must be able to twist, bend and squat periodically. security clearance requirements: must currently hold a security clearance at the secret level. us citizenship is a requirement for clearance at this location. note: • the applicant selected for this position must have and be able to maintain an active security clearance. • u.s. citizenship required. • applicants must have a valid driver’s license for this position and will be subject to verification. • chosen applicants must pass pre-employment drug screening, credit review, and a criminal background check. we are an equal opportunity employer, females/minority/veterans/disabled/sexual orientation/gender identity/religion/national origin #cj requirements:","denver, co",Data Analyst,"['data warehouse', 'etl', 'r', 'sql']","['data warehouse', 'etl', 'r', 'sql']",
"senior data analyst, operations",seatgeek,"seatgeek believes live events are powerful experiences that unite humans. with our technological savvy and fan-first attitude we’re simplifying and modernizing the ticketing industry. we are looking for a senior data analyst to join our analytics team to focus on improving our operational excellence. in this role, you will partner closely with our marketplace operations organization, primarily our customer support team, as well as fulfillment and event operations, to elevate the customer experience across all inbound and outbound interactions. using sql, and advanced analytics techniques across our stack, you’ll transform raw data into actionable insights that guide decision-making, highlight opportunities, and drive measurable improvements. you’ll collaborate cross-functionally with operations, product, and engineering to define success, measure the impact of initiatives, and build the analytical foundations needed for scalable, high-quality support. partnering closely with our marketplace ops team, you will guide ops investment decisions and drive business impact through (among other things): • developing reporting frameworks and building dashboards and metrics that connect support performance to business outcomes (e.g., sla, csat, cost per resolution) • creating data-driven prioritization systems that quantify the expected cost of delayed resolution and inform routing strategies, including identifying candidates for vip/fast-track treatment • evaluating and improving contact volume forecasting by breaking down seasonality, near-term momentum, and historical spikiness patterns to strengthen operational planning • surfacing user insights that connect support quality to broader business outcomes, enabling leaders to understand the downstream implications of support performance on revenue and customer experience • partnering with product and engineering to enact and test preventative product solutions and measuring the impact on support metrics as a member of seatgeek’s analytics team, you will be able to collaborate with and learn from a group of high-performing analysts, data scientists and engineers! what you'll do • own our business relationship with stakeholders on the marketplace ops team, understanding their objectives and aligning our analytical roadmap to achieve those goals • develop our understanding of lifetime customer value by connecting operational excellence, problem-order dynamics, and support experience quality to retention and revenue outcomes • conduct exploratory analysis to identify pockets of under-performance in the support organization, including contact types, workflows, and root-cause categories • analyze chatbot and text-based data (nlp) to understand customer intent, friction points, and automation improvement areas • evaluate different chat bot vendors and plan and conduct a/b tests to improve performance. determine strategy for making chatbot data accessible to dwh • build lasting modeling solutions, automate analyses, and author pipelines via sql-based etl frameworks • improve the hygiene and usability of operational data (zendesk, unitq, and more) enabling us to make decisions based on cost dynamics what you have • 3+ years of relevant experience • a degree or higher in economics, psychology, computer science, statistics, mathematics or another quantitative discipline • expert-level knowledge of sql • experience performing analysis with large datasets • experience building reports and dashboards using business intelligence tools • a track record of influencing business strategy with data • a strong statistical understanding of online testing methodologies and metric development • natural language processing experience - nice to have • proficiency in a scripting language (python or r) - nice to have • domain expertise or past experience in operations - nice to have our stack • scheduling/orchestration: airflow • etl: fivetran, python and dbt • data warehouse: redshift • event stream: mparticle • experimentation: optimizely • dashboarding: looker & hex • code versioning: gitlab • required languages: sql and python • support text exploration: unitq perks • equity stake • flexible work environment, allowing you to work as many days a week in the office as you’d like or 100% remotely • a wfh stipend to support your home office setup • unlimited pto • up to 16 weeks of fully-paid family leave • 401(k) matching program • student loan support resources • health, vision, dental, and life insurance • up to $25k towards family building and reproductive health services • gender-affirming care support program • $500 per year for wellness expenses • subscriptions to headspace (meditation), headspace care (therapy), and one medical • $120 per month to spend on tickets to live events • annual subscription to spotify, apple music, or amazon music the salary range for this role is $105,000 - $147,000 usd. this role is also equity eligible. actual compensation packages within that range are based on a wide array of factors unique to each candidate, including but not limited to skill set, years and depth of experience, certifications, and specific location. seatgeek is committed to providing equal employment opportunities to all employees and applicants for employment regardless of race, color, religion, creed, age, national origin or ancestry, ethnicity, sex, sexual orientation, gender identity or expression, disability, military or veteran status, or any other category protected by federal, state, or local law. as an equal opportunities employer, we recognize that diversity is a positive attribute and we welcome the differences and benefits that a diverse culture brings. to review our candidate privacy notice, click here. #li-remote",anywhere,Data Analyst,"['airflow', 'business intelligence', 'dashboard', 'data warehouse', 'dbt', 'etl', 'excel', 'experimentation', 'looker', 'natural language processing', 'nlp', 'python', 'r', 'redshift', 'scala', 'sql', 'statistics']","['airflow', 'business intelligence', 'dashboard', 'data warehouse', 'dbt', 'etl', 'excel', 'experimentation', 'looker', 'natural language processing', 'nlp', 'python', 'r', 'redshift', 'scala', 'sql', 'statistics']",
ai data analyst,jobgether,"ai data analyst this position is posted by jobgether on behalf of a partner company. we are currently looking for an ai data analyst in the united states.this role focuses on analyzing, interpreting, and optimizing data from ai-powered platforms to help organizations make data-driven decisions. you will work closely with ai architects, product teams, and client stakeholders to uncover actionable insights, improve conversational ai systems, and enhance agent assistance and automation solutions. the position involves designing dashboards, monitoring key performance metrics, and supporting ai model improvements. you will leverage your technical expertise and analytical skills to transform complex datasets into clear, impactful recommendations. this is an excellent opportunity for individuals passionate about ai, data analytics, and business optimization in a collaborative and fast-paced environment.jobgether is a talent matching platform that partners with companies worldwide to efficiently connect top talent with the right opportunities through ai-driven job matching.when you apply, your profile goes through our ai-powered screening process designed to identify top talent efficiently and fairly. our ai evaluates your cv and linkedin profile thoroughly, analyzing your skills, experience, and achievements. it compares your profile to the jobs core requirements and past success factors to determine your match score. based on this analysis, we automatically shortlist the three candidates with the highest match to the role. when necessary, our human team may perform an additional manual review to ensure no strong profile is missed.the process is transparent, skills-based, and free of bias focusing solely on your fit for the role. once the shortlist is completed, we share it directly with the company that owns the job opening. the final decision and next steps (such as interviews or additional assessments) are then made by their internal hiring team.thank you for your interest! by submitting an application to this posting, the applicant acknowledges that jobgether will process their personal data as necessary to evaluate their candidacy, provide feedback, and, when appropriate, share relevant information with potential employers. such processing is carried out on the basis of legitimate interest and pre-contractual measures in accordance with applicable data protection laws. the applicant may exercise their rights of access, rectification, erasure, and objection at any time as provided under the gdpr. #li-cl1",united states,Data Analyst,"['aws', 'dashboard', 'data analytics', 'excel', 'r', 'recommendation']","['aws', 'dashboard', 'data analytics', 'excel', 'r', 'recommendation']",
"online data analyst | freelance, remote (us)",telus digital ai data solutions,"are you a detail-oriented individual with a passion for research and a good understanding of national and local geography? this freelance opportunity allows you to work at your own pace and from the comfort of your own home. a day in the life of an online data analyst: • in this role, you will be working on a project aimed at enhancing the content and quality of digital maps that are used by millions of people worldwide • completing research and evaluation tasks in a web-based environment, such as verifying and comparing data, and determining the relevance and accuracy of information. join us today and be part of a dynamic and innovative team that is making a difference in the world! telus digital ai community our global ai community is a vibrant network of 1 million+ contributors from diverse backgrounds who help our customers collect, enhance, train, translate, and localize content to build better ai models. become part of our growing community and make an impact supporting the machine learning models of some of the world’s largest brands. qualification path no previous professional experience is required to apply to this role; however, working on this project will require you to pass the basic requirements and go through a standard assessment process. this is a part-time long-term project, and your work will be subject to our standard quality assurance checks during the term of this agreement. basic requirements • full professional proficiency in the english language • being a resident in the united states for the last 2 consecutive years and having familiarity with current and historical business, media, sport, news, social media, and cultural affairs in the united states • ability to follow guidelines and conduct online research using search engines, online maps, and website information • flexibility to work across a diverse set of task types, including maps, news, audio tasks, and relevance • daily access to a broadband internet connection, computer, and relevant software assessment in order to be hired into the program, you’ll take an open-book qualification exam that will determine your suitability for the position and complete id verification. our team will provide you with guidelines and learning materials before your qualification exam. you will be required to complete the exam in a specific timeframe, but at your convenience. equal opportunity all qualified applicants will receive consideration for a contractual relationship without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or protected veteran status. at telus digital ai, we are proud to offer equal opportunities and are committed to creating a diverse and inclusive community. all aspects of selection are based on applicants’ qualifications, merits, competence, and performance without regard to any characteristic related to diversity.",anywhere,Data Analyst,"['machine learning', 'r']","['machine learning', 'r']",
program data analyst,developmental pathways,"developmental pathways (dp) has an opportunity for a program data analyst in our program operations (po) department. reporting to the senior director of program operations, you'll play a key role in advancing data-driven decision-making across long term care (ltc) case management. as a vital member of the ltc case management team, you'll collaborate with program managers, frontline staff, leadership, and cross-functional partners, including it, finance, strategic project management, learning & organizational development, and hr, to deliver accurate, meaningful insights through data analysis, reporting, and visualization. key responsibilities: • collect, analyze, and interpret program data to identify trends and compliance metrics, evaluate performance, and inform strategic initiatives • partner with it to ensure accurate data integration and reporting (no dashboard development required) • collaborate with finance and it to streamline workflows and enhance processes • manage data collection workflows and lead process improvements with clear, tactful communication • support both day-to-day operations and long-term planning by turning data into actionable insights that empower teams, improve efficiency, and align with organizational goals our ideal candidate: • strong analytical skills with experience in data interpretation and compliance reporting • excellent communication skills; able to deliver feedback tactfully and foster collaboration • skilled in managing workflows and driving process improvements across teams • comfortable bridging technical and non-technical stakeholders this role offers a 4-day, 40-hour work week, monday-thursday. dp operates a primarily hybrid work environment with in-person requirements at our colorado office. the frequency of on-site work varies by department and is subject to change based on operational needs. application deadline: sunday, december 21st at 11:59 pm (mst) compensation and benefits $68,571 - $76,286 annually • health/dental/vision coverage • employer-paid and supplemental life insurance • short- and long-term disability insurance • generous paid time off and holiday pay • flexible work schedule • monthly remote work stipend • 401(k) investment plan, with an employer match of up to 4% • mileage reimbursement • tuition reimbursement program • certified employer for public service loan forgiveness program • healthcare reimbursement and flexible spending plan • discounts on auto and homeowners' insurance • employee assistance program • credit union membership • employer-paid training you belong! to learn more about our commitment to inclusion and diversity, visit our website: https://www.dpcolo.org/about-us/inclusion-and-diversity/ required • bachelor's degree and two (2) years of related experience, or three (3) to five (5) years of related work experience • 2+ years of experience in a data analyst or similar role • demonstrated ability to analyze and interpret complex data, translating insights into actionable recommendations, in partnership with program leaders and it, as appropriate • strong analytical skills with the ability to translate complex data into actionable insights • excellent communication and presentation skills, with the ability to effectively convey technical concepts to non-technical stakeholders • adaptability and willingness to learn new technologies and methodologies • personal vehicle in good operating condition for use during work as appropriate • valid colorado motor vehicle license and proof of motor vehicle insurance • ability to meet our driving requirements - no suspensions/ no more than two tickets in the past three years upon request, qualifying need, and approval, driving requirements may be waived in lieu of reliable transportation. preferred • proficiency in power bi, with experience building and maintaining reports and dashboards that provide actionable insights and support informed decision-making additional responsibilities • gather and analyze data from various sources to assess program performance and operational effectiveness • ensure data quality, accuracy, and consistency by performing regular audits and validation checks across data sources • identify trends, gaps, and opportunities in program data to support continuous improvement efforts • support and maintain dashboards, reports, and data visualizations that support department goals and stakeholder needs • translate complex data findings into clear, concise summaries for program leadership and external partners • support data requests and ad hoc analyses from program teams and leadership • document data processes, definitions, and reporting methodologies to support transparency and repeatability • collaborate with program staff and cross-functional teams to understand business requirements and translate them into well-designed data-driven insights • ensure compliance with data governance practices and promote data literacy within the case management department • support the maintenance and continuous improvement of organization-wide kpi and metrics in collaboration with operations leaders and it about us started in 1964, developmental pathways (dp) is a nonprofit agency serving more than 15,000 individuals with disabilities and their families in arapahoe, douglas, and elbert counties. we are a designated case management agency (cma) for long-term care services and are one of colorado's community centered boards (ccb), connecting people to federal, state, county, and private funding. we are also colorado's largest early intervention (ei) provider and help connect young children to essential resources, serving the city of aurora in addition. our mission is to enrich the lives of people with disabilities/delays by partnering to provide expertise, support, and advocacy in their pursuit of a meaningful life. at dp, we believe that our people are our greatest asset. we want to give you the ability to grow and do what you love. we are committed to creating an inclusive and dynamic work environment where employees can thrive and make a meaningful impact. if you are looking for a challenging and rewarding career with an organization that values if you need an accommodation with this application process, please contact support-hr@dpcolo.org.","aurora, co",Data Analyst,"['dashboard', 'data analysis', 'excel', 'power bi', 'r', 'recommendation']","['dashboard', 'data analysis', 'excel', 'power bi', 'r', 'recommendation']","68,571–76,286 a year"
senior health data analyst,wellsense health plan,"it’s an exciting time to join the wellsense health plan, a growing regional health insurance company with a 25-year history of providing health insurance that works for our members, no matter their circumstances. job summary: wellsense health plan is seeking a dedicated and experienced senior health data analyst to join our team on a full-time, regular basis. the senior health data analyst is a key member of the team and serves as a strategic partner to various functional areas throughout the organization to meet corporate objectives. under the leadership and guidance of senior director of clinical, operational, and payment analytics, the analyst will focus on planning, developing and building of analytical tools to further assess and manage healthcare kpis during key analytic exercises. the ideal candidate will have a strong background in health data analysis, excellent problem-solving skills, and the ability to communicate complex information clearly to stakeholders. our investment in you: • full-time remote work • competitive salaries • excellent benefits key functions/responsibilities: • work closely with the senior director of clinical, operational, and payment analytics to develop, maintain, and leverage a best in class clinical analytics infrastructure to support the plan’s medical management strategy. • collaborate with cross-functional teams to understand data requirements and ensure the accuracy and integrity of data analysis • lead the development of critical analytic processes comparing plan’s performance against benchmarks to determine areas of focus and opportunity and help maintain industry competitiveness and intelligence. • work closely with supervisor to present data and findings with insights to the clinical leadership teams to help launch performance improvement initiatives. • work closely with all levels of medical management (including case management) leadership in determining how operational and clinical data might assist in addressing their needs. • build and maintain dashboards that are critical to fine tuning operations. • work closely with stakeholders to develop critical drill downs as needed to support the development of initiatives to target the areas of over-utilization. • develop and maintain operational dashboards and performance measurement tools, and reports to measure impact of initiatives • work with internal and external customers to elicit business data requirements; collaborates with data architects and develops code to meet the data needs of all constituents. • understand processes and workflows within medical management operations, and translate clinical and operational metric needs into business reporting requirements • coordinate and participate in medical management operations' technical initiatives, including the development of operational reports and technical specifications in support of all aspects of our um initiatives. • work to ensure continuous improvement and adoption of data management best practices • ensuring compliance with data governance and privacy policies qualifications: education: • bachelor’s degree required experience: • experience in healthcare data analysis and reporting is required. • three or more years conducting advanced analytics using sas and/or sql. • must have deep understanding and hands-on experience with tableau or other data visualization tools • excellent problem solving and analytical skills • self-motivated, takes initiative to identify opportunities for improvement and makes recommendations for improvement • ability to work independently and collaboratively • ability to communicate customers about data needs and explaining report methodologies • working knowledge of utilizing enterprise data warehouse • ability to think out-of-the-box to handle any challenging and complex request • experience managing multiple initiatives or projects at a given time • ability to foster teamwork and positive attitude. certification or conditions of employment: • pre-employment background check competencies, skills, and attributes: • advanced sas and/or sql programming skills. • proficient in microsoft excel. • proficient in tableau (desktop and server) • some knowledge of python scripting • strong analytical and problem solving abilities. ability to use all relevant data to support decision making. • enjoy analytical challenges in a fast paced environment with strong ability for managing multiple projects simultaneously and meeting deadlines. about wellsense wellsense health plan is a nonprofit health insurance company serving more than 740,000 members across massachusetts and new hampshire through medicare, individual and family, and medicaid plans. founded in 1997, wellsense provides high-quality health plans and services that work for our members, no matter their circumstances. wellsense is committed to the diversity and inclusion of staff and their members. qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status. wellsense participates in the e-verify program to electronically verify the employment eligibility of newly hired employees",united states,Data Analyst,"['dashboard', 'data analysis', 'data warehouse', 'excel', 'python', 'r', 'recommendation', 'sas', 'sql', 'tableau']","['dashboard', 'data analysis', 'data warehouse', 'excel', 'python', 'r', 'recommendation', 'sas', 'sql', 'tableau']",
business data analyst- population health,ascension,"details • department: ascension data science institute • schedule: full time • location: remote • salary: $68,450.00 - $95,416.00 per year benefits paid time off (pto) various health insurance options & wellness plans retirement benefits including employer match plans long-term & short-term disability employee assistance programs (eap) parental leave & adoption assistance tuition reimbursement ways to give back to your community benefit options and eligibility vary by position. compensation varies based on factors including, but not limited to, experience, skills, education, performance, location and salary range at the time of the offer. responsibilities as an associate with ascension, you will have the opportunity to serve as a key resource/sme in developing and delivering quality solutions to support business processes. our goal is to improve our data offerings through consolidation and modernization of our data and platforms, while continuing to build our data expertise. the ideal candidate will be comfortable working on the most complex, high visibility projects to bring structure and alignment around the overall solution. responsibilities: • solicit, clarify and analyze customer requirements and develop comprehensive documentation. • interface with the customer to ensure project objectives are achieved. • assist in planning and implementing activities required for the collection and analysis of data related to departmental business needs, including the development of data collection systems. • develop and run basic queries and other data collection spreadsheets to support assigned business areas and organize them in a format for ease of review and use. • participate in project planning; identifying milestones, deliverables and resource requirements; tracks activities and task execution. • utilize bigquery, sql server, python or similar for data validation and transformation. • work with the team to evaluate business needs and priorities, liaise with key business partners and address team needs related to data systems & management. • engages with project teams to understand a project’s needs/requirements to provide technical expertise with data and analytical tools, including legacy and cloud based, to deliver on project goals and timelines what you will need • knowledge of healthcare claims data preferred. • knowledge of data profiling. • ability to work in a fast paced environment, comfortable with frequently shifting priorities. • ability to coordinate and lead peers through requirements, design and testing process as well as application maintenance and support. • ability to work with data and reporting tools to research, organize, and summarize findings. • ability to work with large data sets from multiple data sources • high technical and functional aptitude for problem solving and process flow management. • strong english communication skills, comfortable speaking with and translating between technical and non-technical team members at all levels of the organization. • knowledge of gcp (google cloud platform) a plus. • bigquery, sql, python or similar programming languages are a plus. requirements education: • high school diploma equivalency with 2 years of cumulative experience or associate's degree/bachelor's degree or 4 years of applicable cumulative job specific experience required. additional preferences • bachelor's degree in business, computer science, mathematics or related field preferred. • three years of progressively responsible experience in data management, data modeling and/or decision support analytics preferred. • 1-3 years of experience interpreting user stories and applying the appropriate technical solutions to deliver operational results preferred. • strong business requirement skills preferred. • participation in delivering projects using sdlc and agile methodologies preferred. • experience with fully adjudicated medical and rx claims, premium, provider, and/or supplemental data highly preferred. medicare advantage experience is a plus. #li-remote #adsi #internalops why join our team ascension associates are key to our commitment of transforming healthcare and providing care to all, especially those most in need. join us and help us drive impact through reimagining how we can deliver a people-centered healthcare experience and creating the solutions to do it. explore career opportunities across our ministry locations and within our corporate headquarters. ascension is a leading non-profit, faith-based national health system made up of over 134,000 associates and 2,600 sites of care, including more than 140 hospitals and 40 senior living communities in 19 states. our mission, vision and values encompass everything we do at ascension. every associate is empowered to give back, volunteer and make a positive impact in their community. ascension careers are more than jobs; they are opportunities to enhance your life and the lives of the people around you. equal employment opportunity employer ascension provides equal employment opportunities (eeo) to all associates and applicants for employment without regard to race, color, religion, sex/gender, sexual orientation, gender identity or expression, pregnancy, childbirth, and related medical conditions, lactation, breastfeeding, national origin, citizenship, age, disability, genetic information, veteran status, marital status, all as defined by applicable law, and any other legally protected status or characteristic in accordance with applicable federal, state and local laws. for further information, view the eeo know your rights (english) poster or eeo know your rights (spanish) poster. as a military friendly organization, ascension promotes career flexibility and offers many benefits to help support the well-being of our military families, spouses, veterans and reservists. our associates are empowered to apply their military experience and unique perspective to their civilian career with ascension. please note that ascension will make an offer of employment only to individuals who have applied for a position using our official application. be on alert for possible fraudulent offers of employment. ascension will not solicit money or banking information from applicants. e-verify statement this employer participates in the electronic employment verification program. please click the e-verify link below for more information. e-verify",united states,Data Analyst,"['aws', 'bigquery', 'cloud', 'gcp', 'google cloud', 'python', 'r', 'sql', 'sql server']","['aws', 'bigquery', 'cloud', 'gcp', 'google cloud', 'python', 'r', 'sql', 'sql server']",
data analyst - local to maxico - remote - english must,sais it services,"role: data analyst location: – mexico - remote (candidate based out in maxico) mandatory skills – bigquery, ai/llm tools roles & responsibilities • bigquery sql proficiency - joins, subqueries, ctes, aggregations, functions (st_geog), basic extraction (getting sample data meeting specific criteria) • demonstrates clear & organized communication, visualization, documentation, presentation skills – can effectively summarize findings and recommendations for technical and non-technical stakeholders • ability to conduct complex analysis using multiple datasets to identify similarities, differences, strengths, weaknesses, uniqueness in schema, quality, and content • critical thinking and querying skills to quickly answer data-related questions during discussions • adaptability to ad-hoc data-related requests and investigations • ability to complete assigned work quickly and with high quality output • problem solving & solutioning skills and experience with solving for data anomalies (e.g., identify strategy to resolve duplicate data, outdated data, inaccurate data, etc.) • detail-oriented, diligent in spot checking, and uses initiative to proactively conduct deep data analysis • experience with ai/llm tools to improve how data is analyzed, solve data problems, and uncover insights more efficiently. job type: contract pay: $35.20 - $40.62 per hour expected hours: 40 per week work location: remote",anywhere,Data Analyst,"['bigquery', 'data analysis', 'r', 'recommendation', 'sql']","['bigquery', 'data analysis', 'r', 'recommendation', 'sql']",35.20–40.62 an hour
reporting data analyst,virtualvocations,"a company is looking for a reporting data analyst. key responsibilities collect, clean, and interpret data to identify trends and support business decisions develop and maintain project plans, documentation, and status reports while facilitating meetings collaborate with it teams and business stakeholders to ensure data integrity and quality of reports required qualifications 5+ years of relevant work experience intermediate sql server programming and administration skills strong experience with ssrs (sql server reporting services) understanding of ipa / mso medical management models and healthcare processes healthcare it experience with emphasis on hipaa transaction standards preferred","boulder, co",Data Analyst,"['r', 'sql', 'sql server']","['r', 'sql', 'sql server']",
sales compensation data analyst,plaid,"we believe that the way people interact with their finances will drastically improve in the next few years. we’re dedicated to empowering this transformation by building the tools and experiences that thousands of developers use to create their own products. plaid powers the tools millions of people rely on to live a healthier financial life. we work with thousands of companies like venmo, sofi, several of the fortune 500, and many of the largest banks to make it easy for people to connect their financial accounts to the apps and services they want to use. plaid’s network covers 12,000 financial institutions across the us, canada, uk and europe. founded in 2013, the company is headquartered in san francisco with offices in new york, washington d.c., london and amsterdam. the gtm operations & strategy team partners closely with our go-to-market teams, including sales, account management, marketing, professional services, and support, to drive revenue through cross-functional initiatives. the sales compensation group is responsible for designing, managing, and continuously improving plaid’s global sales incentive programs. we ensure that all plans are fair, accurate, and motivating, while aligning sales behavior with plaid’s strategic growth objectives. in this role, you will be responsible for the accuracy and timeliness of plaid’s variable compensation operations. you’ll own payout validation, crediting audits, and performance reporting to ensure fairness and transparency across our global gtm organization.",united states,Data Analyst,['r'],['r'],"103,968–123,120 a year"
data analyst 2,plic primerica life insurance company,"join our team in 2025, usa today recognized primerica as a top workplace usa for the fifth year in a row, and newsweek named primerica one of america’s greatest workplaces for diversity for the second consecutive year. in 2024, the atlanta journal-constitution named primerica as a top workplace for the eleventh consecutive year, and forbes recognized primerica as one of america’s best employers for women for the fifth year in a row. in addition, for the tenth time primerica has been voted a best employer by gwinnett magazine. primerica is a great place to work! join our team to experience what it’s like to work at “one of the best places to work in the metro atlanta”. about this position data analyst will be responsible for learning the business process and data related to project planning/execution and key performance indicators, the development of compiling appropriate data to perform analysis and effective reports, visuals and dashboards. identify and analyze data to determine trends, make meaningful observations and draw conclusions. solve complex problems, research requests and derive solutions. lead meetings to gather requirements and present findings and drive resolution. this position will adopt and utilize appropriate analytical approaches and methodology in addressing key issues within the business. this is hybrid position, that requires working onsite in the duluth, georgia office and remotely weekly. responsibilities & qualifications job description support all phases of data collection and assessment for reporting and delivery. lead assigned data-related initiatives, including requirements gathering meetings and analysis of business needs. write advanced queries (pulling from multiple tables, calculations, using subqueries, different systems, etc.) using sql, power bi, ms excel, python, or other tools. establish relationships and partner with other teams to deliver client and business value. develop analytical reports, charts, presentations, dashboards, and timelines for internal stakeholders analyze data to identify trends, business landscape, predictive analytics, and provide clear observations. design and maintain complex dashboards for ad hoc and recurring reporting needs. ensure data quality, accuracy, and timely processing across systems. maintain, review and analyze data files, tables, and reports for inconsistencies. perform detailed analysis and summarize findings effectively. present data and provide recommendations to department leadership for their use in making operational decisions, identifying process improvements and developing policy development to support change initiatives. design, develop, and maintain statistical reports on a daily, weekly, monthly, and ad hoc basis. implement changes to improve process efficiency using automation tools (e.g., microsoft access, power bi). identify new research ideas that may be beneficial to the department, gather the data, prepare analysis, and present findings. mentor and guide junior analysts by providing training and direction. required skills: 3-5 years of experience in any combination of analytics-focused role with experience in building and managing analytics bachelor’s degree in business or experience in related analytical/technical field background in data analysis, statistics or related field 3 years of experience developing dashboards and data visualizations using power bi experience creating views and other advanced sql techniques expertise in quantitative data analysis methods and data mining strong strategic thinking skills with the ability to translate trends and analysis into actionable recommendations knowledge of predictive modeling techniques a plus (r and/or python) strong executive presence and collaborative/interpersonal skills experience in the life insurance space is a plus flsa status: this position is exempt (not eligible for overtime pay): yes our benefits: day one health, dental, and vision insurance 401(k) plan with competitive employer match vacation, sick, holiday and volunteer time off life and disability insurance flexible spending account & health savings account professional development tuition reimbursement company-sponsored social and philanthropy events it has been and will continue to be the policy of primerica, inc., and its subsidiaries to be an equal opportunity employer. we provide equal opportunity to all qualified individuals regardless of race, sex, color, religious creed, religion, national origin, citizenship status, age, disability, pregnancy, ancestry, military service or veteran status, genetic or carrier status, marital status, sexual orientation, or any classification protected by applicable federal, state or local laws. at primerica, we believe that diversity and inclusion are critical to our future and our mission – creating a foundation for a creative workplace that leads to innovation, growth, and profitability. through a variety of programs and initiatives, we invest in each employee, seeking to ensure that our people are not only respected as individuals, but also truly valued for their unique perspectives. primerica is a leading provider of financial products and services to middle-income families in the u.s. and canada. since 1977, primerica has helped millions of middle-income households protect their families and save for the future. we insure over 5.5 million lives and have approximately 3.0 million client investment accounts. headquartered in metro atlanta, primerica is a fortune 1000 company with over 2,800 employees located in the u.s. and canada. our employees are an integral part of achieving primerica’s mission in creating more financially independent families and supporting over 151,000 licensed financial representatives across the u.s. and canada. the company’s competitive salaries, award-winning benefits packages, and employee recognition programs are just a few of the reasons the average employee tenure is 13 years. awards and recognitions newsweek’s america’s greatest workplaces usa today’s top workplaces usa atlanta journal-constitution’s top workplaces forbes america’s best insurance companies #1 most trusted life insurance company by investor’s business daily #3 most trusted financial company by investor’s business daily our stock is traded on the new york stock exchange under the symbol “pri.” if you need help with any part of the application process, please email recruiting@primerica.com. www.primerica.com primerica careers don't see a job that you are interested in? introduce yourself and submit your resume to our recruiters. we'll get in touch if there's a role that seems like a good match.",united states,Data Analyst,"['aws', 'classification', 'dashboard', 'data analysis', 'excel', 'power bi', 'python', 'r', 'recommendation', 'sql', 'statistics']","['aws', 'classification', 'dashboard', 'data analysis', 'excel', 'power bi', 'python', 'r', 'recommendation', 'sql', 'statistics']",
senior data analyst,meru health,"our company meru health is setting the new standard in mental healthcare with a holistic online mental health solution to not just help people manage their mental health condition but heal and recover to get their life back. we are on a mission to help 10 million people recover from mental health challenges by 2030, by utilizing the latest science and technology. if you are looking for a place where your work has a strong impact on people’s lives, we invite you to join us on our mission. summary meru health is looking for a senior data analyst who will drive a data first culture across our business. we are looking for a leader who is passionate about using analytics to identify and solve complex business problems within the meru product stack. this role will report to niki kidd, sr. director of operations and partner with c-suite/leadership in engineering, product, business operations, finance and sales. at the intersection of many teams and business decisions, you will play a crucial role at meru by transforming raw data into meaningful insights that drive the strategic direction of our products and services to our customers through existing relationships with fortune 500 partners. key responsibilities strategic impact & storytelling • transform complex data into compelling narratives that directly influence executive decision-making and drive key business outcomes • lead high-impact investigative analyses that uncover hidden opportunities and shape our product strategy • drive data-driven exploration for new product initiatives, helping define our future roadmap analytics leadership & infrastructure • design and maintain executive-level kpi dashboards that become the single source of truth for business performance • build robust data infrastructure and reporting systems that scale with our growing business needs • own and optimize our customer reporting infrastructure to deliver reliable, actionable insights enabling data-driven culture • champion self-service analytics by creating intuitive dashboards and tools that empower stakeholders across the organization • develop new datasets and analytical frameworks that unlock insights for diverse business teams • provide rapid-response analysis for time-sensitive business questions and opportunities mission-critical operations • maintain and enhance critical reporting systems for billing, payroll, and customer operations what we're looking for • 6+ years professional experience in data analytics. • proficient sql user • hands-on experience with bigquery and dbt. • strong alignment ​​with meru values • experience with data visualization tools (e.g., looker, tableau, mode). • experience with any cloud data warehouse technologies and other cloud technologies, e.g., snowflake, databricks. • hands-on experience building functional and user-friendly data products (data models, dashboards, etc.) for both technical and non-technical stakeholders. • ability to collaborate with and manage a variety of stakeholders. • a history of putting structure around ambiguity. pluses if you have these • formal education in research statistics, experimental design, or data science. • experience working in a tech product organization. • experience presenting to executives and leadership. • ability to independently plan and execute on multiple strategic initiatives or research projects. • functional knowledge of financial, sales, or other core business metrics. compensation • salary and equity: $90,000-$145,000 annually based on years of experience and location. • health benefits: top tier cigna insurance with 100% employee coverage and 75% dependent coverage for medical, dental, and vision premiums • paid time off: generous pto policy with 4 weeks in the first year and 5 weeks/year after the first anniversary • wellbeing resources: access to our holistic 12-week mental health program for you and up to two dependents • additional perks: • $400 wellness stipend after 4 months of tenure • $250 home office stipend",united states (+2 others),Data Analyst,"['bigquery', 'cloud', 'dashboard', 'data analytics', 'data warehouse', 'databricks', 'dbt', 'looker', 'r', 'snowflake', 'sql', 'statistics', 'tableau']","['bigquery', 'cloud', 'dashboard', 'data analytics', 'data warehouse', 'databricks', 'dbt', 'looker', 'r', 'snowflake', 'sql', 'statistics', 'tableau']",90K–145K a year
data analyst - not eligible for current or future sponsorship.,brendle group,"about the company brendle group is a sustainability consulting firm committed to solving the problems our clients face today while accelerating equitable climate solutions for the future. our goal is to collaborate on meaningful projects that have measurable impacts. since our founding in 1996, our company’s purpose has been to create lasting impacts that inspire and sustain our communities and our world. our portfolio includes innovative projects that address energy, climate, and water challenges for clients in the public and private sectors. we are a certified b corporation™ and a public benefits corporation (pbc) that not only walks the talk in our own operations but also pioneers new frontiers in sustainability. our team members bring a unique blend of technical expertise, innovation, and agility to help anticipate and solve new sustainability challenges. if this sounds like your kind of challenge, roll up your sleeves with us! there’s so much important work to be done. learn more at brendlegroup.com. about the role brendle group seeks a data analyst to play a pivotal role in advancing our mission of “accelerating equitable solutions to address the climate crisis.” this is an exciting opportunity to apply your technical expertise to projects that make a real-world impact. as a member of our engineering and analytics team, this role will work on diverse, high-impact initiatives that leverage data to drive smarter decisions and innovative solutions. the data analyst will work closely with project teams to conduct analyses that inform design, decision-making, and strategic planning. this role provides comprehensive support for data engineering, analytics, visualizations, and tool development needs across the firm. in collaboration with project teams, the data analyst will help design, build, and maintain analytics tools including qa/qc systems. the position is accountable for supporting data intake and verification, ensuring accurate and seamless integration into tools, databases, and data visualization processes. this position requires experience in programming, problem solving, database design, and effective communication skills. the ideal candidate will bring a continuous learning mindset, a collaborative approach, and interest in optimization, performance, and documentation. position responsibilities: technical consultant (80% time) backend and data engineering • support development of data automation pipelines and extract, transform, load (etl) processes that supply data for both internal and external analytics and data products. • support development and implement workflows and management practices for backend and frontend software development that supports internal processes and external delivery. tool development • collaborate with teams to support the development of analytical tools that inform our work and enhance delivery of work including: • data qa/qc • automation • interactive data visualizations and dashboards (e.g., tableau, power bi) data analysis • performs data analysis to inform decision-making and/or strategy prioritization. • assists with analysis workflows, process design, and quality control procedures. • analyzes water, energy, demographic, and other data sources to create digestible summaries, interesting visualizations, and supportive communication resources such as powerpoint presentations and report content. • comfortable explaining analyses results and key takeaways to non-experts. • supports various project needs and data processing requirements, including data intake and verification and data integration into relevant tools and databases. • explore and leverage ai based tools to support data analysis and/or visualization. other responsibilities (20% time) support other business operations and other professional development opportunities, including: • continuously improve and develop internal tools, skills, and resources - sharing and teaching across the team when applicable. • support company goals, collaboratively engage with the team, and communicate in ways that mitigate conflict and make life easier for the team and our customers. • participate in company-wide, service team, and other meetings. • recognize and uphold established standards to deliver consistent work products, including attention to branding and communications protocols. • other duties as assigned or apparent. required qualifications and experience • 2-6 years of relevant work experience. • sql experience in postgresql, sql server, oracle, and/or other data management technologies. • version control through git or other tools. • programming in python, r, node.js, .net, java, ruby, or other directly related languages. • experience with excel-based tool development and model design. • familiarity with other visualization tools such as tableau and power bi. • experience and understanding across different technologies and end-user experiences. • excellent organizational, decision-making and time management skills with ability to prioritize, work independently, and manage multiple priorities and deadlines. • proficient with microsoft office programs (word, powerpoint, outlook, excel, teams, sharepoint). • eager to learn and expand skillset. • familiarity with and enthusiasm for sustainability topics. preferred qualifications and experience beyond the required qualifications listed above, we value adaptability and collaboration. as a small business, our team members often take on multiple responsibilities to support overall success. for this role, we seek candidates with a broad professional background who can bring additional skills or experience into one or more of the following areas to strengthen team capacity and cross-functional resilience: • prior consulting experience. • prior project and/or task management experience. • demonstrated public speaking skills, including presenting technical information to a diverse audience. • four-year degree with an emphasis in data science, statistics, computer engineering, computer science, or equivalent experience. • experience applying data analytics to develop visualizations, dashboards, and communication tools. • working knowledge of it infrastructure to support data backend processes. • exposure to ai-based tools to support data analysis or visualization. attributes of ideal candidate • people-oriented and client focused. • strong interpersonal communication skills. • reliable and clear communicator. • exhibits professional empathy, perspective, and cultural competence. • ability to function efficiently in a fast-paced and dynamic environment. • consistently produces high quality work. • owns position responsibilities and approaches work as a problem solver and critical thinker. • adapts willingly to changing responsibilities. • tech savvy and confident in learning and using modern technology systems. • self-motivated, ambitious, curious, and innovative team player with a passion for sustainability. salary and benefits brendle group offers a flexible and collaborative workplace with a competitive salary and a generous benefits package. based on position requirements and candidate experience, the base salary range for the data analyst is $72,490-$103,070. other benefits include: • employer-paid health, dental, vision, and disability insurance. • simple retirement plan with employer match. • flexible schedule, with summer hours and hybrid work options. • paid time off, accumulating from your first day. • paid holidays (nine per year) and two annual floating holidays. • annual bonus pool and salary adjustments. location and position conditions the project portfolio for this position is concentrated in the colorado front range and twin cities metropolitan area. therefore, while we will consider remotely located candidates for this position, we will give preference to candidates based in fort collins, co; denver, co; or twin cities metro area, mn. we provide flexible hybrid work options, including remote work opportunities. equal opportunity we are committed to inclusive and equitable hiring practices, recognizing historic inequities in the engineering, planning, and sustainability professions. studies have shown that women and people of color are less likely to apply for jobs unless they believe they can perform every job description task. we want to encourage you to think broadly about your background and if you meet some, but not all the qualifications, yet bring strength in crucial areas, you may be a perfect candidate! we are most interested in finding the best candidate for the job, and that candidate may come from a less traditional background. if you feel your qualifications complement those set forth in this posting and are interested in brendle group, please contact us at hr@brendlegroup.com for additional discussion, referencing this posting. application process please send the following to hr@brendlegroup.com: • resume • three professional references • salary expectations • schedule and office location preference • a compelling cover letter explaining how your experience meets our job requirements. posting dates december 1–december 22, 2025. applications will be evaluated as they are submitted. interviews screening calls are planned for the week of january 12, 2026. • in-person interviews are planned for the week of january 26, 2026. targeted start date mid-february 2026 • the hiring process will include several steps: an initial screening call, followed by interviews—conducted in person for colorado-based applicants and virtually for those outside of colorado or in the twin cities metro area. candidates who advance will undergo reference checks, and final-round applicants will complete a skills assessment to confirm technical proficiency and ensure the best fit for the role. this job is not eligible for sponsorship.","fort collins, co",Data Analyst,"['dashboard', 'data analysis', 'data analytics', 'etl', 'excel', 'java', 'power bi', 'python', 'r', 'sql', 'sql server', 'statistics', 'tableau']","['dashboard', 'data analysis', 'data analytics', 'etl', 'excel', 'java', 'power bi', 'python', 'r', 'sql', 'sql server', 'statistics', 'tableau']","72,490–103,070 a year"
data analyst(remote-wfh),fraser public schools,"job title data analyst (remote-wfh) company name fraser public schools company description fraser public schools is committed to providing high-quality education and fostering a supportive learning environment for students. we are dedicated to innovation and excellence in education, and we are looking for talented individuals to join our team. job location fraser, co years of experience 0-5 job description we are seeking a motivated and analytical data analyst to join our team remotely. the ideal candidate will be responsible for analyzing and interpreting complex data sets to help drive strategic decisions and improve educational outcomes. this role requires strong analytical skills, effective communication, and the ability to work independently in a remote setting. key responsibilities • analyze and interpret complex data sets to identify trends and patterns. • develop and maintain data dashboards and reports to support decision-making. • collaborate with various departments to understand data needs and provide insights. • ensure data accuracy and integrity across all platforms. • communicate findings and recommendations to stakeholders effectively. required skills • strong analytical and problem-solving skills. • proficiency in data analysis tools and software (e.g., excel, sql, python). • excellent communication skills, both written and verbal. • ability to work independently and manage time effectively. • experience with data visualization tools is a plus. additional information this is a remote position based in fraser, co. we offer a flexible work schedule and the opportunity to be part of a dynamic and supportive team. created at 2025-07-21 16:22:52 updated at 2025-07-21 16:24:59 location: in-office (fraser, co); about fraser public schools: fraser public schools is a public school district located in fraser, michigan in the united states. the schools educate about 5,200 students. the majority of students were categorized as white, followed by black, with only 1–2% of the students asian, pacific islander or hispanic. open positions: • fraser public schools - wfh data operator clerk/typist-remote. • students - sample job • fraser public schools - account manager(wfh-remote) • fraser public schools - remote data entry clerk/customer service representative. • fraser public schools - sales representative. • fraser public schools - data entry clerk(wfh-remote) (copy) • fraser public schools - typist • fraser public schools - quality assurance analyst • fraser public schools - remote data entry specialist • fraser public schools - remote content writer/copywriter","fraser, co",Data Analyst,"['dashboard', 'data analysis', 'excel', 'python', 'r', 'recommendation', 'sql']","['dashboard', 'data analysis', 'excel', 'python', 'r', 'recommendation', 'sql']",
financial data analyst - entry level,robert half,"we are seeking a detail-oriented and knowledgeable financial data analyst to support artificial intelligence (ai) annotation initiatives across a range of projects, including research document analysis, document search optimization, and others. the ideal candidate will have a solid understanding of financial markets and strong accounting expertise, ideally with experience in auditing. key responsibilities:review and annotate financial documents and research materials to support ai training and model development.apply financial knowledge to ensure accuracy and consistency in data labeling.collaborate with cross-functional teams to meet project deadlines and quality standards.adapt quickly to shifting priorities and support different projects, including supply chain, as needed.follow established guidelines and maintain high attention to detail.",united states,Data Analyst,['r'],['r'],17–18 an hour
onchain data analyst & researcher,coinbase,"ready to be pushed beyond what you think you’re capable of? at coinbase, our mission is to increase economic freedom in the world. it’s a massive, ambitious opportunity that demands the best of us, every day, as we build the emerging onchain platform — and with it, the future global financial system. to achieve our mission, we’re seeking a very specific candidate. we want someone who is passionate about our mission and who believes in the power of crypto and blockchain technology to update the financial system. we want someone who is eager to leave their mark on the world, who relishes the pressure and privilege of working with high caliber colleagues, and who actively seeks feedback to keep leveling up. we want someone who will run towards, not away from, solving the company’s hardest problems. our work culture is intense and isn’t for everyone. but if you want to build the future alongside others who excel in their disciplines and expect the same from you, there’s no better place to be. while many roles at coinbase are remote-first, we are not remote-only. in-person participation is required throughout the year. team and company-wide offsites are held multiple times annually to foster collaboration, connection, and alignment. attendance is expected and fully supported. coinbase’s unit 0x is a specialized on-chain investigations team at the forefront of securing the crypto ecosystem. we identify, analyze, and mitigate critical on-chain threats—including exploits and illicit activity—using advanced blockchain analytics and investigative techniques. our mission-driven team protects both coinbase and the broader web3 community from emerging risks. as an onchain data analyst & researcher, you’ll lead challenging investigations into illicit activity, proactively research threat actors, and develop scalable methods to detect suspicious patterns. you’ll play a key role in high-risk incidents, special projects, and the continuous improvement of our investigative processes. success in this role requires curiosity, strong blockchain data analysis skills, and a passion for uncovering and neutralizing threats. if you thrive on solving complex problems with data and want to help shape a secure future for blockchain adoption, we want you on our team. onchain is the new online. join us in building a secure future for the next billion users. what you’ll be doing (ie. job duties): • conduct in-depth investigations and research into illicit activity, scams, onchain exploits, and vulnerabilities, using blockchain analytics and clustering techniques to surface suspicious trends and patterns. • analyze and document exploit methodologies, attack vectors, abuse patterns, and incident impacts, providing actionable intelligence to strengthen coinbase’s security posture. • develop and automate scalable methods for detecting and understanding emerging threats, including building dashboards, writing queries, and supporting incident response with clear, quantitative reporting. • collaborate with cross-functional teams to educate and advise on evolving exploit tactics, risk landscapes, potential mitigations and investigative best practices. • monitor and research threat actors, their environments, and emerging blockchain trends to stay ahead of illicit activity and technological developments. • respond to urgent, sensitive cases with discretion and professionalism, and support team members as needed. • handle highly sensitive cases with respective urgency and discretion; • other duties and responsibilities as required or assigned. what we look for in you (i.e., job requirements) • a well-organized self-starter who is able to constantly learn and work autonomously; • ability to handle highly sensitive information; • excellent writing, analytical and communication prowess; • deep knowledge of blockchain ecosystems and; • investigative mindset; • strong sql skills - particularly around on chain analysis in conjunction with working knowledge of abuse vectors; • knowledge to leverage ai to 10x output. nice to haves: • previous experience in corporate osint analysis • experience with dune analytics, flipside, and/or chainbase. position id: p70663 #li-remote pay transparency notice: depending on your work location, the target annual base salary for this position can range as detailed below. full time offers from coinbase also include bonus eligibility + equity eligibility + benefits (including medical, dental, vision and 401(k)). base salary range shown. total compensation also includes equity and bonus eligibility and benefits: $144,500—$170,000 usd please be advised that each candidate may submit a maximum of four applications within any 30-day period. we encourage you to carefully evaluate how your skills and interests align with coinbase's roles before applying. commitment to equal opportunity coinbase is proud to be an equal opportunity employer. all qualified applicants will receive consideration for employment without regard to race, color, religion, creed, gender, national origin, age, disability, veteran status, sex, gender expression or identity, sexual orientation or any other basis protected by applicable law. coinbase will also consider for employment qualified applicants with criminal histories in a manner consistent with applicable federal, state and local law. for us applicants, you may view the employee rights and the know your rights notices by clicking on their corresponding links. additionally, coinbase participates in the e-verify program in certain locations, as required by law. coinbase is also committed to providing reasonable accommodations to individuals with disabilities. if you need a reasonable accommodation because of a disability for any part of the employment process, please contact us at accommodations[at]coinbase.com to let us know the nature of your request and your contact information. for quick access to screen reading technology compatible with this site click here to download a free compatible screen reader (free step by step tutorial can be found here). global data privacy notice for job candidates and applicants depending on your location, the general data protection regulation (gdpr) and california consumer privacy act (ccpa) may regulate the way we manage the data of job applicants. our full notice outlining how data will be processed as part of the application procedure for applicable locations is available here. by submitting your application, you are agreeing to our use and processing of your data as required. ai disclosure for select roles, coinbase is piloting an ai tool based on machine learning technologies to conduct initial screening interviews to qualified applicants. the tool simulates realistic interview scenarios and engages in dynamic conversation. a human recruiter will review your interview responses, provided in the form of a voice recording and/or transcript, to assess them against the qualifications and characteristics outlined in the job description. for select roles, coinbase is also piloting an ai interview intelligence platform to transcribe and summarize interview notes, allowing our interviewers to fully focus on you as the candidate. the above pilots are for testing purposes and coinbase will not use ai to make decisions impacting employment. to request a reasonable accommodation due to disability, please contact accommodations[at]coinbase.com",anywhere,Data Analyst,"['clustering', 'dashboard', 'data analysis', 'excel', 'machine learning', 'r', 'scala', 'sql']","['clustering', 'dashboard', 'data analysis', 'excel', 'machine learning', 'r', 'scala', 'sql']",144K–170K a year
enterprise data analyst,"le001 modsquad, inc.","we could use someone like you in our crew. at modsquad, we’re always pushing the boundaries of what’s possible in the digital space. we’ve built an industry-leading tech stack and we’re looking for an enterprise data analyst (gtm focus) to join our enterprise applications team within the tech department. this role is perfect for a data-driven professional who thrives on turning raw information into actionable insights. if you love digging into numbers, spotting trends, and presenting data in a way that influences decision-making at all levels, we’d love to hear from you. what you’ll do perform in-depth data analysis to uncover trends, patterns, and anomalies, providing clear and actionable insights for cross-functional teams and clients. build and maintain interactive dashboards and reports that make complex findings simple and impactful. collaborate with product, operations, and executive teams to translate business questions into analytical solutions. partner with sales, marketing, and services leadership to enhance engagement and business strategies with data-driven insights. contribute to growth measurement and optimization, aligning with our user and client roadmap. ensure data integrity and accuracy, working closely with the wider data team on quality initiatives. empower teams with self-service analytics tools and training. present findings and recommendations to stakeholders, tailoring communication for different audiences. what we’re looking for bachelor’s degree (or equivalent experience) in data analysis, statistics, business intelligence, or related field. 5+ years of experience in data analysis, reporting, and visualization. hands-on experience with gtm-focused tools like salesforce, hubspot, zoominfo, linkedin, ga4, and ai tools (jasper, etc.). strong sql skills for data extraction and manipulation. experience with modern analytics and visualization platforms such as thoughtspot, sigma, tableau, or power bi. familiarity with data warehousing (snowflake preferred). exposure to saas environments and transactional systems in high-growth companies. highly valued skills include familiarity and hands-on experience with geo (generative seo), gtm journeys, bowtie full funnel focus, account-based marketing, and winning by design frameworks. excellent time management and organizational skills. strong communication skills with the ability to influence and collaborate across teams. creative problem-solver with a “macgyver” mindset for building smart, flexible solutions. high eq, adaptability, and a sense of humor (must-have at modsquad!). benefits & perks flexible, remote-first environment collaborative global team culture opportunities for professional growth and development work with some of the most exciting brands in the world ✨ ready to bring your data superpowers to modsquad? apply today! about modsquad: modsquad has been reinventing the customer experience services industry since 2007. top brands around the globe turn to us for customer support, content moderation, trust and safety, community management, and social media services. we work with startups and fortune 500 companies and everything in between. our client list includes spotify, vsco, vimeo, tourism ireland, and a ton of other companies we aren’t at liberty to talk about. we support over 50 languages in more than 90 countries. we’re primarily a remote company so you’ve already seen our/your office. if you want to work with great people on cool projects for amazing brands, you’ve come to the right place. modsquad is an equal opportunity employer. all qualified applicants will receive consideration for employment without regard to race, color, religion, sex (including pregnancy), gender, national origin, ancestry, age, physical or mental disability, military status, status as a veteran or disabled veteran, sexual orientation, gender identity or expression, marital or family status, genetic information, medical condition, or any other basis protected by applicable federal, state, or local law, ordinance, or regulation. at modsquad, we’ve been changing the customer experience game since 2007. great people are at the center of everything we do, and we’re always looking for talented people to join our team. work for a growth company with amazing clients and awesome colleagues. we offer competitive benefits and most positions are remote. if you still have questions or need accommodations, shoot us an email: recruitment@modsquad.com.",united states,Data Analyst,"['business intelligence', 'dashboard', 'data analysis', 'excel', 'power bi', 'r', 'recommendation', 'snowflake', 'sql', 'statistics', 'tableau']","['business intelligence', 'dashboard', 'data analysis', 'excel', 'power bi', 'r', 'recommendation', 'snowflake', 'sql', 'statistics', 'tableau']",
"data analyst ii visualization at centene corporation cheyenne, wy",centene corporation,"data analyst ii visualization job at centene corporation. cheyenne, wy. you could be the one who changes everything for our 28 million members. centene is transforming the health of our communities, one person at a time. as a diversified, national organization, you'll have access to competitive benefits including a fresh perspective on workplace flexibility. • *position purpose:** responsible for analytical data needs. handle complex data requests, reports, and predictive data modeling. + analyze health management programs including: data collection, validation and outcome measurement. + may include: financial, pharmacy, claims, provider, and member data. + irs, cms, hhsc, hedis reporting. + internal data cleansing and data reconciliation analysis. + trend analysis in various functional areas of health care management. + create and generate reports through ms-excel, ms-access, and sql using business objects interface and direct links to core databases (ods/edw). + produce reports for and interface with senior management and internal and external stakeholders. + gather and interpret business requirements and monitor data trends to proactively identify issues. + execute data changes and update core systems as needed. + handle multiple projects and timelines effectively and communicate risks and issues to manager regularly. + assist with training lower level data analysts. + performs other duties as assigned. + complies with all policies and standards. • *education/experience:** + bachelor's degree in a related field or equivalent experience. + 2+ years of statistical analysis or data analysis experience. + sql experience strongly preferred. + power bi, tableau, or other data visualization tools strongly preferred. + prior experience working with healthcare data and claims analysis strongly preferred. + experience with toad scripting preferred. • *_by applying to this requisition, you acknowledge and understand that you may be considered for other job opportunities for which centene believes you may be qualified._** pay range: $55,100.00 - $99,000.00 per year centene offers a comprehensive benefits package including: competitive pay, health insurance, 401k and stock purchase plans, tuition reimbursement, paid time off plus holidays, and a flexible approach to work with remote, hybrid, field or office work schedules. actual pay will be adjusted based on an individual's skills, experience, education, and other job-related factors permitted by law, including full-time or part-time status. total compensation may also include additional forms of incentives. benefits may be subject to program eligibility. centene is an equal opportunity employer that is committed to diversity, and values the ways in which we are different. all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, veteran status, or other characteristic protected by applicable law. qualified applicants with arrest or conviction records will be considered in accordance with the la county ordinance and the california fair chance act","cheyenne, wy",Data Analyst,"['data analysis', 'excel', 'power bi', 'r', 'sql', 'tableau']","['data analysis', 'excel', 'power bi', 'r', 'sql', 'tableau']",
ai monitoring data quality analyst,fortress information security,"disclaimer: we aren’t hiring for this role just yet, but we expect to in the future. if you're interested in being considered when the role becomes available, we encourage you to submit your application. we appreciate your interest. ai monitoring data quality analyst location: remote compensation: $50,000 - $55,000 per year, depending on experience and qualifications. employment type: full-time what you can expect as a the ai monitoring data quality analyst at fortress: the ai monitoring data quality analyst will review reports created to identify possible third-party risk associated with cybersecurity, regulatory and compliance, financial, offshore, and other security, and supply chain topics. this position is responsible for reviewing, editing, and determining the relevance of data that is collected using public, private, and proprietary tools, analyzing, and evaluating information, and recommending risk levels based on findings. this position may be involved with additional projects as needed. ai monitoring data quality analysts have proven experience utilizing osint tools for cybersecurity third-party vendor research as well as experience reviewing and editing reports. this is an position designed for one to gain experience, knowledge, and relevant skills while determining a potential future career path within fortress. responsibilities include • curate and organize company-specific information from public, private, and proprietary sources through data collection, analysis, evaluation, and risk-scoring with an experienced understanding of the data-driven assessment process. • evaluate information to identify security risk findings. • determine which security risk findings should be reported as part of the aim standard or premium product. • discuss findings with clients as needed • determine the security risk level based on all findings identified. • communicate effectively within the research team. • track assigned work efficiently while meeting given deadlines. • other duties as assigned. minimum qualifications • proven working experience with security research analysis. • demonstrated professional communication and client relationship skills. • attention to detail, sound judgement, logical thinking, and proven ability to follow established scripts/consistency models, drive tasks to completion, meet deadlines in a fast-paced environment, and adapt to changing business environment with periodic supervision. • strong computer skills, including ms office and google products (e.g. word, excel, gmail) and other business software. • understanding of information security frameworks (e.g., nist (national institute of standards and technology) 800-53, nist csf (cybersecurity framework), iso (independent system operators) (independent system operators) 27001). • ambition, drive, sharp vision and a great attitude. preferred experience: • record of accomplishment of success/top performer • proven experience effectively prioritizing schedule and flexing workload to meet tight deadlines and challenging work objectives • experience working in highly regulated industries, such as financial services, energy, and/or healthcare. education • high school or ged • pursuing or recently graduated from bachelor’s or master’s program employee benefits • remote and hybrid working environment • competitive pay structure • medical, dental, vision plans with employees covered up to 90% with highly progressive options for dependents and families • company paid life, short- and long-term disability insurance • employee assistance program • 401(k) match • flexible paid time off • parental leave • access to thousands of learning & development courses that range from mental health and wellbeing, stress, and time management to an array of technical and business-related courses employment perks • we provide each employee with professional growth opportunities through succession planning, up-skilling, and certifications • tuition and certification reimbursement • employee referral programs • company sponsored events fortress is proud to be an equal opportunity employer. all employees and applicants will receive consideration for employment without regard to age, color, disability, gender, national origin, race, religion, sexual orientation, gender identity, protected veteran status, or any other classification protected by federal, state, or local law. fortress information security takes part in the e-verify process for all new hires. for positions located in the us, the following conditions apply. if you are made a conditional offer of employment, you will have to undergo a drug test. ada disclaimer: in developing this job description care was taken to include all competencies needed to successfully perform in this position. however, for americans with disabilities act (ada) purposes, the essential functions of the job may or may not have been described for purposes of ada reasonable accommodation. all reasonable accommodation requests will be reviewed and evaluated on a case-by-case basis.",anywhere,Data Analyst,"['classification', 'excel', 'r']","['classification', 'excel', 'r']",50K–55K a year
cybersecurity data analyst,hunter strategy,"about hunter strategy hunter strategy has a unique philosophy to technical project delivery. we treat all our customers like mission partners because they rely on our team to meet their objectives through complex software engineering, cloud operations, and cyber risk management solutions. hunter strategy was founded on the premise that it is 21st century infrastructure - critically important but only instrumentally valuable. accordingly, our teams look at problems with a single objective: the identification and enablement of the right capability to address the most vexing problems our mission partners face. we continue to support our partners' success by leveraging the right technology, with the right plan, and the right team to address tomorrow's challenges today. hunter strategy is seeking an assessment data analyst to support cybersecurity assessment initiatives. the analyst will play a key role in aggregating and interpreting large cybersecurity datasets to measure the overall posture of business units and the organization. this role requires strong statistical analysis capabilities, familiarity with cybersecurity assessment frameworks, and the ability to turn complex data into actionable insights that enhance cybersecurity effectiveness. key responsibilities • support the aggregation and analysis of cybersecurity assessment data to evaluate organizational cybersecurity posture. • develop and author splunk spl queries to interrogate large datasets and identify trends, anomalies, and deviations. • monitor cybersecurity assessment data flows and recommend quality improvement initiatives. • develop mappings between existing controls and assessment metrics. • perform statistical analyses on large (including massive) datasets—such as computing standard deviations, trends, anomalies, and deviations—using tools such as spreadsheets, databases, business intelligence platforms, visualization tools, or programming languages. • * note: use of ai-assisted tools (e.g., chatgpt) is prohibited. • identify and map data flows within enterprise networks to support assessment and analysis activities. • conduct event log analysis to determine telemetry, sequences of events, impacts, threats, and mitigation or recovery steps—leveraging splunk as a key siem tool. • use the mitre att&ck framework to support threat hunting and detection-building in splunk (preferred). • provide data-driven insights and recommendations to improve cybersecurity posture. required qualifications • at least 1 year of experience performing statistical analysis on large or massive datasets. • demonstrated ability to author splunk spl queries from scratch (e.g., writing queries to identify unusual login times, isolate anomalies, or detect irregular patterns). • demonstrated ability to conduct statistical analysis, including identifying standard deviations and interpreting trends, without ai-assisted tools. • minimum 2 years of experience conducting federal cybersecurity assessments measuring compliance with nist 800-53 rev. 4 or newer. • minimum 2 years of experience performing fisma compliance assessments. • minimum 2 years in a customer-facing role, with experience assessing compliance, documenting findings, and providing actionable recommendations. • experience identifying and mapping enterprise data flows. • experience with event log analysis and siem tools such as splunk. • experience with the mitre att&ck framework (preferred). • experience as a soc analyst using splunk (preferred). • active public trust clearance or the ability to obtain a government security clearance. nice-to-have qualifications • working knowledge of large-scale or distributed data and analytics solutions such as apache spark, hadoop, splunk spl, azure data lake, or similar tools. • at least 3 years of experience in network operations or cybersecurity, with at least 1 year focused on data analysis. • strong communication and customer relationship skills.",anywhere,Data Analyst,"['azure', 'business intelligence', 'cloud', 'data analysis', 'data lake', 'hadoop', 'r', 'recommendation', 'spark']","['azure', 'business intelligence', 'cloud', 'data analysis', 'data lake', 'hadoop', 'r', 'recommendation', 'spark']",
senior data analyst,rocky vista university,"rocky vista university is hiring a senior data analyst with 3 - 5 years of experience. based in united states - parker, co and with in-office ways of working. job description and responsibilities: the senior data analyst for the physician assistant program will be responsible for data analysis and data management projects and operations that support data-informed strategic planning, decision making, and assessment reporting for the physician assistant program's innovative competency-by-design curriculum. this senior level staff position will work independently and as a member of a team to plan and execute assessment and evaluation projects. essential job functions • work with the pd and faculty in the planning and implementation of all phases of assessment and evaluation projects, including design, quantitative and qualitative data collection, analysis, and reporting. • provide leadership and critical support for development and assessment of processes that ensure efficient, effective implementation of core program data analysis functions • plan and run statistical analysis on student and program data, including but not limited to correlation, regression, stratification, for use in developing predictive modeling tools to predict student success and program outcomes. • perform qualitative analysis to identify positive and negative trends noted within program and course-level evaluations. • present findings visually, orally and in writing, as appropriate, and contribute visual and written materials to reports, publications and proposals. • design and produce charts and graphs that describe and interpret findings, and efficiently and effectively reveal spatial and statistical relationships with actionable insights. • respond to ad-hoc research- and evaluation-related queries. • attend and participate in required team meetings via zoom or in person as needed. • coordinate strategic and operational planning and outcomes analysis within the program – analyze, and present to faculty, findings of student and cohort competency progression to mitigate risk of slowed development. • conceive strategies, develop tools and processes, and coordinate resources required to support curriculum tracking objectives. • provide benchmarking information to program director as a means of assessing effectiveness of program processes/systems. • collects, manages, scrubs, and distributes a variety of program data to inform and support the ongoing assessment reporting and planning processes of the program • manage on-time completion of core program functions, including but not limited to didactic, clinical, and admissions timelines. • consult with the program director and serve as liaison to university-level administrators to promote, coordinate, and monitor data and analytics required by institutional accrediting bodies. • serve on relevant program committees and communicate data-driven recommendations to all members of the team. • provide the ongoing data analysis and summary reports required to fulfill accreditation requirements. • train faculty and staff in the use of tracking, reporting, and data repository tools. • identify and provide oversight of budget needs associated with securing and implementing new systems and tools to enhance data analysis and management. • serve as liaison for any curriculum tracking tool, data analytics, or data repository development by external or internal vendors. • coordinate with tool vendors to meet cost, schedule, and technical requirements of any project management, data analytic, or data repository tools required within the program. • oversee data integrity and integration processes between outside vendors and university systems. requirements and qualifications: required knowledge, skills, and abilities • proven research and evaluation experience and expert knowledge of quantitative and qualitative research methods. track record of undertaking and/or commissioning research and evaluation that has informed practice and/or policy development. • demonstrated ability to analyze quantitative and qualitative data, interpret findings, discern implications, draw conclusions, and make recommendations. • demonstrated proficiency in statistical analysis software, including ability to write complex syntax. • advanced critical thinking skills, exhibited by the ability to synthesize and summarize complex information from a range of sources in an accessible way. • ability to translate technical data into messages and graphic representations meaningful to a broad audience. • in-depth understanding of modern database design, systems and information technologies. • designing, developing, and modifying data infrastructure to accelerate the processes of data analysis and reporting. • developing standards of operation when handling and archiving data. • maintaining data management plans and instructions for operating complex business systems. • assessing system performance and making recommendations for hardware, software, and data storage improvements. • ensuring data and information security by integrating and upholding digital security systems. • good relational skills (including with people from a diverse range of backgrounds) and ability to work collaboratively as part of a team. personable and able to influence others whilst maintaining supportive, collegial relationships. ability to establish positive working relationships at all levels of an organization and across departments • ability to work independently, possess strong time management and prioritization skills, and operate with a high level of attention to detail and accuracy. • effective meeting and facilitation skills. • demonstrated decision-making ability and desire to lead • demonstrated commitment to valuing equity, diversity and contributing to an inclusive working and learning environment. minimum qualifications • bachelor's degree in a medical or technical field • at least five years relevant experience in an educational or research setting. experience in industry can also be transferable to this position. • experience in statistical analysis and predictive modeling. • skilled with data visualization and analysis tools (e.g. tableau, powerbi, excel, etc.) • strong working knowledge of microsoft office applications (outlook powerpoint, excel, onedrive, sharepoint) preferred qualifications • master's degree or other advanced degree in statistics and measurement • professional background in academic and medical environments • experience working in higher education • experience with medical education-specific it systems (learning management, clinical tracking platforms, assessment platforms) • experience working directly with software tool vendors","parker, co",Data Analyst,"['data analysis', 'data analytics', 'excel', 'r', 'recommendation', 'regression', 'statistics', 'tableau']","['data analysis', 'data analytics', 'excel', 'r', 'recommendation', 'regression', 'statistics', 'tableau']",60K–74.9K a year
data analyst intern,booz allen hamilton,"data analyst intern the opportunity: are you excited at the prospect of unlocking the secrets held by a data set? are you fascinated by the possibilities presented by the iot, machine learning, and artificial intelligence advances? in an increasingly connected world, massive amounts of structured and unstructured dataopen upnew opportunities. as a data analyst intern, you can turn these complex data sets into useful information to solve global challenges. across private and public sectors - from fraud detection, to cancer research, to national intelligence - you know the answers are in the data. we have an opportunity for you to use your analytical skills to improve our business. you'll work closely with your customer to understand their questions and needs, and then dig into their data-rich environment to find the pieces of their information puzzle. you'll apply analytical skills and use the right combination of tools and frameworks to turn that set of disparate data points into objective answers to help clients make informed decisions. you'll provide your customer with a deep understanding of their data, what it all means, and how they can use it. join us as we use data science for good. join us. the world can't wait. you have: • experience with ai/ml analytics • knowledge of data manipulation, database management, statistical analysis, data visualization, and web scraping • ability to solve problems to overcome programming and analysis challenges • ability to obtain a secret clearance • scheduled to obtain a doctorate degree in an engineering or data science field by may 2028 nice if you have: • experience with research and development • knowledge of time series analysis and natural language processing clearance: applicants selected will be subject to a security investigation and may need to meet eligibility requirements for access to classified information. compensation at booz allen, we celebrate your contributions, provide you with opportunities and choices, and support your total well-being. our offerings include health, life, disability, financial, and retirement benefits, as well as paid leave, professional development, tuition assistance, work-life programs, and dependent care. our recognition awards program acknowledges employees for exceptional performance and superior demonstration of our values. full-time and part-time employees working at least 20 hours a week on a regular basis are eligible to participate in booz allen's benefit programs. individuals that do not meet the threshold are only eligible for select offerings, not inclusive of health benefits. we encourage you to learn more about our total benefits by visiting the resource page on our careers site and reviewing our employee benefits page. salary at booz allen is determined by various factors, including but not limited to location, the individual's particular combination of education, knowledge, skills, competencies, and experience, as well as contract-specific affordability and organizational requirements. the projected compensation range for this position is $48,000.00 to $98,000.00 (annualized usd). the estimate displayed represents the typical salary range for this position and is just one component of booz allen's total compensation package for employees. this posting will close within 90 days from the posting date. identity statement as part of the application process, you are expected to be on camera during interviews and assessments. we reserve the right to take your picture to verify your identity and prevent fraud. work model our people-first culture prioritizes the benefits of flexibility and collaboration, whether that happens in person or remotely. • if this position is listed as remote or hybrid, you'll periodically work from a booz allen or client site facility. • if this position is listed as onsite, you'll work with colleagues and clients in person, as needed for the specific role. commitment to non-discrimination all qualified applicants will receive consideration for employment without regard to disability, status as a protected veteran or any other status protected by applicable federal, state, local, or international law.",united states,Data Analyst,"['machine learning', 'natural language processing', 'r', 'time series']","['machine learning', 'natural language processing', 'r', 'time series']",98K a year
healthcare data analyst,colorado springs cardiology,"colorado springs cardiology (cosc), in partnership with heart & vascular partners, seeks a hard-working, team-oriented individual to join our team and serve as a healthcare data analyst. as a healthcare data analyst you are responsible for collecting, analyzing, and interpreting healthcare data to support decision-making, improve patient outcomes, and optimize operational efficiency within the organization. this role involves working with clinical, administrative, and it teams to ensure that data is used effectively to improve healthcare services, identify trends, and meet regulatory compliance standards. the healthcare data analyst will develop reports, dashboards, and analytical models to provide actionable insights for clinical and operational teams. essential functions of the role: • data collection & management: collect, clean, and organize data from multiple healthcare systems (e.g., electronic health records [ehr], billing systems, patient surveys) to ensure accuracy and completeness. develop and maintain databases, data systems, and data pipelines to support analytics functions. ensure data integrity by identifying data quality issues and collaborating with it and clinical teams to resolve discrepancies. • data analysis & reporting: create and present regular reports, dashboards, and visualizations of key performance indicators (kpis) to healthcare teams and leadership. provide insights on patient outcomes, clinical performance, operational efficiency, and financial performance to optimize organizational performance. prepare ad-hoc reports and perform deep dives into specific data sets as requested by management. • decision support & strategic recommendations: use data analytics to support strategic initiatives, such as improving patient care quality, reducing operational costs, and increasing revenue through optimization of workflows and resource management. provide actionable insights that contribute to evidence-based decision-making for clinical and operational teams. collaborate with department leaders to understand business needs and tailor data solutions to meet those needs. • quality improvement & clinical analytics: support quality improvement programs by analyzing clinical data to assess patient outcomes, adherence to guidelines, and quality metrics (e.g., hedis, meaningful use). provide data-driven recommendations to improve patient care processes and enhance the patient experience. participate in initiatives to monitor and improve clinical performance, including tracking compliance with regulatory standards and accreditation requirements. • compliance & regulatory reporting: ensure data practices comply with healthcare regulations such as hipaa, hitech, and cms requirements. assist in the preparation of reports for regulatory bodies, including government agencies, payers, and accrediting organizations. maintain up-to-date knowledge of healthcare laws, standards, and industry best practices related to data analysis and healthcare analytics. qualifications: • bachelor’s degree in healthcare administration, health informatics, statistics, data science, or related field required. • 2-4 years of experience in data analysis, healthcare analytics, or a related field. • experience working with healthcare data sources (ehrs, claims data, patient satisfaction surveys) is highly preferred. • strong analytical and problem-solving skills, with the ability to interpret large sets of healthcare data. • proficiency in data analysis tools (e.g., sql, excel, tableau, power bi, r, python). • familiarity with healthcare systems and databases, including ehr, billing systems, and claims data. • knowledge of healthcare industry standards, regulations, and metrics (e.g., hedis, macra, meaningful use). • strong communication skills, with the ability to present complex data findings to a non-technical audience. • detail-oriented with a focus on data quality and accuracy. • certified health data analyst (chda) or other relevant certifications are a plus. • healthcare-related certifications or experience with healthcare compliance and regulatory standards is preferred. physical requirements as a healthcare data analyst with cosc, you must be able to: • travel between multiple practice locations as needed • stand or walk for periods when observing staff, conducting rounds, and/or participating in staff meetings and training sessions at different practice locations. • sit for extended periods during meetings, report reviews, or administrative tasks, such as managing schedules, budgets, and compliance documentation. • use office equipment (computers, phones, and fax machines) and other technology tools required for administrative tasks. frequently lifting light materials (up to 20-25 pounds) and occasionally lifting up to 50 pounds. • demonstrate the ability to work in a high-pressure, fast-paced environment with the capacity to handle a variety of administrative responsibilities across multiple locations. this includes responding to patient complaints and addressing any compliance or regulatory concerns quickly. company benefits cosc offers its teammates: • competitive wage • 401k with match up to 4%, vested immediately at 100% once contributions start. • flexible work options • pro-rated paid time off with graduated pto for years of service, up to 5 weeks a year equal employment opportunity statement cosc/heart & vascular partners provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. this policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. the statements contained herein are intended to describe the general nature and level of work performed by the healthcare data analyst but is not a complete list of the responsibilities, duties, or skills required. other duties may be assigned as business needs dictate. reasonable accommodations may be made to enable qualified individuals with disabilities to perform the essential functions.","colorado springs, co",Data Analyst,"['aws', 'dashboard', 'data analysis', 'data analytics', 'data pipeline', 'excel', 'power bi', 'python', 'r', 'recommendation', 'sql', 'statistics', 'tableau']","['aws', 'dashboard', 'data analysis', 'data analytics', 'data pipeline', 'excel', 'power bi', 'python', 'r', 'recommendation', 'sql', 'statistics', 'tableau']",
"data analyst, 4+ years of experience",snap inc.,"snap inc is a technology company. we believe the camera presents the greatest opportunity to improve the way people live and communicate. snap contributes to human progress by empowering people to express themselves, live in the moment, learn about the world, and have fun together. the company’s three core products are snapchat, a visual messaging app that enhances your relationships with friends, family, and the world; lens studio, an augmented reality platform that powers ar across snapchat and other services; and its ar glasses, spectacles. the data science & insights team seeks to leverage our massive data sets into actionable insights that deliver business value. as truth seekers and truth tellers, we relay what the data says without bias, even when inconvenient or difficult. we aim to improve decision-making quality and speed across the organization to enable a deeper understanding of our customers and products. we focus on efficient and effective analytic execution, positioning ourselves as the go-to experts with deep knowledge and expertise, constantly striving for better, faster insights from data. we’re looking for a data analyst to join our data science & insights team at snap! what you’ll do: drive impactful enhancements across snap’s data ecosystem, optimizing etl processes and data distribution while providing robust documentation and governance. deliver strategic insights by deeply understanding key business metrics, influencing product development and audience growth strategies. lead the design, execution, and automation of a/b and multivariate experiments, offering actionable business recommendations through robust data analysis. collaborate with cross-functional teams, including data engineering, product, and business units, to develop high-impact analytical solutions that drive strategic decision-making. enhance and maintain advanced data frameworks that optimize reporting and monitor key performance indicators, ensuring data accuracy and accessibility. proactively develop data quality checks, ensuring rigorous qa processes to maintain data integrity and reliability. create and manage scalable data pipelines, performing sql and etl tuning to increase efficiency and reduce system costs. lead the creation of detailed documentation, metadata management, and facilitate comprehensive data adoption strategies. communicate complex data stories to various stakeholders using advanced visualization techniques, elevating data literacy and driving business impact. knowledge, skills & abilities: proven leadership capabilities with substantial experience driving strategic data-driven projects and initiatives. expert proficiency in sql and python, with the ability to design complex queries and maintain robust data architectures. strong understanding of advanced statistical methods, data modeling, and growth accounting methodologies. excellent strategic thinking skills with the ability to interpret market insights and transform them into actionable business strategies. superior communication skills, capable of crafting compelling narratives that articulate data insights across the organization. minimum qualifications: bachelor's degree in analytics, data science, information systems, computer science, business, marketing, communications, economics, or a related area of study or equivalent years of experience 4+ years of relevant professional experience in an analytical field. 3+ years of experience using sql preferred qualifications: experience in additional programming languages like python. experience in web application development is a strong plus. demonstrated ability to adapt to dynamic environments and cross-functional collaboration. experience with advanced a/b testing and deep understanding of metrics. if you have a disability or special need that requires accommodation, please don’t be shy and provide us some information. ""default together"" policy at snap: at snap inc. we believe that being together in person helps us build our culture faster, reinforce our values, and serve our community, customers and partners better through dynamic collaboration. to reflect this, we practice a “default together” approach and expect our team members to work in an office 4+ days per week. at snap, we believe that having a team of diverse backgrounds and voices working together will enable us to create innovative products that improve the way people live and communicate. snap is proud to be an equal opportunity employer, and committed to providing employment opportunities regardless of race, religious creed, color, national origin, ancestry, physical disability, mental disability, medical condition, genetic information, marital status, sex, gender, gender identity, gender expression, pregnancy, childbirth and breastfeeding, age, sexual orientation, military or veteran status, or any other protected classification, in accordance with applicable federal, state, and local laws. eoe, including disability/vets. we are an equal opportunity employer and will consider qualified applicants with criminal histories in a manner consistent with applicable law (by example, the requirements of the san francisco fair chance ordinance and the los angeles fair chance initiative for hiring, where applicable). our benefits: snap inc. is its own community, so we’ve got your back! we do our best to make sure you and your loved ones have everything you need to be happy and healthy, on your own terms. our benefits are built around your needs and include paid parental leave, comprehensive medical coverage, emotional and mental health support programs, and compensation packages that let you share in snap’s long-term success! compensation in the united states, work locations are assigned a pay zone which determines the salary range for the position. the successful candidate’s starting pay will be determined based on job-related skills, experience, qualifications, work location, and market conditions. the starting pay may be negotiable within the salary range for the position. these pay zones may be modified in the future. zone a (ca, wa, nyc): the base salary range for this position is $142,000-$214,000 annually. zone b: the base salary range for this position is $135,000-$203,000 annually. zone c: the base salary range for this position is $121,000-$182,000 annually. this position is eligible for equity in the form of rsus. a decade of snap: learn about our origin story, values, mission, culture of innovation, and more. citizensnap: in our third annual citizensnap report, we demonstrate progress towards our environmental, social, and governance (esg) goals, and we lay out our plans looking forward. the dei innovation summit: watch highlights from the 2nd annual dei innovation summit, which brings together thought leaders and dei experts for a day of courageous conversations to enable bold action. snap news: stay up to date on the latest and greatest product and innovation news at snap applicant and candidate privacy policy",united states,Data Analyst,"['a/b testing', 'aws', 'classification', 'data analysis', 'data pipeline', 'etl', 'excel', 'python', 'r', 'recommendation', 'scala', 'sql']","['a/b testing', 'aws', 'classification', 'data analysis', 'data pipeline', 'etl', 'excel', 'python', 'r', 'recommendation', 'scala', 'sql']",
automation data & insights analyst,insight global,"senior automation data & insights analyst duration: 12-16 month contract location: onsite 1 day a month in dallas, tx pay range: around 60-70/hr the senior automation data & insights analyst is responsible for enabling data-driven decision-making that accelerates the client's cxo digital transformation and automation strategy. this role drives analytics and performance insights across automation, ai, and self-service initiatives, providing business intelligence that fuels operational excellence and value realization. as part of the business solutions & transformation (bs&t) organization within cxo, this role serves as a trusted partner to operations, technology, and product leadership teams — ensuring the client's automation investments deliver measurable impact to both the customer and the enterprise. key responsibilities • analyze contact center data to identify trends, inefficiencies, and improvement opportunities. • lead the development and automation of performance dashboards, scorecards, and kpi reporting across ai, rpa, and digital initiatives. • translate complex data into clear insights and visualizations that inform strategic decision-making at the leadership level. • define, track, and validate value realization metrics for automation programs, including cost savings, efficiency, and customer experience impact. • integrate and analyze data across salesforce, five9, verint cloud, and ai platforms to provide a holistic view of cxo performance. • conduct advanced analytics to identify automation opportunities and performance trends. • partner with product, technology, and operations teams to define and evolve the automation analytics roadmap. • streamline analytics and reporting processes through automation using power bi, sql, python, and tableau. • support quarterly business reviews, automation performance reviews, and quantum leap transformation updates. • champion data quality, consistency, and governance across cxo reporting assets."" required qualifications • bachelor’s degree in business analytics, computer science, data science, or related field (master’s preferred). • 7+ years of relevant experience in data analytics, automation insights, or digital transformation analytics. • proven experience supporting contact center analytics, preferably within a transformation or automation context. • strong understanding of contact center kpis and metrics. • proficiency in power bi, tableau, sql, and data modeling. • experience with rpa platforms (uipath, blue prism), ai/chatbot analytics, or verint/five9 data integration. • demonstrated ability to communicate analytical insights effectively to non-technical and executive audiences. • knowledge of machine learning and ai applications in contact centers. preferred skills & qualifications • experience supporting automation and ai analytics (rpa, nlp, auto-qa, conversational ai). • strong proficiency in power bi, tableau, and sql for data visualization and reporting automation. • working knowledge of five9, verint cloud, and salesforce service cloud data environments. • proven ability to measure roi, value realization, and performance of automation initiatives. • familiarity with uipath or other rpa platforms and related analytics tools. • excellent data storytelling and executive presentation skills. • strong collaboration and communication skills across business and technology teams. • passion for continuous improvement, automation innovation, and data-driven decision-making. physical requirements • general office environment; hybrid schedule (in-office and remote work). • occasional travel (10%) may be required for collaboration across sites.""",united states,Data Analyst,"['business intelligence', 'cloud', 'dashboard', 'data analytics', 'excel', 'machine learning', 'nlp', 'power bi', 'python', 'r', 'sql', 'tableau']","['business intelligence', 'cloud', 'dashboard', 'data analytics', 'excel', 'machine learning', 'nlp', 'power bi', 'python', 'r', 'sql', 'tableau']",50–70 an hour
"sr data analyst(tableau, ms power bi) remote - richardson, tx",ahu technologies inc,"job description 100% telecommute 8-5 pm cst • more than five (5) years of experience developing, implementing, and maintaining complex tableau or ms power bi dashboards, visualizations, and analytics • proven strong development of high performing sql with more than five years of experience developing complex sql scripts (teradata/db2/oracle load utilities) or stored procedures • at least two years developing complex business intelligence solutions • at least two years of data warehouse project experience • at least two years of teradata experience, including sql assistant/teradata studio • excellent verbal/written communication skills, end client-facing skills, team collaboration, and mentoring skills • bachelors or advanced degree in a related field such as information technology/computer science, mathematics/statistics, analytics, business technology • strong organization skills and ability to set priorities and schedule work deadlines • adapting to and collaborating with the team and organizational culture preferred qualifications: • more than two years of experience analyzing and delivering complex analytics using medicaid claims, and enrollment data • more than two years of tableau server administrative experience • experience in sap business objects or sas is nice to have • experience with state medicaid / medicare / healthcare industry • prior experience in tableau or bo version upgrades • experience in agile tableau and sql - 5 years of experience. healthcare domain knowledge is plus. what skills/attributes are preferred (what will set a candidate apart)? healthcare domain skills. does this position require a professional license or certification? no what does the ideal candidate background look like? 5+ years of business intelligence reporting experience. which would you consider the top skills? tableau sql what experience will set candidates apart from one another? strong dashboard developer. this is a remote position.",anywhere,Data Analyst,"['business intelligence', 'dashboard', 'data warehouse', 'excel', 'power bi', 'r', 'sas', 'sql', 'statistics', 'tableau']","['business intelligence', 'dashboard', 'data warehouse', 'excel', 'power bi', 'r', 'sas', 'sql', 'statistics', 'tableau']",30–36 an hour
lead technology data analyst,"100 raymond james & associates, inc.","job description this position will follow our hybrid work model, we expect the selected candidate to be in office 2-3 days a week at one of the following office locations: st. petersburg, fl; summary: the enterprise data & analytics analyst works as part of the enterprise data team and will be responsible for developing data integration solutions in support of a critical data platform. the analyst plays a key role in the journey of raymond james to develop a leading wealth management platform. this position will have extensive contact with multiple application development teams and other shared services teams. the analyst will be responsible for the deliveries of the analyst team with respect to requirements and high-level design of the data domains. the position is based out of our headquarters in st. petersburg, fl. responsibilities: creates detailed business requirements for functional (e.g., business processes, rules) and non-functional (e.g., data, security) capabilities. gathers and interprets information from multiple sources (including databases, interviews, etc.); resolve complex analytical challenges, independently analyze information; and make recommendations based on analysis. provides support for application development teams including documenting business processes. build strong working relationships with teams, stakeholders, and senior management. incorporate needs, wants, and goals from different business unit perspectives into project specifications. translates technical concepts to a business audience and business information to a technical audience. attend to detail while maintaining a big-picture orientation. solve complex problems and model the business and financial impact of proposed scenarios. participates in developing estimates, project schedules and implementation plans for technical solutions. interpret and apply policies and identify and recommend changes. performs other duties and responsibilities as assigned. skills: business analysis, data analysis, project management. – required understanding or skilled in sql, at an intermediate level. business intelligence skills, such as the creation of tableau, thoughtspot or qlik reports is preferred. product management experience is preferred. conducting interviews with customers and subject matter experts. managing technology products through their lifecycle. motivate and influence others to achieve desired outcomes without organizational authority. partner with other functional areas to accomplish objectives. vendor management and project management is desirable financial services experience or equities / investment banking knowledge is preferred. education bachelor’s: computer systems analysis, high school (hs) (required), master's: computer and information science work experience general experience - 3 to 6 years certifications travel less than 25% workstyle hybrid at raymond james our associates use five guiding behaviors (develop, collaborate, decide, deliver, improve) to deliver on the firm's core values of client-first, integrity, independence and a conservative, long-term view. we expect our associates at all levels to: • grow professionally and inspire others to do the same • work with and through others to achieve desired outcomes • make prompt, pragmatic choices and act with the client in mind • take ownership and hold themselves and others accountable for delivering results that matter • contribute to the continuous evolution of the firm at raymond james – as part of our people-first culture, we honor, value, and respect the uniqueness, experiences, and backgrounds of all of our associates. when associates bring their best authentic selves, our organization, clients, and communities thrive. the company is an equal opportunity employer and makes all employment decisions on the basis of merit and business needs. #li-sa1 raymond james is a diversified financial services company providing wealth management, capital markets, asset management, banking and other services to individuals, corporations and municipalities. founded in 1962 in st. petersburg, florida, rather than on wall street, we have always embraced being a different kind of financial services firm. today, raymond james has locations and subsidiaries across the united states, canada, the united kingdom and germany, and is listed on the new york stock exchange under the symbol rjf. thanks for your interest in working with raymond james. while we might not have the perfect role for you today, we'd love to keep in touch. join our talent network to stay up to date on career opportunities that may be a good fit for you. our business is deeply focused on people and their financial well-being. we're committed to helping individuals, corporations and institutions achieve their goals, while also supporting successful professionals and helping our communities prosper. we believe doing well and doing good aren't mutually exclusive. as an established but ever-evolving company, you can start – or continue – growing your career here. we invest in you with wide-ranging benefits and the support of leaders and colleagues who care. from development opportunities and enriching networking groups to prioritizing diversity, inclusion and the power of different perspectives, raymond james is where good people grow. our people-first culture is outlined in our culture blueprint. check it out to see why many choose to work at raymond james – and why they stay.",united states,Data Analyst,"['business intelligence', 'data analysis', 'r', 'recommendation', 'sql', 'tableau']","['business intelligence', 'data analysis', 'r', 'recommendation', 'sql', 'tableau']",
business data analyst for financial modeling systems,intelagile,"edit job duties: the position serves as an analyst in a high-performing software development team responsible for developing and supporting freddie mac’s financial forecasting applications. job duties include but are not limited to the following: • * conduct data forensic analysis to explain data anomalies by digging into data sources, data flow and transformation logics, software implementation process, and system functionality changes • * troubleshoot and resolve any data issues by connecting the dots and identifying the patterns • * interfacing with business users, modelers, and developers to collect, interpret and document business, data, and system requirements for the enhancement of the applications • * perform user acceptance testing to ensure application changes meet business requirements • * product support for business users’ inquiries qualifications: • * bachelor’s degree or higher in computer science, information systems, business or data analytics, financial engineering, or related field • * smart in data and analysis. able to connect dots and see patterns in seemingly random data and events • * take initiative to learn and deliver rather than simply do what is being told • * experience and knowledge of python, excel, and sql • * experience with linux and command-line operations • * familiar with software development process lifecycle (sdlc, waterfall, and agile) • * experience with aws, big data principles preferred • * knowledge of statistical and machine learning models preferred • * knowledge of fixed-income finance and mortgage preferred (e.g. cfa 1 passed) • * years of work experience flexible h1b and green card sponsorship are available for qualified candidates within the u.s.a. to apply, send a cover letter and resume to victor@entagile.com business data analyst for financial model system",united states,Data Analyst,"['aws', 'data analytics', 'excel', 'machine learning', 'python', 'r', 'sql']","['aws', 'data analytics', 'excel', 'machine learning', 'python', 'r', 'sql']",
senior data analyst - full-time,cardinal health,"**_what data analytics brings to cardinal health:_** the data & analytics function oversees the analytics life-cycle in order to identify, analyze and present relevant insights that drive business decisions and anticipate opportunities to achieve a competitive advantage. this function manages analytic data platforms, the access, design and implementation of reporting/business intelligence solutions, and the application of advanced quantitative modeling. we are currently hiring for a **senior data analyst** within **navista - data & advanced analytics** team to support the growth of our navista application suite and the integrated oncology network (ion). as a **senior data analyst** , you will work cross-functionally to solve problems by performing analysis, recommending solutions, and building analytic and predictive products to enable better decision making in the following areas: + revenue cycle management + practice performance insights + metrics/performance reporting at navista, our mission is to empower community oncology practices to deliver patient-centered cancer care. navista, a cardinal health company, is an oncology practice alliance co-created with oncologists and practice leaders that offers advanced support services and technology to help practices remain independent and thrive. true to our name, our experienced team is passionate about helping oncology practices navigate the future. • *_responsibilities_** + applies knowledge of business processes, systems, data, and analytical tools to explore and identify root causes of problems. + system monitoring to ensure processes are executing as expected and correcting issue to ensure purchase orders and receiving not impacted by errors. + partner with cross-functional project teams to ideate, develop, and recommend solutions. listen to partners and ask questions to clearly define and document business problems. + perform exploratory analysis to determine root causes of problems and confirm / reject hypotheses. + apply advanced analytics methodologies to model, analyze and clean datasets + recommend solutions. summarize the financial and operational impact of the recommendation and pros/cons of alternatives considered. + work with data engineers to create data models for analysis, reporting and help create standard reporting of metrics. + lead analytics projects and initiatives that drive meaningful roi. guide business teams/partners towards solving their analytical problems. + acts as a mentor to less experienced colleagues + independently determines method for completion of new projects + works on or may lead complex projects of large scope + self-starter, team player, high eq. • *_qualifications_** + bachelor's degree in related field, or equivalent work experience, preferred + 8-12 years of experience, preferred + 5+ years of experience in business intelligence tools such as power bi, preferred. + healthcare analytics, value-based care experience, preferred + experience in writing complex sql queries, stored procedures, etc. + have excellent verbal and oral communication skills + experience in agile methodologies preferred + experience in version control and ci/cd pipelines + oncology data experience + rcm experience + multi-cloud experience (azure, gcp) • *_what is expected of you and others at this level_** + applies advanced knowledge and understanding of concepts, principles, and technical capabilities to manage a wide variety of projects + participates in the development of policies and procedures to achieve specific goals + recommends new practices, processes, metrics, or models + works on or may lead complex projects of large scope + projects may have significant and long-term impact + provides solutions which may set precedent + independently determines method for completion of new projects + receives guidance on overall project objectives + acts as a mentor to less experienced colleagues • *anticipated salary range:** $105,100 - $150,100 • *bonus eligible:** yes • *benefits:** cardinal health offers a wide variety of benefits and programs to support health and well-being. + medical, dental and vision coverage + paid time off plan + health savings account (hsa) + 401k savings plan + access to wages before pay day with myflexpay + flexible spending accounts (fsas) + short- and long-term disability coverage + work-life resources + paid parental leave + healthy lifestyle programs • *application window anticipated to close:** 12/15/2025 *if interested in opportunity, please submit application as soon as possible. the salary range listed is an estimate. pay at cardinal health is determined by multiple factors including, but not limited to, a candidate’s geographical location, relevant education, experience and skills and an evaluation of internal pay equity. \#li-remote _candidates who are back-to-work, people with disabilities, without a college degree, and veterans are encouraged to apply._ _cardinal health supports an inclusive workplace that values diversity of thought, experience and background. we celebrate the power of our differences to create better solutions for our customers by ensuring employees can be their authentic selves each day. cardinal health is an equal_ _opportunity/affirmative_ _action employer. all qualified applicants will receive consideration for employment without regard to race, religion, color, national origin, ancestry, age, physical or mental disability, sex, sexual orientation, gender identity/expression, pregnancy, veteran status, marital status, creed, status with regard to public assistance, genetic status or any other status protected by federal, state or local law._ _to read and review this privacy notice click_ here (https://www.cardinalhealth.com/content/dam/corp/email/documents/corp/cardinal-health-online-application-privacy-policy.pdf) • *_what data analytics brings to cardinal health:_** the data & analytics function oversees the analytics life-cycle in order to identify, analyze and present relevant insights that drive business decisions and anticipate opportunities to achieve a competitive advantage. this function manages analytic data platforms, the access, design and implementation of reporting/business intelligence solutions, and the application of advanced quantitative modeling. we are currently hiring for a **senior data analyst** within **navista - data & advanced analytics** team to support the growth of our navista application suite and the integrated oncology network (ion). as a **senior data analyst** , you will work cross-functionally to solve problems by performing analysis, recommending solutions, and building analytic and predictive products to enable better decision making in the following areas: + revenue cycle management + practice performance insights + metrics/performance reporting at navista, our mission is to empower community oncology practices to deliver patient-centered cancer care. navista, a cardinal health company, is an oncology practice alliance co-created with oncologists and practice leaders that offers advanced support services and technology to help practices remain independent and thrive. true to our name, our experienced team is passionate about helping oncology practices navigate the future. • *_responsibilities_** + applies knowledge of business processes, systems, data, and analytical tools to explore and identify root causes of problems. + system monitoring to ensure processes are executing as expected and correcting issue to ensure purchase orders and receiving not impacted by errors. + partner with cross-functional project teams to ideate, develop, and recommend solutions. listen to partners and ask questions to clearly define and document business problems. + perform exploratory analysis to determine root causes of problems and confirm / reject hypotheses. + apply advanced analytics methodologies to model, analyze and clean datasets + recommend solutions. summarize the financial and operational impact of the recommendation and pros/cons of alternatives considered. + work with data engineers to create data models for analysis, reporting and help create standard reporting of metrics. + lead analytics projects and initiatives that drive meaningful roi. guide business teams/partners towards solving their analytical problems. + acts as a mentor to less experienced colleagues + independently determines method for completion of new projects + works on or may lead complex projects of large scope + self-starter, team player, high eq. • *_qualifications_** + bachelor's degree in related field, or equivalent work experience, preferred + 8-12 years of experience, preferred + 5+ years of experience in business intelligence tools such as power bi, preferred. + healthcare analytics, value-based care experience, preferred + experience in writing complex sql queries, stored procedures, etc. + have excellent verbal and oral communication skills + experience in agile methodologies preferred + experience in version control and ci/cd pipelines + oncology data experience + rcm experience + multi-cloud experience (azure, gcp) • *_what is expected of you and others at this level_** + applies advanced knowledge and understanding of concepts, principles, and technical capabilities to manage a wide variety of projects + participates in the development of policies and procedures to achieve specific goals + recommends new practices, processes, metrics, or models + works on or may lead complex projects of large scope + projects may have significant and long-term impact + provides solutions which may set precedent + independently determines method for completion of new projects + receives guidance on overall project objectives + acts as a mentor to less experienced colleagues • *anticipated salary range:** $105,100 - $150,100 • *bonus eligible:** yes • *benefits:** cardinal health offers a wide variety of benefits and programs to support health and well-being. + medical, dental and vision coverage + paid time off plan + health savings account (hsa) + 401k savings plan + access to wages before pay day with myflexpay + flexible spending accounts (fsas) + short- and long-term disability coverage + work-life resources + paid parental leave + healthy lifestyle programs • *application window anticipated to close:** 12/15/2025 *if interested in opportunity, please submit application as soon as possible. the salary range listed is an estimate. pay at cardinal health is determined by multiple factors including, but not limited to, a candidate’s geographical location, relevant education, experience and skills and an evaluation of internal pay equity. \#li-remote _candidates who are back-to-work, people with disabilities, without a college degree, and veterans are encouraged to apply._ _cardinal health supports an inclusive workplace that values diversity of thought, experience and background. we celebrate the power of our differences to create better solutions for our customers by ensuring employees can be their authentic selves each day. cardinal health is an equal_ _opportunity/affirmative_ _action employer. all qualified applicants will receive consideration for employment without regard to race, religion, color, national origin, ancestry, age, physical or mental disability, sex, sexual orientation, gender identity/expression, pregnancy, veteran status, marital status, creed, status with regard to public assistance, genetic status or any other status protected by federal, state or local law._ _to read and review this privacy notice click_ here (https://www.cardinalhealth.com/content/dam/corp/email/documents/corp/cardinal-health-online-application-privacy-policy.pdf)","cheyenne, wy",Data Analyst,"['azure', 'business intelligence', 'cloud', 'data analytics', 'excel', 'gcp', 'power bi', 'r', 'recommendation', 'sas', 'sql']","['azure', 'business intelligence', 'cloud', 'data analytics', 'excel', 'gcp', 'power bi', 'r', 'recommendation', 'sas', 'sql']",
"analyst, data strategy & analytics (healthcare)",kepler group,"kepler executes engineered marketing, where every message and ad delivered helps create a more personalized and productive relationship between brands and their consumers. we do this by acting as clients' agency of record, as their in-house team, or some hybrid of the two - and by harnessing data and technology across all paid digital media and data-driven crm channels. kepler prides itself on being a great place to work. in fact, we're proud to share that adage recognized kepler among the best place to work in 2022, validating our investment in our team and our clients. • we're transparent with our employees. you'll hear updates on company financials, how we're performing against bonus goals, and how we're responding to challenges we face. • we're growing. for you, that means unparalleled growth opportunities and a role in shaping the direction of the company. • we're fun. you'll work with and learn from the smartest people in the industry and have a blast doing it. the analyst, data strategy & analytics plays a critical role in ensuring client and company success, designing sophisticated measurement plans, measuring digital campaign performance, and developing deep insights to drive performance. this exciting, entry-level position requires strong quantitative skills, an analytical mindset, and a desire to be part of a tightly-knit team that's out to change the industry. expect to learn a lot about cool technologies, digital media, and marketing strategy quickly. what you will do: while each client engagement is unique, you may: • partner with clients and internal teams to understand business and marketing goals, leveraging analytics to advance these goals • develop data visualizations (datorama) for 2-3 brands, consolidating a variety of disparate data sources into a single, one-stop dashboard brands can use to keep a pulse on real-time performance • support data storytelling efforts, interpreting marketing performance data and crafting a holistic and succinct cross-channel performance story with actionable insights • cultivate an understanding of clients' key business questions and success drivers and contribute to strong client relationships through consistent delivery of high quality reporting and custom deliverables • learn to use measurement and analytics tools used by the team (google tag manager, google analytics, datorama, sql, python, etc.) to support reporting, analysis, and process development desired skills and experience: • bachelor's degree with background in business, marketing, or other analytical discipline • strong quantitative skills, and comfort with ms excel, data analysis and internet technologies • strong interest and/or experience in marketing, data analysis, and data visualization, ideally in digital media (including display, mobile, social and search) • excellent listening and communication skills with strong ability to create and build relationships • ability to self-manage, juggle multiple priorities, and pay strong attention to details • ability to contribute to fast-paced, entrepreneurial, team-based environment • some knowledge of programming languages to aid in analysis and automation (e.g. sql, python, r) is helpful, but not required transparency is fundamental to kepler's culture. our compensation strategy is designed to attract, reward, and retain the talented employees that drive kepler's growth and success. we aim to offer competitive direct compensation and a rich indirect compensation program that demonstrates the value we place on our employees and their wellbeing. compensation: • $28.25/hr - $33.89/hr benefits: • healthcare/dental/vision • unlimited pto • 401k contributions • $75/mo wellness stipend • $100/mo mobile phone stipend • $50/mo internet stipend • $500/yr annual learning stipend • $2,000/yr annual tuition stipend • one-time $200 new hire home office equipment stipend • parental leave - 16 week primary caregiver / 6 week secondary caregiver leave • annual work from anywhere 4 weeks per year kepler is a people first organization. if this role piques your interest but you may not check every box, we still encourage you to apply! studies show that imposter syndrome can prevent women and people of color from applying unless they meet every single qualification. we welcome all who are interested to apply, you just might be a great candidate for this role or others. protect yourself from recruitment fraud. the only way to apply for a position at kepler is by submitting a direct application via the keplergrp.com website or working with a recruiter employed by kepler with a @keplergrp.com email address. learn how to stay safe by clicking here",united states,Data Analyst,"['dashboard', 'data analysis', 'excel', 'python', 'r', 'sql']","['dashboard', 'data analysis', 'excel', 'python', 'r', 'sql']",28.25–33.89 an hour
data analyst sr,dayforce,"dayforce is a global human capital management (hcm) company headquartered in toronto, ontario, and minneapolis, minnesota, with operations across north america, europe, middle east, africa (emea), and the asia pacific japan (apj) region. our award-winning cloud hcm platform offers a unified solution database and continuous calculation engine, driving efficiency, productivity and compliance for the global workforce. our brand promise - makes work life better™ - reflects our commitment to employees, customers, partners and communities globally. effective november 1, 2025 this position is not open to residents of quebec; applicants must reside in a province or territory of canada other than quebec to be considered. any roles available in quebec will be posted separately. about the opportunity the senior data analyst at dayforce is a key member of the decision intelligence team, responsible for delivering end-to-end reporting and analytics solutions that are not only accurate and insightful, but also intuitive, elegant, and user-centric. this role is critical in transforming data into actionable insights that drive strategic and operational decisions. the senior data analyst will partner closely with business leaders across sales, marketing, and other key functions to define kpis, establish consistent reporting frameworks, and deliver high-quality dashboards and analyses. a strong emphasis is placed on power bi ui/ux design excellence, ensuring reporting products reflect best-in-class usability, visual clarity, and accessibility. this individual will leverage advanced sql and power bi skills to design, build, and maintain scalable data solutions that balance technical rigor with polished, intuitive user experiences. additionally, they will lead validation and documentation efforts to ensure all reporting meets dayforce’s standards for quality, reliability, and design consistency. what you'll get to do • lead the end-to-end reporting process—from requirements gathering through publication—ensuring accuracy, consistency, and a refined user experience across all deliverables. • collaborate with department leadership to identify key metrics and develop dashboards and reports that surface meaningful business insights with exceptional visual clarity. • utilize sql and power bi to design and maintain scalable data models, reports, and visualizations, with a strong focus on ui/ux best practices, layout standards, and intuitive information architecture. • apply user-centered design principles to create dashboards that are elegant, easy to navigate, accessible, and optimized for diverse audiences and levels of data literacy. • clean, transform, and validate data to ensure completeness, accuracy, and adherence to governance standards. • conduct detailed analyses to identify trends, opportunities, and risks that inform business strategies. • prepare and deliver clear, visually compelling reports and presentations for senior management and cross-functional stakeholders. • continuously evaluate existing reporting assets to identify opportunities for ux improvements, performance optimization, automation, and retirement of unused reports. • document all reporting assets—including data sources, assumptions, calculations, design standards, and validation procedures—to ensure transparency and reproducibility. • partner with data strategists, engineers, and business partners to ensure alignment on data definitions, availability, and reporting best practices. skills and experience we value • bachelor’s degree in data science, statistics, mathematics, economics, or a related analytical field. • 4+ years of experience in a data analyst or business intelligence role, preferably in a fast-paced enterprise environment. • advanced proficiency in sql and power bi, including deep experience with data modeling, dax, and modern dashboard ui/ux design principles. • demonstrated ability to design clean, intuitive, user-friendly reporting experiences—ideally supported by a portfolio of power bi dashboards showcasing thoughtful layout, navigation, and visual storytelling. • proven analytical, problem-solving, and critical-thinking skills with the ability to translate complex data into actionable insights. • excellent written and verbal communication skills, including experience presenting findings and guiding stakeholders through dashboard usability. • experience developing and maintaining business reports, dashboards, and kpis that drive measurable business impact. • familiarity with data governance, validation, and documentation processes. • experience collaborating cross-functionally with both technical and non-technical stakeholders to achieve business objectives. what’s in it for you dayforce is fueled by the diversity of our talented employees. we are an equal opportunity employer and consider and embrace all individuals and what makes them unique. we believe our employees should be happy and healthy, with peace of mind and a sense of fulfillment. we encourage individuals to apply based on their passions. dayforce encourages personal and professional growth. we offer excellent time away from work programs, comprehensive wellness initiatives and recognition through competitive pay and benefits. with a commitment to community impact, including volunteer days and our charity, dayforce cares we provide opportunities for you to thrive both in your career and personal life. our focus is not just on your job but on supporting you to be the best version of yourself. about the salary ranges please note that the salary range mentioned in this job description should serve simply as a guide. the final compensation offered may vary based on a variety of factors, including bonuses and/or incentives, or a candidate’s experience, skills, budget and location. our company is committed to providing a fair, equitable, and competitive package that reflects the value an individual brings to the organization. proficiency in english is required for this position as this role will regularly interact with english-speaking stakeholders, co-workers, managers and/or clients across the world. further, our back office support teams, including but not limited to human resources, are primarily english speaking. employees need to be able to communicate with these departments in english to appropriately administer their business relationship. due to the significant high volume of interactions with these english-speaking co-workers, managers, stakeholders and/or clients, which is inherent to this position, it is not possible to reorganize the company's activities to avoid this requirement. fraudulent recruiting beware of fraudulent recruiting. legitimate dayforce contacts will use an @dayforce.com email address. we do not request money, checks, equipment orders, or sensitive personal data during the recruitment process. if you have been asked for any of the above, or believe you have been contacted by someone posing as a dayforce employee, please refer to our fraudulent recruiting statement found here: https://www.dayforce.com/be-aware-of-recruiting-fraud dayforce actively monitors all job applications to ensure authenticity. submissions determined to be fraudulent or misleading will be declined from the recruitment process",canada,Data Analyst,"['business intelligence', 'cloud', 'dashboard', 'excel', 'power bi', 'r', 'scala', 'sql', 'statistics']","['business intelligence', 'cloud', 'dashboard', 'excel', 'power bi', 'r', 'scala', 'sql', 'statistics']",$80.5K–$144K a year
data & analytics practice,fdm group,"about the role not just another entry level career opportunity: receive coaching, mentorship, and support at every step of your career journey. we invest in you so you can thrive and succeed. launch your tech career with fdm – any degree welcomed your degree got you here, your passion can take you further. start a tech career that’s both dynamic and impactful. as an fdm consultant, you’ll get the chance to work with global companies across banking, consulting, retail, insurance, and more. you’ll add big names to your resume and gaining hands-on experience from day one. serving a wide variety of industries, we can offer our consultants a career path in their most suited field. we help you develop and build on your strengths, behaviors, knowledge, and ability with hands-on, industry relevant coaching, and place you on assignments with leading global companies. we match your strengths to the right role. opportunities could include, but not limited to: bi developer transform complex datasets into clear visualizations and performance metrics using sql and bi tools, enabling teams to track progress and uncover opportunities. data engineer design scalable systems to process large datasets from multiple sources, enabling teams to access clean, consistent data for reporting and machine learning. ai developer use generative ai frameworks to create smart applications that solve business challenges, from automating workflows to improving user engagement. data analyst interpret complex datasets and present findings through dashboards and reports, helping teams understand patterns and make informed choices. about you what’s in it for you? • fast-track your career with 6-12 weeks initial upskilling courses facilitated by our expert coaches, followed by practice-based learning catered to industry demand • opportunity to work on exciting, business-critical projects with top-tier companies in banking & finance, insurance, retail, and more • dive into delivering solutions across the application lifecycle • no tech experience? no problem. if you’re into interpreting data, enjoy solving problems, and want to explore tech, even without coding experience, we’ll help you build a future in data & analytics • with hands-on support and real client projects, you’ll be assignment-ready and can grow into senior roles in just a few years • build a diverse and rewarding career path with ongoing coaching through our skills lab • consultant experience team to support your wellbeing, health, and happiness • potential to qualify for relocation support what we look for • you hold a university degree level (bachelor or higher) • able to commit to at least two years of employment with fdm as a consultant following the initial training period • strong problem-solving and analytical skills, paired with great interpersonal and communication skills • eligible to work in canada • please note that all client assignments will require on-site work in client offices. office locations may include, but are not limited to, toronto, montreal, halifax, waterloo, and calgary about us fdm powers the people behind tech and innovation. we spot trends, find top talent, and help businesses stay ahead. with 35+ years of experience, we coach, mentor, and launch fresh thinkers from diverse backgrounds into world-class careers. partnering with top global companies, we deliver the right talent at the right time—while guiding our people toward exponential growth. 🌍 global impact – 19 centers across north america, apac, the uk, and europe 🚀 25,000+ careers launched – and counting 🤝 300+ trusted client partners committed to diversity, equity, and inclusion tech careers should be for everyone. with 80+ nationalities represented, fdm thrives on diversity, fuels innovation through unique perspectives, and celebrates success together. as an equal opportunity employer and ftse4good-listed company, we ensure every qualified applicant gets a fair shot—no barriers, just opportunities. please note that fdm utilizes artificial intelligence to assist in evaluating candidates for its career development program. attention: montreal openings: due to the operational requirements of these roles, including frequent communication with clients and fdm employees located outside quebec, the duties associated with these positions require proficiency in english. / attention: offres à montréal: en raison des exigences opérationnelles de ces postes, y compris la communication fréquente avec des clients et des employés de fdm situés en dehors du québec, les tâches associées à ces postes exigent une maîtrise de l'anglais.",canada,Data Analyst,"['bi tools', 'dashboard', 'machine learning', 'r', 'scala', 'sql']","['bi tools', 'dashboard', 'machine learning', 'r', 'scala', 'sql']",
"data analyst, digital experience and personalization",company 1 - the manufacturers life insurance company,"the digital experience and personalization data analyst position offers a unique opportunity to drive our mission of enhancing customer engagement through data-driven insights. this role is pivotal in shaping personalized digital experiences that align with our strategic goals. by joining our team, you will have the chance to influence the future of digital personalization and grow your career in a supportive environment. position responsibilities: analyze and interpret data to enhance digital experiences. develop and implement personalization strategies. collaborate with cross-functional teams to optimize customer engagement. utilize advanced analytics tools to derive actionable insights. monitor and report on key performance indicators related to digital personalization. required qualifications: 3+ years of experience in data analysis or a related field in enterprise environment. proven experience with analytics tools such as google analytics, adobe analytics, etc. bachelor's degree in data science, marketing, or a related discipline. relevant certifications in data analysis or digital marketing are preferred. experience with python scripting, sql, tableau, power bi preferred qualifications: experience with machine learning models for personalization. familiarity with a/b testing and multivariate testing. strong understanding of customer journey mapping. excellent communication and presentation skills. ability to work in a fast-paced, dynamic environment. when you join our team: we’ll empower you to learn and grow the career you want. we’ll recognize and support you in a flexible environment where well-being and inclusion are more than just words. as part of our global team, we’ll support you in shaping the future you want to see. about manulife and john hancock manulife financial corporation is a leading international financial services provider, helping people make their decisions easier and lives better. to learn more about us, visit https://www.manulife.com/en/about/our-story.html. manulife is an equal opportunity employer at manulife/john hancock, we embrace our diversity. we strive to attract, develop and retain a workforce that is as diverse as the customers we serve and to foster an inclusive work environment that embraces the strength of cultures and individuals. we are committed to fair recruitment, retention, advancement and compensation, and we administer all of our practices and programs without discrimination on the basis of race, ancestry, place of origin, colour, ethnic origin, citizenship, religion or religious beliefs, creed, sex (including pregnancy and pregnancy-related conditions), sexual orientation, genetic characteristics, veteran status, gender identity, gender expression, age, marital status, family status, disability, or any other ground protected by applicable law. it is our priority to remove barriers to provide equal access to employment. a human resources representative will work with applicants who request a reasonable accommodation during the application process. all information shared during the accommodation request process will be stored and used in a manner that is consistent with applicable laws and manulife/john hancock policies. to request a reasonable accommodation in the application process, contact recruitment@manulife.com. referenced salary location toronto, ontario working arrangement hybrid salary range is expected to be between $83,400.00 cad - $133,400.00 cad if you are applying for this role outside of the primary location, please contact recruitment@manulife.com for the salary range for your location. the actual salary will vary depending on local market conditions, geography and relevant job-related factors such as knowledge, skills, qualifications, experience, and education/training. employees also have the opportunity to participate in incentive programs and earn incentive compensation tied to business and individual performance. manulife offers eligible employees a wide array of customizable benefits, including health, dental, mental health, vision, short- and long-term disability, life and ad&d insurance coverage, adoption/surrogacy and wellness benefits, and employee/family assistance plans. we also offer eligible employees various retirement savings plans (including pension and a global share ownership plan with employer matching contributions) and financial education and counseling resources. our generous paid time off program in canada includes holidays, vacation, personal, and sick days, and we offer the full range of statutory leaves of absence. if you are applying for this role in the u.s., please contact recruitment@manulife.com for more information about u.s.-specific paid time off provisions. we're manulife. and we’re on a mission to make decisions easier and lives better. better is what drives us. it’s what inspires us to find new ways to support customers and colleagues in living longer and healthier lives. it’s the reason we’re dedicated to investing in digital innovation and accelerating a sustainable and economically inclusive future. joining us means you’ll be empowered to learn and grow your career. we’ll recognize and support you in a flexible environment where well-being and inclusion are more than just words. and as part of our global team, you’ll help shape the future you want to see – and discover that better can take you anywhere you want to go. we’re proud of our accomplishments and recognitions. recent awards include: 2024 gallup exceptional workplace award winner manulife named one of forbes world’s best employers 2023 best companies to work for in asia 2023 we’ve been recognized as one of canada’s top 100 employers (2024) manulife included in bloomberg’s 2023 gender-equality index to receive our latest job opportunities directly to your inbox, create an account or sign in and navigate to the ‘job alerts’ section located in the top right corner of the page. from there, you can sign up to receive job alerts. our ambition is to be the most digital, customer-centric global company in our industry. learn more at https://www.manulife.com/.",canada,Data Analyst,"['a/b testing', 'aws', 'data analysis', 'excel', 'machine learning', 'power bi', 'python', 'r', 'sql', 'tableau']","['a/b testing', 'aws', 'data analysis', 'excel', 'machine learning', 'power bi', 'python', 'r', 'sql', 'tableau']",
digital data analyst - contract (hybrid role),architech,"hourly rate: cad $60- 65 6-month hybrid contract, potential for extension. join us in building the future at architech, we don’t just ship software. we partner with north america’s leading brands to modernize legacy platforms, embed ai into real operations, and launch digital products that transform business outcomes. our engineers and designers harness cloud-native tools, autonomous agents, data-driven insights, and genai to drive measurable impact - replatforming systems on azure or gcp, optimizing customer journeys, or accelerating ai adoption across the enterprise. you’ll work at the intersection of strategy and execution, solving complex problems alongside smart, curious teammates across canada and poland. backed by 20+ years of experience, a drive for excellence, and a culture rooted in growth and collaboration, this is where you thrive if you’re looking to deliver meaningful, high-stakes software solutions. we’re building a more inclusive tech industry we believe diversity leads to better outcomes. nearly half of our team was born outside of canada, and we speak 19+ languages. we’re 31% women, 57% bipoc, and 14% lgbtqia+. we’ve doubled the number of women in tech roles in the past year, and maintain a 0% gender pay gap across our delivery and technology teams. inclusion here isn’t a buzzword, it’s backed by data, policy, and accountability. how we work together we’re a close-knit, collaborative group who care about doing excellent work, and doing it with integrity. our values shape how we show up every day: • think big – dream it, plan it, ship it • be open & collaborate – diverse minds build better solutions • never fail a client – own the outcome • grow our people – feedback, learning, leadership • do the right thing – even when it’s hard • embrace change – adapt fast, stay curious our people say it best: “employees of different backgrounds interact well within our company” - and 97% agree. another 96% say “architech respects individuals and values their differences.” we support continuous growth with learning budgets, internal bootcamps, certification bonuses, summit days, and more. it’s not just work - it’s a place to grow, lead, and build things that matter. and this is just the beginning. position overview: this position will play a key role in driving data-informed decision-making across our digital platforms. this role is focused on leveraging digital data to analyze user behavior, optimize digital experiences and buyflows, and provide actionable insights that align with business objectives to improve the digital user experience and reduce areas of friction. the ideal candidate is a data enthusiast who excels in uncovering trends, identifying optimization opportunities, and crafting insights that guide personalization, content strategy, and performance improvements across web and mobile digital applications. what you will do: data analysis & insights: • analyze digital user behavior across platforms, including websites, mobile apps, and internal applications, using tools such as adobe analytics, google analytics, dynatrace, quantum metric and other data platforms. • provide actionable insights by identifying trends, patterns, and areas of opportunity to enhance the digital experience and improve kpis, such as engagement, conversion, retention, and satisfaction. • develop and maintain dashboards and reports to track key performance metrics, delivering clear, concise insights to stakeholders at all levels. cross-functional collaboration: • work closely with product and development teams to ensure data insights are integrated into decision-making processes and digital product strategies. • support the personalization team with data segmentation and user behavior analysis to drive targeted and tailored digital experiences. • partner with customer service and support teams to analyze and enhance digital service journeys, focusing on customer satisfaction and efficiency. data governance & management: • ensure the accuracy and integrity of data collected through tagging and tracking, working closely with developers to implement and maintain analytics setups. • maintain compliance with data privacy regulations, ensuring analytics practices align with organizational and legal standards. • proactively troubleshoot data issues, ensuring consistency and reliability across reports and dashboards. innovation & trends: • stay updated on industry trends, tools, and best practices in digital analytics, applying new techniques to enhance data analysis and insights. • experiment with emerging analytics tools and methodologies, such as predictive analytics, to identify potential opportunities for innovation. qualifications: • education: bachelor’s degree in data science, statistics, marketing, or a related field (master’s degree preferred). • experience: 3+ years of experience in digital data analytics, with a proven ability to analyze and report on web and mobile app performance. • technical skills: proficiency in analytics tools such as google analytics, adobe analytics, dynatrace, quantum metric and firebase. familiarity with data visualization tools like tableau, power bi, or looker is a plus. • data tracking & implementation: strong understanding of tracking/tagging implementation using google tag manager, adobe launch, or similar platforms. • programming & querying skills: knowledge of sql and python, particularly for data manipulation, querying, and advanced analytics use cases. • retail & telecommunications expertise: experience in the retail and telecommunications industry is an asset, with an understanding of ecommerce and service journey analytics considered a bonus. • analytical thinking: strong quantitative and analytical skills, with the ability to synthesize data into actionable insights. • collaboration skills: excellent communication and interpersonal skills, with a proven ability to present complex data to non-technical audiences. • project management: experience managing multiple projects and deadlines in a fast-paced environment. • ai & predictive analytics: familiarity with ai-driven tools and predictive analytics is an advantage. architech is an equal opportunity employer committed to diversity. should you require any accommodations prior to or during the interview process, please indicate this during the interview process. we strongly encourage applications from racialized people, people with disabilities, people from gender and sexually diverse communities and/or people with intersectional identities.",canada,Data Analyst,"['azure', 'cloud', 'dashboard', 'data analysis', 'data analytics', 'excel', 'gcp', 'looker', 'power bi', 'python', 'r', 'sql', 'statistics', 'tableau']","['azure', 'cloud', 'dashboard', 'data analysis', 'data analytics', 'excel', 'gcp', 'looker', 'power bi', 'python', 'r', 'sql', 'statistics', 'tableau']",
master data management business analyst,snaphunt,"the role : the master data management business analyst will play a critical role in managing and optimizing the organization's master data. this position involves collaborating with various departments to ensure data accuracy and consistency across systems. • analyze and document business requirements related to master data management. • develop and maintain data governance policies and procedures. • collaborate with it and business teams to implement data management solutions. • monitor data quality and integrity across systems. • provide training and support to users on data management practices. team structure: you will work closely with data stewards, it professionals, and business stakeholders to ensure effective data management. ideal profile : the ideal candidate will possess a blend of technical and analytical skills, along with a strong understanding of data management principles. skills: • mdm, business analysis, p&c insurance domain exp is must. • core competencies: data analysis & sql, mdm project exposure, agile project delivery: active contributor in agile teams, participating in sprint planning, daily stand-ups, and retrospectives. • familiar with scrum and kanban methodologies. jira & user story creation, sql server, oracle, snowflake informatica mdm, talend, or other mdm platforms jira, confluence excel, power bi, tableau (for data visualization) what's on offer : this contract position offers competitive compensation and the opportunity to work in a dynamic environment. • growth opportunities within the organization. • supportive company culture focused on innovation. • flexible remote work options. • access to learning and development resources.",canada,Data Analyst,"['data analysis', 'excel', 'power bi', 'r', 'snowflake', 'sql', 'sql server', 'tableau']","['data analysis', 'excel', 'power bi', 'r', 'snowflake', 'sql', 'sql server', 'tableau']",
data analyst - canada,enbridge,"· looking for experienced data analysts · contract opportunities with a high possibility of extension based on performance and other factors. · multiple locations - canada at enbridge, our goal is to be the first-choice energy delivery company in north america and beyond—for customers, communities, investors, regulators and policymakers, and employees. to meet that goal, enbridge is partnering with raise—a leading recruitment firm that specializes in it, technical, and engineering staffing. together, raise and enbridge are building teams that are rising to meet the growing energy needs of north america. as a result, the enbridge contingent talent community is starting to pool candidates for future openings - we’re currently accepting applications in advance of several upcoming openings. by applying now, you’ll ensure you are the first in line for opportunities as they come up. if you want to join a successful and growing company that is always creating new opportunities, this is your chance. enbridge provides a wide array of jobs within a data analysts portfolio. if you are an experienced data analyst, please apply now so that we can contact you as soon as an opportunity aligned with your skills/experience arises. responsibilities: • the data analyst serves as technical expert on assigned area's applications, including code, interfaces and data flows • responsible for development and maintenance of application programs & interfaces; coordinates with project teams and business analysts to translate detailed specifications into new or enhanced application solutions • demonstrates command of multiple programming languages, basic systems analysis techniques, testing, debugging, documentation standards, file design, storage, and interfacing • maintains peer relationships across it areas (infrastructure, operations, coe, etc.) to support effective implementations qualifications: • full system development life cycle hands-on experience: analysis, designing, coding, testing, performance tuning and documentation • experience with process quality methodology • degree in information systems, computer science or related technical discipline or equivalent - 4-7 years of it program development experience or equivalent",canada,Data Analyst,['r'],['r'],
finance data analyst – financial planning & analysis,alberta blue cross,"alberta blue cross® is an alberta based organization dedicated to delivering exceptional customer experience and community leadership. we’re committed to providing the best health coverage to over 1.8 million members and take an active role in promoting wellness. we believe in what we do—and place trust in our employees to deliver our vision. working at alberta blue cross® means having a career where you’ll be recognized for your contributions. we value diversity, encourage our team members to maintain a healthy work-life balance and provide opportunities for career growth. overview: alberta blue cross® has an opening for a finance data analyst to join our team. reporting to the manager, finance intelligence and analytics, the data analyst will be quality focused, passionate, a quick learner, self-motivated, analytical and detail oriented. they will ensure a high caliber of data quality and will support and collaborate with the other data teams in the organization. in this flexible work style arrangement, you will be able to work from home and go into office when required. what you will do: • collect and extract financial and operational data from internal systems and external sources. • clean, validate, and prepare datasets to ensure accuracy and reliability for analysis. • develop financial reports and analytics to support business and finance decision-making. • interpret data trends and explain insights, variances, and findings to the finance team. • participate in finance data projects, automation, and system enhancements. • collaborate with enterprise it and analytics teams to create and maintain data pipelines and data governance for finance. • support the maintenance of financial data models, metrics, and reporting tools. • ensure adherence to data quality standards and contribute to continuous improvement. what you will have: • bachelor’s degree in finance, data analytics, economics, statistics, or a related field. • 3 years’ experience in a similar role in a finance department. • strong foundation in quantitative analysis, data management, and financial concepts. • analytically minded with hands on skills in advanced excel, sql, python, power bi and the ability to work with large and diverse sets of data. experience with other power platform tools is an asset. • experience in data extraction. • demonstrated ability to deal with change and excel in high-stress situations. • ability to manage multiple responsibilities with varying priority to deliver timely results. • commitment to a highly collaborative team environment including cross-training. this position will remain open until a suitable candidate is selected. alberta blue cross® is an inclusive employer committed to a workplace that reflects the diversity of the communities we serve. we empower and are advocates for our team members by welcoming, respecting and valuing their unique perspectives, backgrounds, and experiences. we offer the opportunity to work in an innovative, high-energy team-focused environment. if you have the qualifications we are looking for, apply online at careers.ab.bluecross.ca",canada,Data Analyst,"['data analytics', 'data pipeline', 'excel', 'power bi', 'python', 'r', 'sql', 'statistics']","['data analytics', 'data pipeline', 'excel', 'power bi', 'python', 'r', 'sql', 'statistics']",
sql developer/data analyst,vdart inc,"title: sql developer/data analyst location: halifax, ca (remote) type: contract job description • we need a decision analyst (contractor) to support analysis for a real-time decisioning platform. deliverables include weekly insights, dashboards, documentation, and data investigations. • 8+ years of experience in sql, data analysis, and interpreting rule/model outputs. • 5+ years of experience in producing reports, dashboards, a/b test support, and documentation. • 5+ years of experience in using of genai tools (cursor, windsurf, copilot) is required",canada,Data Analyst,"['dashboard', 'data analysis', 'r', 'sql']","['dashboard', 'data analysis', 'r', 'sql']",$38–$40 an hour
data analyst – gaming analytics & ml,zynga,"a leading gaming company is seeking a data analyst for its words with friends team. this entry-level role involves analyzing player behaviors, designing etl pipelines, and enhancing gaming metrics through data-driven insights. candidates should have a strong background in data science, familiarity with sql and python, and a passion for gaming. the position offers opportunities for growth and the chance to work in a dynamic entertainment environment. #j-18808-ljbffr",canada,Data Analyst,"['etl', 'python', 'r', 'sql']","['etl', 'python', 'r', 'sql']",
online data analyst - remote in canada (english speakers),telus digital ai data solutions,"this a full remote job, the offer is available from: canada are you a detail-oriented individual with a passion for research and a good understanding of national and local geography? this freelance opportunity allows you to work at your own pace and from the comfort of your own home. a day in the life of an online data analyst: • in this role, you will be working on a project aimed at enhancing the content and quality of digital maps that are used by millions of people worldwide • completing research and evaluation tasks in a web-based environment such as verifying and comparing data, and determining the relevance and accuracy of information. join us today and be part of a dynamic and innovative team that is making a difference in the world! telus digital ai community our global ai community is a vibrant network of 1 million+ contributors from diverse backgrounds who help our customers collect, enhance, train, translate, and localize content to build better ai models. become part of our growing community and make an impact supporting the machine learning models of some of the world’s largest brands. qualification path no previous professional experience is required to apply to this role; however, working on this project will require you to pass the basic requirements and go through a standard assessment process. this is a part-time long-term project and your work will be subject to our standard quality assurance checks during the term of this agreement. basic requirements • full professional proficiency in the english language • being a resident in canada for the last 2 consecutive years and having familiarity with current and historical business, media, sport, news, social media, and cultural affairs in canada. • ability to follow guidelines and conduct online research using search engines, online maps, and website information • flexibility to work across a diverse set of task types, including maps, news, audio tasks, and relevance • daily access to a broadband internet connection, computer, and relevant software assessment in order to be hired into the program, you’ll take an open book qualification exam that will determine your suitability for the position and complete id verification. our team will provide you with guidelines and learning materials before your qualification exam. you will be required to complete the exam in a specific timeframe but at your convenience. equal opportunity all qualified applicants will receive consideration for a contractual relationship without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or protected veteran status. at telus digital ai, we are proud to offer equal opportunities and are committed to creating a diverse and inclusive community. all aspects of selection are based on applicants’ qualifications, merits, competence, and performance without regard to any characteristic related to diversity. this offer from ""telus digital ai data solutions"" has been enriched by jobgether.com and got a 77% flex score.",canada,Data Analyst,"['machine learning', 'r']","['machine learning', 'r']",
senior data analyst/ corporate research /telework/hybrid/,cbc/radio-canada,"position title: senior data analyst, corporate research (telework/hybrid) status of employment: contractee long-term (fixed term) position language requirement: language skills: work at cbc/radio-canada at cbc/radio-canada, we create content that informs, entertains and connects canadians on multiple platforms. our successes and accomplishments are driven by embodying and upholding values, which include creativity, integrity, inclusiveness and relevance. do you think you have the ability and drive to keep up with this exciting, ever-changing industry? whether it be in front of the camera, on air, online or behind the scenes, you would be joining a team that thrives on making connections and telling stories that are important to canadians. unposting date: 2025-12-17 11:59 pm this role is a hybrid work arrangement. work schedule to be discussed with the hiring manager according to the guidelines defined by the department. primary location: ottawa or toronto contract (2 years). your role join the corporate research group and serve as a crucial contributor integrating data engineering, advanced analytics, and strategic insight generation. this role directly supports cbc/radio-canada’s strategic initiatives, specifically by designing and delivering high-quality data and analytics. you will be expected to lead data projects, provide robust solutions to other teams, and deliver clear, actionable business narratives. reporting to: manager, data science & industry analysis, corporate research. responsibilities: • own and manage data projects from conception to delivery, ensuring successful execution, documentation, and clear communication with internal and cross-functional stakeholders, including senior leadership. • lead the end-to-end development, validation, and maintenance of robust data pipelines, aggregated data tables, and internal data products (e.g., self-service dashboards and datamarts). • serve as an analytics partner for internal teams. collaborate with data operations teams to identify requirements for audience data infrastructure and processes to maximize 360-degree insights on audiences. • evaluate third-party data vendors and new technical solutions. support with vendor relationship management to enable centralized data acquisition and sharing. • explore opportunities for side projects focused on new innovations or iterative process improvements for the benefit of the corporation. technical & analytical skills: • proven ability to programmatically solve data problems using languages we prioritize (sql and python within environments like databricks). experience with other statistical or programming languages is valuable, but a strong foundation in table-based database concepts is critical. • demonstrated experience with data infrastructure management, automation of pipelines, and designing/building data products. • experience creating intuitive dashboards and reports using powerbi (preferred) or similar platforms (tableau, looker studio). must possess strong storytelling skills to transform data into digestible insights. • ability to apply a thoughtful, structured approach to understanding, interpreting, evaluating, and presenting complex data and business cases. • familiarity with generative ai tools and concepts (including prompt engineering) to support workflow optimization, code generation, and accelerate data analysis and research efforts. • proficiency with productivity suites, including ms excel, ms powerpoint, and google suite (sheets and slides). familiarity with project management tools (jira) and documentation tools (confluence) is a bonus. we are looking for a candidate with the following: • bilingualism (french and english) is preferred due to the need to work with french and english teams across the organization. • university degree from a recognized university with a specialization in data science, computer science, statistics, engineering, business, or a closely related program. • minimum of 2 years of professional experience in a hands-on analytics, data science, software engineering, or business intelligence role, with experience managing projects in a data environment. • excellent communication skills (oral and written) with the comfort and ability to represent the team effectively to senior leaders and technical experts. • demonstrated ability to organize, drive, and take ownership of projects from conception through to delivery, operating with a high degree of self-management and entrepreneurial initiative. • a strong willingness to collaborate and actively work with colleagues within our team and across the corporation. must possess a high degree of openness to discuss challenges, issues, and mistakes, fostering a culture of trust and continuous improvement. • commitment to understanding and anticipating stakeholder needs, providing excellent experiences, and building strong working relationships with internal clients and cross-functional partners. • ability to think systemically and solve problems independently. • high attention to detail. • enthusiasm and a positive attitude candidates may be subject to skills and knowledge testing. we thank all applicants for their interest, but only candidates selected for an interview will be contacted. as part of our recruitment process, candidates who advance to the next step will be asked to complete a background check. this includes: • a mandatory criminal record check. • other background checks may be conducted based on the operational requirements of the position. cbc/radio-canada is committed to being a leader in reflecting our country’s diversity. that’s because we can only create and tell the stories that connect canadians, by having a workforce that mirrors the ever-changing makeup of our country. that’s why we, as an employer, value equal opportunity and nurture an inclusive workplace where our individual differences are not only recognized and valued, but also extend to and pervade all the services we provide as canada’s public broadcaster. for more information, visit the diversity and inclusion section of our website. if you have accommodation needs at this stage of the recruitment process, please inform us as soon as possible by sending an e-mail to recruitment@cbc.ca. you are invited to consult and familiarize yourself with our code of conduct, which can be found on our corporate website. all employees must adhere to the code as a condition of employment. we also invite you to take a look at our policy on conflicts of interest. in the event that you become an employee, it will be important to inform us, as quickly as possible, of any situation that, because of your hiring, constitutes or could appear to constitute a conflict of interest. primary location: 181 queen street, ottawa, ontario, k1p 1k9 number of openings: 1 work schedule: full time original job senior data analyst/ corporate research /telework/hybrid/ posted on grabjobs ©. to flag any issues with this job please use the report job button on grabjobs.",canada,Data Analyst,"['business intelligence', 'dashboard', 'data analysis', 'data pipeline', 'databricks', 'excel', 'looker', 'python', 'r', 'sql', 'statistics', 'tableau']","['business intelligence', 'dashboard', 'data analysis', 'data pipeline', 'databricks', 'excel', 'looker', 'python', 'r', 'sql', 'statistics', 'tableau']",
data analyst for high tech luxury venture backed startup,freelancejobs,"we're looking for a sharp, resourceful data analyst who can turn messy data into clean insights and crisp stories. you should be equally comfortable writing complex sql, doing light data engineering, building looker dashboards, and translating findings into recommendations the business can act on. what you'll do write efficient sql queries across large datasets (bigquery preferred). build and maintain looker dashboards + looks with clean modeling (lookml a plus). do light data engineering: create clean tables/views, manage pipelines, optimize schemas. use python for analysis, automation, data cleaning, and small etl scripts. break down data into simple, visual narratives that drive decisions. proactively identify trends, opportunities, and issues across product, growth, and finance. deliver weekly insights summaries with clear ""so what?"" takeaways. partner with me (cfo) to improve reporting, forecasting inputs, and kpi frameworks. the ideal fit 3–6 years in analytics, bi, or data engineering (startup experience is a plus). advanced sql (ctes, window functions, optimization). hands-on looker development (lookml modeling is a bonus). python experience for data manipulation + light scripting. strong communicator who can simplify complex analyses into crisp insights. bias toward action, can work quickly, iterate, and self-manage. comfortable working with ambiguity and building structure from scratch. tools we use bigquery / sql looker python airflow/zapier (nice to have) sheets / excel contract duration of less than 1 month. with 30 hours per week. mandatory skills: microsoft power bi, data visualization, sql, microsoft excel, business intelligence, power query, microsoft power bi data visualization, looker studio, tableau, analytics dashboard, microsoft power bi development, data analysis, report, interactive data visualization, microsoft power automate",canada,Data Analyst,"['airflow', 'bigquery', 'business intelligence', 'dashboard', 'data analysis', 'etl', 'excel', 'looker', 'power bi', 'python', 'r', 'recommendation', 'sql', 'tableau']","['airflow', 'bigquery', 'business intelligence', 'dashboard', 'data analysis', 'etl', 'excel', 'looker', 'power bi', 'python', 'r', 'recommendation', 'sql', 'tableau']",
senior data analyst: finance & payments (remote),jane app,"a remote-first technology company is hiring a sr. data analyst to support finance and payments teams. this role involves complex sql/dbt modeling and creating dashboards for financial insights. candidates should have 5+ years in data analysis, strong sql and looker skills, and excellent communication abilities. the annual salary range is $100,000 to $156,300, with new hires typically starting at $118,800. a commitment to an inclusive workplace is essential. #j-18808-ljbffr",canada,Data Analyst,"['dashboard', 'data analysis', 'dbt', 'excel', 'looker', 'r', 'sql']","['dashboard', 'data analysis', 'dbt', 'excel', 'looker', 'r', 'sql']",
"manager, data analyst, cfo group",0000050007 royal bank of canada,"job description what is the opportunity? this is an exciting opportunity in cfo group where you will play an instrumental role in leveraging rbc’s strategic data assets to build automation solutions and data analytics models. we are currently seeking for an individual with strong data/business analysis background who is quick to adapt to new technologies and enthusiastic about developing innovative solutions to support the digital transformation in finance. this role will work with various stakeholders in helping to identify business problems and prioritize them to unlock business value. the data enablement team is focused on developing strategic solutions to support key report to shareholder and regulatory reporting initiatives across the bank. our team consists of highly accomplished professionals with various backgrounds in computer science, data engineering, business analytics, accounting and finance to deliver innovative solutions that supports data-driven decisions. we are hiring multiple manager roles with areas of focus including: capital markets business and products international banking and insurance (u.s. and caribbean) - corporate support business and products wealth management business and products what will you do? data analysis work closely with subject matter experts to document detailed business requirement, and ensures requirements are in line with business goals and objectives analyze large data sets and system extracts to build business rules behind automated data pipelines using sql to understand data sets about different systems and extract data identify, analyze implications and communicate program issues, risks and dependencies, so that they can be appropriately mitigated work collaboratively with cross-functional teams and develop and manage relationship with relevant stakeholders manage, communicate and author changes to the requirements solution design design, develop and test reporting solutions and reconciliation processes using python develop and execute tests, log and track issues and drive them to resolution with verification of bug fixes, document test results and research findings develop scripts as needed to assist in the systems validation process integrate with existing data storage solutions coordinate with front-end developers and data engineers what do you need to succeed? must-have demonstrated experience with data extraction, transformation, and loading from a wide variety of data sources using sql and related technologies (oracle, vertica, hive, etc). working knowledge in implementing and performance tuning etl pipelines using dagster, apache airflow or prefect. proficiency in python, specifically pandas, sqlalchemy, numpy and selenium packages. excellent analytical skills & critical thinking skills to investigate system and business issues, determine root cause, and deliver sustainable solutions. commitment to continuous improvement; specifically, ability to identify existing processes and gaps across the current system landscape. nice to have experience in rbc it systems and service platforms (e.g. core, confluence, servicenow, access manager). open to learn and innovate using tech stack or framework like flask, django and any front-end framework. experience in managing windows servers using iis. proficiency in excel vba. experience with business intelligence reporting & visualizations using tableau. basic knowledge of git, apis, docker and/or devops. what’s in it for you? it provides the opportunity to be a part of a high performing experienced team with progressive thinking to deliver trusted advice to help our clients thrive and communities prosper. opportunities to do challenging work utilizing latest technologies opportunities to see measurable results in relatively short term opportunities to take on progressively greater accountabilities opportunities to building close relationships with counterparts across the bank job skills artificial intelligence (ai), big data management, data mining, data science, decision making, machine learning (ml), natural language processing (nlp), predictive analytics, python (programming language), statistical analysis additional job details address: royal bank plaza, 200 bay st:toronto city: toronto country: canada work hours/week: 37.5 employment type: full time platform: office of the cfo job type: regular pay type: salaried posted date: 2025-12-11 application deadline: 2026-01-30 note: applications will be accepted until 11:59 pm on the day prior to the application deadline date above inclusion and equal opportunity employment at rbc, we believe an inclusive workplace that has diverse perspectives is core to our continued growth as one of the largest and most successful banks in the world. maintaining a workplace where our employees feel supported to perform at their best, effectively collaborate, drive innovation, and grow professionally helps to bring our purpose to life and create value for our clients and communities. rbc strives to deliver this through policies and programs intended to foster a workplace based on respect, belonging and opportunity for all. join our talent community stay in-the-know about great career opportunities at rbc. sign up and get customized info on our latest jobs, career tips and recruitment events that matter to you. expand your limits and create a new future together at rbc. find out how we use our passion and drive to enhance the well-being of our clients and communities at jobs.rbc.com. royal bank of canada is a global financial institution with a purpose-driven, principles-led approach to delivering leading performance. our success comes from the 84,000+ employees who bring our vision, values and strategy to life so we can help our clients thrive and communities prosper. as canada’s biggest bank, and one of the largest in the world based on market capitalization, we have a diversified business model with a focus on innovation and providing exceptional experiences to more than 16 million clients in canada, the u.s. and 34 other countries. learn more at rbc.com.‎ we are proud to support a broad range of community initiatives through donations, community investments and employee volunteer activities. see how at rbc.com/community-social-impact.",canada,Data Analyst,"['airflow', 'business intelligence', 'data analysis', 'data analytics', 'data pipeline', 'etl', 'excel', 'machine learning', 'natural language processing', 'nlp', 'numpy', 'pandas', 'python', 'r', 'sql', 'tableau']","['airflow', 'business intelligence', 'data analysis', 'data analytics', 'data pipeline', 'etl', 'excel', 'machine learning', 'natural language processing', 'nlp', 'numpy', 'pandas', 'python', 'r', 'sql', 'tableau']",
"data analyst intern, risk & fraud (8 months)",super.com,"**about super.com** we started super.com to help maximize lives–both the lives of our customers and the lives of our employees– so that everyone can experience all that life has to offer. for our employees, our promise is that super.com is more than just a job; it’s an opportunity to unlock one’s potential, where learning is celebrated and impact is realized. we are more than a fast-paced, high-growth tech company; we care about our people and take career progression seriously. this is your career and our aim is to supercharge it through the people, the work, and the programs that fuel who we are. • *about the role** we are looking for a data analytics intern, risk & fraud to join our growing data team for the winter 2026 term. as an analytics intern at super, you will be responsible for using data to answer strategic questions and support our business strategy and rapid growth. with a strong analytical and data-driven approach you’ll help our team organize, visualize and understand large data sets relating to various performance marketing channels. this is an exciting opportunity that will allow you to run deep-dive analysis, create regression or segmentation models, build dashboards and generate reporting to drive business growth and draw strategic insights from product, user, and operational data. • *challenges you'll solve** - data-driven culture: you will be an ambassador for our data-driven culture, taking data and using it to answer business questions and drive strategy & decision making - data management: you will access, clean and organize raw data in a way that makes sense for you, your team and your stakeholders - visualizations: you will help us to see and understand our data in a way that makes sense, creating automated dashboards and funnels for various internal teams - hard questions: you’ll be taking our data and using it to answer business questions, drive strategy and decision making. this will include running experiments and impact analysis to understand the effects of changes, drawing conclusions from data, and tying insights back to key business metrics or kpis. you’ll also explore user behaviour patterns to make informed recommendations that improve product performance and reduce risk • *about you** - you can pull, clean and manipulate large sets of complex data to turn them into stories that business stakeholders can understand - you can write sql queries and perform data analysis with python, and know your way around analytics tools (looker, amplitude dbt and snowflake) - you are naturally curious and have a solid understanding of common statistical methods and how they relate to real world applications - you are able to work independently while also embracing mentorship and guidance, confidently addressing stakeholder requests and contributing valuable insights - you take ownership of your work, leveraging the support provided to solve problems and deliver impactful results - you are comfortable navigating ambiguous or changing priorities and can adjust your approach to meet new challenges - you understand how data insights connect to broader business goals and are eager to learn how to make strategic contributions • *we've got you covered** - compensation: we pay our interns top-of-market - $300 one-time home office set up allowance - $25/week ubereats allowance on fridays - $300/term learning and development allowance - $120/ term fitness/wellness allowance - top talent: work with the best in the world, including engineers and leadership from google, facebook, wish, youtube, uber, and more - build something great: most importantly, build a product used by millions around the world - have ownership, have impact, and do great work we believe in equal opportunity we are an equal opportunity employer and value diversity at our company. we do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. accommodations are available on request for candidates taking part in all aspects of the selection process. if needed, please notify our talent acquisition partner. we believe in equal opportunity we are an equal opportunity employer and value diversity at our company. we do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. accommodations are available on request for candidates taking part in all aspects of the selection process. if needed, please notify our talent acquisition partner. compensation for this internship will be determined based on the number of prior work terms or internships you have successfully completed, in alignment with our intern pay structure.",canada,Data Analyst,"['dashboard', 'data analysis', 'data analytics', 'dbt', 'looker', 'python', 'r', 'recommendation', 'regression', 'snowflake', 'sql']","['dashboard', 'data analysis', 'data analytics', 'dbt', 'looker', 'python', 'r', 'recommendation', 'regression', 'snowflake', 'sql']",
"data analyst, medical transportation (internal only)",first nations health authority,"fnha overview the first nations health authority is a diverse and transformational health organization of professional, innovative, and dedicated team members and leaders. the first of its kind in canada, fnha works as a health-and-wellness partner with bc first nations to support self-determination and decision-making to improve health outcomes. fnha is proud to have been named one of bc’s top employers for 2025. learn more about why we were selected here. title: data analyst, medical transportation (internal only) salary: $66,597 - $83,246 - $91,571 per year (pay range explained below) contract: temporary – until march 31, 2026 hours: 37.5 hours (mon-fri core hours 9am-3pm) location: alberni office - located on the traditional, ancestral, and unceded territory of the coast salish peoples, including the territories of the musqueam, squamish, and tsleil-waututh nations - hybrid (hybrid working is an option in line with fnha’s flex work policy). about the role as the data analyst, medical transportation, you will support data entry, analysis and reconciliation of medical transportation reporting to reconcile data quality and compliance issues and create reports. you will review data quality concerns and coordinate with fnha communities and external partners to resolve discrepancies. as well as you will conduct analysis of health benefits medical transportation data and provide meaningful insights. additionally, you will collaborate with team members to develop standard operating procedures and process documentation. in addition to, you will evaluate medical transportation reporting against program policies to ensure that communities are adhering to travel guidelines. about you education • bachelor’s degree in a relevant field of study, or an equivalent combination of education, training and experience experience • 3-5 years of significant, recent and relevant experience in analytical work, preferably in healthcare • experience working with microsoft office productivity software, with a focus on excel • experience in analytical work involving financial and operational reporting data, preferably in healthcare • experience with excel-based tools such as sql, macros and/or visual basic is an asset • experience developing standard operating procedures and process documentation is an asset knowledge • knowledge of health related programs and services within first nations communities is an asset • knowledge of the fnha health benefits program is an asset please refer to job specification section of the job description for more details. working at fnha as a member of the team, you will have access to a wide range of employee benefits, including: • municipal pension plan with an employer contribution of 9.31% • 3 weeks’ vacation, plus 5 personal days, 1 volunteer day, and 14 public holidays including national indigenous people’s day, national day of truth and reconciliation, easter monday and boxing day • access to group health benefits and a health care spending account through canada life on day 1 of employment • and more! the first nations health authority is committed to respecting diversity within our workforce; preference will be given to individuals who identify as first nations, inuit, or métis. leading with culture, all fnha employees complete san’yas indigenous cultural safety training. we support our family to be leaders in wellness and all staff members develop a yearly wellness plan as part of their performance partnership and goal-setting. the pay range is the minimum and maximum annual salary based on full time equivalent hours. incumbents are typically hired, transferred or promoted between the minimum and midpoint of the range based on their knowledge, skills, abilities and experience in relation to the role requirements. the top 10% of the pay range is for the incumbents who are industry experts in the job with the combination of exceptional experience and competencies needed to perform all duties and responsibilities at a superior capability level. internal employee compensation will be determined as per the fnha employee total compensation policy. how to apply if this sound likes the opportunity for you, please click on the 'apply for job' button. closing date: december 21, 2025 (at 23:59) resources for more information about us, please visit: www.fnha.ca/about/work-with-us for information about https://www.fnha.ca/what-we-do/cultural-safety-and-humility and for information about land acknowledgements if you have any inquiries or issues please contact fnha recruitment at careers@fnha.ca follow us at: facebook twitter linkedin instagram",canada,Data Analyst,"['excel', 'r', 'sql']","['excel', 'r', 'sql']","$66,597–$91,571 a year"
analytics team lead,exchange solutions,"the technical lead, advanced analytics is responsible for delivering analytics and insights to support the ongoing enhancement and performance of a large loyalty program operated by a pharmacy retailer, as well as supporting initiatives related to the company’s saas products. this role reports to the senior manager, analytics the ideal candidate will have hands-on experience with python for data manipulation and analysis, proficiency in working with large datasets, and a strong understanding of database structures (sql, snowflake). expertise in advanced analytics techniques such as predictive modeling (using machine learning) and segmentation, as well as business case development, test design and campaign analysis (including effectiveness and roi), is essential. in addition, the candidate should have experience designing reports and other business intelligence tools, along with the ability to support and mentor team members in methodology design and review for deep-dive analytics projects. experience with deploying machine learning models into production (automated training and scoring) would be desired. this role presents a unique opportunity to join a dynamic team of analytical professionals who partner with both business and technology teams to design innovative, value-adding customer engagement strategic solutions for clients. it is a great fit for motivated individuals looking to further develop their expertise in retail, customer behavior, and loyalty program analytics. primary responsibilities • develop custom analytics solutions: create and deliver custom analytics solutions to drive strategic value in understanding customer behavior and campaign performance. deep dives into the customer lifetime value drivers such as acquisition, growth, retention and reactivation with supporting campaign strategy or category analysis of customer purchase behaviour to drive up-sell, cross-sell or pricing decisions are some examples of work typical for the role. • business case development & test design: design and develop loyalty-based financial modelling for loyalty program mechanics, product pricing scenarios, campaign performance estimates., etc • campaign & customer segmentation support: assist clients in determining targeting criteria for promotional campaigns, customer segmentation, and conducting behavior change deep dives. fluent in different campaign measurement techniques such as pre/pot analysis, test and control and 4-point methods. segmentation methods include rfm analysis, clustering, decision trees, etc. • machine learning models and genai solutions: develop machine learning models and support implementation and automation into production and leverage llms where appropriate to support analysis. models will generally follow customer lifecycles looking at purchase propensity, churn, reactivation, etc. • lead & oversee analytics methodologies: design and support the execution of analytical methodologies to ensure consistency, accuracy, and innovation across all projects. conduct peer review sessions and qc the work of other analysts. • support saas product performance analysis: provide support for the company’s saas products by testing measurement methodologies, assisting in program performance reviews, and offering insights for continuous improvement. • reporting & documentation: design and develop standard and custom reports, and ensure thorough documentation is created. work closely with the bi and data team to define requirements, develop logic and validate results. • ad-hoc analysis & data requests: fulfill ad-hoc analysis and data requests from clients, including gathering requirements, designing custom queries (python, sql), and conducting data integrity checks on large datasets. • communication of insights: share findings with peers in the analytics team and externally to clients with clear visualization and impactful story telling in presentations. • process standardization & automation: contribute to process standardization, automation, and optimization leveraging genai to aid with productivity. develop comprehensive documentation and follow existing processes while identifying improvement enhancements. • team collaboration & company culture: contribute to the team’s overall operations and the company culture by embodying the organization’s values and adhering to its policies. opportunity to participate in committees, engage in peer recognition, attend social events, etc. capability requirements – education, skills & experience • graduate degree in statistics, mathematics, or other quantitative fields; supplementary education in business is a bonus • 5+ years of analytical, data mining, predictive modeling experience, preferably with some exposure to retail industry, and loyalty program performance analytics including campaign analytics, customer behaviour analysis, etc. • familiarity with saas product performance metrics is a plus. • well versed with a solid understanding and experience of the structure of relational databases • can create complex custom queries using python, sql or other data-mining tools based on multiple data sources • proven ability to design and develop financial models and business cases • can demonstrate the use of genai to aid in overall productivity and/or enhance analysis • ability to develop analytical frameworks and generate / communicate business insights • strong attention to detail, high aptitude for problem solving and a natural interest in understanding and explaining consumer behavior / business • excellent communication skills, both verbal and written and strong presentation skills",canada,Data Analyst,"['business intelligence', 'clustering', 'excel', 'machine learning', 'python', 'r', 'snowflake', 'sql', 'statistics']","['business intelligence', 'clustering', 'excel', 'machine learning', 'python', 'r', 'snowflake', 'sql', 'statistics']",
"senior data analyst: shape strategy with data (remote, stock)",medium,"a leading tech company in canada is seeking a senior data analyst to drive data-informed decision-making. the role involves analyzing complex datasets to derive insights and developing dashboards. ideal candidates have a degree in data science, 5+ years of experience, and expertise in sql and bi tools. the estimated salary range is cad 103,700 – 140,300, with comprehensive benefits including unlimited vacation and employee stock options.",canada,Data Analyst,"['bi tools', 'dashboard', 'r', 'sql']","['bi tools', 'dashboard', 'r', 'sql']",$103K–$141K a year
"data analyst (independent contractor, remote)",green brick labs,"**data analyst (contractor)** • *location:** remote (canada preferred) • *type:** contractor (10–20 hours/month, ongoing) • *about maverick games** maverick games is a fast-growing ontario-licensed igaming operator offering an online casino and sportsbook experience. we operate in a dynamic, data-rich environment where rapid insights power decisions across marketing, product, operations, and finance. you’ll join a lean team where your work will have real impact. • *role overview** we’re looking for a data analyst who will become our go-to person for deriving business intelligence across multiple domains: player activity, marketing funnels, payments/withdrawals, player retention & churn, and regulatory reporting. you’ll build dashboards, manage data pipelines, partner with cross-functional teams (marketing, product, finance, compliance) and help shape our data-driven culture. • *key responsibilities** - acquire, clean, validate and aggregate data from varied sources (gaming platforms, crm, payment providers, google analytics, internal systems) - build, maintain and improve dashboards and regular reports tracking key igaming metrics: ggr, ngr, deposit/withdrawal trends, arpu, churn, retention, promotional roi, etc. - support regulatory and compliance reporting for ontario (igo) and other jurisdictions as needed - perform ad-hoc and structured analysis: player segmentation, acquisition channel effectiveness, campaign attribution, product funnel drop-off analysis, payment/withdrawal patterns - collaborate with marketing to measure and optimize campaigns, with product/operations to understand player behaviour and with finance to support forecasting, reconciliation and financial insights - automate data flows and reporting processes where possible (etl, apis, scripts) - identify data quality issues/gaps and propose improvements to tracking, instrumentation and reporting infrastructure - present findings and recommendations clearly to stakeholders (non-technical and technical alike) • *requirements** - bachelor’s degree (data science, statistics, analytics, business or equivalent) - 2+ years of experience as a data analyst (ideally in gaming, fintech, e-commerce or related) - strong proficiency in sql and spreadsheet tools (excel/google sheets) - experience with bi/data-visualisation tools (power bi, tableau, looker studio, etc.) - ability to work with large datasets, perform cleaning/validation, draw insights and communicate them clearly - familiarity with financial / gaming metrics (e.g., ggr, ngr, arpu, churn) is a strong plus - comfortable working in a small, fast-paced environment where priorities may shift • *nice-to-have** - experience with data warehouse / big-query environments (e.g., google bigquery, snowflake) - familiarity with scripting/automation tools (python, r) - prior igaming or sportsbook experience (platform integrations, player-behaviour analytics, payments) - marketing analytics experience (utm tracking, conversion funnels, attribution modelling) - experience with compliance/regulatory analytics in gaming • *additional details** - contractor role, 10–20 hours per month (flexible schedule). - permanent ongoing engagement, we’re looking for someone who can grow with us. - **please include your hourly or monthly rate when applying.**",canada,Data Analyst,"['bigquery', 'business intelligence', 'dashboard', 'data pipeline', 'data warehouse', 'etl', 'excel', 'looker', 'power bi', 'python', 'r', 'recommendation', 'snowflake', 'sql', 'statistics', 'tableau']","['bigquery', 'business intelligence', 'dashboard', 'data pipeline', 'data warehouse', 'etl', 'excel', 'looker', 'power bi', 'python', 'r', 'recommendation', 'snowflake', 'sql', 'statistics', 'tableau']",
data analyst finance (it),abc benefits corporation,"finance data analyst - financial planning & analysis page is loaded## finance data analyst - financial planning & analysislocations: hybrid: albertatime type: full timeposted on: posted 3 days agojob requisition id: r1023alberta blue cross is an alberta based organization dedicated to delivering exceptional customer experience and community leadership. we’re committed to providing the best health coverage to over 1.8 million members and take an active role in promoting wellness. we value diversity, encourage our team members to maintain a healthy work-life balance and provide opportunities for career growth.**overview:**alberta blue cross has an opening for a finance data analyst to join our team. reporting to the manager, finance intelligence and analytics, the data analyst will be quality focused, passionate, a quick learner, self-motivated, analytical and detail oriented. they will ensure a high caliber of data quality and will support and collaborate with the other data teams in the organization. in this flexible work style arrangement, you will be able to work from home and go into office when required.**what you will do:*** collect and extract financial and operational data from internal systems and external sources.* clean, validate, and prepare datasets to ensure accuracy and reliability for analysis.* develop financial reports and analytics to support business and finance decision-making.* interpret data trends and explain insights, variances, and findings to the finance team.* participate in finance data projects, automation, and system enhancements.* collaborate with enterprise it and analytics teams to create and maintain data pipelines and data governance for finance.* support the maintenance of financial data models, metrics, and reporting tools.* ensure adherence to data quality standards and contribute to continuous improvement.**what you will have:*** bachelor’s degree in finance, data analytics, economics, statistics, or a related field.* 3 years’ experience in a similar role in a finance department.* strong foundation in quantitative analysis, data management, and financial concepts.* analytically minded with hands on skills in advanced excel, sql, python, power bi and the ability to work with large and diverse sets of data. experience with other power platform tools is an asset.* experience in data extraction.* commitment to a highly collaborative team environment including cross-training.alberta blue cross is an inclusive employer committed to a workplace that reflects the diversity of the communities we serve. if you have the qualifications we are looking for, apply online atlocations: licensed to abc benefits corporation for use in operating the alberta blue cross plan. † alberta blue cross is an alberta based not-for-profit, dedicated to delivering exceptional customer experience and community leadership. we are committed to providing the best health coverage to over 1.8 million members and take an active role in promoting the wellness of all albertans. #",canada,Data Analyst,"['data analytics', 'data pipeline', 'excel', 'power bi', 'python', 'r', 'sql', 'statistics']","['data analytics', 'data pipeline', 'excel', 'power bi', 'python', 'r', 'sql', 'statistics']",
hr data analyst,robert half,"on behalf of our client, we are seeking a highly skilled hr data analyst to support a large-scale hr transformation initiative involving data governance, system standardization, and end-to-end data migration. this role is ideal for someone who brings a strong technical foundation paired with a business-oriented mindset, and who thrives in project-driven environments. this is a hybrid contract position based in calgary, with preference for candidates who can be onsite regularly. standard work hours are monday–friday, 8 am–5 pm with some flexibility. about the role as the hr data analyst, you will play a critical role in migrating hr and timekeeping data to a new enterprise platform while ensuring compliance with global data governance standards. you will work closely with hr, it, and pmo teams to validate data, streamline processes, and support system integrations. this work is hands-on—you will be directly executing data migration activities, conducting validation, and troubleshooting issues. this project is part of a multi-year transformation and has potential to extend into 2027. key responsibilities data migration • lead the planning, execution, and validation of hr data migration activities. • create and manage detailed migration plans, milestones, and deliverables. • perform data mapping, reconciliation, quality checks, and error resolution. • execute hands-on data migration using available tools and custom-built functionality. • ensure data accuracy, consistency, and compliance throughout all stages of migration. • provide post-migration support including documentation and training. data governance & system improvements • apply and uphold global hr data governance standards and best practices. • review system changes and enhancements to ensure alignment with governance frameworks. • support integrations, particularly within successfactors and ideally fieldglass. • conduct audits, profiling, and data quality assessments across hr systems. • collaborate with cross-functional teams to resolve discrepancies and optimize processes. • document procedures, findings, and recommendations for stakeholders. required qualifications • proven experience in hr data analysis, data governance, and full-cycle data migration. • strong analytical mindset with exceptional attention to detail. • experience with hris and time systems; familiarity with successfactors strongly preferred. • proficiency with hana, power query, and advanced excel. • ability to work independently with minimal oversight; proactive and self-directed. • excellent communication and stakeholder collaboration skills. • ability to manage competing priorities in a fast-paced project environment. preferred qualifications • 5+ years of hr data governance and migration experience. • experience with successfactors employee central payroll (ecp) and/or fieldglass integration. • knowledge of global hr processes, compliance, and data privacy standards.",canada,Data Analyst,"['data analysis', 'excel', 'r', 'recommendation']","['data analysis', 'excel', 'r', 'recommendation']",
facilities student program - data analyst,stellantis,"job id: 2013495 career area: manufacturing job category: salaried location: remote –, us date posted: november 20, 2025 brand: fca group student program format: the format for this student program will be remote from september through april, meaning the student will work virtually. remote work must be completed at a location based in the united states. students who live in the metro-detroit area are welcome to work out of the stellantis headquarters and technology center in auburn hills, michigan, but this is not mandatory. students are expected to work 24 hours per week, during normal business hours, monday through friday. candidates who are selected for the program will work with their manager to determine a consistent weekly work schedule where candidates fulfill the 24-hour requirement, while balancing their class schedule. during the summer months (may through august), students may be relocated to the stellantis headquarters and technology center in auburn hills, mi and may potentially work 40 hours per week in a hybrid/in-person format, reporting onsite 3-5 days per week, depending on business needs. this determination will be at management’s discretion. applications are accepted year-round, and student program cohorts start on a quarterly basis in january, april, june, and september, and students are intended to stay in the program until graduation. stellantis student program highlights: the stellantis student program offers a unique alternative to traditional summer internships by providing students with the opportunity to work year-round throughout their academic journey. this continuous engagement allows students to apply their classroom learning in real-world settings on a daily basis, fostering deeper integration within their teams and organizations. as they progress through the program, students are empowered to take on increasing levels of responsibility, gaining valuable experience, and making meaningful contributions along the way. students will be assigned to a department based on the candidate’s background and skill set. the assignments are project-oriented, meaningful to the department, and allow the student to provide valuable contributions. students experience a sense of personal accomplishment and learn about the corporate business culture through work assignments, structured activities, and exclusive student networking events. student program benefits: • exposure to cutting-edge projects and technologies • collaborative work environment • mentorship from experienced professionals • networking opportunities with peers and leaders • skill development workshops and training sessions • paid us holidays • stellantis employee advantage vehicle discount program • eligible students may be able to participate in the company vehicle employee lease program department details: the stellantis facilities, general services and environmental compliance organization – data and business analytics student program allows individuals to gain experience and exposure across the many different facets in facilities, work plant conditions, environmental and energy strategic initiatives. students will work in a dynamic, fast-paced environment with people who work together as a team to continually think bigger and bolder. the data and business analytics student will gain industry experience, interface with all levels within our organization, and apply their coursework to drive results.| position benefits: • exposure to cutting-edge projects and technologies • collaborative work environment • mentorship from experienced professionals • networking opportunities with peers and leaders • skill development workshops and training sessions • what to expect: • a real assignment, with deliverables that matter • exposure to approachable, fun, and intelligent people who can make remarkable things happen • a knowledgeable coach to help facilitate learning, growth, and network development • experiences that build company acumen and individual technical skills eoe/disability/veteran at stellantis, we assess candidates based on qualifications, merit, and business needs. we welcome applications from all people without regard to sex, age, ethnicity, nationality, religion, sexual orientation, disability, or any characteristic protected by law. we believe that diverse teams reflect our identity as a global company, enabling us to better address the evolving needs of our customers and care for our future. our benefits reflects the stellantis commitment to helping you reach your personal and professional goals. in addition to an environment that promotes career development, we offer benefits for a healthy lifestyle and a rewarding future, designed to take care of you and your family, in various stages of life. as a global company, our employee packages will vary by country, customary norms and the legal entity into which you are hired. we care about your privacy. for more information on how your personal data is processed, please read the specific privacy statement provided by the respective entities whose job offer you have selected.",canada,Data Analyst,['r'],['r'],
data analyst (0 experience required),peroptyx,"this a full remote job, the offer is available from: canada for thousands of years, maps have provided humans with the knowledge they need to make decisions. as a maps evaluator, you will have the opportunity to provide ground truth for your town, city or country. at peroptyx, we are looking for data analysts who will review mapping data for digital mapping applications. your research capabilities will validate and ensure that the navigation of certain routes are accurate and safe. as part of this role you will verify that business names and opening hours are correct. you will check that the distance from a starting point to an end destination is listed accurately resulting in better user experiences. with this job you can plan your days around this highly flexible working schedule, work weekends or late evenings, all from the comfort of your own office. the flexibility of our roles minimizes the impact on your daily routine. so, whether you are a student looking to earn as you learn, a retiree looking for a new challenge a part-time/full time professional or a work from home parent, peroptyx has the right role for you! benefits • work up to 20 hours per week. • earn a competitive rate of pay. • develop your research skills. • avoid the long commute. • work from the comfort of your home office. • enjoy the flexibility of setting your own working hours! ideal candidate • fluent in english • excellent research skills. • excellent local knowledge of your home country. • good understanding and general knowledge of the geography and culture of canada. • analytical mindset. job requirements • must be living in canada for a minimum of 5 consecutive years. • must pass an online open-book exam that can verify your full understanding of the material and concepts. • must be willing to work a minimum of 10 hours and up to 20 hours per week depending on task availability. • good working knowledge of search engines, map applications and familiarity with social media platforms. • strong ability to learn, understand and apply multiple sets of different instructions. • all work must be of an independent nature. apply online today! work from home - wfh - remote - freelance - telecommuting - flexible this offer from ""peroptyx"" has been enriched by jobgether.com and got a 74% flex score.",canada,Data Analyst,"['excel', 'r']","['excel', 'r']",
sr. data analyst | full-time | remote,oak view group,"oak view group: oak view group is the global leader in venue development, management, and premium hospitality services for the live event industry. offering an unmatched, 360-degree solution set for a collection of world-class owned venues and a client roster that includes the most influential, highest attended arenas, convention centers, music festivals, performing arts centers, and cultural institutions on the planet. position summary: oak view group is hiring a **sr. data analyst** based remote in canada. reporting to the sr. manager, data & analytics. as we continue to grow and redefine the landscape of sports, live entertainment, and hospitality, oak view group is adding a data analyst to our dynamic team. this role supports our ongoing growth and leadership in the sports, live entertainment, hospitality, and venue management industries by providing data-driven insights and analyses. the sr. data analyst will play a crucial role in leveraging analytical methods to support decision-making and strategic initiatives across our growing portfolio of canadian venues, including but not limited to td coliseum, rogers stadium, and canada life place and partnership with liberty hospitality group. this position involves collaboration with stakeholders across sports, live entertainment, and hospitality to enhance customer experiences and optimize operational performance worldwide. this person will be responsible for maintaining data consistency, building data visualizations, and working with ovg’s venue subject matter experts to analyze core kpis of the venue and event management business. this role pays an annual salary of $111,600-$145,000 cad and is bonus eligible • *benefits for full-time roles: health, dental and vision insurance, pension matching, and paid time off (vacation days, sick days, and 11 holidays)** • *this position will remain open until january 9, 2026.** responsibilities: • *data analysis & modeling** - analyze complex data sets to develop and refine analytical frameworks and models that influence business strategies. - collaborate closely with business intelligence teams to transform raw data into actionable insights, enhancing operational efficiency and customer satisfaction • *reporting & visualization** - assist in the design and deployment of interactive dashboards and reporting solutions that provide real-time insights to stakeholders. - contribute to the development and maintenance of a centralized data dictionary or metric framework. • *stakeholder collaboration** - support broader organizational initiatives to integrate advanced analytics into daily operations, fostering a data-informed culture within the organization. - partner with global venue teams to ensure alignment of kpis and data definitions across the portfolio. • *innovation & partnership** - contribute to managing external partnerships and vendor relations to expand data capabilities and technological advancements. - support ad hoc analysis requests across departments including marketing, operations, and finance. qualifications: - bachelor's degree in statistics, mathematics, computer science, engineering, or a related field, with 3+ years of data analytics experience - proficiency with analytical tools (e.g., sql, python, r) and experience with bi platforms (e.g., powerbi) - skilled in statistical modeling, forecasting, and data mining techniques - demonstrated ability to manage and analyze large data sets - excellent problem-solving skills and the ability to communicate complex data in a clear and concise manner to non-technical stakeholders - familiarity with collaboration tools (e.g., jira, planner, confluence, or slack) • *preferences:** - experience in an azure environment - experience with databricks - experience working with event, ticketing, f&b, or fan engagement data; familiarity with sports, entertainment, or hospitality industry data. - experience with data governance, data quality, or data lineage. - experience working with cross-venue or multi-entity data in a centralized analytics team. strengthened by our differences. united to make a difference: at ovg, we understand that to continue positively disrupting the sports and live entertainment industry, we need a diverse team to help us do it. we also believe that inclusivity drives innovation, strengthens our **people**, improves our **service**, and raises our **excellence**. our success is rooted in creating environments that reflect and celebrate the diverse communities in which we operate and serve, and this is the reason we are committed to amplifying voices from all different backgrounds.",canada,Data Analyst,"['azure', 'business intelligence', 'dashboard', 'data analysis', 'data analytics', 'databricks', 'excel', 'python', 'r', 'sql', 'statistics']","['azure', 'business intelligence', 'dashboard', 'data analysis', 'data analytics', 'databricks', 'excel', 'python', 'r', 'sql', 'statistics']",
product growth data analyst,deel,"who we are is what we do. deel is the all-in-one payroll and hr platform for global teams. our vision is to unlock global opportunity for every person, team, and business. built for the way the world works today, deel combines hris, payroll, compliance, benefits, performance, and equipment management into one seamless platform. with ai-powered tools and a fully owned payroll infrastructure, deel supports every worker type in 150+ countries—helping businesses scale smarter, faster, and more compliantly. among the largest globally distributed companies in the world, our team of 7,000 spans more than 100 countries, speaks 74 languages, and brings a connected and dynamic culture that drives continuous learning and innovation for our customers. why should you be part of our success story? as the fastest-growing software as a service (saas) company in history, deel is transforming how global talent connects with world-class companies – breaking down borders that have traditionally limited both hiring and career opportunities. we're not just building software; we're creating the infrastructure for the future of work, enabling a more diverse and inclusive global economy. in 2024 alone, we paid $11.2 billion to workers in nearly 100 currencies and provided healthcare and benefits to workers in 109 countries—ensuring people get paid and protected, no matter where they are. our momentum is reflected in our achievements and customer satisfaction: cnbc disruptor 50, forbes cloud 100, deloitte fast 500, and repeated recognition on y combinator’s top companies list – all while maintaining a 4.83 average rating from 15,000 reviews across g2, trustpilot, captera, apple and google. your experience at deel will be a career accelerator. at the forefront of the global work revolution, you'll tackle complex challenges that impact millions of people's working lives. with our momentum—backed by a $17.3 billion valuation and $1 b in annual recurring revenue (arr) in just over five years—you'll drive meaningful impact while building expertise that makes you a sought-after leader in the transformation of global work. responsibilities • collaborate closely with product managers, product operations and dev teams on different problems to create actionable insights • monitor product and feature release performance, define baselines, success metrics and provide recommendations for optimisation • develop/apply extensive domain knowledge in your dedicated product areas in order to proactively derive meaningful recommendations and insights that drive and inform product strategy • analyze market data, identify customer behavior and trends • help design and evaluate a/b tests to improve customer journeys • create and promote effective self-service analytics infrastructure for your product areas • develop user friendly dashboards and high quality data visualizations that drive actions and support decision making • explore large and complex datasets to create actionable insights • build data models in dbt with the support of analytics engineers to create the cleanest data structures and kpis for your product areas • make intelligent choices for the adoption of new technologies based on your extensive experience • contribute to the development and growth of fellow product analysts by proactively sharing knowledge, providing mentoring, etc. qualifications • bachelor degree in computer science, math, statistics, economics or equivalent degree • 5+ years’ experience in product analytics team • hands on experience in at least one modern bi tools (preferably looker) • proficiency in sql and product analytics tools (mixpanel / heap / amplitude) • solid experience with data modelling (dbt) • excellent multitasking and problem-solving abilities • effective communication skills and experience dealing with stakeholders of varying seniority • experience with snowflake and lookml is a plus total rewards our workforce deserves fair and competitive pay that meets them where they are. with scalable benefits, rewards, and perks, our total rewards programs reflect our commitment to inclusivity and access for all. some things you’ll enjoy • stock grant opportunities dependent on your role, employment status and location • additional perks and benefits based on your employment status and country • the flexibility of remote work, including optional wework access at deel, we’re an equal-opportunity employer that values diversity and positively encourage applications from suitably qualified and eligible candidates regardless of race, religion, sex, national origin, gender, sexual orientation, age, marital status, veteran status, disability status, pregnancy or maternity or other applicable legally protected characteristics. unless otherwise agreed, we will communicate with job applicants using deel-specific emails, which include @deel.com and other acquired company emails like @payspace.com and @paygroup.com. you can view the most up-to-date job listings at deel by visiting our careers page. deel is an equal-opportunity employer and is committed to cultivating a diverse and inclusive workplace that reflects different abilities, backgrounds, beliefs, experiences, identities and perspectives. deel will provide accommodations on request throughout the recruitment, selection and assessment process for applicants with disabilities. if you require accommodations, please inform our talent acquisition team via this link and a team member will be in touch to ensure your equal participation. if you have difficulty accessing the form, please email at recruiting@deel.com. we use covey as part of our hiring and/or promotional processes. as part of the evaluation process, we provide covey with job requirements and candidate-submitted applications. certain features of the platform may qualify it as an automated employment decision tool (aedt) under applicable regulations. for positions in new york city, our use of covey complies with nyc local law 144. we began using covey scout for inbound on march 30, 2025. for more information about our data protection practices, please visit our privacy policy. you can review the independent bias audit report covering our use of covey here: https://getcovey.com/nyc-local-law-144",canada,Data Analyst,"['bi tools', 'cloud', 'dashboard', 'dbt', 'excel', 'looker', 'r', 'recommendation', 'scala', 'snowflake', 'sql', 'statistics']","['bi tools', 'cloud', 'dashboard', 'dbt', 'excel', 'looker', 'r', 'recommendation', 'scala', 'snowflake', 'sql', 'statistics']",
online data analyst - english canada (wfh),telus digital ai data solutions,"are you a detail-oriented individual with a passion for research and a good understanding of national and local geography? this freelance opportunity allows you to work at your own pace and from the comfort of your own home. a day in the life of an online data analyst: • in this role, you will be working on a project aimed at enhancing the content and quality of digital maps that are used by millions of people worldwide • completing research and evaluation tasks in a web-based environment such as verifying and comparing data, and determining the relevance and accuracy of information. join us today and be part of a dynamic and innovative team that is making a difference in the world! telus digital ai community our global ai community is a vibrant network of 1 million+ contributors from diverse backgrounds who help our customers collect, enhance, train, translate, and localize content to build better ai models. become part of our growing community and make an impact supporting the machine learning models of some of the world’s largest brands. qualification path no previous professional experience is required to apply to this role; however, working on this project will require you to pass the basic requirements and go through a standard assessment process. this is a part-time long-term project and your work will be subject to our standard quality assurance checks during the term of this agreement. basic requirements • full professional proficiency in the english language • being a resident in canada for the last 2 consecutive years and having familiarity with current and historical business, media, sport, news, social media, and cultural affairs in canada. • ability to follow guidelines and conduct online research using search engines, online maps, and website information • flexibility to work across a diverse set of task types, including maps, news, audio tasks, and relevance • daily access to a broadband internet connection, computer, and relevant software assessment in order to be hired into the program, you’ll take an open book qualification exam that will determine your suitability for the position and complete id verification. our team will provide you with guidelines and learning materials before your qualification exam. you will be required to complete the exam in a specific timeframe but at your convenience. equal opportunity all qualified applicants will receive consideration for a contractual relationship without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or protected veteran status. at telus digital ai, we are proud to offer equal opportunities and are committed to creating a diverse and inclusive community. all aspects of selection are based on applicants’ qualifications, merits, competence, and performance without regard to any characteristic related to diversity.",canada,Data Analyst,"['machine learning', 'r']","['machine learning', 'r']",
remote senior data analyst — aws & quicksight,accion labs,a tech consulting firm in canada is seeking an hr specialist with extensive data analytics experience for a contract role. you will be responsible for supporting business users through analytics and designing dashboards in amazon quicksight. the ideal candidate will have strong sql skills and experience with complex data models. this position is remote and requires availability during eastern time hours from 9 am to 5 pm. #j-18808-ljbffr,canada,Data Analyst,"['dashboard', 'data analytics', 'r', 'sql']","['dashboard', 'data analytics', 'r', 'sql']",
data analyst (gcp) – analytics & dashboards expert,stacktics announces partnership,"a leading data analytics firm in canada is seeking a data analyst to drive quality and efficiency in service and data analytics. the role involves designing strategies, creating dashboards, and collaborating on data pipelines. candidates should have 4+ years in data analytics, proficiency in python and sql, and a strong understanding of marketing analytics platforms. flexibility to work remotely and a comprehensive benefits package are provided. #j-18808-ljbffr",canada,Data Analyst,"['dashboard', 'data analytics', 'data pipeline', 'python', 'r', 'sql']","['dashboard', 'data analytics', 'data pipeline', 'python', 'r', 'sql']",
"sr. data analyst (remote, canada)",fairygodboss inc.,"**the muse** • *senior data analyst** location: remote, canada; other locations will be considered on a case-by-case basis • *our mission** the muse, including its platforms themuse.com, fairygodboss.com, and purposejobs.com attracts 70 million annual users, primarily gen z and millennial professionals, to connect with companies and find a career that matches their values. founded in 2011, we were named one of fast company's 50 most innovative companies in the world. we have a highly diverse userbase of millennial & gen z jobseekers who we help match with companies as wide-ranging as meta, nike and the fbi, on the strength of those companies' values, cultures, and employee experiences. our users come to themuse.com for original career advice from prominent experts, access to the best coaches, and a behind-the-scenes look at job opportunities. in 2022, the muse acquired fairygodboss, the largest online career community for women, and 2024, the muse acquired purpose jobs, the go-to platform for values-driven job discovery. the muse operates the three brands as a single organization. • *the role** we are looking for an experienced senior analyst with outstanding active listening, communication, and presentation skills. the candidate will work with a close-knit team of professionals, including stakeholders across multiple functional groups in email marketing, editorial, account management, sales, marketing, tech, data engineering, and product, and be able to drive decision-making with actionable insights and deliverables. you will create innovative analyses to help the team understand how the muse is performing and drive analytical products to meet the needs of our customers. you will also help us tell our clients the story of how our users engage with their content, what job seekers are looking for, and the role that the muse plays in bringing and matching our job seekers to them. you will be responsible for deriving insights from data at the intersection of client success, sales, product, and operations and help lead our efforts to improve and grow how we evaluate client success. your ability to clearly calibrate and communicate with different stakeholders and with different levels of data expertise is critical to success. we work remotely and are looking for teammates who can work primarily during our business hours of 9-5pm (gmt-5). please note: this is not a data engineering role. we are looking for an experienced data analyst who can go beyond engineering and reporting; actionable insights and high-fidelity presentation skills are a must. • *responsibilities** \-provide actionable reporting, analysis and recommendations to help the organization improve performance and achieve enterprise objectives; this includes presenting to executive leadership and sales functions in a way that is clear and concise to a non-technical audience \-design and define analytical standards, data definitions, dashboards & processes to provide timely insights \-establish innovative analytical reports, developing new ways of thinking about our data and presenting findings in a clear and concise manner to help drive commercial success and strategy \-build, develop and implement enhancements to reporting and analytics processes \-translate data requests into actionable data plans and work across the data platform and engineering teams to implement \-deliver dashboards that enable data-driven decision making and conduct more in depth analysis using sql, r, looker & other business intelligence tools • *qualifications and requirements** \-3-5 years of related work experience in analytics (not just data engineering), presenting data to and discussing findings with non-technical audiences, including executives, and a proven track record of providing valuable insights from data that drives strategic thinking with business objectives. \-excellent written and verbal communication skills in english and can fluently discuss analytical findings \-excited to not only provide answers but also to help people learn how to do their own analysis \-must be a ""data detective"" and will investigate issues until you've identified the root cause, ensuring good quality data. \-strong relationship management and interpersonal abilities \-comfortable working in sql, r or python for data analysis and can switch to excel or sheets to do some pivot tables and vlookups. in addition, you are well-versed with looker or other bi tools. \-innovative team player who can work independently and autonomously and develop and drive data initiatives to uplevel the organization. \-ability to work cross-functionally, converting requests from stakeholders and into technical requirements to ensure that business needs are met. \-passionate about the customer experience and have an empathetic, analytical, and modern product practice mindset. \-manages multiple projects simultaneously, prioritize, and meet deadlines while working in a fast-paced environment. \-leverages analytics to understand audience preferences to expand reach, improve engagement, keep users coming back, and identify new audience segments \-familiarity with seo and performance marketing techniques \-helps make sure the right content reaches the right audience in the most effective format \-comfortable in a highly collaborative, matrixed, and distributed (remote) workplace. \-media, content, publishing, job platform and/or other consumer-facing website experience is a plus \-passion for the mission of the muse and its success • *salary commensurate with experience and background.** at the muse, we believe that great ideas come from anywhere. we support a collaborative environment and value open participation from individuals with different ideas, experiences, and perspectives. we believe having a diverse team makes the muse a more interesting and innovative place to work, and we strive every day to make the muse a welcoming and inclusive place for all. $75,000 - $85,000 a year",canada,Data Analyst,"['bi tools', 'business intelligence', 'dashboard', 'data analysis', 'excel', 'looker', 'python', 'r', 'recommendation', 'sql']","['bi tools', 'business intelligence', 'dashboard', 'data analysis', 'excel', 'looker', 'python', 'r', 'recommendation', 'sql']",
data analyst specialist,freelancejobs,"data analyst role overview the data analyst will support a multi-phase asset valuation and restructuring initiative involving manufacturing equipment, real estate holdings, intellectual property, and potential future acquisition activities. the analyst will gather data, conduct financial modeling, evaluate risks, and build dynamic tools that support strategic decision-making. this role requires strong analytical skills, advanced excel modeling, and the ability to structure and interpret complex or incomplete data. core responsibilities • equipment & manufacturing asset analysis • extract and organize all available factory equipment data from filings and supporting documents. • build a structured equipment valuation model including: ○ asset descriptions and specifications ○ original purchase cost ○ estimated fair market value ○ forced-sale / liquidation value estimates ○ accelerated depreciation schedules • review financial statements to determine depreciation already taken. • ensure valuation outputs integrate into higher-level summary dashboards. • real estate data gathering & valuation a. data collection • collect all property and mortgage documentation available. • pull public data from: ○ county tax assessor ○ county recorder / clerk ○ lien databases ○ property tax records • identify and summarize: ○ mortgages and encumbrances ○ liens ○ restrictions or covenants ○ any litigation flags b. valuation modeling • build land and development valuation worksheets including: ○ land value ○ structure/improvement value ○ development cost assumptions ○ market comparables c. scenario analysis create models for multiple exit strategies: • sell completed homes individually. • sell the project as a build-to-rent portfolio. d. integration • ensure all real estate models can be rolled up into an overarching summary workbook. • exit strategy & capital structure modeling • construct flexible scenario modules that evaluate: ○ bringing on equity partners ○ joint venture structures ○ construction management participation ○ liquidity creation strategies • build fields and formulas that can adapt as new kpis are defined. • intellectual property valuation support • develop a framework for estimating the value of proprietary assets. • prepare models that support potential licensing structures. • integrate outputs into the master summary workbook. • organizational & entity-level modeling • create integrated models representing: ○ manufacturing operations ○ real estate holdings ○ intellectual property assets ○ higher-level holding structures • ensure data flows upward from individual workbooks into a central dashboard. • maintain flexibility as restructuring strategies evolve. • legal agreement data extraction (analytical support) note: the analyst does not perform legal interpretation, only structured data extraction. • review documents such as: ○ mortgages ○ loan agreements ○ contractor agreements ○ warranty or service contracts • create structured tables summarizing: ○ key terms ○ restrictive or potentially problematic clauses ○ obligations and expiration dates ○ counterparty risks • flag items requiring review by legal or executive teams. technical deliverables excel / modeling deliverables • equipment valuation workbook • real estate valuation workbook • intellectual property valuation framework • exit strategy scenario model • consolidated organizational/asset structure workbook • master summary dashboard including: ○ valuations ○ liabilities ○ scenario results ○ sensitivities ○ flexible kpi placeholders document review deliverables • summaries of mortgages, liens, and contractual obligations • risk/issue flags requiring executive attention skills & competencies required • advanced excel financial modeling • asset valuation & depreciation analysis • real estate financial modeling and comps analysis • ability to gather and interpret public record data • strong data cleaning and structuring skills • ability to analyze incomplete or inconsistent documents • competence in scenario and sensitivity modeling • clear written communication for summarizing complex findings workflow overview • data ingestion from filings, financials, public databases. • data organization into structured models. • asset valuation across equipment, real estate, and ip. • scenario modeling for exit and partnership strategies. • integration into centralized dashboards. • document summarization for legal/strategic use. • ongoing refinement as new information and metrics emerge. contract duration of more than 6 months. with 40 hours per week. mandatory skills: data analysis, customer service analytics, operations analytics, marketing analytics, product analytics, growth analytics, analytical presentation, data analysis consultation, data visualization, business intelligence",canada,Data Analyst,"['business intelligence', 'dashboard', 'data analysis', 'excel', 'r']","['business intelligence', 'dashboard', 'data analysis', 'excel', 'r']",
oakville fire data analyst,town of oakville,"the town of oakville's fire department is focused on improving service efficiency and policy development through data-driven insights. as a critical member of the team, the fire data analyst will collaborate with various departments and external partners to analyze and communicate performance measures effectively. in this role you can expect to have the responsibilities: • analyze structured and unstructured datasets to aid business decisions. • prepare, visualize, and communicate performance measures across departments. • collaborate with leadership and it teams to transform data into actionable insights. • provide core application data support for decision-making and policy development. this role comes with the following benefits: this role requires you to have: • three-year diploma or degree in mathematics, statistics, information management, or a related field. • minimum of 3 years of experience in data analysis and reporting. • proficiency in using data analysis tools such as qlikview, ms power bi, google analytics, tableau. you would benefit from having: • experience with compliance reporting to emergency management ontario or similar organizations. • dama certified data management professional (cdmp) or tdwi certified business intelligence professional (cbip) certification. • strong understanding of statistical research, data trends, and forecasting. candidates must demonstrate their qualifications during the application and interview process, requiring a minimum score of 75%.",canada,Data Analyst,"['business intelligence', 'data analysis', 'power bi', 'r', 'statistics', 'tableau']","['business intelligence', 'data analysis', 'power bi', 'r', 'statistics', 'tableau']",
financial data analyst,university of waterloo,"at the university of waterloo, we create and promote a culture where everyone can reach their full potential. as an employee, you get support & opportunities that empower you to advance your career. explore how we can bring big ideas to life, together. the university is a welcoming workplace for those of all abilities, interests, and expertise. as part of our workforce, you can do what you do best, every day. learn more about our recruitment process. job requisition id: 2025-00893 time type: full time employee group: staff job category: analytics and reporting employment type: temporary department: faculty of mathematics - dean of mathematics office - financial operations hiring range: $78,214.70 - $97,768.38 posting information: this is an external contract, term 2 years. job description: primary purpose reporting to the associate director, faculty financial operations, the financial data analyst (fda) is responsible for the development of databases, models, analyses, and reporting as well as communicating the results. the fda will proactively contribute to providing data, and analytical planning support to the senior leadership in the faculty of mathematics to inform prioritization, resource allocation, budgeting and planning related decisions. key accountabilities data analysis and support · develops and prepares analytical models, reports, and relevant data visualizations (including dashboards, graphs, presentations and web applications) to inform evidence-based decision-making at the faculty level. · supports indicator development and ongoing reporting for operational and strategic measures. · analyzes trends and patterns in the faculty of mathematics performance over time using various benchmarks and comparative data. · supports and maintains faculty of mathematics planning and budgeting model (vena) related data and reporting tools. · prepares standard and customized reports and accountability submissions as may be needed by the dean and/or central university administration. data management: · ensures that faculty data is accurate, consistent, properly maintained and shared in compliance with relevant privacy protection, confidentiality and other ethical principles. · identifies and addresses data integrity/reliability issues and uses data cleaning processes to achieve required data quality standards. · works with large data sets to perform data mining and complete statistical analyses. · transforms, synthesizes, cleanses data and identifies opportunities to reduce duplication and errors and to ensure consistency in data; identifies data integrity issues and proposes data cleansing processes to develop clear and consistent data quality standards. · designs and develops user interfaces for accountability reporting using web applications and best practices in data visualization, for example, power bi communication: · develops accurate graphs, reports, dashboards and presentations to convey complex information in an understandable and compelling manner. · creates business/analytical models and related user interfaces that allow stakeholders to self-serve and explore planning parameters. · ensures data and information is placed into the proper context by combining faculty data with both qualitative and quantitative relevant environmental/external data and appropriate narrative. · communicates with technical and non-technical users to gather data and data system requirements, reporting requirements, consolidate and document technical data requirements, transform data into visualizations, and assist with project planning. · ensures timely communication of essential data to internal and external users as appropriate. · serves on faculty, and as appropriate, non-faculty committees and groups, offering financial data and related analytical expertise fostering data information sharing and collaborative approaches on a variety of projects. · liaises with departments within mathematics providing technical expertise and data support to enable informed decision making · develops compelling narratives through data analysis; supports marketing and advancement initiatives. evaluation, reporting and planning: · uses various quantitative and qualitative methods to administer surveys, develop sampling procedures, analyze survey results, or to gather and analyze data for special requests (literature reviews, environmental scans, needs assessments, focus group and key informant interviews). · maintains data to evaluate the faculty’s progress towards goals and priorities, and/or routine monitoring of faculty performance. · develops and maintains databases; creates standard operating procedures for data maintenance and storage with appropriate documentation. · prepares data to support enrolment planning and forecasting in relation to both short and long range revenue projections, and support resource allocation decisions. leadership, relationships and collaboration · maintains current knowledge of best practices across all domains of the role, in the post-secondary sector and beyond, to effectively advise faculty leadership. · investigates new tools, systems, and practices to recommend improvements across all domains of the role · understands the needs of each stakeholder group in order to assess and scope projects fully and to make recommendations on timelines and priorities for each. · communicates effectively with multiple, varied clients (senior leadership, faculty members, and staff) to understand needs, scope projects and project terms, update progress, and present findings. required qualifications education · bachelor’s degree in statistics, economics, computer science, social science research or another related discipline is required. · an equivalent combination of experience and education may be considered. · preference will be given to members of cpa ontario. experience · minimum five years of related experience in data analytics, management reporting, financial planning, and analysis. · experience in an academic environment strongly preferred. · supervisory/management experience is an asset. knowledge/skills/abilities · advanced data management, data transformation and interpretation, and analysis skills. · knowledge and capabilities of a variety of reporting and data visualization tools, such as power bl. · knowledge of sharepoint and other web development applications · advanced excel capabilities. · has proven ability to apply knowledge and expertise creatively · excellent written and verbal skills for procedural documentation and demonstrated experience with report writing. · participates in continuing education such as workshops. seminars, study sessions or conferences. · experience processing data with jupyter notebooks(python), rstudio ® or equivalent. · basic proficiency with sql. · ability to work independently and as part of a team. · takes initiative and be proactive in identifying issues of consequence and fosters creative and innovative approaches to problem solving in a team leader role. · ability to manage large workloads and prioritize competing deadlines. · ability to identify and apply a combination of quantitative and qualitative research methods. · builds productive and collaborative relationships and provides advice and guidance to individuals formally and informally. equity statement the university of waterloo acknowledges that much of our work takes place on the traditional territory of the neutral, anishinaabeg, and haudenosaunee peoples. our main campus is situated on the haldimand tract, the land granted to the six nations that includes six miles on each side of the grand river. our active work toward reconciliation takes place across our campuses through research, learning, teaching, and community building, and is coordinated within the office of indigenous relations. the university values the diverse and intersectional identities of its students, faculty, and staff. the university regards equity and diversity as an integral part of academic excellence and is committed to accessibility for all employees. the university of waterloo seeks applicants who embrace our values of equity, anti-racism and inclusion. as such, we encourage applications from candidates who have been historically disadvantaged and marginalized, including applicants who identify as first nations, métis and/or inuk (inuit), black, racialized, a person with a disability, women and/or 2slgbtq+. positions are open to qualified candidates who are legally entitled to work in canada. the university of waterloo is committed to accessibility for persons with disabilities. if you have any application, interview, or workplace accommodation requests, please contact human resources at hrhelp@uwaterloo.ca or 519-888-4567, ext. 45935. at the university of waterloo, we think differently, we act with purpose, and we work together. we advance learning and knowledge through teaching, research, and scholarship, nationally and internationally, in an environment of free expression and enquiry. we are a leading global research-intensive university, renowned for entrepreneurship and innovation, providing co-op and work-integrated learning at scale with impact. learn more about waterloo current employees, please apply via the job hub. internal candidates are reviewed first before external applicants are considered. due to the volume of applications, only those selected for an interview will be contacted.",canada,Data Analyst,"['dashboard', 'data analysis', 'data analytics', 'excel', 'power bi', 'python', 'r', 'recommendation', 'sql', 'statistics']","['dashboard', 'data analysis', 'data analytics', 'excel', 'power bi', 'python', 'r', 'recommendation', 'sql', 'statistics']",
data analyst/ measurement / programmatic support,broadsign careers,"data analyst, measurement & programmatic support are you looking to join a global tech leader? then we have an exciting opportunity for you in montreal or toronto, remote! about broadsign broadsign is a growing software company with a mission to make buying, selling, and delivering out-of-home media easier than ever. our software is operated by some of the most successful out-of-home businesses and powers impactful, compelling campaigns seen across the world. come light up the world as a data analyst, measurement & programmatic support. what to expect we’re looking for a data analyst to support our growing work in campaign measurement, audience analytics, and programmatic operations. you’ll play a key role in executing uplift and attribution studies (brand lift, store visit uplift, exposure-to-visit models), supporting client reporting, and helping the programmatic account management team ensure smooth campaign delivery and optimization. this is a hands-on role ideal for someone who loves data storytelling, is comfortable working with campaign data, and wants to grow in the intersection of data, media, and programmatic advertising. reporting to arno buskop, senior director, data strategy. job responsibilities: measurement & analytics • support the execution of uplift and attribution studies for client campaigns (brand lift, foot traffic, sales impact). • collect, clean, and analyze campaign performance data using internal tools and partner platforms. • build reports and visual dashboards summarizing campaign outcomes and data insights. • conduct qa on incoming datasets from measurement partners, ensuring consistency and data integrity. • contribute to the development of benchmarks and effectiveness metrics for dooh campaigns. • collaborate with the director of data strategy to improve internal processes and analytical models. • translate complex analytical findings and technical concepts into clear, actionable presentations for clients and internal partners, fostering strong cross-functional collaboration. programmatic campaign support • collaborate with the programmatic account management team to monitor live campaigns and flag pacing or delivery issues. • support campaign setup and reporting workflows across broadsign dsp and broadsign reach (ssp). • assist in preparing and validating campaign performance reports for clients and internal teams. • identify process improvement opportunities across campaign management, data handling, and reporting. what you need to perform in this job : • 3 years of experience in data analytics, media operations, or programmatic advertising. • strong analytical skills and attention to detail - you enjoy making sense of complex datasets. • familiarity with digital or programmatic advertising metrics (impressions, cpms, ctr, reach, frequency, conversions). • proficiency in excel / google sheets; experience with sql, python, bigquery or data visualization tools like looker is a plus. • excellent organizational and problem-solving skills, with a proactive approach and getting things done mentality. • collaborative mindset - able to work cross-functionally with data, product, and account teams. • experience or coursework in statistics, marketing analytics, or data science is an asset. • understanding of ooh or dooh media is a plus, but not required. • great technical communication and collaboration • strong communication skills what we bring to the table : • comprehensive benefits: complete company insurance plan (health, dental, vision, travel) effective from day one (100% employer-paid). $500 annual health care savings account (hcsa) for additional health-related expenses.unlimited access to virtual healthcare platform (telus health). • wellness: $500 annual wellness fund for mental/physical health and office-related expenses. • paid time off: minimum 3 weeks vacation, plus an additional week off during the holidays, 5 sick/personal days, and 2 volunteer days. • retirement savings: group rrsp with a 50% employer matching up to 4% of your salary. • financial perks: transportation reimbursement for travel to a broadsign office. • family support: parental leave salary supplement. • growth opportunities: training & development opportunities with a yearly budget to support professional growth. at broadsign, we value the varied social identities that make up our community. we recognize talent comes in different forms and encourage applications that reflect different backgrounds and experiences. our promise is to be an inclusive employer and partner, open to learning, with thoughtful strategies and practices that amplify the different voices of our industry. knowledge of french is required for positions permanently located in quebec so incumbents can communicate with their colleagues and partners in quebec as necessary. french-language training is offered to all incumbents in permanent positions in quebec who do not have a good knowledge of french. fluent english is required for this position in order to communicate with colleagues, clients and partners (or suppliers) located outside quebec and to understand the technical and scientific documentation used in our industry. original job data analyst/ measurement / programmatic support posted on grabjobs ©. to flag any issues with this job please use the report job button on grabjobs.",canada,Data Analyst,"['bigquery', 'dashboard', 'data analytics', 'excel', 'looker', 'python', 'r', 'sql', 'statistics']","['bigquery', 'dashboard', 'data analytics', 'excel', 'looker', 'python', 'r', 'sql', 'statistics']",
business analyst ii (audit/data attestation),career nexus,"job title: business analyst ii (audit/data attestation) job id: 5845 location: montreal, qc or mississauga, on job type: contract pay rate: $600/day work authorization: you must be eligible to work in canada without sponsorship. positions available: 1 required experience: 7+ years principal duties and responsibilities: • document and evaluate existing data controls. • obtain critical data elements and data quality requirements from business users. • based on data lineage documentation, reach out to control owners (it and business) to understand and obtain documentation on existing controls. • document existing controls along with evidence for their existence and execution (system procedure/user documents, controls run reports, etc.). • assess: • control design. • control effectiveness based on the data quality requirements (is it the right control? is it being run with the frequency relevant to the end report? etc.). • document gaps between existing controls design & effectiveness vs. data quality requirements by the business users. • incorporate gaps into the bank’s data quality issue management framework. • conduct root cause analysis of identified issues. • understand appropriate team to designate as issue remediation owner. • reach consensus on a suitable course of action (tactical and/or strategic remediation). • assign appropriate team(s) to handle the remediation activities. • follow up with control owners to obtain remediation plans and track them until required new controls are established and gaps in existing controls are remediated. • track remediation activities to issue resolution, report, and escalate as necessary. • work collaboratively with the bank’s data quality pillar, data stewards, business users, it teams, and data owners. qualifications: • at least 7+ years working experience in the financial industry as a business or data analyst. • strong knowledge of banking and investment products. • ideal: experience with data attestation process, inclusive of evaluating data controls, and/or former audit experience. • strong communication skills: both written and oral with technical and non-technical staff. • familiarity with issues workflow management tools such as jira. • at least 3+ years working with databases such as sql server & oracle. • experience working with a data management team and monitoring data quality and/or performing data quality issue remediation activities, inclusive of conducting root cause analysis. • advanced excel skills, familiarity with visio and powerpoint. • detail-oriented, organized, and thorough. • ability to thrive in a team-based environment. industry: financial services job category: data analysis / audit",canada (+2 others),Data Analyst,"['data analysis', 'excel', 'r', 'scala', 'sql', 'sql server']","['data analysis', 'excel', 'r', 'scala', 'sql', 'sql server']",
"trade compliance specialist, business data analyst (l3)",l3harris technologies,"l3harris is dedicated to recruiting and developing high-performing talent who are passionate about what they do. our employees are unified in a shared dedication to our customers’ mission and quest for professional growth. l3harris provides an inclusive, engaging environment designed to empower employees and promote work-life success. fundamental to our culture is an unwavering focus on values, dedication to our communities, and commitment to excellence in everything we do. l3harris technologies is the trusted disruptor in the defense industry. with customers’ mission-critical needs always in mind, our employees deliver end-to-end technology solutions connecting the space, air, land, sea and cyber domains in the interest of national security. • *trade compliance specialist, business data analyst** reference #27779 waterdown, ontario • *about the role** the trade compliance specialist, business data analyst will be a part of the wescam trade compliance team supporting trade compliance process analytics to help align strategic goals into actionable plans. communicates with both internal and external stakeholders to explain and interpret operational processes, practices, and procedures. recommends enhancements to systems and processes, and works to achieve operational targets for specific programs and projects. the trade specialist will provide process, data analytics and technical expertise to ensure that all trade compliance related interactions meet regulatory, corporate, and local compliance requirements to support data analysis, reporting, and process improvements. conducts training sessions for employees and assists in developing and implementing policies and procedures to ensure compliance with federal regulations. develops and conducts audits of internal controls and procedures by examining artifacts to ensure compliant transactions. supports external audits and inquiries from us governmental control agencies such as customs, dos, doc, and global affairs canada through data gathering and corrective action execution. prepares agreements in accordance with federal regulations to support international business initiatives, including: employment of foreign persons, manufacturing abroad, and sourcing and development relationships with foreign entities. interprets governmental responses and provides guidance to stakeholders. establishes and maintains relations with government licensing authorities to ensure the company's position on licensing and technology is conveyed, understood and accepted. • *primary responsibilities** - collect, inspect, cleanse data from multiple data sources, and analyze data to improve reporting, suggest and implement automation. - works to achieve operational targets for specific programs and projects and recommends enhancements to systems and processes to enhance effectiveness. - support l3harris e3 - continuous improvement initiatives, such as developing and updating business process, training materials, reports, and erp improvement projects - work with internal / external stakeholders to obtain supporting documents, for canadian export and brokering permits and u.s. export, re-export and re-transfer license applications. - draft and submit canadian export and brokering permit applications to global affairs canada - monitor canadian export / brokering permit, update internal stakeholders of permit status and update excol with quarterly and semi-annual reports - draft and submit u.s. department of state (dos) and u.s. department of commerce (doc) export, re-export and re-transfer license applications to the global trade group for review prior to submission to the united states government - monitor, review and release blocked shipments and determine if a valid export authorization can be used to release the shipment in the erp system - support with the management of technical assistance agreements (taa), warehouse distribution agreements (wda) and technology control plans (tcp) - validate export classification and jurisdiction of purchased and manufactured items (i.e. eccn, usml, ecl, bcl, cgp) with support from engineering, procurement and ims segment classification teams - conduct restricted party screening (rps) for all l3harris business partners - assist with trade compliance investigations, risk mitigation, audits and self-assessments. • *required capabilities** - requires keen attention to detail, with the ability to learn and comprehend detailed laws and regulations. - proficiency in bi power or other reporting applications - very good analytical skills working with large amounts of data - knowledge of l3harris products and export control status would be an asset - excellent written and oral communication skills at the professional level - excellent analytical, problem-solving, negotiation, and organizational skills - effectively manages multiple tasks and can prioritize to meet deadlines - passion for continuous improvement and education - ability to build strong relationships with people at all levels. - perform duties with minimum supervision, and participate in cross-functional projects as assigned. - proactive problem solver, action and results oriented with a willingness to see things through to the end. - ability to work efficiently under pressure and to tight deadlines. • *desired education and experiences** - requires proficient knowledge of job area - may have practical knowledge of project management - bachelor’s degree and a minimum of 4 years of prior related experience. - graduate degree or equivalent with 2 years of prior related experience. - in lieu of a degree, minimum of 8 years of prior related experience. - knowledge of & experience with the following regulations: - canada’s export & import permits act (eipa), - canada’s controlled goods program (cgp), - international traffic and arms regulations (itar), - export administration regulations (ear) at l3harris, we foster an inclusive and equitable workplace. l3harris is committed to treating all employees and applicants for employment with respect and dignity and maintaining a workplace that is free from unlawful discrimination. for applicants with disabilities, we will provide you with accommodation so that you have what you need in order to be at your best. l3harris performs background checks prior to employment as all applicants must be eligible for registration with the controlled goods program and obtain and maintain a positive security assessment. some positions may require a government of canada “reliability” status and/or level 2 (secret) security clearance. in addition, l3harris performs pre-employment substance abuse testing where required.",canada,Data Analyst,"['aws', 'classification', 'data analysis', 'data analytics', 'excel', 'r']","['aws', 'classification', 'data analysis', 'data analytics', 'excel', 'r']",
data analytics- summer intern,standardaero,"job description job title: data analytics- summer intern st. johns, nl build an aviation career you’re proud of at standardaero, we use our ingenuity and know-how to find solutions for the simple to the most complex challenges in aviation. together, we get the job done and done well. our stability, resources, and respectful culture supports you in building a solid career with a great team you can count on day in and day out for the long term. what’s it like at standardaero we are looking for a data driven, motivated team player who enjoys working in a fast-paced environment to join our it team in st. john’s nl as a data analytics intern for the 2026 summer term. the successful candidate will have a keen interest in data analytics and an eagerness to learn and contribute to developing data analytics products within standardaero’s cloud-based enterprise data analytics architecture (power bi and snowflake). as a data analytics intern you will work closely with the data analytics team to gather and analyze data, then develop reporting/ visualizations that allow for data driven decision making throughout the organization. what you’ll do • work closely with data analytics team members to develop power bi reports/ dashboards. • assist with maintenance of data architecture which contains data from multiple data sources. • perform data validation to ensure data accuracy. • assist with user acceptance testing of changed datasets, reports and dashboards. • follow best practices for bi report/ visualization development and data management. • ensure data access is secure by enforcing the organizations security policies. • assist with troubleshooting helpdesk tickets related to data analytics. basic qualification • completed coursework in computer science, data analytics, information systems, or equivalent. • actively pursuing a degree or diploma in computer science, data analytics, information systems, or equivalent • introductory knowledge of microsoft power bi (or similar toolset) through completed coursework or class projects. • introductory knowledge of data pipelines and data models in an enterprise data warehouse setting. • proficient in sql and data querying for data aggregation/ manipulation. • experience working with databases: (eg. snowflake, oracle, ms sql) • ability to follow software development processes and work in an agile environment. • strong analytical and problem-solving skills. desired skills • knowledge of microsoft power bi architecture (pbi desktop, pbi service, dax, etc) • experience in developing power bi reports and dashboards considered an asset. • strong written and verbal communication skills, demonstrated through presentations, reports, or class projects. • self-starter with demonstrated ability to work independently or in group settings, as shown through team projects or class assignments. • self-motivation to develop skills in data analytics and learn new technologies and toolsets. raising the standard of excellence since 1911 with over a century of proven excellence, standardaero has become an industry leader in mro services and customized solutions in the aerospace field. our shared values and learning-based culture inspire our team to exceed their potential and power our customers’ missions worldwide. with on-the-job training, advancement opportunities, and excellent benefits, standardaero invites you to experience a fulfilling and meaningful career with us. inclusivity is our standard standardaero offers equal employment opportunities for all. our supportive environment celebrates diversity with no room for harassment or discrimination of any kind. we invite you to be who you are and experience our welcoming culture.",canada,Data Analyst,"['cloud', 'dashboard', 'data analytics', 'data pipeline', 'data warehouse', 'excel', 'power bi', 'r', 'snowflake', 'sql']","['cloud', 'dashboard', 'data analytics', 'data pipeline', 'data warehouse', 'excel', 'power bi', 'r', 'snowflake', 'sql']",
fleet associate and data analyst,clearway,"fleet asscociate & data analyst at clearway we are committed to every project. we pride ourselves on a keen work ethic and more importantly, the talent and skill of our people. in fact, the clearway staff is the reason why so many organizations turn to us for the most complex construction assignments. that is how we’ve evolved from a sewer and watermain contractor, to one of canada’s most reputable construction companies with as many capabilities as there are needs. clearway has been operating for more than 50 years, specializing in sewers and watermains, transportation, dry utilities, shoring, road reconstruction, concrete forming, marine, environmental remediation and treatment plants / pumping stations. if you are looking to work for a growing company who values the contribution of its employees and you demonstrate creativity, ingenuity, initiative and a high-level commitment then we are the company for you. we offer challenging and rewarding career opportunities, with room to grow, learn and excel. job summary we’re looking for a motivated and detail-oriented fleet associate & data analyst to join our fleet & administration team. this position blends hands-on fleet coordination with data analysis and reporting; ideal for someone who enjoys both operational problem-solving and working with numbers. reporting to the director of fleet systems and administration, the fleet associate and data analyst will support daily fleet activities such as dispatching, compliance tracking, and equipment coordination, while also analyzing data related to utilization, fuel efficiency, and maintenance costs to help drive smarter operational decisions. location: clearway equipment office & yard – 69 freshway dr, concord, on l4k 1r9. hours of operation: 8:00 am to 5:00 pm (est), monday through friday(on site) compensation range: $60,000 - $70,000 annual salary essential duties & responsibilities data collection, reporting & analysis • gather, clean, and consolidate data from multiple sources (erp, fuel cards, telematics, gps systems, etc.) into structured reports. • develop and maintain weekly, monthly, and quarterly reports on: • equipment and vehicle utilization • fuel efficiency and idle time • maintenance and downtime trends • cost per hour/kilometer analysis • asset aging and lifecycle performance • build and maintain dashboards or summary sheets that highlight fleet trends and performance indicators for management review. • identify variances, anomalies, or trends in data (ex. high idle time, overconsumption of fuel, underutilized assets) and make recommendations for corrective actions. • support budgeting and forecasting activities by providing accurate utilization and cost data. • work with administration to ensure all fleet-related expenses are captured under correct g/l codes and cost centers. fleet administration & coordination • support day-to-day fleet operations including vehicle and equipment assignments, registration renewals, insurance updates, and new asset onboarding. • track and maintain cvor compliance records, annual inspections, driver abstracts, and other regulatory requirements. • serve as a point of contact for field staff, drivers, and supervisors regarding vehicle status, dispatch needs, and maintenance scheduling. • support in coordination of dispatching and scheduling of equipment, float moves, and company vehicles to meet operational timelines and project demands. • maintain the master fleet database, ensuring all records (vin, license plate, asset numbers, driver assignments, etc.) are accurate and current. • assist with equipment recalls, warranty claims, and service scheduling, coordinating with the maintenance shop or vendors as needed. • manage incoming and outgoing fleet documentation such as fuel card setups, vehicle transfers, insurance certificates, and repair authorizations. compliance & process improvement • assist with internal and external audits related to fleet compliance, insurance, cvor, and safety records. • monitor upcoming license, permit, and inspection expirations and ensure timely renewals. • help standardize fleet documentation templates (asset checklists, dispatch forms, mileage logs, etc.) to ensure consistency across divisions. • identify and implement process improvements to increase efficiency in fleet tracking, dispatch communication, and data management. • collaborate with internal software teams to optimize use of fleet systems. • support leadership in data-driven decision-making related to equipment replacement and utilization optimization. communication & collaboration • liaise between intercompany departments to ensure alignment on asset status, cost tracking, and reporting accuracy. • provide clear, concise updates to management on fleet kpis, performance issues, and improvement opportunities. • assist in preparing presentations and summary reports for monthly or quarterly fleet review meetings. • promote a culture of safety, accountability, and continuous improvement within fleet operations. our ideal candidate: • post-secondary education in business administration, data analytics, logistics, or a related field. • 2–4 years’ experience in fleet management, operations coordination, or data analysis (construction, logistics, or transportation preferred). • knowledge of erp systems, telematics, gps tracking, and fuel card programs. • familiarity with cvor compliance, insurance, and vehicle registration processes. • strong data analysis and reporting skills; advanced in excel and familiar with power bi or similar tools. • experience with fleet management or maintenance tracking software. • organized, detail-focused, and able to manage multiple deadlines. • excellent communication and collaboration skills across teams. benefits: • competitive salary with annual cost of living increases • annual bonus program • group rrsp with 100% matching • 100% company paid benefits • extended health care • dental care • vision care • disability insurance • vacation & personal days, plus paid holiday shutdown • parental leave top-up program • employee assistance program (eap) • life insurance • wellness programs • free on-site parking please, no agencies",canada,Data Analyst,"['dashboard', 'data analysis', 'data analytics', 'excel', 'power bi', 'r', 'recommendation']","['dashboard', 'data analysis', 'data analytics', 'excel', 'power bi', 'r', 'recommendation']",$60K–$70K a year
senior data analyst (business intelligence),forward financing,"overviewapplication forward financing is a financial technology company based in boston, massachusetts with team members throughout the united states, dominican republic, and canada. the company is on a mission to unlock the capital that fuels small businesses across america. recognized as a best place to work by built in boston and certified as a great place to work®, forward is investing in its employees, technology, and customer experience – with long-term success in mind every step of the way. as a senior data analyst on our analytics team, you will leverage the company’s data and modern data stack to provide valuable insights that enhance operations and business outcomes across all departments. you will report to the manager of business intelligence. • *why you should apply** - meaningful work. you will make a lasting impact, helping us achieve our mission of helping small businesses. you will see your work directly impact the decision making process of departments across forward financing, and contribute to the long term success of forward financing. - you love working with data! in this role, you will work with a variety of datasets to provide teams with deeper insight, enabling operational and strategic decisions. you will work with our modern data stack (fivetran + snowflake + dbt + tableau + census) to deliver a combination of automated reports and dashboards, self-service tools, and ad-hoc analyses. - flexibility is a top priority. our teams are empowered to do what works for them. this opportunity has 100% fully remote flexibility. • *in this role you will** - develop expertise in navigating, and generating insights from, the company’s internal databases (including salesforce). - partner with various departments to identify and outline data needs and requirements. - work with tableau and sql to build customized dashboards, visual analyses, and self-serve tools for key stakeholders. - contribute to work across the analytics department, including analytics engineering and data science. • *role requirements** (even if you don’t check every box, but see yourself contributing, please apply.) - bachelor's degree (or higher) in a quantitative discipline (e.g. computer science, statistics, mathematics, etc.) - 4+ years of experience in an analytics role - ability to perform complex joins and transformations on data using sql - experience building data visualizations with tableau or similar tools (e.g., looker, powerbi) - strong knowledge of excel (vlookup, index/match, pivot tables, calculations) - bonus: experience using snowflake, dbt, git, and python • *forward financing core values** - drive the mission - we believe in financial opportunity for underserved small businesses. we say “yes” when others say “no.” - keep it real - we value direct communication, candid feedback, and authenticity. we are an open book. - act with kindness - we create an environment where caring is cool and helping is the norm. we do the right thing. - shoot for extraordinary - we are inspired by innovative thinking and continuous improvement. we never settle for yesterday’s best. • *about us** forward financing is a financial technology company based in boston, massachusetts with team members throughout the united states, dominican republic, and canada. the company is on a mission to unlock the capital that fuels small businesses across america. whether facing challenges accessing traditional financing or simply needing a convenient, flexible solution, forward is committed to funding more of the millions of small businesses nationwide. forward offers revenue-based financing – delivering an upfront sum of working capital in exchange for a set amount of the business’s future revenue. by simplifying the requirements, streamlining the process, and using advanced proprietary technology, forward is often able to deliver funds same day – giving more businesses the financial opportunity they need to thrive. plus, with their dedicated teams and award-winning service, customers get personalized support when they need it most. since 2012, forward has expanded access to capital by providing over $4.1 billion in funding to more than 80,000 small businesses. the company is a+ rated by the better business bureau with an excellent / 4.7 stars rating on trustpilot.com. recognized as a best place to work by built in boston and certified as a great place to work®, forward is dedicated to empowering both its team and the customers they serve, helping them succeed and thrive. • *perks & benefits** at forward, our team members are at the heart of our company, and we are committed to taking care of them as people, not just employees. we offer a comprehensive benefits package, including but not limited to: medical, dental, vision, and commuter benefits, a flexible time-off policy, paid parental leave, 401k match for us employees, wellness reimbursement, volunteering days, annual professional development budget, and charitable donation match. workplace flexibility is a top priority at forward too. our employee choice policy means that almost all of our employees get to decide where they work. as a business, we are focused on impact; we are more concerned with your contributions to the success of the company than where you get your work done. if face-to-face time is desired, people managers are empowered to find a cadence that works for their team. when we aren’t collaborating to drive business and support our customers, we’re finding virtual and in-person ways to get to know our colleagues, celebrate team wins, and have fun together! • *us equal opportunity employment information** forward financing is proud to be an equal opportunity employer, and is committed to fostering a fair and inclusive culture built on a foundation of high performance and exceptional customer experience. with a laser focus on employee impact, we’re able to reduce biases and ensure the right people are in the right jobs to contribute to our mission. the varied perspectives of our people fuel innovation and make us a stronger team. by embracing what makes each of us unique, we create a supportive environment where people feel valued, accepted, and empowered to thrive. california employee privacy policy",canada,Data Analyst,"['business intelligence', 'dashboard', 'dbt', 'excel', 'looker', 'python', 'r', 'snowflake', 'sql', 'statistics', 'tableau']","['business intelligence', 'dashboard', 'dbt', 'excel', 'looker', 'python', 'r', 'snowflake', 'sql', 'statistics', 'tableau']",
"senior data analyst (sql/nosql, python)",trafilea,"about trafilea trafilea is a consumer tech platform for transformative brand growth. we’re building the ai growth engine that powers the next generation of consumer brands. with over $1b+ in cumulative revenue, 12m+ customers, and 500+ talents across 19 countries, we combine technology, growth marketing, and operational excellence to scale purpose-driven, digitally native brands. we own and operate our own digitally native brands (not an agency), with presence in walmart, nordstrom, and amazon, and a strong global d2c footprint. why trafilea we’re a tech-led ecommerce group scaling our own globally loved dtc brands, while helping ambitious talent grow just as fast. 🚀 we build and scale our own brands. 🦾 we invest in ai and automation like few others in ecom. 📈 we test fast, grow fast, and help you do the same. 🤝 be part of a dynamic, diverse, and talented global team. 🌍 100% remote, usd competitive salary, paid time off, and more. key responsibilities the mission of the data analyst is to leverage data, computational science, and technology to deliver insights that enhance business performance . by combining statistical analysis, data visualization, and knowledge of population segments, this role guides the business in optimizing decisions, marketing performance, and strategic outcomes. • support definition of project scope, objectives, and data requirements. • deliver insights and recommendations to marketing acquisition and stakeholders. • identify trends and growth opportunities through data interpretation. • partner with acquisition, data science, and marketing to ensure analytical alignment. • contribute to the design of methodologies for campaign testing, attribution, and econometric/statistical kpis. • work with bi teams to design and validate dashboards for digital performance. • support a/b tests, lift tests, simulations, forecasting, and ad-hoc analysis. • improve attribution/media mix methodologies through data analysis. • prepare technical reports, briefings, and presentations. • maintain clear documentation of acquisition methodologies and data science initiatives. • bachelor’s in computer science, engineering, statistics, economics, or related field. • 4+ years years in data analysis (sql/nosql & python) • proficient in sql & data warehouse environments. • strong python analytics skills. • data visualization expertise (tableau, dashboards). • strong analytical and quantitative skills. • ability to manage multiple projects simultaneously. • proactive, detail-oriented, and ownership-driven. • strong communicator with ability to simplify complexity. • enthusiastic about learning and applying new data technologies. • experience in marketing or e-commerce analytics (preferred).",canada,Data Analyst,"['dashboard', 'data analysis', 'data warehouse', 'excel', 'python', 'r', 'recommendation', 'sql', 'statistics', 'tableau']","['dashboard', 'data analysis', 'data warehouse', 'excel', 'python', 'r', 'recommendation', 'sql', 'statistics', 'tableau']",
online data analyst - estonian (ca),telus digital,"this a full remote job, the offer is available from: canada are you a detail-oriented individual with a passion for research and a good understanding of national and local geography? this freelance opportunity allows you to work at your own pace and from the comfort of your own home. a day in the life of an online data analyst: • in this role, you will be working on a project aimed at enhancing the content and quality of digital maps that are used by millions of people worldwide • completing research and evaluation tasks in a web-based environment such as verifying and comparing data, and determining the relevance and accuracy of information. join us today and be part of a dynamic and innovative team that is making a difference in the world! telus digital ai community our global ai community is a vibrant network of 1 million+ contributors from diverse backgrounds who help our customers collect, enhance, train, translate, and localize content to build better ai models. become part of our growing community and make an impact supporting the machine learning models of some of the world’s largest brands. qualification path no previous professional experience is required to apply to this role, however, working on this project will require you to pass the basic requirements and go through a standard assessment process. this is a part-time long-term project and your work will be subject to our standard quality assurance checks during the term of this agreement. basic requirements • full professional proficiency in estonian and english language • being a resident in canada or the last 2 consecutive years and having familiarity with current and historical business, media, sport, news, social media, and cultural affairs in canada • ability to follow guidelines and conduct online research using search engines, online maps, and website information • flexibility to work across a diverse set of task types, including maps, news, audio tasks, and relevance • daily access to a broadband internet connection, computer, and relevant software assessment in order to be hired into the program, you’ll take an open book qualification exam that will determine your suitability for the position and complete id verification. our team will provide you with guidelines and learning materials before your qualification exam. you will be required to complete the exam in a specific timeframe but at your convenience. equal opportunity all qualified applicants will receive consideration for a contractual relationship without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or protected veteran status. at telus digital ai, we are proud to offer equal opportunities and are committed to creating a diverse and inclusive community. all aspects of selection are based on applicants’ qualifications, merits, competence, and performance without regard to any characteristic related to diversity. contact email sourcingteam121@telusinternational.ai this offer from ""telus digital"" has been enriched by jobgether.com and got a 77% flex score.",canada,Data Analyst,"['machine learning', 'r']","['machine learning', 'r']",
remote latvian online data analyst; freelance,telus digital ai data solutions,"position: remote latvian online data analyst (freelance) a leading digital solutions company in canada seeks a detail-oriented sourcing analyst for a part-time, remote role. the ideal candidate should be proficient in latvian and english, have a solid understanding of canadian affairs, and be flexible with various tasks. this entry-level position involves improving the quality of digital maps through research and data evaluation. candidates must complete an open-book qualification exam. join us for this exciting freelance opportunity! #j-18808-ljbffr",canada,Data Analyst,['r'],['r'],$60K–$80K a year
data analyst - sql / bi (h/f),textnow,"textnow is looking for a motivated senior data analyst to join our analytics & insights team. you’ll drive data-informed decision-making across the organization by translating business problems into analytical solutions, designing insightful dashboards, and uncovering trends that shape strategic actions. this role is perfect for someone with strong analytical skills, deep business acumen, and a passion for using data to tell stories that inspire action. develop and maintain dashboards, reports, and data visualizations using tools like looker, tableau, power bi, or redash conduct ad hoc analyses to support product, marketing, and operations initiatives partner with data engineering teams to ensure data quality, integrity, and availability develop and maintain kpi frameworks and performance measurement systems assist in building scalable data models and automation pipelines translate business questions into data requirements and present insights and recommendations to senior leadership mentor junior analysts and foster a culture of data-driven decision-making bachelor’s degree in data science, statistics, mathematics, economics, computer science, or a related field (master’s preferred) 5+ years of experience in data analytics or business intelligence proficiency in sql and at least one programming language (e.g., python or r) experience with modern bi tools (looker, tableau, power bi, mode, or redash) strong understanding of a/b testing, statistical analysis, and data modeling excellent communication and storytelling skills with data attention to detail, analytical rigor, and curiosity for continuous improvement familiarity with machine learning concepts and predictive analytics understanding of etl processes and data warehousing fundamentals canada (cad): $103,700 – $140,300 us – national (usd): $114,800 – $155,300 final compensation will be determined based on a number of factors, including skills, experience, location, and on-the-job performance. we recognize that exceptional talent may fall outside of these ranges;",canada,Data Analyst,"['a/b testing', 'bi tools', 'business intelligence', 'dashboard', 'data analytics', 'etl', 'excel', 'looker', 'machine learning', 'power bi', 'python', 'r', 'recommendation', 'scala', 'sql', 'statistics', 'tableau']","['a/b testing', 'bi tools', 'business intelligence', 'dashboard', 'data analytics', 'etl', 'excel', 'looker', 'machine learning', 'power bi', 'python', 'r', 'recommendation', 'scala', 'sql', 'statistics', 'tableau']",$104K–$140K a year
business analysis consultant,confidential jobs,"business analytics consultant our client is looking for a detail-driven, solutions-focused analytics professional to support their corporate development team. in this role, you’ll enhance reporting, systems, and processes that drive revenue optimization, operational efficiency, and data-driven decisions across a global organization. key responsibilities • build and manage global performance dashboards that support forecasting and deliver actionable insights. • align financial reporting and performance management across corporate and commercial finance. • improve profitability and operational discipline through process audits and data-driven enhancements. • lead cross-functional projects to elevate bi, crm, and workflow systems. • coach team members and ensure high-quality project delivery. • analyze markets and performance trends to identify revenue and cost-saving opportunities. • partner with global leaders on scenario planning, financial modeling, and business cases. • maintain data integrity across regions and deliver advanced analytics to senior leadership. what you bring • degree in business, accounting, computer science, or related field. • 5–7 years in business development, forecasting, analytics, or statistical modeling. • strong understanding of business processes within complex organizations. • proficiency in sap, sql, vba, python, bi/bw, and advanced excel tools. • excellent communication skills and the ability to turn analysis into clear recommendations. • ability to manage multiple projects, work cross-functionally, and build influential relationships.",canada,Data Analyst,"['dashboard', 'excel', 'python', 'r', 'recommendation', 'sql']","['dashboard', 'excel', 'python', 'r', 'recommendation', 'sql']",
senior healthcare resarch & data analyst,clarivate,"overviewclarivate provides innovative data and analytical solutions to the largest biopharmaceutical and medical technology companies in the world. clarivate's medtech data team harnesses real-world healthcare data and identifies meaningful insights from large data and metadata sources to help medical device companies make some of their most important business decisions. we are seeking a senior analyst for our medtech data team with a background in healthcare or life sciences and strong quantitative analysis skills. the ability to effectively collaborate with internal colleagues, including product managers, sales leaders and technical experts is required. this is a heavy client-facing and solutions-oriented role where demonstrated communication skills will be put to the test in explaining complex ideas with customers. about you - experience, education, skills and accomplishments: bachelor's degree with a concentration in sciences, healthcare, data analytics, or business. at least 5 years of hands-on experience, utilizing sql and relational databases with both structured and unstructured data. proven experience working with complex data sets, including the ability to investigate data issues, identify trends/gaps, and translate findings into actionable insights. at least 4 years of experience with excel and sql, including advanced functions, and capable of working with large, multi-dimensional data sources. expertise in handling client requirementsit would be great if you have. an advanced degree - masters or mbafamiliarity with additional data languages (e. g. , python, r), which would help expand analytical capabilities and improve efficiency in working with complex healthcare datasetsexposure to or experience with project management responsibilities, including planning, coordination, and stakeholder communication across cross-functional teamsexpertise working on healthcare or lifesciences domain projects (pharmaceutical, medtech or medical devices)expertise in understanding data variables and connecting the dots in various datasets. expertise in handling and manipulating large datasets. proficiency with written and oral communications and must be able to communicate complex quantitative ideas to internal and external stakeholders. what will you be doing in this role?understand the worldview and pain points of clarivate customers, working closely with them as a problem-solver. from fortune 500 companies to smaller startups, you will converse with high-profile clients regularly and be expected to both present and defend conclusions to them. design, develop, and maintain processes and systems to analyze structured & unstructured 'big data' sources using tools like tableau, sql, python, and other analytic softwaredevelop and code software programs, algorithms and automated processes to cleanse, integrate and evaluate large datasets from multiple disparate sources with an emphasis on us medical devices and supplies markets. evaluate data outputs with a keen eye for details, proactively finding potential errors and omissions. identify opportunities and issues for data analysis and experiments, with bias towards driving customer delight. contribute your vision; influence the evolution of our products, data models, and data usage strategy. about the teamthe pricetrack team at clarivate provides market-leading insights into medtech product pricing and market dynamics. by leveraging deep industry expertise and robust data resources, the team delivers actionable intelligence that helps clients build winning pricing strategies, track market share, and stay ahead in a highly competitive landscape. hours of workfull-time permanentposition primarily working core business hours in your time zone, with flexibility to adjust to various global time zones as neededmust live within a commutable distance to our toronto, on office with a hybrid work schedule (2-3 days/week on site)at clarivate, we are committed to providing equal employment opportunities for all qualified persons with respect to hiring, compensation, promotion, training, and other terms, conditions, and privileges of employment. we comply with applicable laws and regulations governing non-discrimination in all locations. •",canada,Data Analyst,"['aws', 'data analysis', 'data analytics', 'excel', 'python', 'r', 'sql', 'tableau']","['aws', 'data analysis', 'data analytics', 'excel', 'python', 'r', 'sql', 'tableau']",
data analyst (mass appraisal) permanent and temporary assignment up to 18 months,mpac,do you want to be part of a team helping to strengthen every community across ontario are you interested in seeing the importance of your work firsthand become adata analyst (mass appraisal)- permanent and temporary assignment up to 18 monthswith mpac the municipal property assessment corporation (mpac) is made up of a team of experts who understand local communities and assess every property in ontario. what we do provides the very foundation that municipalities use to base the property taxes needed to pay for the services we use every day. equity diversity inclusion and anti-racism mpac is committed to equity diversity inclusion and anti-racism. we are taking important steps towards ensuring that all voices are respected valued and being heard. mpac is dedicated to fostering an environment where employees can bring their full unique and authentic selves and are inspired to do their best work. job description what makes you great for this role you are an agile individual who can develop valuation parameters using industry standard computer assisted mass appraisal (cama) techniques applying direct comparison approach to value for assigned market areas and property types. we have: • a team of highly skilled dedicated and collaborative staff to work with • leadership that supports you • an enterprise that embraces change • a work environment that has flexible work locations (including hybrid) hours of work and overall employee wellness support you have: • completion of post-secondary education in a related field • at least 3 years progressively responsible combined experience in at least one of the following areas: • computer assisted mass appraisal and valuation • multiple regression and statistical analysis • data analytics and computer programming • knowledge and experience in the application of various valuation tools including data collection manuals corporate databases and valuation systems spatial analysis and mpac policy and procedures • understanding of property valuation processes policies and procedures appraisal techniques and approaches to value market and economic factors that affect property types and familiarity with mpac technology-based products and services • demonstrated research analytical problem solving and logic skills to undertake comprehensive market analysis associated with mass appraisal process and systems • proficient use of various software programs and databases including statistical geospatial software programs (spss r python arcview qgis) microsoft office (e.g. word excel powerpoint) and databases including sql and nosql • good verbal and written communication and interpersonal skills to effectively prepare and communicate detailed valuation reports briefing notes and responses to client inquiries nice to have: • completion or working towards an appraisal designation/certificate from one of the following accrediting organizations: ubc real property assessment ima aic iaao iaao or rics what you will do: • apply valuation parameters to appropriate corporate database and valuation system to value the universe of properties in assigned market area and property type • identify the supply and demand factors that impact the value of real property through statistical analysis reviewing local economic indicators other geographical and spatial data • edit programming code (e.g. spss r python) to document market analysis maintain and create analytical data and associated output files • prepare narrative reports for internal and external use (e.g. market valuation reports) as well as public information and reports for ratepayers under the information and data sharing policy and freedom of information (foi) programs • value and ensure new parcels are valued correctly by assigning proper valuation neighbourhoods and required site information is identified • perform quality checks on property data and value estimates for new parcels to ensure corporate quality standards have been met identify issues and develop recommendations to resolved • support research initiatives to improve and enhance existing valuation systems and data leveraging open source technologies and advanced spatial and mass appraisal analytics • liaise with internal staff and legal counsel in order to provide support in the preparation for appeals involving the mass appraisal process interpreting sales ratio studies and other statistical interpretations additional information: • requisition id: 2812 • job type: union • closing date: november 20 2025 mpac has a number of office locationsthroughout ontario. as part of the hiring process the successful candidate may be placed at the mpac office in closest proximity to an individuals residence or a location as determined by mpac. ready to apply please note only applications submitted through the applicant tracking system will be accepted. successful candidates will be required to undergo a background verification with mintz global screening. by applying to this job posting you are providing your consent to mpac to share your name email address and phone number with mintz to conduct the criminal and driver abstract check and for mintz to disclose the results to mpac should you be the successful incumbent. mpac is committed to fostering an inclusive accessible environment where all employees and members of the public feel valued respected and supported. we are dedicated to building a workforce that reflect the diversity of the public and communities we serve. persons with disability who need accommodation in the application process or those needing job postings in an alternative format may email their request to we thank all applicants for their interest however only those selected for further consideration will be contacted. please note that mpac prohibits employees from holding a current/valid real estate license and successful applicants must at least suspend their real estate licenses during their period of employment at mpac as outlined in our employee code of conduct. job advertisements for positions that have been designated bilingual will be posted in both english and french on our website. positions that are not designated bilingual are not translated and are only posted in english on the english version of our website. stay connected: mpac on linkedin mpac on twitter mpac on youtube mpac on facebook required experience: ic,canada,Data Analyst,"['data analytics', 'excel', 'python', 'r', 'recommendation', 'regression', 'sql']","['data analytics', 'excel', 'python', 'r', 'recommendation', 'regression', 'sql']",
data analyst - customer operations and risk,pavarini construction inc,"pavarini construction co. is a full-service construction management and general contracting firm dedicated to providing outstanding client services. the role you will be responsible for : contributing to and driving continuous process improvement initiatives to meet business needs. establishing project plans, resources, budgets and time-frames, and assigning tasks. gathering, analysing, defining and formalising business requirements and processes into project / system specifications. identifying, tracking and communicating progress, milestones, deliverables, risks and issues. managing vendor relationships and deliverables. preparing project feasibility studies, cost-benefit analysis and proposals and obtain required approvals from it management and project sponsors. managing the team’s performance of project tasks and providing technical advice and guidance. managing a team of 1-5 people. ideal profile you have at least 1 year experience including solid experience in a similar role within real estate. you have good interpersonal and communication skills and are adept at working with multiple stakeholders to drive desired outcomes. you are highly goal driven and work well in fast paced environments you are a strong mentor and coach who builds high performing teams you possess strong analytical skills and are comfortable dealing with numerical data what's on offer? flexible working options opportunity to make a positive impact work in a company with a solid track record of performance",canada,Data Analyst,['r'],['r'],
senior data analyst - slots,zynga,"level up your career with zynga! at zynga, we bring people together through the power of play. as a global leader in interactive entertainment and a proud label of take-two interactive, our games have been downloaded over 6 billion times—connecting players in 175+ countries through fun, strategy, and a little friendly competition. from thrilling casino spins to epic strategy battles, mind-bending puzzles, and social word challenges, our diverse game portfolio has something for everyone. fan-favorites and latest hits include farmville™, words with friends™, zynga poker™, game of thrones slots casino™, wizard of oz slots™, hit it rich! slots™, wonka slots™, top eleven™, toon blast™, empires & puzzles™, merge dragons!™, csr racing™, harry potter: puzzles & spells™, match factory™, and color block jam™—plus many more! founded in 2007 and headquartered in california, our teams span north america, europe, and asia, working together to craft unforgettable gaming experiences. whether you're spinning, strategizing, matching, or competing, zynga is where fun meets innovation—and where you can take your career to the next level. join us and be part of the play! position overview join the slots analytics team wizard of oz slots is a beloved and iconic social casino game that brings the magic and adventure of the classic film to millions of players globally. as a senior data analyst on our team, you'll contribute to a visually stunning and highly engaging game that offers diverse slot experiences, captivating narratives, and a fun, communal experience for fans of the franchise. if you are passionate about combining sophisticated analysis with interactive entertainment, wizard of oz slots offers an unparalleled opportunity to influence a popular title in mobile gaming! wizard of oz slots is seeking a senior data analyst to help drive decisions that impact millions of players worldwide. at wizard of oz slots, analytics is central to our success. we drive business decisions and optimize the player journey by leveraging vast amounts of data and a/b testing. whether you're designing the analysis plan for a live-ops event, modeling the long-term impact of a new monetization feature on our virtual economy, or diving deep into player segmentation to improve retention and engagement, your work will directly shape the user experience. join us if you thrive on using data to solve complex, high-impact problems in a fast-paced environment here's how you'll shape the player experience • product strategy: analyze long and short-term trends to deliver insights that influence product direction • experimentation: design statistically sound experiments that drive measurable impact • business intelligence: maintain comprehensive dashboards and etl pipelines to keep the organization informed on game performance • game integrity: enhance fraud detection and bad actor models to maintain a healthy gaming ecosystem join us if you love solving analytical puzzles that impact millions! what you'll do • collaborate with the product team to enhance revenue, retention, and engagement metrics. this includes conducting a/b testing, performing insights analysis, and developing dashboards. • design and maintain etl pipelines to extract, transform, and load data from multiple sources, ensuring source of truth for analyses and business intelligence. • design game telemetry to capture meaningful player interaction data to improve the game experience. • analyze player behavior to understand motivations and preferences, partnering with game design and consumer insights teams to translate findings into actionable experience improvements. • actively play and engage with our games to maintain first-hand understanding of player experience, game mechanics, and potential pain points. • build, monitor, and maintain the integrity of a wide range of tools and models used to streamline workflows and personalize the player experience, in collaboration with other data analysts and data scientists on the team. • participate in and contribute to the analytics org’s ai initiative, which aims to significantly increase each analyst’s impact through the power of llms. • continuously learn about analytics best practices, and actively share and promote these practices across various games and teams. what you bring • bachelor's degree in computer science, mathematics, statistics, economics, or a related quantitative field. advanced degrees (masters or phd) are a plus. • minimum of 4 years in data science or analytics roles/2 years with masters or phd. • proficiency in python and sql. • demonstrated experience in data mining, data visualization, experimental design, statistical analysis, and machine learning. • familiarity with business intelligence (bi) tools like tableau and version control systems such as git is advantageous. • strong ability in both written and oral communication. • highly self-motivated with a keen interest in problem-solving. • willingness to take ownership of challenges. • proven ability to work collaboratively as part of a team. • familiarity with mobile games bonus points • experience/ interest in improving analytics team's processes and work efficiency using llms. • familiarity with slots gameplay and terminology the pay range for this position in california at the start of employment is expected to be between $108,900 and $161,160 per year. however, base pay offered is based on market location, and may vary further depending on individualized factors for job candidates, such as job-related knowledge, skills, experience, and other objective business considerations. subject to those same considerations, the total compensation package for this position may also include other elements, including a bonus and/or equity awards and eligibility to participate in our 401(k) plan and employee stock purchase program. regular, full-time employees are also eligible for a range of benefits at the company, including: medical, dental, vision, and basic life insurance coverage; 14 paid holidays per calendar year; paid vacation time per calendar year (ranging from 15 to 25 days) or eligibility to participate in the company’s discretionary time off program; up to 10 paid sick days per calendar year; paid parental and compassionate leave; wellbeing programs for mental health and other wellness support; family planning support through maven; commuter benefits; and reimbursements for fitness-related expenses. we are proud to be an equal opportunity employer, which means we are committed to creating and celebrating diverse thoughts, cultures, and backgrounds throughout our organization. employment with us is based on substantive ability, objective qualifications, and work ethic – not an individual’s race, creed, color, religion, sex or gender, gender identity or expression, sexual orientation, national origin or ancestry, alienage or citizenship status, physical or mental disability, pregnancy, age, genetic information, veteran status, marital status, status as a victim of domestic violence or sex offenses, reproductive health decision, or any other characteristics protected by applicable law. as an equal opportunity employer, we are committed to providing the necessary support and accommodation to qualified individuals with disabilities, health conditions, or impairments (subject to any local qualifying requirements) to ensure their full participation in the job application or interview process. please contact us at accommodationrequest@zynga.com to request any accommodations or for support related to your application for an open position. please be aware that zynga does not conduct job interviews or make job offers over third-party messaging apps such as telegram, whatsapp, or others. zynga also does not engage in any financial exchanges during the recruitment or onboarding process, and will never ask a candidate for their personal or financial information over an app or other unofficial chat channel. any attempt to do so may be the result of a scamp or phishing attack, and you should not engage. zynga’s in-house recruitment team will only contact individuals through their official company email addresses (i.e., via a zynga.com, naturalmotion.com, smallgiantgames.com, themavens.com, gram.gs email domain).",canada,Data Analyst,"['a/b testing', 'business intelligence', 'dashboard', 'etl', 'experimentation', 'machine learning', 'python', 'r', 'sql', 'statistics', 'tableau']","['a/b testing', 'business intelligence', 'dashboard', 'etl', 'experimentation', 'machine learning', 'python', 'r', 'sql', 'statistics', 'tableau']",
data analyst placement programme,itol recruit,"please note this is a training course and fees apply are you looking to benefit from a new career in data analysis? if you are detail orientated, perceptive, organised, competent, analytical and can communicate well with those around you; you could have a truly rewarding future as a data analyst we do this using our specialised data analyst career programme which looks to assist and place qualified candidates into a career pathway in data analysis. please note this career program is designed for entry level individuals with limited or no experience, so please do not apply if you are already an experience data analyst as we will be looking primarily at entry level roles. demand for data analysts has grown 20% year on year with experienced analysts easily commanding salaries of 50k+. all business decisions rely on data to ensure correct business decisions are made and therefore the role of the data analyst in the new digital world has become essential for business owners. below are current average salaries in the sector for lower-level positions and fully trained data analysts: • junior data analyst - 30,000 • data analyst - 50,000 • business data analyst - 67,500 • data analytics analyst - 80,000 • business analysts - 60,000 using our experience in providing data analysis and business analysis training online and through our expert recruitment consultants, we can provide a seamless journey and often fast-tracked route into a new career in data analysis. the courses in the package have been identified by our recruitment partners as industry standards for the uk/european and international data analysis sector. skills shortages across all sectors are increasing the demand for qualified, entry-level career seekers and career changers. whether you are working full-time, part-time or are unemployed, this job and recruitment package has the flexibility to be completed at a pace that suits you and can be completed in a few weeks. your job and career goals are completed in 4 easy steps listed below. step 1 - comptia data+ qualification the first step is completing the comptia data+ qualification accredited by comptia. this professional international industry-recognised qualification teaches you the essential skills of a data analyst. you will cover skills such as data mining, manipulating data, visualising data, and reporting on data. study time for the qualification is approximately 30 hours and qualification is achieved by completing a 1-hour multiple choice exam. the course is provided online and comes complete with exam simulators and revision tools. you will be provided with access to a tutor and a 1-hour online revision workshop prior to sitting your exam. this qualification will set you on a route to becoming a data analyst. step 2 - data administration training data is extracted using various method which are normally it driven. therefore, we will provide you with access to the following additional courses to ensure you are suitably employable as a data analyst. • microsoft excel - to expert level. • learn sql - the programming language used for extracting data from more complex databases. • learn python 3 - python 3 is a simple to use programming language used by many analysts. • learn tableau - tableu is a tool which analysts use to build visually appealing dashboards for complex data making it easier for stakeholders to comprehend. study time for these courses is approximately 30-60 hours and qualification is achieved through completion of the course with no exam. step 3 - business analysis foundation business analysts and data analysts often work very closely together and indeed many data analysts become business analysts and vice versa. adding a business analyst foundation certificate will make you much more employable and enable you to cross over into business analyst roles and hybrid roles. business analysis foundation accredited by the bcs (the british computer society also known as the chartered institute of it). study time for this course is approximately 15 hours and qualification is achieved through an online bcs exam. step 4 - recruitment support now you are qualified for an entry level data analyst position our recruitment support team will now begin collaborating with you to help you secure your first a suitable role based on your new qualifications and any other experience you may have. we have been helping candidates start and build careers in the project management industry since 2007 and have a 4.8 trustscore on trustpilot. our recruitment support team will help you work through job applications, interviews and provide you with a full cv review based around your new qualification and written to maximise your chances of obtaining a role in the project management sector. they will provide you with guidance as to which roles our most suitable for you as an entry level project sector worker aiming to become a project manager. this will include mock job interviews as well as any help you feel you need. our money back guarantee if after 1 year of passing your formal qualifications, we have been unable to help you secure a role, we will refund your study fees minus the cost of the exams. however, we are normally able to help candidates find their first role within 6 months of qualifying and for locations based close to major cities this is often reduced to less than 3 months. check our testimonials from the hundreds of candidates we have already helped. what now? to accommodate candidates, the training element of the package is available on finance terms of up to 1 year. this enables you to get qualified and start in your new role without having to fund all the training costs up front. enquire now and one of our experienced career consultants will contact you within 4 working hours to answer any questions that you may have and to assist you in taking the first step towards your data analyst career aspirations.",united kingdom,Data Analyst,"['dashboard', 'data analysis', 'data analytics', 'excel', 'python', 'r', 'sql', 'tableau']","['dashboard', 'data analysis', 'data analytics', 'excel', 'python', 'r', 'sql', 'tableau']",
data analyst training programme (not a job — training with job guarantee,uptrail,"location: remote (uk-based applicants only)we are offering a flexible 4–6 month online training pathway designed to prepare individuals for entry-level data analyst roles. the programme can be completed part-time or full-time depending on your pace. you will gain practical skills in excel, sql, python, power bi, data visualisation, business intelligence, and real-world portfolio projects used for job applications. no prior experience is required, training starts from beginner level.career support is included, with cv and linkedin optimisation, mock interviews, job search guidance, mentor-led sessions, and access to an online peer and alumni community. a job guarantee is provided after completion, if you do not secure a data analyst role within the agreed timeframe after graduating, your full programme fee will be refunded (terms explained during the application stage).this opportunity is suitable for graduates, career changers, and working professionals seeking to enter the data industry. to be eligible, you must be based in the uk, have the right to work in the uk, possess basic computer skills, and be able to commit at least 10 hours per week. the programme is remote and self-paced, offering flexible study hours.programme summary:duration: 4–6 months totalstudy format: remote (uk-only)schedule: part-time or full-timejob support: up to 12 monthsjob guarantee: after graduationtraining content: data analysis tools and practical projectsindustry relevance: focused on real employer requirementsfees: payment plans available (full details provided during screening)how to apply:click apply<",glasgow,Data Analyst,"['business intelligence', 'data analysis', 'excel', 'power bi', 'python', 'r', 'sql']","['business intelligence', 'data analysis', 'excel', 'power bi', 'python', 'r', 'sql']",£27k–£35k a year
project data analyst,tria recruitment,"project data analyst up to £450/day (inside ir35) central london - 2-3x per week 6-month contract are you a data analyst with experience working within an it project environment? do you have strong excel skills as well as some power bi reporting experience? the project analyst will join a household name at a period of significant modernisation across the entire it estate. you'll join a busy it project team that is delivering key projects across cloud, infrastructure and euc. the role will suit a detail-oriented and analytically minded excel & power bi specialist who will be developing dashboards, streamlining reporting processes, analysing data, and supporting business decision-making. we are looking for someone with prior experience working in large, complex it project environments. reporting & data support assist in creating and updating power bi dashboards and excel reports. run routine reports and ensure data is accurate and up to date. help organize and clean datasets for analysis. support stakeholders with basic data requests and reports. data preparation use excel (including pivot tables and formulas) to analyse and prepare data. perform basic data transformations in power query. help maintain source data files and documentation. dashboard & visualization assistance contribute to building simple power bi visualizations and layouts. make updates based on feedback from stakeholders ensure dashboards are easy to understand and visually consistent. process improvement identify opportunities to simplify manual reporting tasks. support basic automation efforts with power query or templates. document updates, steps, and processes clearly. for further information, please apply today!",united kingdom,Data Analyst,"['cloud', 'dashboard', 'excel', 'power bi', 'r']","['cloud', 'dashboard', 'excel', 'power bi', 'r']",£450 a year
fully remote work: online data analyst - latvian speakers in uk,telus digital,"are you a detail-oriented individual with a passion for research and a good understanding of national and local geography? this freelance opportunity allows you to work at your own pace and from the comfort of your own home. tasks a day in the life of an online data analyst: • in this role, you will be working on a project aimed at enhancing the content and quality of digital maps that are used by millions of people worldwide • completing research and evaluation tasks in a web-based environment such as verifying and comparing data, and determining the relevance and accuracy of information. join us today and be part of a dynamic and innovative team that is making a difference in the world! requirements qualification path no previous professional experience is required to apply to this role, however, working on this project will require you to pass the basic requirements and go through a standard assessment process. this is a part-time long-term project and your work will be subject to our standard quality assurance checks during the term of this agreement. basic requirements • full professional proficiency in latvian and english language • being a resident in the united kingdom or the last 2 consecutive years and having familiarity with current and historical business, media, sport, news, social media, and cultural affairs in the united kingdom • ability to follow guidelines and conduct online research using search engines, online maps, and website information • flexibility to work across a diverse set of task types, including maps, news, audio tasks, and relevance • daily access to a broadband internet connection, computer, and relevant software assessment in order to be hired into the program, you’ll take an open book qualification exam that will determine your suitability for the position and complete id verification. our team will provide you with guidelines and learning materials before your qualification exam. you will be required to complete the exam in a specific timeframe but at your convenience. benefits • earn extra income working remotely from the comfort of your own home • have the freedom to choose your own working hours to suit your own lifestyle • be a part of a community and access our well-being initiatives • contribute to the development of the ai ecosystem. equal opportunity all qualified applicants will receive consideration for a contractual relationship without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or protected veteran status. at telus digital ai, we are proud to offer equal opportunities and are committed to creating a diverse and inclusive community. all aspects of selection are based on applicants’ qualifications, merits, competence, and performance without regard to any characteristic related to diversity.",anywhere,Data Analyst,['r'],['r'],
data analyst trainee,itol recruit,"are you looking to benefit from a new career in data analysis? if you are detail orientated, perceptive, organised, competent, analytical and can communicate well with those around you; you could have a truly rewarding future as a data analyst we do this using our specialised data analyst career programme which looks to assist and place qualified candidates into a career pathway in data analysis. please note this career program is designed for entry level individuals with limited or no experience, so please do not apply if you are already an experience data analyst as we will be looking primarily at entry level roles. demand for data analysts has grown 20% year on year with experienced analysts easily commanding salaries of £50k+. all business decisions rely on data to ensure correct business decisions are made and therefore the role of the data analyst in the new digital world has become essential for business owners. below are current average salaries in the sector for lower-level positions and fully trained data analysts: • junior data analyst - £30,000 • data analyst - £50,000 • business data analyst - £67,500 • data analytics analyst - £80,000 • business analysts - £60,000 using our experience in providing data analysis and business analysis training online and through our expert recruitment consultants, we can provide a seamless journey and often fast-tracked route into a new career in data analysis. the courses in the package have been identified by our recruitment partners as industry standards for the uk/european and international data analysis sector. skills shortages across all sectors are increasing the demand for qualified, entry-level career seekers and career changers. whether you are working full-time, part-time or are unemployed, this job and recruitment package has the flexibility to be completed at a pace that suits you and can be completed in a few weeks. your job and career goals are completed in 4 easy steps listed below. step 1 - comptia data+ qualification the first step is completing the comptia data+ qualification accredited by comptia. this professional international industry-recognised qualification teaches you the essential skills of a data analyst. you will cover skills such as data mining, manipulating data, visualising data, and reporting on data. study time for the qualification is approximately 30 hours and qualification is achieved by completing a 1-hour multiple choice exam. the course is provided online and comes complete with exam simulators and revision tools. you will be provided with access to a tutor and a 1-hour online revision workshop prior to sitting your exam. this qualification will set you on a route to becoming a data analyst. step 2 - data administration training data is extracted using various method which are normally it driven. therefore, we will provide you with access to the following additional courses to ensure you are suitably employable as a data analyst. • microsoft excel - to expert level. • learn sql - the programming language used for extracting data from more complex databases. • learn python 3 - python 3 is a simple to use programming language used by many analysts. • learn tableau - tableu is a tool which analysts use to build visually appealing dashboards for complex data making it easier for stakeholders to comprehend. study time for these courses is approximately 30-60 hours and qualification is achieved through completion of the course with no exam. step 3 - business analysis foundation business analysts and data analysts often work very closely together and indeed many data analysts become business analysts and vice versa. adding a business analyst foundation certificate will make you much more employable and enable you to cross over into business analyst roles and hybrid roles. business analysis foundation accredited by the bcs (the british computer society also known as the chartered institute of it). study time for this course is approximately 15 hours and qualification is achieved through an online bcs exam. step 4 - recruitment support now you are qualified for an entry level data analyst position our recruitment support team will now begin collaborating with you to help you secure your first a suitable role based on your new qualifications and any other experience you may have. we have been helping candidates start and build careers in the project management industry since 2007 and have a 4.8 trustscore on trustpilot. our recruitment support team will help you work through job applications, interviews and provide you with a full cv review based around your new qualification and written to maximise your chances of obtaining a role in the project management sector. they will provide you with guidance as to which roles our most suitable for you as an entry level project sector worker aiming to become a project manager. this will include mock job interviews as well as any help you feel you need. our money back guarantee if after 1 year of passing your formal qualifications, we have been unable to help you secure a role, we will refund your study fees minus the cost of the exams. however, we are normally able to help candidates find their first role within 6 months of qualifying and for locations based close to major cities this is often reduced to less than 3 months. check our testimonials from the hundreds of candidates we have already helped. what now? to accommodate candidates, the training element of the package is available on finance terms of up to 1 year. this enables you to get qualified and start in your new role without having to fund all the training costs up front. enquire now and one of our experienced career consultants will contact you within 4 working hours to answer any questions that you may have and to assist you in taking the first step towards your data analyst career aspirations. please note this is a training course and fees apply",midlothian,Data Analyst,"['dashboard', 'data analysis', 'data analytics', 'excel', 'python', 'r', 'sql', 'tableau']","['dashboard', 'data analysis', 'data analytics', 'excel', 'python', 'r', 'sql', 'tableau']",
service improvement – data analyst,baltimore consulting ltd,"service improvement – data analyst contract length: 3-month initial interim contract location: east midlands local authority full-time opportunity hybrid working pay rate: £350 per day ir35 status: inside ir35 i am currently working with an east midlands based local authority in their search for an experienced service improvement – data analyst for an initial 3-month interim contract. this role sits within the adult social care service improvement team and will play a key part in driving performance insight, data-led improvements and transformation across asc services. interviews to take place as soon as possible. your duties as a service improvement – data officer: • lead the development and delivery of asc performance information, ensuring accurate insight into current performance, projected outturns and progress against service plans. • develop dashboards and balanced scorecards to present key performance indicators including sickness, h&s, workforce indicators and business plan progress. • design and implement information management systems that support effective data analysis, evidence-based research and asc service improvement. • work closely with service areas to analyse complex datasets, identify best practice and drive improvements through meaningful performance insight. • identify, assess and manage risks and issues, producing timely exception reports and recommendations. • create and maintain project documentation, ensuring robust business cases and justification for ongoing service improvement projects. • support service managers in developing stretching kpis and growth targets aligned with asc business plans. • utilise market intelligence, legislation and external benchmarking to identify opportunities for service development and efficiency. • lead and coordinate regular performance review meetings. • undertake specific service improvement projects as required. experience needed as a service improvement – data officer: • strong experience producing accurate, clear and concise performance information within local authority settings—ideally adult social care. • demonstrable experience in performance management, quality assurance frameworks and driving service improvement. • ability to work confidently with quantitative and qualitative data, interpreting findings to inform strategic decision making. • highly proficient in it systems, including extensive use of performance software and strong skills across ms excel, access, powerpoint and word. • ability to interrogate management systems to extract data, build reports and provide meaningful insights. • excellent organisational skills with the ability to manage competing deadlines in a fast-paced environment. • strong communication and stakeholder engagement skills, able to influence senior managers and work collaboratively across teams. • self-motivated, resilient and able to work independently within an evolving, improvement-focused environment. if you are interested in this opportunity or know a colleague who may be suitable, please contact me asap for more information. alternatively, if this role doesn’t match your skill set and you’re proactively looking for a new assignment, please send an updated cv or contact me on 0117 313 7110 and i’d be happy to connect you with the most relevant specialist consultant. (we offer a senior referral scheme upon successful placement of your recommendation.) baltimore consulting group is an executive & senior appointments specialist. we provide pivotal recruitment consultancy services to public sector clients across the uk. i personally recruit within the health & social care market, so if this role is not of interest but you would like to discuss other opportunities, please get in touch. our clients are passionate about fostering a diverse workforce and do not discriminate against any employee or applicant. recruitment decisions are based on experience and skills, and any reasonable adjustments required during the hiring process will be fully supported.",united kingdom,Data Analyst,"['dashboard', 'data analysis', 'excel', 'r', 'recommendation']","['dashboard', 'data analysis', 'excel', 'r', 'recommendation']",
data analyst,the great & the good,"senior data analyst are you an ambitious and curious-minded senior data analyst ready to drive strategic growth for high-profile brands? about the company our client is a major creatively-driven advertising agency that helps world class brands drive significant business advantage. at the heart of the operation is an exceptional data and strategy team that helps brands understand their worlds and ultimately influences informed, intelligent and effective marketing strategy. the agency is one of the most creatively awarded shops in london: you will most definitely definitely know their work. location beautiful offices in central london. the opportunity the role of data analyst sits integrally at the intersection of numbers, client strategy, and e-commerce: transforming complex data into compelling commercial action. you will be the go-to expert for understanding real-time brand sales activity, campaign effectiveness and leading the data narrative for key accounts (including: automotive and retail). your role • understand the data: analyse market trends, consumer behaviour, and competitor activity • bring the data to life: develop, create, and optimise detailed trading plans to maximise sales activity and roi. • help everyone else understand the data: deliver clear presentations and proactive recommendations to senior client contacts and agency leadership. • manage the data: manage and deliver critical weekly reports, fulfilling ad hoc client requests that feed directly into key business decisions. • make the data meaningful: work closely with the wider agency teams to build and implement integrated campaign briefs. you a driven, detail-focused individual who thrives in a busy environment. you are comfortable in the weeds of complex data but capable of seeing the big picture and telling a persuasive commercial story. experience in retail, automotive or telcos sectors would be useful. • you bring recent experience as a data analyst, either from an agency or in-house. • you have deep understanding of how data demonstrates consumer behaviour and a genuine interest for all things related to marketing and consumer data insights. • exceptional analytical skills with proven experience translating complex data set • naturally, you bring strong proficiency in excel. experience with the following will be a bonus: • sql • power bi, tableau • brandwatch, sprout • great communication skills: proven ability to build strong working relationships, and happy presenting data to clients and colleagues. the great & the good represents talented people who love what they do. we recruit for the great and the good of the marketing, pr, digital, advertising and creative industries.",united kingdom,Data Analyst,"['excel', 'power bi', 'r', 'recommendation', 'sql', 'tableau']","['excel', 'power bi', 'r', 'recommendation', 'sql', 'tableau']",
senior data analyst - crm,wise,"senior data analyst - crm wise wisefinancial services senior data analyst - crm united kingdom , londonapply now wise is a global technology company, building the best way to move and manage the world’s money. min fees. max ease. full speed. whether people and businesses are sending money to another country, spending abroad, or making and receiving international payments, wise is on a mission to make their lives easier and save them money. as part of our team, you will be helping us create an entirely new network for the world's money. for everyone, everywhere. more about our mission and what we offer. we are seeking an experienced data analyst to join our crm analytics team, supporting both personal and business customer segments. as a key member of the marketing analytics organisation, you will report to the senior manager of crm analytics and play a critical role in driving our lifecycle marketing capabilities through rigorous data analysis and actionable insights. this is a high-impact role where you'll collaborate closely with our crm, product, data science, and growth teams. you will help define and execute the measurement framework for crm initiatives, ensure data quality is top-notch, and provide analytical expertise to truly optimise customer engagement strategies. your work will directly influence how we acquire, engage, and retain customers by building and running data-driven crm programs. key responsibilities: • translate complex business questions into clear analytical frameworks, delivering insights that directly drive strategic decisions. • design and execute in-depth analyses to evaluate campaign performance, customer segmentation, and engagement patterns across the customer lifecycle. • apply advanced testing methodologies (a/b testing, incrementality) to measure and significantly improve the effectiveness of crm campaigns. • develop and maintain critical dashboards and reporting tools that track key crm metrics and provide actionable insights for marketing teams. • collaborate closely with analytics engineering, crm technology, and product teams to ensure data integrity and help define requirements for new initiatives. • build and optimise customer segmentation models to enhance targeting and personalisation strategies. • contribute to the development of the overall crm analytics strategy and measurement framework. required qualifications: • proven, hands-on experience in marketing analytics, specifically within crm or lifecycle marketing domains. • you'll need a strong technical skillset, especially advanced sql proficiency to handle complex data manipulation in a modern data warehouse (like snowflake). • experience with data modeling (dbt) and visualisation tools to create insightful and easy-to-digest dashboards and reports. • comfortable with a programming language such as python or r for data analysis. • solid understanding of experimental design and statistical testing methodologies. • excellent communication skills with the ability to translate complex analyses into clear, actionable business recommendations. • experience working cross-functionally with marketing and product teams. preferred qualifications: • knowledge of marketing automation platforms and crm tools. • familiarity with various customer segmentation techniques and personalisation frameworks. qualifications what do we offer: • salary: £60,000 - £75,000 • company restricted stock units • numerous great benefits in our london office key benefits: • hybrid working • paid annual holiday, sick days, parental leave and other leave opportunities • 6 weeks of paid sabbatical after 4 years at wise on top of annual leave additional information for everyone, everywhere. we're people building money without borders — without judgement or prejudice, too. we believe teams are strongest when they are diverse, equitable and inclusive. we're proud to have a truly international team, and we celebrate our differences. inclusive teams help us live our values and make sure every wiser feels respected, empowered to contribute towards our mission and able to progress in their careers. if you want to find out more about what it's like to work at wise visit wise.jobs. keep up to date with life at wise by following us on linkedin and instagram. create an accountor log in to save this jobcreate an accountlogin",united kingdom,Data Analyst,"['a/b testing', 'dashboard', 'data analysis', 'data warehouse', 'dbt', 'excel', 'python', 'r', 'recommendation', 'snowflake', 'sql']","['a/b testing', 'dashboard', 'data analysis', 'data warehouse', 'dbt', 'excel', 'python', 'r', 'recommendation', 'snowflake', 'sql']",
remote ai data analyst,jobs ai,"employment type: remote (part-time/contract) compensation: estimated range: usd 5,000-8,000 per month, depending on experience, project scope, and performance expectations for a full-time schedule. about the role as an ai data analyst, you will play a vital role in supporting various operational, research, and ai initiatives. your contributions will ensure efficient workflows and project delivery within a collaborative remote team environment. key responsibilities • contribute to project-related tasks including data entry, content creation, online research, analysis, and ai-output review. • maintain and update information meticulously to ensure accuracy. • communicate effectively with team members, providing regular updates on your tasks and progress. skills & qualifications • strong proficiency in written english and professional communication. • familiarity with digital tools such as spreadsheets and project management platforms. • analytical skills with a keen attention to detail. • self-motivated with the ability to prioritize tasks and work independently. • a genuine interest in research, content operations, or data management is a plus; training will be provided. what we offer • flexible remote work arrangements with a schedule that aligns with team needs. • weekly payments through secure methods, with transparent compensation structures. • opportunities for skill development in research, content operations, and data workflows. • a supportive and encouraging work culture that fosters learning and professional growth. (legal authorization to work in your country is required.) we welcome candidates from diverse backgrounds and evaluate applicants based on qualifications, skills, and business requirements, adhering to applicable employment laws. apply now to join our team!",united kingdom,Data Analyst,"['aws', 'r']","['aws', 'r']",
advanced analytics analyst - sql,datatech analytics,"data analyst - perm & ftc opportunity available middlesex offices/home - hybrid working 3 days p/w in the office salaries in the region of £35,000 - 50,000 doe j13028 candidates must have working rights without sponsorship requirements fantastic opportunity for a graduate with 1 + years’ proven commercial application experience in an analyst role to join a highly respected global company. candidate needs to have proven skills with sql, analysing large data sets and working collaboratively as part of a team. degrees such as mathematics, statistics, physics, morse, economics etc are suitable - with a strong numerical and problem solving content. you should have experience delivering business insights and stakeholder engagement. you will be expected to provide analytical expertise and communicate technical data to non-technical audiences to develop the data agenda in line with business priorities. a positive thinker with plenty of curiosity would be ideal for this role. duties • partner with core business areas to gain a deep understanding of their data, reporting, visualisation and analysis needs • manage a portfolio of dashboards, visualisations and data sources, and the continuous improvement of these • deliver robust and accurate data sets and visualisations within expected timescales • structure problems and design and develop numerical models to inform decisions • proactively consult and bring together multiple stakeholders and gain buy-in to ideas and approaches • provide deep insight for critical business questions using a variety of analytical tools e.g., sql, tableau, python and excel skills • proven advanced analytical skills • comfortable challenging and influencing senior management with conflicting views • creativity in recommending solutions and commitment to driving delivery • proven ability to lead the direction of analytical projects • excellent presentation and communication skills • strong business acumen and commercial awareness • proven technical skills including sql, excel and python (or similar) • experience in design and creation of data visualisations and dashboards e.g., tableau experience • 1+ year in an analyst role • analysing complex issues, packaging findings and presenting effectively to stakeholders • managing databases and/or blending data • designing data for management information purposes • visualising data and presenting trends and findings for broad audiences if this sounds like the role for you then please apply today! alternatively, you can refer a friend or colleague by taking part in our fantastic referral schemes! if you have a friend or colleague who would be interested in this role, please refer them to us. for each relevant candidate that you introduce to us (there is no limit) and we place, you will be entitled to our general gift/voucher scheme. datatech is one of the uk’s leading recruitment agencies in the field of analytics and host of the critically acclaimed event, women in data. for more information, visit our website: www.datatech.org.uk",united kingdom,Data Analyst,"['dashboard', 'excel', 'python', 'r', 'sql', 'statistics', 'tableau']","['dashboard', 'excel', 'python', 'r', 'sql', 'statistics', 'tableau']",
data analyst,sse,"base location: glasgow or eurocentral glasgow-motherwell salary: £38,966 - £45,905 + a range of benefits to support your finances, wellbeing and family. working pattern: permanent | full time | flexible first options available the role the procurement & commercial (p&c) function is on a journey to become a leader in data-driven decision-making. our goal is to embed trusted, intelligent, and connected data services into everyday workflows enabling smarter, faster and more sustainable operations. the data & insights team plays a pivotal role in this transformation. we've already delivered high-impact initiatives like the datahub (our central reporting & insights platform), a comprehensive data governance programme and launched ai solutions to business challenges. to scale our impact and meet growing demand from our users, we're expanding our team. you will • deliver tailored insights through dashboards, kpis and ad hoc analysis that support strategic sourcing, supplier management, and commercial performance. • collaborate with stakeholders across procurement, commercial and it teams to understand user needs and translate them into data solutions. • support delivery of the datahub roadmap, including the rollout of tailored role-focused dashboards and integration of external data and intelligence sources. • contribute to data governance, helping improve data quality, stewardship and metadata management across procurement domains. • design, manage, and optimise data workflows in databricks, building and maintaining robust data catalogues and scalable, automated pipelines that enable reporting, analytics, and advanced insights across the entire procurement lifecycle. • enable a data-driven culture by supporting literacy initiatives and acting as a trusted partner to users across the department. you have we’re looking for individuals who combine technical capability with strong business acumen and a collaborative mindset: • excellent communication skills with proven ability to engage stakeholders effectively. • strong experience in data analysis, reporting, and business intelligence using tools like power bi and excel. • skilled in python and sql for data transformation and handling large datasets. • ability to translate complex data into clear insights and compelling data stories. • knowledge of data governance, quality management and a creative problem-solving approach. about sse sse has a bold ambition – to be a leading energy company in a net zero world. we're investing around £10 million a day in homegrown energy to help power a cleaner, more secure future. our investment will see us build the world's largest offshore wind farm and transform the grid to deliver greener electricity to millions. our procurement & commercial teams help us get the best service and value from partners, while ensuring suppliers meet compliance, contractual and business obligations. flexible benefits to fit your life enjoy discounts on private healthcare and gym memberships. wellbeing benefits like a free online gp and 24/7 counselling service. interest-free loans on tech and transport season tickets, or a new bike with our cycle to work scheme. as well as generous family entitlements such as maternity and adoption pay, and paternity leave. work with an equal opportunity employer sse will make any reasonable adjustments you need to ensure that your application and experience with us is positive. please contact francesca.palmer@sse.com / 01738 344763 to discuss how we can support you. we're dedicated to fostering an open and inclusive workplace where people from all backgrounds can thrive. we create equal opportunities for everyone to succeed and especially welcome applications from those who may not be well represented in our workforce or industry. ready to apply? start your online application using the apply now box on this page. we only accept applications made online. we'll be in touch after the closing date to let you know if we'll be taking your application further. if you're offered a role with sse, you'll need to complete a criminality check and a credit check before you start work. #li-ft1 #li-hybrid",renfrew,Data Analyst,"['business intelligence', 'dashboard', 'data analysis', 'databricks', 'excel', 'power bi', 'python', 'r', 'scala', 'sql']","['business intelligence', 'dashboard', 'data analysis', 'databricks', 'excel', 'power bi', 'python', 'r', 'scala', 'sql']",
data analyst programmer,the university of edinburgh,"full time: 35 hours per week fixed term: 42 months the opportunity we are seeking a methodological, rigorous and service-oriented individual to support data processing in sebi-l and provide data support services to deliver to the gates foundation. the post-holder will write code that is scalable, tested and validated to process project data. this post is full-time (35 hours per week) and we are open to considering requests for hybrid working (on a non-contractual basis) that combines a mix of remote and regular on-campus working. the post holder is expected to attend our weekly in-person team meetings at easter bush campus. the post is available on a 42-month fixed-term contract basis. sebi-livestock is a dynamic and innovative organisation tasked with improving data and evidence in low- and middle-income countries to support better informed decision making. we provide an extensive data driven monitoring and learning service to the gates foundation in tracking their livestock investments. in addition, we convene a large livestock data network, livestock data for decisions (ld4d) to help achieve common goals. the postholder will work on building a scalable, tested and validated data pipeline to process livestock and aquaculture data. this role will play a crucial part in supporting the sebi-l team in transforming and importing livestock data into the sebi-l platform. responsibilities will include: • the writing, running and maintenance of scripts to automate and test data processing workflows; • ensuring code is version controlled, validated and robust in collaboration with other team members; • supporting the automating and running of livestock models and analyses. within this role, there is an opportunity to develop data visualization skills and to work on projects in machine learning and natural language processing. this position is based within sebi-livestock, the centre for supporting evidence based interventions in livestock, which is hosted by the roslin institute and royal (dick) school of veterinary studies, university of edinburgh. sebi-livestock offers a collaborative, fast-paced environment with opportunities for learning, growth and the chance to contribute to addressing real-world challenges. your skills and attributes for success: • educated to degree level in a numerical discipline such as bioinformatics, computer science, mathematics or equivalent; • demonstrable experience of r programming and modular code; • basic knowledge and experience of python and sql databases; • good knowledge and experience of data science technologies and of good software engineering practices including use of version control and implementation of unit and integration tests; • an interest in exploring the application of new technologies including natural language processing and machine learning apply before: 05/01/2026, 23:59 £34,610 to £39,906 per annum",anywhere,Data Analyst,"['data pipeline', 'machine learning', 'natural language processing', 'python', 'r', 'scala', 'sql']","['data pipeline', 'machine learning', 'natural language processing', 'python', 'r', 'scala', 'sql']","£34,610–£39,906 a year"
workforce data analyst,harvey nash uk,"data workforce analyst | 6 month contract | (inside ir35) | hybrid, 40% - glasgow/durham/london | starting asap day rate: £400 about the role: to maintain accurate, trusting resource data and provide high-quality reporting that supports the transformation programme in the client's decision making for capacity and capability management and workforce planning. the role is delivery-focussed, supporting the programme resource manager by ensuring the right information is captured, validated and on time. you'll be working in a small and friendly team (programme manager, workforce planning manager and hr manager). this is an exciting and fast paced role to benefit the improvement of the clients transformation programme resourcing data and reporting - to get it accurate to support strategic decisions linked to capacity and capability management and workforce planning. main duties • maintain, update and quality assure resourcing data across multiple workstreams, ensuring accuracy and consistency. in turn, maintain version control, audit trails and clear documentation of changes to resourcing data • gather resourcing updates from programme managers, team leaders and suppliers to keep workforce and capacity data current • manage excel resourcing data to support integration of data across power bi and power automate workflow • prepare resourcing reports, dashboards and mi within excel and power bi to support stakeholders i.e. resource manager, finance, hr and programme management office. • maintain and update organisation charts and process charts in visio on behalf of the transformation programme • track resource movement (joiners, leavers, extensions, role changes) and ensure pos are raised and ensure governance around this is followed. in turn, support the resource manager in maintaining controls for approvals, (statement of works, pre-invoicing) governance and supplier oversight • provide reliable, timely data to finance for forecasting and month-end reconciliation • coordinate with suppliers to ensure timely submission of resourcing information and timesheet • support improvements to processes and tooling by providing insights from day-to-day resourcing operational experience essential skills & experience: • experience working with structured data, ideally in a programme management office, hr or resourcing operations environment • strong excel skills: pivot tables, formulas, data validation and conditional formatting • ability to manage and update sharepoint lists and create structured datasets for reporting into power bi and creating power automate flows. • experience creating clear, accurate resourcing reports and dashboards in excel and power bi. • proven experience in maintaining visio organisational charts for complex programmes and process diagrams for resourcing governance. desirable qualifications • strong attention to detail, ensuring data accuracy and completeness • ability to follow establish processes and maintain consistent documentation and audit trails • good organisational skills with ability to manage multiple data requests and deadlines • ability to interpret workforce/capacity data and highlight gaps or inconsistencies. stakeholder and team skills: • comfortable engaging with range of colleagues to gather updates and clarify data issues • good communication skills, able to explain data queries clearly • collaborative team player willing to support across pmo, hr and programme teams • proactive in identifying errors, risks or opportunities to streamline processes and work effectively with resource, hr and workforce planning manager to articulate this desirable: • experience supporting a pmo or working in a transformation programme • familiarity with resource planning, capacity reporting or workforce mi this role has been deemed inside ir35 by the client. applicants must hold, or be happy to apply for, a valid basic disclosure scotland. please click the link to apply.",glasgow,Data Analyst,"['dashboard', 'excel', 'power bi', 'r']","['dashboard', 'excel', 'power bi', 'r']",
launch your data analyst career: entry-level training,itol recruit,"a leading training provider in the uk is offering a comprehensive program for aspiring data analysts. this entry-level course includes essential qualifications, such as comptia data+ and business analysis foundation, alongside practical skills in excel, sql, python, and tableau. graduates receive recruitment support to enhance their chances of landing their first role. ideal for those seeking a new career path in data analysis, the program accommodates flexible training options and offers a money-back guarantee if employment is not secured within a year. #j-18808-ljbffr",carlisle,Data Analyst,"['data analysis', 'excel', 'python', 'r', 'sql', 'tableau']","['data analysis', 'excel', 'python', 'r', 'sql', 'tableau']",€40k a year
data analyst | hybrid,vivo,"who we are vivo provides facilities management and accommodation maintenance for the uk military and its partners. vivo embodies both experience and innovation. what we do we put our customers and families first. they are the driving force behind everything we do. we drive forward improvements for them to ensure the service they receive, whether reactive repairs or a major project, is completed safely and on time. in order to achieve this, we have 4 core values: open, caring, agile and collaborative. these are at the heart of everything we do. contract: full time, permanent location: larkhill or aldershot (hybrid: typically 3 days office / 2 days home, or 2 days office / 3 days home depending on the week) working hours: monday to friday, 8:30am–5:00pm (no weekends or out-of-hours work) are you passionate about turning raw data into powerful insights? do you thrive in a role where every day brings new challenges, new reports, and new opportunities to influence decision-making? if so, we want you to join our team as our dedicated data analyst. this is an exciting and dynamic role supporting our operations—especially within the construction and facilities management environment. you’ll be the go-to expert for all things data, working independently while collaborating with key stakeholders and contributing to high-quality reporting across the business. about the role as our data analyst, your day-to-day work will be varied and fast paced. you’ll be responsible for daily reporting, including cash reports and progress updates, as well as manipulating and interpreting data to create meaningful outputs. you’ll review, update, and improve internal and external trackers across excel and other programmes, ensuring information remains accurate, accessible, and up to date. you’ll manage data exporting, cleansing, and analysis, and you’ll regularly present your findings to smaller stakeholder groups or distribute reports via email—making this a forward-facing, high-visibility role. expect a blend of routine reporting deadlines and reactive tasks that arise day by day. what you’ll need success in this position requires strong experience in excel, including vlookups, pivot tables, and advanced data-handling techniques. prior experience in a data analyst role is essential, and familiarity with maximo, fm environments, or mod projects is highly advantageous. your ability to manage deadlines, maintain excellent timekeeping, work independently, and communicate effectively with stakeholders will be key. innovation, accuracy, and attention to detail are at the heart of this role. bpss clearance will be required. you’ll collect, process, and analyse data from a range of sources to uncover trends and insights. you’ll build and maintain clear, engaging dashboards and reports—particularly using tools like power bi—and collaborate across teams to understand their data needs. you’ll also conduct data validation checks, apply statistical methods where needed, and present your findings in a concise and impactful way. you will need to be able to pass bpss clearance to be eligible for this role. if you are not based within an hour of larkhill or aldershot you will not be eligible for this role. if you’re ready to take on a role where your expertise stands out, your ideas are valued, and your work drives meaningful outcomes, we’d love to hear from you. apply today and turn data into action! we offer; • up to 6% contributory pension scheme • 25 days annual leave • volunteer leave • established reward and recognition scheme • one paid professional subscription • life assurance policy • employee discount and reward schemes vivo defence services the role you have applied for is with vivo defence services, a joint venture between serco and equans. by applying for this role, please be aware that information contained within your cv may be shared between vivo defence services, serco and equans during the recruitment process. at vivo, we are committed to building a diverse and inclusive organisation that supports the needs of all. therefore, we will make reasonable adjustments at interview through to employment for our candidates and strongly encourage applications from a diverse candidate pool. we are open to discussions around flexibility and flexible working and operate a hybrid work structure in many of our business areas. we are proudly a disability confident leader. disabled applicants who meet the minimum criteria for the job will be given the opportunity to demonstrate their abilities at an interview.",united kingdom,Data Analyst,"['dashboard', 'excel', 'power bi', 'r']","['dashboard', 'excel', 'power bi', 'r']",
online data analyst,apply4u,"job description telus digital ai-data solutions partners with a diverse and vibrant community to help our customers enhance their ai and machine learning models. the work of our ai community contributes to improving technology and the digital experiences of many people around the world. our ai community works in our proprietary ai training platform handling all data types (text, images, audio, video and geo) across 500+ languages and dialects. we offer flexible work-from-home opportunities for people with passion for languages. the jobs are part-time, and there is no fixed schedule. whoever you are, wherever you come from, come join our global ai community. www.telusdigital.com we are hiring freelance (english speaking) online data analysts for a project aimed at improving the content and quality of digital maps, which are used by millions of users globally. the job would suit someone who is detail-oriented, likes doing research and has a good knowledge of national and local geography. this is a freelance position on a flexible schedule - you can work in your own time whenever work is available. you will be completing research and evaluation tasks in a web-based environment, eg verifying and comparing data, determining the relevance and accuracy of information. you will be provided with guidelines for each task, which need to be followed. the project offers a variety of tasks, and work is paid per task. requirements • full professional proficiency in english • you must be living in the uk the last 2 consecutive years • ability to follow guidelines and do research online using search engines, online maps and website information • you must have familiarity with current and historical business, media, sport, news, social media and cultural affairs in the uk • being open to work across a diverse set of task types (e.g. maps, news, audio tasks, relevance) • applicants must be 18 years or over. working on this project will require you to go through a standard recruitment process (including passing an open book assessment). this is a long-term project and your work will occasionally be subject to quality assurance checks. why join the telus digital ai community? • earn additional income with flexible hours to fit your lifestyle • better work-life balance • be your own boss • remote work & location independence • complimentary well-being package encompassing a wealth of well-being resources. • be part of an online community what’s next? if this sounds like a role you’d be interested in taking on, please apply below.",stirling,Data Analyst,"['machine learning', 'r']","['machine learning', 'r']",
lead data analyst - insights that drive growth,corecom consulting,"a leading organization in property and legal technology is seeking a lead data analyst to drive an insight-led culture and influence product, marketing, and commercial strategies. the ideal candidate will have over eight years of experience in analytics, excellent stakeholder influencing skills, and strong technical capabilities including sql and python or r. this role offers a chance to work end-to-end across the analytics lifecycle, shaping data-driven decisions and impacting multiple brands.",united kingdom,Data Analyst,"['excel', 'python', 'r', 'sql']","['excel', 'python', 'r', 'sql']",£85k a year
online data analyst - bengali speakers (uk),telus digital,"are you a detail-oriented individual with a passion for research and a good understanding of national and local geography? this freelance opportunity allows you to work at your own pace and from the comfort of your own home. tasks a day in the life of an online data analyst: • in this role, you will be working on a project aimed at enhancing the content and quality of digital maps that are used by millions of people worldwide • completing research and evaluation tasks in a web-based environment such as verifying and comparing data, and determining the relevance and accuracy of information. join us today and be part of a dynamic and innovative team that is making a difference in the world! requirements no previous professional experience is required to apply to this role, however, working on this project will require you to pass the basic requirements and go through a standard assessment process. this is a part-time long-term project and your work will be subject to our standard quality assurance checks during the term of this agreement. basic requirements • full professional proficiency in bengali and english language • being a resident in the united kingdom or the last 2 consecutive years and having familiarity with current and historical business, media, sport, news, social media, and cultural affairs in the united kingdom • ability to follow guidelines and conduct online research using search engines, online maps, and website information • flexibility to work across a diverse set of task types, including maps, news, audio tasks, and relevance • daily access to a broadband internet connection, computer, and relevant software assessment in order to be hired into the program, you’ll take an open book qualification exam that will determine your suitability for the position and complete id verification. our team will provide you with guidelines and learning materials before your qualification exam. you will be required to complete the exam in a specific timeframe but at your convenience. benefits • earn extra income • access to our community wellbeing initiative • remote work & location independence • be your own boss • flexible hours to fit in with your lifestyle • be a part of an online community",anywhere,Data Analyst,['r'],['r'],
online data analyst,telus digital ai data solutions,"telus digital ai-data solutions partners with a diverse and vibrant community to help our customers enhance their ai and machine learning models. the work of our ai community contributes to improving technology and the digital experiences of many people around the world. our ai community works in our proprietary ai training platform handling all data types (text, images, audio, video and geo) across 500+ languages and dialects. we offer flexible work-from-home opportunities for people with passion for languages. the jobs are part-time, and there is no fixed schedule. whoever you are, wherever you come from, come join our global ai community. www.telusdigital.com we are hiring freelance (english speaking) online data analysts for a project aimed at improving the content and quality of digital maps, which are used by millions of users globally. the job would suit someone who is detail-oriented, likes doing research and has a good knowledge of national and local geography. this is a freelance position on a flexible schedule - you can work in your own time whenever work is available. you will be completing research and evaluation tasks in a web-based environment, eg verifying and comparing data, determining the relevance and accuracy of information. you will be provided with guidelines for each task, which need to be followed. the project offers a variety of tasks, and work is paid per task. requirements • full professional proficiency in english • you must be living in the uk the last 2 consecutive years • ability to follow guidelines and do research online using search engines, online maps and website information • you must have familiarity with current and historical business, media, sport, news, social media and cultural affairs in the uk • being open to work across a diverse set of task types (e.g. maps, news, audio tasks, relevance) • applicants must be 18 years or over. working on this project will require you to go through a standard recruitment process (including passing an open book assessment). this is a long-term project and your work will occasionally be subject to quality assurance checks. why join the telus digital ai community? • earn additional income with flexible hours to fit your lifestyle • better work-life balance • be your own boss • remote work & location independence • complimentary well-being package encompassing a wealth of well-being resources. • be part of an online community what’s next? if this sounds like a role you’d be interested in taking on, please apply below. https://www.telusinternational.ai/cmp/contributor/jobs/available/105970",stirling,Data Analyst,"['machine learning', 'r']","['machine learning', 'r']",
student systems and data analyst,university of stirling,"the post the university is embarking on an exciting transformation of our student systems infrastructure over the next two years and during the project we are committed to maintaining our business-as-usual activities and require additional resources to continue to deliver the service. this fixed term role will provide support for the ongoing statutory development, operation and maintenance of the tribal sits product, working in partnership with the stakeholders and subject experts to provide effective technical solutions across all system users. this fixed term post is part of academic registry and sits within the student systems and data area of the team which is responsible for ongoing maintenance and continued enhancement of the university’s student and curriculum systems and the management of module evaluation and statutory data sharing and reporting, including student returns to hesa. there are two student systems and data analyst posts in the team and both contribute across the range of activity of the student systems and data area, particularly managing activity in respect to the maintenance in our core student and curriculum systems which support the operational administrative teams with business processes across the student lifecycle from application to graduation. this post takes a leading role working alongside the other team members under direction form the student systems manager in managing the system infrastructure and data quality to support the student lifecycle, and operates in collaboration with other areas of student academic and corporate services, and with colleagues from other service areas in order to support a streamlined approach. this post is based at our stirling campus with weekly attendance and hybrid arrangements. description of duties • conduct systems and business analysis and provide business process support to software upgrades and new functionality implementation, plan, co-ordinate and schedule bau testing and sign off in line with software release schedules, issue resolutions and liaising with the it product development team on further resolution from uat testing to live system • provide system infrastructure support to business areas and troubleshoot complex systems and data errors across the suite of student systems. which includes: • amendments to srl configurations • creation or amendments of srl’s, vistas, workflows and batch processes • creation and amendment of operational reports • implementation of new and revised functionality • troubleshoot system integration errors • data imports • provide end-user systems support including managing the student systems support requests and service desk resolution within service level agreements, liaising with internal information services and or software supplier helpdesk as to technical support where required • plan an active role in system governance, ensuring regulations and deadlines are met, including maintaining the organisational hierarchies and appropriate reference data within student systems, to ensure they correctly reflect the university’s organisational data coding structures • work in line with data quality arrangements and procedures to contribute to ensuring the quality of the data held in the student record system • contribute as required to the management of the module evaluation process and system infrastructure, including in relation to user interfaces, permissions, system data retention, and the creation of module evaluation surveys via extraction transfer and data manipulation from source systems, survey data management and dashboard reporting and required system • enhancements • work comfortably with numerous forms of complex data, designing and preparing criteria for reports, and supporting colleagues to produce routine types of information and respond to relevant freedom of information requests • provide support to the student systems and data officers in maintaining security profiles and user permissions across systems, to ensure users have the appropriate level of access, and that appropriate user security documentation is maintained and work collaboratively with colleagues in information services on data security compliance as required • perform regular system administration and housekeeping activities to ensure the integrity of data and system performance. including coordinating high level system roll forward for a variety of data sets in the student management system ensuring system functionality is maintained essential criteria • educated to degree level or equivalent or extensive employment experience across a range of roles with increasing levels of responsibility • demonstrable experience of utilising a range of complex it systems to an advanced level, including knowledge of activities such as system security, data cleansing, system auditing and control procedures and advanced excel • experience of tribal sits or a student record system • experience of creating system tups, tasking, srl’s and vistas • proven ability to resolve problems with expediency, intelligence and sensitivity • good numerical skills and ability to interpret complex management information • highly organised with the ability to work with strong attention to detail, accuracy, within specified deadlines and with a high-volume and complex workload • experience of identifying and making improvements to existing processes and procedure • ability to work independently and with initiative but to also operate successfully in team and wider collaborative contexts using excellent skills in communicating • ability to produce effective, robust reports based on understanding and manipulating complex statistical data • proven analytical and problem-solving skills • ability to work independently and with initiative but to also operate successfully in team and wider collaborative contexts • ability to simultaneously work on multiple projects • familiarity with reporting tools and sql desirable criteria • experience of working with sits templates • knowledge of prince2 practitioner or itil practitioner, or any recognised repeatable project management methodology • an interest in streamlining and improving business processes • experience of working successfully in the area of student administration within an he environment additional information full time fixed term until august 2027 the closing date for applications is midnight on sunday 04 january 2026. interviews are expected to take place on the week commencing monday 12 january 2026. there is an expectation that work will be undertaken in the uk. for the purposes of sponsorship, this role may be eligible depending on candidate circumstances under soc code 2133. the university of stirling recognises that a diverse workforce benefits and enriches the work, learning and research experiences of the entire campus and greater community. we are committed to removing barriers and welcome applications from those who would contribute to further diversification of our staff and ensure that equality, diversity and inclusion is woven into the substance of the role. we strongly encourage applications from people from diverse backgrounds including gender, identity, race, age, class, and ethnicity. for a full description of duties and essential/desirable criteria please click the apply button, which will take you directly to the university website.",stirling,Data Analyst,"['dashboard', 'excel', 'r', 'sql']","['dashboard', 'excel', 'r', 'sql']",
data analyst,bam uk & ireland,"building a sustainable tomorrow bam fm is recruiting a data analyst to join our fm systems team. working 37.5 hour per week. this role is a hybrid role working from home and out of any or our uk office locations. so why not become part of our forward-thinking fm systems team as a data analyst and help shape the future of data-driven solutions. we’re looking for a motivated professional with a passion for transforming data into meaningful insights. in this role, you’ll be responsible for designing, implementing, and maintaining system integrations to ensure smooth connectivity across platforms. you’ll bring proven experience in creating solutions with microsoft power bi, powerapps, and power automate. working closely with internal stakeholders and external clients, you’ll develop innovative tools that enhance efficiency and deliver real impact. making possible • design, implement, and maintain system integrations.• collaborate with the head of fm systems & reporting to identify and implement process efficiencies through new integrations.• translate business requirements into scalable, user-friendly solutions using power bi and powerapps.• design, develop, and maintain data models, queries, and interactive dashboards to support decision-making.• perform in-depth data analysis to uncover trends and drive business performance improvements.• work with stakeholders to identify improved ways of working, including integrated systems and new technologies.• engage with application vendors to assess capabilities and compare with existing solutions.• develop automated workflows to streamline data integration across systems.• collaborate with cross-functional teams to support data integration and governance initiatives.• follow software development life cycle (sdlc) principles and support application lifecycle management processes.• provide end-user support, training, and guidance on power bi functionality and best practices.• conduct rigorous testing to ensure solution accuracy, reliability, and compliance.• proactively identify and resolve data quality issues and discrepancies.• champion it security by ensuring all development and administrative processes adhere to security protocols. your team working as part of our fm systems team. what’s in it for you? • company car or car allowance.• contributory pension• bupa • life assurance• 26 days holiday (increases with length of service) plus 2 wellbeing days and 1 volunteering day.• gym subsidy and bam social club membership.• health and well being programme.• learning and development opportunities. what do you bring to the role? • degree in computer science, information systems, or a related field.• minimum 2 years of experience working with the microsoft power platform.• strong foundation in software development principles.• proven experience in data modelling, visualisation, and report development.• proficiency in: • power bi desktop, power bi service, power query. • power automate, powerapps. • sql and sql server reporting services (ssrs). • sql server integration services (ssis).• experience with etl (extract, transform, load) tools is a plus.• strong attention to detail and commitment to high-quality deliverables.• excellent communication skills with the ability to simplify complex data insights.• comfortable working independently and collaboratively.• positive, proactive mindset with a strong sense of ownership. about bam building a sustainable tomorrow. that’s our mission and our promise at bam. it’s how we engineer vital infrastructure and construct high-quality buildings as one of the largest construction companies in europe. we strive to create an environment where everybody feels welcome and valued. we’re on an exciting journey to employ the best talent to join us regardless of social background, race, colour, religion, national or ethnic origin, sexual orientation, gender identity or expression, age, disability or other characteristics. the application process bam is committed to ensuring a fully inclusive recruitment and onboarding process, so if at any time you feel we need to do something to make it more accessible to you, do not hesitate to speak with one of our team, and we will do our best to support you. dbs will be required. ""join us in making possible"" closing date for applications is friday 5th december 2025.",stepps,Data Analyst,"['dashboard', 'data analysis', 'etl', 'excel', 'power bi', 'r', 'scala', 'sql', 'sql server']","['dashboard', 'data analysis', 'etl', 'excel', 'power bi', 'r', 'scala', 'sql', 'sql server']",
hris & payroll - senior systems / data analyst - erp systems,interface recruitment uk,"hris & payroll – senior systems / data analyst – erp systems gbp60,000 to gbp70,000 (commensurable with experience) work hours: 37.5 hours – flexi hours plus remote or hybrid responsibilities integration, migration, and analysis + reporting. services and qualifications ind standard. data, erp, and systems analysis & reporting. additional benefits any hours worked over are given off in lieu. region west yorkshire senior systems and data analyst – uk or ireland (9 months fixed term) having acquired multiple acquisitions with circa. 1,000 staff across numerous employing entities and 14 payrolls (predominantly uk and ireland), we now need to identify the best solution to align our different employment terms and conditions, benefits, and amalgamation of employing entities into a streamlined, standardised offering that is easier to manage, cost neutral and accompanies our strategic direction. this will be a complicated exercise. in addition to a number of legacy terms and conditions, plus non–/contractual benefits, there will undoubtedly be individuals with contractual arrangements who do not align with an entity's standard offering. the expected timeframe to achieve alignment is within nine months from now. purpose of the role/key responsibilities: • interpret various data inputs from our multiple payrolls, files, and sage people hris, in conjunction with financial costs–benefit analysis to create a comprehensive record for each individual • generate business–critical reports and in–depth organisational modelling down to an individual level, to enable data–driven, big–picture decision making • work with internal hr, finance, payroll, employee benefits, and it colleagues to ensure uniform/master data architecture and integrity • work with these colleagues to determine the design and development of configuration requirements, within a roadmap prioritisation for our hris third party vendor • fulfil time–driven turnaround of quality data for external reporting (e.g. gender pay gap, esg assessments, tender submissions, employee benefits and insurance renewals) • work with power bi individual to design and feed the group's management information dashboard reports • manage our external v. internal headcount and associated salary benchmarking, plus all other remuneration factors • proactively manage the seamless collection and migration of data during implementation phase, so that nothing breaks when moving people and payroll data, and creating mail–merged letters experience and skill set required for the role: • minimum 5 years of complex payrolls, benefits and/or hr systems and data analysis • extensive business data modelling involving complex analysis to form critical business decisions • can map, sequence and deliver multiple data migrations • strong communication skills – both written and oral – to present recommended management information that drives key decision making • works on own initiative and to tight deadlines, whilst collaborating with others to formulate and seamlessly implement the best approach we are an equal opportunities employer and welcome applications from all qualified candidates.",united kingdom,Data Analyst,"['dashboard', 'data analysis', 'power bi', 'r']","['dashboard', 'data analysis', 'power bi', 'r']",
senior data analyst - claims,arthur recruitment,"senior data analyst – claims location: london (hybrid) 70k + bonus + benefits about we are working with a leading global provider of specialty lines insurance and reinsurance, renowned for intelligent risk-taking, excellent client service, and delivering strong returns. role overview we are seeking a senior data analyst – claims to turn complex claims data into actionable insights that drive strategic decision-making, operational efficiency, and improved customer outcomes. you will support the global claims function and work closely with other departments relying on claims data to deliver meaningful results. key responsibilities design and deliver data insights • collaborate with report developers to create impactful dashboards and reports. • develop and maintain performance monitoring tools, visualisations, and predictive analytics to support claims forecasting, fraud detection, and decision-making. • integrate and analyse data from multiple sources across geographies and markets. collaborate with stakeholders • engage with internal stakeholders to understand business needs and translate them into data-driven insights. • work cross-functionally to gather quantitative and qualitative data. • present findings clearly to support operational, performance, and financial improvements. support data transformation initiatives • assist with testing, reporting, and validating data flows for market-wide projects. • maintain claims data products, ensuring quality, consistency, and integrity. • identify opportunities to improve reporting and analytics processes. required qualifications • experience within lloyd’s and the london market, with strong understanding of claims processes, systems, and insurance products. • familiarity with london market bureau processes, xchanging, lloyd’s v5 bordereaux, and london market claims systems.claims • strong analytical and problem-solving skills with attention to detail. • ability to build strong relationships and collaborate across departments. • comfortable managing multiple priorities and committed to professional development. preferred qualifications • experience with data visualisation tools such as power bi or tableau. • ability to identify and escalate potential risks impacting projects or operations. sound good? apply now!!!",united kingdom,Data Analyst,"['dashboard', 'excel', 'power bi', 'r', 'scala', 'tableau']","['dashboard', 'excel', 'power bi', 'r', 'scala', 'tableau']",£60k–£70k a year
student systems and data analyst (sis project),university of stirling,"location: stirling campus the post the university is embarking on an exciting transformation of our student systems infrastructure over the next two years and during the project require additional resources for the implementation and configuration of the new student information system – ellucian. the project will operate on a capability led deployment and this fixed term role will provide crucial input to the project across all capabilities as the project evolves and will be delivering configuration and developing workflow, system automation and reporting. this post as part of the core project team will work with our external partners, internal business process capability leads and subject experts to deploy and configure the system processes across the applicant and student lifecycle, to support a streamlined approach to system processes and maximise system automation. description of duties • configuration of system infrastructure including: • contribute to the system security user access role profiling and general person record configuration • experience cards and workflows with rules and automatic processes across the capabilities that are confirmed agreed deliverables • creation of system extensions where required ie additional fields • creating agreed system configuration to produce output including transcripts, applicant offer letters, certificate of enrolments • configuration of requests forms and workflows and automation to support for example withdrawals, leave of absence business processes • configuration of application forms and associated workflow, document management, system archiving, indexing and purging and data retention based on agreed policy • undertaking functional testing of above configuration before transferring to user acceptance testing within the capabilities. identifying test cases and escalating defects resolution • issue resolution and liaising with the project team (both internal and external stakeholders) as well as ellucian user support • creation and amendment of operational reports within the ellucian insights capabilities, ensuring reporting infrastructure on a capability basis essential criteria • educated to degree level or equivalent or extensive employment experience across a range of roles with increasing levels of responsibility • demonstrable experience of utilising a range of complex it systems to an advanced level, including knowledge of activities such as system security, data cleansing, system auditing and control procedures and advanced excel • experience of ellucian banner, tribal sits or a student record system • experience of system configuration • proven ability to resolve problems with expediency, intelligence and sensitivity for further information, including a full description of duties, essential criteria and details on how to apply, please see vacancy details | university of stirling £38,784 to £46,049 per annum (grade 7)",stirling,Data Analyst,"['excel', 'r', 'scala']","['excel', 'r', 'scala']","£38,784–£46,049 a year"
13439- data analyst programmer,university of edinburgh,"grade ue06: £34, 610- £39,906 per annum royal (dick) school of veterinary studies full time: 35 hours per week fixed term: 42 months the opportunity we are seeking a methodological, rigorous and service-oriented individual to support data processing in sebi-l and provide data support services to deliver to the gates foundation. the post-holder will write code that is scalable, tested and validated to process project data. this post is full-time (35 hours per week) and we are open to considering requests for hybrid working (on a non-contractual basis) that combines a mix of remote and regular on-campus working. the post holder is expected to attend our weekly in-person team meetings at easter bush campus. the post is available on a 42-month fixed-term contract basis. sebi-livestock is a dynamic and innovative organisation tasked with improving data and evidence in low- and middle-income countries to support better informed decision making. we provide an extensive data driven monitoring and learning service to the gates foundation in tracking their livestock investments. in addition, we convene a large livestock data network, livestock data for decisions (ld4d) to help achieve common goals. the postholder will work on building a scalable, tested and validated data pipeline to process livestock and aquaculture data. this role will play a crucial part in supporting the sebi-l team in transforming and importing livestock data into the sebi-l platform. responsibilities will include: the writing, running and maintenance of scripts to automate and test data processing workflows; ensuring code is version controlled, validated and robust in collaboration with other team members; supporting the automating and running of livestock models and analyses. within this role, there is an opportunity to develop data visualization skills and to work on projects in machine learning and natural language processing. this position is based within sebi-livestock, the centre for supporting evidence based interventions in livestock, which is hosted by the roslin institute and royal (dick) school of veterinary studies, university of edinburgh. sebi-livestock offers a collaborative, fast-paced environment with opportunities for learning, growth and the chance to contribute to addressing real-world challenges. your skills and attributes for success: educated to degree level in a numerical discipline such as bioinformatics, computer science, mathematics or equivalent; demonstrable experience of r programming and modular code; basic knowledge and experience of python and sql databases; good knowledge and experience of data science technologies and of good software engineering practices including use of version control and implementation of unit and integration tests; an interest in exploring the application of new technologies including natural language processing and machine learning",edinburgh,Data Analyst,"['data pipeline', 'machine learning', 'natural language processing', 'python', 'r', 'scala', 'sql']","['data pipeline', 'machine learning', 'natural language processing', 'python', 'r', 'scala', 'sql']",
data analyst - red cell immunohaematology (rci),nhs blood and transplant,"this vacancy is not eligible for visa sponsorship. please do not apply unless you have another means of establishing a right to work, or you are already employed by the nhs and would transfer to the pay point at the top of band 6. job summary this is a great opportunity to work in nhs blood transplant as a data analyst with the red cell immunohaematology (rci) function, a national reference service supporting the safe and clinically effective use of red cells and other blood components. you will provide data analysis expert advice to rci management and other stakeholders nhsbt. as part of the rci management team, you will provide expert data analysis, interpretation, and reporting to inform operational and strategic decision-making across the service. your analytical insight will support workforce planning, quality improvement, and service development, ensuring rci remains a recognised centre of excellence in reference transfusion science. your organisational and leadership skills will ensure the transfusion service continues to deliver a cost effective and responsive service to patients and users of the service. nhsbt will offer training and development opportunities to maintain and enhance your personal development. main duties of the job in this role you will be a data analyst responsible for the analysis of business performance data. responsibilities include: • gathering data from a variety of sources, producing analysis and recommendations. • managing a portfolio of scheduled and ad hoc reporting tasks, including weekly, monthly, and quarterly outputs. • reporting of trends to support strategic planning, service improvement and business growth. • supporting the assistant director of rci, members of the senior management team and the wider function to achieve challenging performance and growth targets with a focus on improving efficiency/productivity of the function. • supporting the delivery of the rci strategic plan by ensuring that insight from data is aligned with business objectives, that performance is monitored across all sites, and that data is used to inform improvements, operational planning, and stakeholder engagement. • leading and contributing to data-driven workstreams, such as performance reporting, benchmarking, forecasting and dashboard development, ensuring that they are of the highest professional standards and compliant with all relevant internal and external standards, regulation and instructions. you will be required to travel and spend time away from base, which will involve working irregular hours and overnight stays when required, with prior notice about you experience and knowledge • demonstrate experience in the use of analytical, reporting and statistical software e.g. power bi, r, sas, spss, s+, microsoft access, excel, sql server and business objects. • demonstrable experience of working with budgets and the use of financial/activity information to support the management of services. • experience of producing analysis and identifying trends from large quantities of complex and often conflicting data. • experience of developing/assisting with the creation of strategic plans / business cases. • experience of working in the delivery of projects as part of a multi-disciplinary team. • experience in providing training to others on an area of specialism. • experience of working with information technology using microsoft office packages (outlook, word, excel, teams and powerpoint). qualifications and training • educated to degree level in a relevant subject (e.g. data science, statistics, health informatics) or equivalent level qualification or demonstrable work-based experience in a range of procedures and practices in the specialism to degree level. • post graduate qualification in statistical or numerate discipline or significant demonstrable experience equivalent to post graduate level. • demonstrates commitment to own continued professional development (cpd) about us it takes all types of people to deliver the kind of service that saves and improves lives. at nhs blood and transplant, you’ll join a team of more than 6,000 people who are making a genuine difference to communities, families, friends, relatives and more across the uk. we play a unique and special role in the nhs by helping people do something extraordinary- donate blood, blood products, organs, tissues, or stem cells to save someone in need. our three core values are what set us apart. they guide and inspire everything we do. by being caring, expertly meeting the needs of our patients and our people, and accepting nothing less than the best quality, we can do extraordinary work – and help our people to do something extraordinary in their career, too. three small words, one big difference - caring, expert and quality. together we'll save and improve more lives than ever. you will join us on our journey to create an inclusive workplace and aim to reflect the diverse communities we work with, and we positively encourage applications from all sectors of the community. what we offer: • nhsbt promotes flexible working opportunities where the role will allow. • 27 days annual leave (pro rata for part-time) plus bank holidays, increasing to 29 days after 5 years’ service and to 33 days after 10 years. • nhs pension scheme. the nhs pension scheme is a defined benefit scheme (not dependent on investment returns) further details and outline of benefits can be found at: www.nhsbsa.nhs.uk/pensions ‎ • we’ve fostered a culture of continuous learning where colleagues are well-led, engaged, and encouraged to grow. we support you in reaching your full potential, both in your current role and future career. our thrive program embodies our commitment to learning and development, offering a wide range of activities to support your personal and professional growth. it’s open to everyone at nhsbt, ensuring you have the resources to succeed and shine in your role. to learn more, please see our recruitment profile, which provides a summary of the job description and person specification. this can be accessed via the link below, or by clicking ‘apply’ if you are viewing this advert on another site. the successful candidate may be based at any of our main centres in england, by agreement. this is a fixed term/secondment for 2 years. this vacancy will close at 23:59 on sunday 14th december 2025. interviews are anticipated to be held week commencing 12th january 2026 – subject to confirmation. for informal enquiries please contact katie yandell, senior pa, via email at katie.yandell@nhsbt.nhs.uk",united kingdom,Data Analyst,"['dashboard', 'data analysis', 'excel', 'power bi', 'r', 'recommendation', 'sas', 'sql', 'sql server', 'statistics']","['dashboard', 'data analysis', 'excel', 'power bi', 'r', 'recommendation', 'sas', 'sql', 'sql server', 'statistics']",
ai content and data analyst,talent pulse,"job overview: join our dynamic team as an ai content and data analyst in a fully remote capacity. we are on the lookout for engaged individuals to assist in developing high-quality content, conducting in-depth research, and efficiently organizing data. your contributions will play a vital role in collaborative projects alongside cross-functional teams, ensuring our brand maintains consistency and excellence. key responsibilities: • craft, edit, and proofread captivating content for diverse digital platforms. • conduct extensive research to ensure the relevance and accuracy of our content. • systematically organize and maintain data and documentation while upholding meticulous attention to detail. • support various project tasks including content creation, data analysis, and digital workflow management. • work closely with teams to foster coherent messaging and a unified brand voice. qualifications: • exceptional written and verbal communication skills in english. • analytical thinker with a keen eye for detail and precision. • self-motivated with effective time-management skills in a remote setting. • flexible to adapt to different work arrangements (internship, part-time, or full-time). • background in writing, communications, marketing, or related fields is advantageous. benefits: • enjoy a flexible remote working schedule suited to your lifestyle. • access to professional mentorship and opportunities for skills enhancement. • build a meaningful portfolio and gain invaluable experience through diverse projects. apply now!",united kingdom,Data Analyst,"['data analysis', 'excel', 'r']","['data analysis', 'excel', 'r']",
data analyst internship,jay technologies ltd,"overview we are seeking a highly skilled data analyst to join our dynamic team. the successful candidate will be responsible for transforming complex data into actionable insights, supporting strategic decision-making processes across various departments. this role offers an excellent opportunity to utilise advanced analytical tools and techniques, contributing to organisational growth and efficiency. the ideal applicant will possess a strong background in data analysis, database management, and visualisation, with the ability to communicate findings effectively to both technical and non-technical stakeholders. responsibilities • analyse large datasets using sql, r, python, and other relevant tools to identify trends, patterns, and anomalies. • develop and maintain interactive dashboards and reports using tableau and power bi to facilitate data-driven decision making. • design and optimise database structures in oracle, microsoft sql server, ensuring data integrity and efficiency. • conduct business analysis to understand organisational needs and translate them into technical specifications. • manage projects related to data collection, processing, and reporting within the sdlc framework. • create detailed visualisations and documentation using visio for process mapping and system design. • collaborate with cross-functional teams on database design, system integration, and process improvements. • utilise vba and bash (unix shell) scripting for automation of routine tasks and data manipulation. experience • proven experience as a data analyst or similar role with a strong portfolio of completed projects. • proficiency in sql programming with experience in microsoft sql server or oracle databases. • advanced skills in data visualisation tools such as tableau and power bi. • experience with r and python for statistical analysis and data modelling. • knowledge of sdlc methodologies and project management principles. • strong understanding of business analysis processes alongside database design expertise. • familiarity with vba scripting, bash (unix shell), visio, and other related tools is highly desirable. • excellent analysis skills with the ability to interpret complex datasets accurately and efficiently. this position offers a stimulating environment where analytical expertise is valued, providing opportunities for professional growth within a collaborative team dedicated to innovation through data-driven insights. job types: full-time, part-time, permanent, temporary, fixed term contract, temp to perm, apprenticeship, zero hours contract, graduate, volunteer, internship contract length: 6 months pay: £21,395.40-£45,000.00 per year expected hours: 20 per week benefits: • flexitime • work from home work location: remote",anywhere,Data Analyst,"['dashboard', 'data analysis', 'excel', 'power bi', 'python', 'r', 'sql', 'sql server', 'tableau']","['dashboard', 'data analysis', 'excel', 'power bi', 'python', 'r', 'sql', 'sql server', 'tableau']","£21,395.40–£45,000.00 a year"
people systems & data analyst,funding circle uk,"people systems & data analyst we are looking for a people systems & data analyst in our people operations team. the people operations team sits within the wider people team and is critical to keeping all of our hr systems and processes flowing smoothly. who are we? we’re funding circle. we back small businesses to succeed. at funding circle, we believe the world needs small businesses. that’s why we’ve made it our mission to help them get the finance they need to grow. with more than a decade of expertise under our belt, we’ve built a game-changer of a platform with cutting-edge data and technology that’s reshaping the landscape of sme lending. say goodbye to lengthy applications and hello to lightning-fast decisions! in just minutes, smes across the uk can get a decision, giving them access to competitive funding in a flash. we know that good business is about good people. so we pride ourselves on providing meaningful, human support as well as fast, hassle free processes to deliver an unbeatable customer experience. the role in this role, you will work with all systems and data across our people team (hr) and will be responsible for maintenance and accuracy of our data. • systems management: • be the ""data gatekeeper"" for the hris (hibob), ats (ashby), and lms (learn amp) • manage relationships with other system providers • drive continuous improvement and manage system updates • partner with the people team to ensure processes and systems are functioning correctly • data & reporting: • ensure data integrity and accuracy through regular audit checks and analysis • create, maintain, and run key reports to communicate people data to the business • help analyze and report on key people metrics • team support: • support employee lifecycle events (e.g., engagement survey, performance reviews) • support the wider people team with queries related to people tools and systems. proactively share relevant data and insights with key stakeholders • collaborate with the compliance team on monitoring and reporting compliance training completion. assist with uploading and maintaining training materials • proactively problem solving, identifying and confidently mitigating any risks, issues or control weaknesses that arise in your day-to-day what we’re looking for • excellent attention to detail and a passion for data • prior experience with data or systems and high organisation/prioritisation skills • comfortable with google sheets and/or excel (g-suite is predominantly used) • systems savvy, inquisitive, and familiar with systems like hibob, ashby, learn amp, and g-suite • ability to work under pressure and pivot between tasks • strong desire to learn new things and willing to question and partner with stakeholders at funding circle we are committed to building diverse teams so please apply even if your past experience doesn’t align perfectly with the requirements. want to learn more? • we have a huge impact on the businesses that borrow through our platform, the communities they serve and the overall economy (last year £6.9bn of gdp generated). you can read our full impact report here: https://www.fundingcircle.com/uk/impact • to see what our customers think, visit our trustpilot page: https://uk.trustpilot.com/review/fundingcircle.com • and we’re still evolving! our award-winning multi-product platform is solving more sme finance challenges than ever before. we think big, rally together and meet the needs of sme customers like no other. why join us? at funding circle, we celebrate and support the differences that make you, you. we’re proud to be an equal-opportunity workplace and affirmative-action employer. we truly believe that diversity makes us better. as a flexible-first employer we offer hybrid working at funding circle, and we've long believed in a 'best of both' approach to in-office collaboration and non-office days. we expect our teams to be in our london office three times a week, where you can take advantage of our newly refurbished hybrid working space, barista made coffee and subsidised lunches (via justeat) every day! we back our circlers to build their own incredible career, making a difference to small businesses every day. our circler proposition is designed to support employees both in and out of work, and it is anchored around four pillars: health, wealth, development & lifestyle. a few highlights: • flexibility: we provide you with a benefit allowance so that you can tailor your benefit selection to you and your family’s needs. • health: private medical insurance, dental insurance, health cash plan, health assessments (including female fertility health assessments), eyecare vouchers, flu jabs, wellhub (for discounted flexible gym membership and access to wellbeing apps) access to a free employee assistance programme and free digital gp for yourself and any children under 16. • wealth: life assurance, income protection, critical illness cover, financial coaching through octopus money coach, a tax-advantaged share scheme, a free mortgage advisor partnership and discounts across numerous retailers through perks at work. • development: dedicated annual learning & development allowance and full access to internal learning platform. • lifestyle: electric car scheme, cycle to work scheme, season ticket loans, and more! and finally, we have award winning parental leave policies supporting parents through enhanced maternity, partner and adoption leave, as well as additional leave for parental bereavement and for fertility treatments. ready to make a difference? we’d love to hear from you. #li-cs1",united kingdom,Data Analyst,"['elt', 'excel', 'r']","['elt', 'excel', 'r']",
data analyst business partner,c4s search pro,"job title: data analytics business partner location: swindon salary: £50,000 way of working: 2/3 days in office the business: our client is a leading provider in at home eye care and hearing care for patients that find it challenging to visit high street stores. the data analytics business partner opportunity: we’re looking for a data analytics business partner to turn complex data into clear, actionable insights for senior leaders. you’ll act as the bridge between data and decision-making, shaping strategy and promoting a culture of data-driven thinking. this isn’t a coding-heavy role - it’s about applying analysis to real business challenges and presenting findings that drive change. with a new single customer view database in place, there’s huge scope to unlock deeper insights and influence the future direction of the business. responsibilities • act as the senior data contact across all departments, understanding needs and turning them into actionable insights. • partner with department leaders to spot opportunities where data can drive growth, efficiency, and better outcomes. • translate complex data into clear, business-friendly narratives through reports, dashboards, and presentations. • lead data-driven projects and champion a culture of evidence-based decision making, ensuring data is reliable and widely used. the ideal candidate will have: • background in analytics or business partnering role within a service led environment. • strong commercial financial awareness with the ability to see the bigger picture and understand the business impact of insights. • confident communicator and presenter, able to influence at all levels and translate complex information into clear, practical actions. • hands-on with data tools such as power bi, excel, and sql, and skilled at turning datasets into meaningful stories. • adaptable and organised, able to balance strategic thinking with day-to-day delivery while managing multiple priorities. benefits inclusive of 25 days holiday + bank holidays, private medical insurance, pension scheme matched contributions up to 5%, life assurance, free sight and hearing tests plus discounts, flexible hybrid working arrangements and a supporting inclusive team culture where your input will directly shape the business. if you are interested in the role, please send an application detailing proof of the above or contact amy burton in our office. c4s search connect talent with leading organisations and we are always keen to hear from those who work in the tech industry.",united kingdom,Data Analyst,"['dashboard', 'data analytics', 'excel', 'power bi', 'r', 'sql']","['dashboard', 'data analytics', 'excel', 'power bi', 'r', 'sql']",
data analyst placement programme,itol recruit,"• please note this is a training course and fees apply* are you looking to benefit from a new career in data analysis? if you are detail orientated, perceptive, organised, competent, analytical and can communicate well with those around you; you could have a truly rewarding future as a data analyst we do this using our specialised data analyst career programme which looks to assist and place qualified candidates into a career pathway in data analysis. please note this career program is designed for entry level individuals with limited or no experience, so please do not apply if you are already an experience data analyst as we will be looking primarily at entry level roles. demand for data analysts has grown 20% year on year with experienced analysts easily commanding salaries of £50k+. all business decisions rely on data to ensure correct business decisions are made and therefore the role of the data analyst in the new digital world has become essential for business owners. below are current average salaries in the sector for lower-level positions and fully trained data analysts: • junior data analyst - £30,000 • data analyst - £50,000 • business data analyst - £67,500 • data analytics analyst - £80,000 • business analysts - £60,000 using our experience in providing data analysis and business analysis training online and through our expert recruitment consultants, we can provide a seamless journey and often fast-tracked route into a new career in data analysis. the courses in the package have been identified by our recruitment partners as industry standards for the uk/european and international data analysis sector. skills shortages across all sectors are increasing the demand for qualified, entry-level career seekers and career changers. whether you are working full-time, part-time or are unemployed, this job and recruitment package has the flexibility to be completed at a pace that suits you and can be completed in a few weeks. your job and career goals are completed in 4 easy steps listed below. step 1 - comptia data+ qualification the first step is completing the comptia data+ qualification accredited by comptia. this professional international industry-recognised qualification teaches you the essential skills of a data analyst. you will cover skills such as data mining, manipulating data, visualising data, and reporting on data. study time for the qualification is approximately 30 hours and qualification is achieved by completing a 1-hour multiple choice exam. the course is provided online and comes complete with exam simulators and revision tools. you will be provided with access to a tutor and a 1-hour online revision workshop prior to sitting your exam. this qualification will set you on a route to becoming a data analyst. step 2 - data administration training data is extracted using various method which are normally it driven. therefore, we will provide you with access to the following additional courses to ensure you are suitably employable as a data analyst. • microsoft excel - to expert level. • learn sql - the programming language used for extracting data from more complex databases. • learn python 3 - python 3 is a simple to use programming language used by many analysts. • learn tableau - tableu is a tool which analysts use to build visually appealing dashboards for complex data making it easier for stakeholders to comprehend. study time for these courses is approximately 30-60 hours and qualification is achieved through completion of the course with no exam. step 3 - business analysis foundation business analysts and data analysts often work very closely together and indeed many data analysts become business analysts and vice versa. adding a business analyst foundation certificate will make you much more employable and enable you to cross over into business analyst roles and hybrid roles. business analysis foundation accredited by the bcs (the british computer society also known as the chartered institute of it). study time for this course is approximately 15 hours and qualification is achieved through an online bcs exam. step 4 - recruitment support now you are qualified for an entry level data analyst position our recruitment support team will now begin collaborating with you to help you secure your first a suitable role based on your new qualifications and any other experience you may have. we have been helping candidates start and build careers in the project management industry since 2007 and have a 4.8 trustscore on trustpilot. our recruitment support team will help you work through job applications, interviews and provide you with a full cv review based around your new qualification and written to maximise your chances of obtaining a role in the project management sector. they will provide you with guidance as to which roles our most suitable for you as an entry level project sector worker aiming to become a project manager. this will include mock job interviews as well as any help you feel you need. our money back guarantee if after 1 year of passing your formal qualifications, we have been unable to help you secure a role, we will refund your study fees minus the cost of the exams. however, we are normally able to help candidates find their first role within 6 months of qualifying and for locations based close to major cities this is often reduced to less than 3 months. check our testimonials from the hundreds of candidates we have already helped. what now? to accommodate candidates, the training element of the package is available on finance terms of up to 1 year. this enables you to get qualified and start in your new role without having to fund all the training costs up front. enquire now and one of our experienced career consultants will contact you within 4 working hours to answer any questions that you may have and to assist you in taking the first step towards your data analyst career aspirations.",livingston,Data Analyst,"['dashboard', 'data analysis', 'data analytics', 'excel', 'python', 'r', 'sql', 'tableau']","['dashboard', 'data analysis', 'data analytics', 'excel', 'python', 'r', 'sql', 'tableau']",£28k–£40k a year
data analyst,diageo,"the role of data controller sits within brand change, part of the gbs technical function and reports to the data control team leader. the key purpose of the role is to support all brand change activity through accurate creation, development and maintenance of all product data covering scotland, baileys, mexico and china hub operations. this includes core product data, bill of material set up as well as creation of all new packaging component and liquid specifications. the role is critical to ensure all new product introductions are delivered within project timelines to meet brand change kpi targets. there will also be scope within the role to develop change management skills by supporting/leading improve and transform opportunities within our area. you will be responsible for: • to develop and deliver all project data requirements for small to large- scale brand change solutions on behalf of the project team. • stakeholder engagement and communication key to ensure project timelines are adhered to and rft kpi’s are met • to pro-actively manage the product life cycle i.e. creating, developing and maintaining all production data, including creation of all new material and liquid specifications – across a variety of systems and applications such as sap, plm, excel • ensure adherence to all relevant processes for data closedown of all brand change projects, work with project manager to meet project closedown kpi • manage shared inbox requests for product data and site data requests on a rotational basis. generate new barcodes for any new product introductions on a rotational basis. responsible for leading efficiency improvements, and supporting process review and updates. • a keen eye for detail and understand the importance of accurate data. • ability to work quickly and accurately under pressure. • strong communication and organisation skills. • take ownership and accountability for project data. • knowledge of sap ecc & plm (preferred but not essential) • skilled in microsoft office, particularly excel and power automate. as a temporary contractor, you'll be employed by our managed service provider, magnit global, who manage most of our temporary contract hiring, you will not be employed by diageo qualification/licensure work authorization : us citizen preferred years of experience : 1 years travel required : no travel required shift timings :",menstrie,Data Analyst,"['excel', 'r']","['excel', 'r']",
trainee data analyst - elevation programme,frasers group plc t/a sports direct international,"at frasers group we’re rethinking retail. through digital innovation and unique store experiences, we’re serving our consumers with the world’s best sports, premium and luxury brands globally. as a leader in the industry, we’re elevating the retail experience for our consumers through our collection of established brands, including sports direct, flannels, usc, frasers, and game. why join us? our vision we are building the worlds most admired and compelling brand ecosystem our purpose – we are elevating the lives of the many with access to the world’s best brands and experiences at frasers group, we fear less and do more. our people are forward thinkers who are driven to operate outside of their comfort zone to change the future of retail, embracing challenges along the way. the potential to elevate your career is massive, the experience unrivalled. to be able to make the most of it you need to live and breathe our principles: • think without limits - think fast, think fearlessly, and take the team with you • own it and back yourself - own the basics, own your role and own the results • be relevant - relevant to our people, our partners and the planet are you ready to join the fearless? job description about the programme the elevation programme is designed to help you live and breathe our principles. you’ll discover what it means to think without limits, own it, be relevant and be the next champion of analytics. this is a unique opportunity to deliver tangible performance for the uk’s biggest multi brand retailer. what you’ll do year 1: • exposure to commercial / retail / digital / warehouse analytical functions, through dedicated rotations working in the heart of each department. • each rotation you'll be exposed to advanced analytical techniques, data modelling, data best practice and will be given the opportunity to be at the forefront of ai adoption and innovation for a leading retailer. • the objective of year one is to understand how a product flows throughout the business from a physical perspective and a data perspective. year 2: you’ll use your new technical skillset with a rounded business knowledge to specialise in a particular analytical function, while being accountable for high value projects across the entire organisation. your development to support your journey towards becoming a future leader, you’ll have: • early access to our leadership and management training programme • exclusive insights from senior leaders across frasers group • continuous professional development designed to accelerate your career qualifications • valid working rights for the uk • no degree required but any degrees disciplines are considered • 1 year+ work experience in any industry (if no degree) • gcse grade a*-c (or equivalent) in maths & english skills • the ability to put commerciality at the forefront of your decision making • we are looking for people with a real passion for retail, spotting customer trends and helping us push the boundaries within this industry. • inquisitive and technical mindset with strong problem solving skills. additional information • one-year free accommodation, as well as subsidised food and laundry • discounted gym membership • 20% discount across our brands, including sports direct, jack wills, flannels, everlast, game, frasers and many more… • frasers champion – a peer-nominated recognition scheme where winners receive double pay for one month. this award celebrates individuals who have demonstrated exceptional behaviours by thinking without limits, owning their actions, and staying relevant. • frasers festival – an event like no other! our frasers festival is our celebration for head office and retail staff across the uk and europe – hosting a mega brand village, guest speakers from the world's biggest brands, evening entertainment, the ultimate frasers fearless fitness challenge and much more. employee welfare frasers fit – our everlast gyms team are on a mission to make our workforce the best, and fittest on the planet! we run free gym classes for employees as well as discounted memberships to our clubs. frasers fit is our wellbeing programme which aims to support and improve colleagues physical, financial & mental wellbeing. the app is accessible employees and includes training, nutrition and lifestyle advice- all completely free. retail trust – we know that it’s not just about physical health, mental wellness is equally important which is why all our employees get free access and support from the retail trust charity. this includes a 24-hour wellbeing helpline, wellness hub, counselling and financial/legal support. selection process • application review (response within 30 days) • online assessment (analytical skills, logic, values) • video interview (showcase your fearless qualities) • onsite assessment centre at shirebrook hq (march 2026) • offered issued in april • programme starts sept 2026 with an immersive orientation week. additional information we are unable to accept applications from candidates on dependent visa’s, student tier-4 visa’s, tier-2 skilled worker visa’s, or those holding a post-study work visa. please apply only if you have permanent uk working rights.",united kingdom,Data Analyst,['r'],['r'],
workday people reporting & data analyst,first bus,"welcome to first bus first bus is one of the country's largest bus operators – taking 1.6 million customers to their destinations every day. we're a forward-looking business at the forefront of bus technology, leading the way on contactless payments, mobile apps and real time information. our investment in state-of-the-art, new buses is just one part of our transition to a low-carbon future. it's an exciting time to be here. the workday people reporting & data analyst is responsible for delivering high quality insights through workday reporting and analytics to support data-driven decision-making across first bus. the role ensures hr data is accurate, consistent, and accessible, enabling leaders and teams to make informed decisions based on trusted information. in this role you will work closely with hr, payroll, finance, operations, and technology stakeholders to develop robust reporting solutions, define meaningful kpis, and enhance overall data quality within workday. key responsibilities: • develop, maintain, and enhance reports, dashboards, and analytics assets using workday report writer, composite reports, and workday prism. • interpret data and produce meaningful analysis to support business decisions, hr initiatives, and operational performance. • support cyclical and regular reporting needs (e.g., monthly kpis, workforce metrics, absence, turnover, headcount, compliance). • respond to ad hoc reporting requests, translating stakeholder needs into accurate and timely insights. • manage scheduled reports, ensuring they are delivered consistently and to the correct audiences. • troubleshoot reporting issues and ensure outputs are reliable and aligned with business requirements. • work with hr, finance, and operational leaders to define meaningful kpis and reporting requirements. • partner with business teams to understand their objectives and convert these into clear reporting and data specifications. • present insights and recommendations in a clear and compelling way to technical and non-technical audiences. • ensure all reporting outputs adhere to data governance, audit, and security standards. • perform data validation, identify inconsistencies, and work with relevant teams to resolve data quality issues. skills, experience & qualifications: • solid experience in hr reporting, analytics, or data insights roles with workday. • strong experience with workday reporting tools (report writer, calculated fields, composite reports). familiarity with workday prism is desirable. • proficiency with data visualisation and reporting platforms (e.g., power bi, tableau, workday dashboards). • excellent analytical skills with the ability to interpret workforce data, spot trends, and tell a clear story using insights. • experience applying hr data governance principles and managing sensitive employee data. • experience supporting large or complex organisations with diverse reporting needs. • knowledge of sql, data modelling, or etl concepts (advantageous but not essential). rewards & benefits • your health is important to us which is why we offer simply health & smart health for you and your dependents at no cost to you! this offering includes cash back benefits across optical & dental as well as virtual gp appointments, mental health support and nutrition and fitness advise. our eap provision offers 24hr, 365 days a year mental health support • we are also committed to your financial wellbeing, and we offer a platform with many discounts on high street brands and supermarkets etc • working at first you also receive discounted train travel for you and your family after 6 months service as well as a free bus pass • cycle to work scheme • 25 days' holiday + bank holidays • pension scheme and chance to buy discounted first group shares • enhanced maternity / paternity pay • flexible working we put a big focus on physical and mental wellbeing at first bus. we recognise that anyone can be affected by the stresses and strain of work, or life outside it. you'll find a range of health-promotion initiatives, and you will have 24/7 access to our confidential employee assistance programme. please apply now. we look forward to receiving your application. public transport serves everyone, whatever their differences. at first bus, we want to be an employer open to you, no matter what your differences are. we aspire to be an inclusive organisation because diverse backgrounds, thinking and experiences bring so many benefits to our customers, communities and people. we welcome applications from all. applicants are advised that background checks will be conducted as a standard part of our recruitment process.",united kingdom,Data Analyst,"['dashboard', 'etl', 'excel', 'power bi', 'r', 'recommendation', 'sql', 'tableau']","['dashboard', 'etl', 'excel', 'power bi', 'r', 'recommendation', 'sql', 'tableau']",
data analyst trainee,apply4u,"job description are you looking to benefit from a new career in data analysis? if you are detail orientated, perceptive, organised, competent, analytical and can communicate well with those around you; you could have a truly rewarding future as a data analyst we do this using our specialised data analyst career programme which looks to assist and place qualified candidates into a career pathway in data analysis. please note this career program is designed for entry level individuals with limited or no experience, so please do not apply if you are already an experience data analyst as we will be looking primarily at entry level roles. demand for data analysts has grown 20% year on year with experienced analysts easily commanding salaries of £50k+. all business decisions rely on data to ensure correct business decisions are made and therefore the role of the data analyst in the new digital world has become essential for business owners. below are current average salaries in the sector for lower-level positions and fully trained data analysts: • junior data analyst - £30,000 • data analyst - £50,000 • business data analyst - £67,500 • data analytics analyst - £80,000 • business analysts - £60,000 using our experience in providing data analysis and business analysis training online and through our expert recruitment consultants, we can provide a seamless journey and often fast-tracked route into a new career in data analysis. the courses in the package have been identified by our recruitment partners as industry standards for the uk/european and international data analysis sector. skills shortages across all sectors are increasing the demand for qualified, entry-level career seekers and career changers. whether you are working full-time, part-time or are unemployed, this job and recruitment package has the flexibility to be completed at a pace that suits you and can be completed in a few weeks. your job and career goals are completed in 4 easy steps listed below. step 1 - comptia data+ qualification the first step is completing the comptia data+ qualification accredited by comptia. this professional international industry-recognised qualification teaches you the essential skills of a data analyst. you will cover skills such as data mining, manipulating data, visualising data, and reporting on data. study time for the qualification is approximately 30 hours and qualification is achieved by completing a 1-hour multiple choice exam. the course is provided online and comes complete with exam simulators and revision tools. you will be provided with access to a tutor and a 1-hour online revision workshop prior to sitting your exam. this qualification will set you on a route to becoming a data analyst. step 2 - data administration training data is extracted using various method which are normally it driven. therefore, we will provide you with access to the following additional courses to ensure you are suitably employable as a data analyst. • microsoft excel - to expert level. • learn sql - the programming language used for extracting data from more complex databases. • learn python 3 - python 3 is a simple to use programming language used by many analysts. • learn tableau - tableu is a tool which analysts use to build visually appealing dashboards for complex data making it easier for stakeholders to comprehend. study time for these courses is approximately 30-60 hours and qualification is achieved through completion of the course with no exam. step 3 - business analysis foundation business analysts and data analysts often work very closely together and indeed many data analysts become business analysts and vice versa. adding a business analyst foundation certificate will make you much more employable and enable you to cross over into business analyst roles and hybrid roles. business analysis foundation accredited by the bcs (the british computer society also known as the chartered institute of it). study time for this course is approximately 15 hours and qualification is achieved through an online bcs exam. step 4 - recruitment support now you are qualified for an entry level data analyst position our recruitment support team will now begin collaborating with you to help you secure your first a suitable role based on your new qualifications and any other experience you may have. we have been helping candidates start and build careers in the project management industry since 2007 and have a 4.8 trustscore on trustpilot. our recruitment support team will help you work through job applications, interviews and provide you with a full cv review based around your new qualification and written to maximise your chances of obtaining a role in the project management sector. they will provide you with guidance as to which roles our most suitable for you as an entry level project sector worker aiming to become a project manager. this will include mock job interviews as well as any help you feel you need. our money back guarantee if after 1 year of passing your formal qualifications, we have been unable to help you secure a role, we will refund your study fees minus the cost of the exams. however, we are normally able to help candidates find their first role within 6 months of qualifying and for locations based close to major cities this is often reduced to less than 3 months. check our testimonials from the hundreds of candidates we have already helped. what now? to accommodate candidates, the training element of the package is available on finance terms of up to 1 year. this enables you to get qualified and start in your new role without having to fund all the training costs up front. enquire now and one of our experienced career consultants will contact you within 4 working hours to answer any questions that you may have and to assist you in taking the first step towards your data analyst career aspirations.",edinburgh,Data Analyst,"['dashboard', 'data analysis', 'data analytics', 'excel', 'python', 'r', 'sql', 'tableau']","['dashboard', 'data analysis', 'data analytics', 'excel', 'python', 'r', 'sql', 'tableau']",
senior data analyst,rick stein head office,"senior data analyst • £40,000-£45,000 per annum. • plus, up to £2,400 per year in tips (paid weekly, based on last year’s earnings), giving total potential earnings of £47,400 per year. • permanent contract 40hr week. we’re looking for a talented and driven senior data analyst to play a key role in transforming how we use data to make smarter business decisions. you’ll be responsible for ensuring the consistency, accuracy, and integrity of our data across multiple operational systems, developing insightful analysis, and building robust forecasting models that directly support commercial and operational performance. this is a fantastic opportunity for someone who thrives on problem-solving, enjoys working collaboratively, and wants to make an impact by improving the way data informs strategy and drives performance. duties and responsibilities • create and maintain a consistent data structure across all operational systems (access, fourth, mapal, alert65), establishing rules and audit checks to ensure ongoing compliance and data integrity. • oversee performance reporting and analysis, translating operational and business data into actionable insights. • build reliable analytical models to improve forecasting and demand planning for centralised stock, providing recommendations to the commercial team. • develop performance forecasting templates to enhance both financial and non-financial decision-making. • produce clear training materials and “how-to” guides to maintain high data quality across teams and systems. • support supplier onboarding by ensuring catalogue data is accurate and consistent. • maintain and manage data across core systems, including: products & promotions in epos, recipes and product catalogues in pw • introduce and integrate third-party data sources to enhance business insight and decision-making. • identify opportunities for efficiency through automation, ai, improved structure, and governance. • monitor market trends and performance, providing comparative analysis and actionable recommendations to enhance competitiveness. experience: • proven experience in a data analysis or business intelligence role, ideally within a commercial or operational environment. • strong skills in data modelling, reporting, and visualisation (e.g. power bi, excel, sql). • excellent attention to detail and commitment to maintaining high data quality. • experience managing data across multiple systems or platforms. • strong communication skills, with the ability to explain insights to non-technical stakeholders. • proactive, collaborative, and adaptable approach to problem-solving.",united kingdom,Data Analyst,"['business intelligence', 'data analysis', 'excel', 'power bi', 'r', 'recommendation', 'sql']","['business intelligence', 'data analysis', 'excel', 'power bi', 'r', 'recommendation', 'sql']",£40k–£45k a year
13487- postdoctoral data analyst,university of edinburgh,"grade ue07 - £41 604 – £48,822 per annum college of medicine and veterinary medicine / school of population health sciences / usher institute full time (35hrs per week) fixed term: available from january 2026 to 31st december 2030 the opportunity: the centre for medical informatics at the usher institute within the university of edinburgh is looking for a postdoctoral data analyst who will use scottish data infrastructure of linked electronic healthcare records to investigate short term and long term outcomes on infants from respiratory syncytial virus (rsv) maternal vaccine the post holder will be working with large amounts of highly complex confidential electronic health data to develop and implement data driven approaches to the above study aims. we will also consider requests for hybrid working (on a non-contractual basis) that combines a mix of remote and regular (weekly) on-campus working. the usher institute expects a minimum of 40%* on campus working. • can be increased depending on business requirements / role. informal enquiries may be directed to dr ting shi, senior lecturer (ting.shi@ed.ac.uk) your skills and attributes for success: • data linkage • electronic health records • infectious disease epidemiology • respiratory syncytial virus vaccine",edinburgh,Data Analyst,['r'],['r'],
data analyst and coach,dudley group of hospitals nhs trust,"job summary do you love data and strive to make data accessible to all? the improving together team at the dudley group nhs foundation trust are looking for a data analyst and coach who is passionate about improvement and using data to focus our curiosity. you will be joining an established quality improvement team who support the trust to develop an improvement mindset, empowering staff to make change, improving our patients and staffs experience. the improving together team provides support for all staff to develop improvement skills and behaviour with data at the centre of their thinking. we provide engaging and interactive training and support for all staff and colleagues. the data coach will be integral in the delivery of the data collection, analysis and delivery of our vision of 'creating a culture of continuous improvement delivering safe, high quality and compassionate efficient care'. a large part of the role will include face to face coaching with stakeholders to support the use of data for insight and decision making. based on our core principles of respect for people and a coaching style of leadership. main duties of the job as the improving together data coach and analysist you will • research, analyse and present data in an engaging way. • coach staff in the use of data for improvement. • prepare, deliver and maintain data packs for improvement events and performance boards • support the data analysis and deliver for the management system. • present data and train data tools in an engaging and interactive way. • provide high quality visual management for the improving together team. about us at the dudley group our patients and staff are at the heart of all that we do and that is to provide a world class service that aligns with our vision of 'excellent health care, improved health for all'. we are seeking to recruit staff who share our vision and values of making dudley group an incredible workplace. as part of the nhs people promise, your wellbeing is our priority, and we are committed to ensuring our employees achieve a healthy work-life balance supported by our flexible working options and by making reasonable adjustments where possible. we believe no-one should have to sacrifice family, friends, or their personal interests for work. we are a modern employer able to attract and retain high quality staff through our commitment to the nhs people promise. as an employer, we foster an inclusive environment where everyone feels valued, supported, and empowered to contribute their best. we want to ensure that our workforce is representative of the population we serve. more information - inclusive practice people promise. dudley group offer many opportunities for our staff to develop and grow within their roles through our learning & od team. these include topics such as communication, wellbeing, team development, cultural competency, and values. please click below to view our edi page: equality, diversity, and inclusion - the dudley group nhs foundation trust (dgft.nhs.uk) details date posted 05 december 2025 pay scheme agenda for change band band 6 salary £38,682 to £46,580 a year per annum contract permanent working pattern full-time reference number 253-1225-7640703 job locations russell hall hospital pensnett road dudley dy1 2hq job description job responsibilities job summary professional skills the improving together data analyst and coach is responsible for supporting clinical and operational teams in identifying opportunities to continually improve clinical workflows, increase clinical coding quality and provide analytical support. the data analyst & coach will build relationships with both clinical and operational staff at all levels across the trust to ensure the needs of the organisation are understood and met, and that all improvements are measurable across the domains of delivery, quality, cost and morale. the data coach will identify and recommend opportunities for improvements to clinical workflow and business processes through the exploitation of technology and improved use of applications or changes to business processes. the post holder is responsible for delivering the following activities: • collecting, analysing and reporting data measures associated with improvement activity being undertaken by the improving together team. preparing data reports in an accessible and engaging format. • coaching staff in the use of the data relevant to their service or department. • requirements gathering - determine reporting requirements through a consultative process. • development develop the data architecture to support data mining and drill down reporting by roles and across pathways. • analytics identify data analytic techniques to support analyses and optimise the design of clinical workflows, define measures to support operational improvements. • preparing and maintaining data packs for improvement events, and performance boards. • providing data to support the dudley groups management systems. • delivering high quality engaging and interactive training including plot the dots training and data presentations at improvement events. • creating and updating bi dashboards, presenting data to in an engaging and accessible way with an improvement focus to provide insight and support decision making. • provide high-quality visual management for the improving together team, also supporting the maintenance and development of social media and intranet pages in a creative and visually engaging manner. behavioural skills a high competency in all forms of communication is essential; one to one, in meetings, written, verbal, teaching and presentation skills. it is also important to be able to choose a style of communication appropriate to the situation and audience ranging from operational and clinical staff to presenting formal reports to the clinical leads. as part of the team the data analyst is required to demonstrate continuously high standards of customer care as well as the ability to respond efficiently and effectively to rapidly changing technical and business developments and ever-expanding user expectations. solid organisational skills, multi-tasking and prioritising are fundamental to the post-holders success. there will also be an opportunity in this role to develop improvement workshop facilitation skills. knowledge and experience the post holder will continually develop their working understanding of the analytical techniques and tools, evaluate business processes in action in an acute hospital to facilitate effective engagement and collaborate with operational staff to analyse processes and develop data analytics to identify opportunities and optimise practice. the post holder will also keep themselves up to date with population health developments and big data analytical methods. job description job responsibilities job summary professional skills the improving together data analyst and coach is responsible for supporting clinical and operational teams in identifying opportunities to continually improve clinical workflows, increase clinical coding quality and provide analytical support. the data analyst & coach will build relationships with both clinical and operational staff at all levels across the trust to ensure the needs of the organisation are understood and met, and that all improvements are measurable across the domains of delivery, quality, cost and morale. the data coach will identify and recommend opportunities for improvements to clinical workflow and business processes through the exploitation of technology and improved use of applications or changes to business processes. the post holder is responsible for delivering the following activities: • collecting, analysing and reporting data measures associated with improvement activity being undertaken by the improving together team. preparing data reports in an accessible and engaging format. • coaching staff in the use of the data relevant to their service or department. • requirements gathering - determine reporting requirements through a consultative process. • development develop the data architecture to support data mining and drill down reporting by roles and across pathways. • analytics identify data analytic techniques to support analyses and optimise the design of clinical workflows, define measures to support operational improvements. • preparing and maintaining data packs for improvement events, and performance boards. • providing data to support the dudley groups management systems. • delivering high quality engaging and interactive training including plot the dots training and data presentations at improvement events. • creating and updating bi dashboards, presenting data to in an engaging and accessible way with an improvement focus to provide insight and support decision making. • provide high-quality visual management for the improving together team, also supporting the maintenance and development of social media and intranet pages in a creative and visually engaging manner. behavioural skills a high competency in all forms of communication is essential; one to one, in meetings, written, verbal, teaching and presentation skills. it is also important to be able to choose a style of communication appropriate to the situation and audience ranging from operational and clinical staff to presenting formal reports to the clinical leads. as part of the team the data analyst is required to demonstrate continuously high standards of customer care as well as the ability to respond efficiently and effectively to rapidly changing technical and business developments and ever-expanding user expectations. solid organisational skills, multi-tasking and prioritising are fundamental to the post-holders success. there will also be an opportunity in this role to develop improvement workshop facilitation skills. knowledge and experience the post holder will continually develop their working understanding of the analytical techniques and tools, evaluate business processes in action in an acute hospital to facilitate effective engagement and collaborate with operational staff to analyse processes and develop data analytics to identify opportunities and optimise practice. the post holder will also keep themselves up to date with population health developments and big data analytical methods. person specification skills essential • ability to communicate information in an accessible and engaging format. experience essential • experience of data analytics and reporting using sql, excel and microsoft power bi. • coaching - experience of helping others in the use of data and teaching non-technical staff to produce and understand data reporting tools such as spc charts. qualifications essential • degree level qualification or equivalent experience in relevant subject person specification skills essential • ability to communicate information in an accessible and engaging format. experience essential • experience of data analytics and reporting using sql, excel and microsoft power bi. • coaching - experience of helping others in the use of data and teaching non-technical staff to produce and understand data reporting tools such as spc charts. qualifications essential • degree level qualification or equivalent experience in relevant subject disclosure and barring service check this post is subject to the rehabilitation of offenders act (exceptions order) 1975 and as such it will be necessary for a submission for disclosure to be made to the disclosure and barring service (formerly known as crb) to check for any previous criminal convictions. certificate of sponsorship applications from job seekers who require current skilled worker sponsorship to work in the uk are welcome and will be considered alongside all other applications. for further information visit the uk visas and immigration website (opens in a new tab). from 6 april 2017, skilled worker applicants, applying for entry clearance into the uk, have had to present a criminal record certificate from each country they have resided continuously or cumulatively for 12 months or more in the past 10 years. adult dependants (over 18 years old) are also subject to this requirement. guidance can be found here criminal records checks for overseas applicants (opens in a new tab). additional information disclosure and barring service check this post is subject to the rehabilitation of offenders act (exceptions order) 1975 and as such it will be necessary for a submission for disclosure to be made to the disclosure and barring service (formerly known as crb) to check for any previous criminal convictions. certificate of sponsorship applications from job seekers who require current skilled worker sponsorship to work in the uk are welcome and will be considered alongside all other applications. for further information visit the uk visas and immigration website (opens in a new tab). from 6 april 2017, skilled worker applicants, applying for entry clearance into the uk, have had to present a criminal record certificate from each country they have resided continuously or cumulatively for 12 months or more in the past 10 years. adult dependants (over 18 years old) are also subject to this requirement. guidance can be found here criminal records checks for overseas applicants (opens in a new tab). employer details employer name the dudley group nhs foundation trust address russell hall hospital pensnett road dudley dy1 2hq employer's website https://www.dgft.nhs.uk/ (opens in a new tab) employer details employer name the dudley group nhs foundation trust address russell hall hospital pensnett road dudley dy1 2hq employer's website https://www.dgft.nhs.uk/ (opens in a new tab)",united kingdom,Data Analyst,"['dashboard', 'data analysis', 'data analytics', 'excel', 'power bi', 'r', 'sas', 'sql']","['dashboard', 'data analysis', 'data analytics', 'excel', 'power bi', 'r', 'sas', 'sql']",
remote work: online data analyst - odia (uk),telus digital,"tasks are you a detail-oriented individual with a passion for research and a good understanding of national and local geography? this freelance opportunity allows you to work at your own pace and from the comfort of your own home. a day in the life of an online data analyst: • in this role, you will be working on a project aimed at enhancing the content and quality of digital maps that are used by millions of people worldwide • completing research and evaluation tasks in a web-based environment such as verifying and comparing data, and determining the relevance and accuracy of information. join us today and be part of a dynamic and innovative team that is making a difference in the world! telus digital ai community our global ai community is a vibrant network of 1 million+ contributors from diverse backgrounds who help our customers collect, enhance, train, translate, and localize content to build better ai models. become part of our growing community and make an impact supporting the machine learning models of some of the world’s largest brands. requirements qualification path no previous professional experience is required to apply to this role, however, working on this project will require you to pass the basic requirements and go through a standard assessment process. this is a part-time long-term project and your work will be subject to our standard quality assurance checks during the term of this agreement. basic requirements • full professional proficiency in odia and english language • being a resident in the united kingdom or the last 2 consecutive years and having familiarity with current and historical business, media, sport, news, social media, and cultural affairs in the united kingdom • ability to follow guidelines and conduct online research using search engines, online maps, and website information • flexibility to work across a diverse set of task types, including maps, news, audio tasks, and relevance • daily access to a broadband internet connection, computer, and relevant software assessment in order to be hired into the program, you’ll take an open book qualification exam that will determine your suitability for the position and complete id verification. our team will provide you with guidelines and learning materials before your qualification exam. you will be required to complete the exam in a specific timeframe but at your convenience. equal opportunity all qualified applicants will receive consideration for a contractual relationship without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or protected veteran status. at telus digital ai, we are proud to offer equal opportunities and are committed to creating a diverse and inclusive community. all aspects of selection are based on applicants’ qualifications, merits, competence, and performance without regard to any characteristic related to diversity.",anywhere,Data Analyst,"['machine learning', 'r']","['machine learning', 'r']",
senior market data analyst,fentexa,"role senior market data analyst position contract 12 months extendable location london hybrid rates good day rates my client a leading investment bank are looking for a seasoned market data sme to act as there go-to authority for all aspects of market data ingestion, management, vendor relationships, licensing, and data governance. the successful candidate will work across trading, quant, risk, operations, and technology teams to ensure high-quality, reliable and compliant market data coverage across asset classes (e.g. equities, fixed income, fx, derivatives, commodities, as relevant). role • own the end-to-end lifecycle of market data — from vendor selection and licensing, ingestion, validation, transformation, storage, to publication/use across internal systems. • serve as the liaison between business stakeholders (trading desks, quant, risk, operations), it / data engineering teams, and external vendors. • ensure data quality, consistency, robustness, and timely availability. • maintain metadata catalogues, data dictionaries, and documentation of data flows, entitlements, usage rights, and compliance policies. • provide expert advice and support on market data vendor offerings, licensing models, cost optimisation, vendor negotiation and contract review (for firms where licensing/commercial terms are relevant). skills needed • demonstrable expertise in market data — including understanding of different asset classes (equities, fixed income, derivatives, fx, commodities, as relevant). • strong working knowledge of major market data vendors and data feeds, licensing models, reference data, and real-time data services.",united kingdom,Data Analyst,['r'],['r'],
data analyst/ editor (m/w/d),wallstreetdocs,"data analyst/ editor (m/w/d) eintritt: zum nächstmöglichen zeitpunkt wir suchen eine engagierte persönlichkeit zur verstärkung unseres daten-teams – mit einem ausgeprägten interesse an finanzmärkten, einer schnellen auffassungsgabe und einem blick für datenqualität. in dieser position bist du teil eines projektteams und übernimmst die analyse, erfassung und qualitätssicherung strukturierter finanzprodukte wie zertifikate und optionsscheine in unseren zentralen stammdatenbanken. deine aufgaben: • pflege und laufende aktualisierung von produktstammdaten • sicherstellung der datenqualität durch standardisierte prüfmechanismen • erstellung, betreuung und weiterentwicklung von datenimportprozessen • bearbeitung interner und externer supportanfragenzusammenarbeit mit dem eigenen team sowie weiteren spannenden fachabteilungen dein profil: • abgeschlossene ausbildung im kaufmännischen oder it-nahen bereich (z. b. bankkaufmann/-frau, it-systemkaufmann/-frau) – motivierte quereinsteiger sind herzlich willkommen • interesse an finanz- und börsenthemen, idealerweise erste berührungspunkte mit strukturierten finanzprodukten • it-affinität und idealerweise erste erfahrungen im umgang mit datenbanken, sql-kenntnisse von vorteil • analytisches denken und hohe genauigkeit im umgang mit komplexen datenstrukturen • sehr gute deutsch- und gute englischkenntnisse in wort und schrifteigenverantwortliche, proaktive und lösungsorientierte arbeitsweise was wir bieten: • ein motiviertes, kollegiales team mit flachen hierarchien • eine strukturierte einarbeitung in alle aufgabenbereiche • spannende einblicke in das dynamische umfeld der strukturierten finanzprodukteraum für persönliche weiterentwicklung und den ausbau technischer kompetenzen wenn sie über die nötigen fähigkeiten, erfahrungen und die motivation verfügen, in dieser anspruchsvollen und lohnenden position erfolgreich zu sein, freuen wir uns auf ihre bewerbung. bewerben sie sich noch heute und machen sie den nächsten schritt in ihrer karriere mit uns! wsd is an employer that values diversity. we highly encourage applications from appropriately qualified and eligible candidates irrespective of age, race, religion, national origin, gender, sexual orientation, gender identity and/or expression, veteran status, disability, or any other status protected by applicable law.",anywhere,Data Analyst,"['r', 'sql']","['r', 'sql']",
bi developer & data analyst,the wise group,"position job bi developer & data analyst location flexible working - principally home based with some travel to our glasgow office as required salary £40,000 + excellent benefits hours 35 hours per week (flexible working) status permanent closing date 9 december 2025 do analysis that matters. at the wise group, we support more than 50,000 households each year – transforming lives and lifting people out of poverty. we create stability in households, improve quality of life, and put money back into people’s pockets. join us and use your skills to turn rich, sector-leading data into decisions that change lives every day. why this role is special data, dashboards, ai, and analysis are critical in our 5-year strategy. you will be integral to the delivery of that strategy. your work will improve performance, prove our efficacy and credibility, maximise the improvement in people’s lives, and increase our influence nationally. the bi dashboards and adhoc data analysis you carry out will be used every day by dozens of colleagues, and external stakeholders. you will turn findings into actionable recommendations that tell clear stories with the data. your work will be at the heart of our decision making. you’ll influence what we do, why we do it, and improve the lives of the people we support. we’re a small team. you’ll work directly with our director of data & insights, wearing multiple hats across bi development, data engineering, analytics and ai. there’s plenty of variety and opportunity to enhance your skills. what you’ll do • build and evolve our microsoft-centric data platform alongside internal colleagues and trusted partners. • design robust pipelines (elt/etl) for new data sources, and manage existing ones. • create insightful power bi reporting, working with stakeholders to co-create the requirements. • deliver analysis that drives decisions. from exploratory deep-dives to forecasting and clear recommendations, you will tell memorable stories with data — not just numbers. • champion governance and quality. be a steward of our definitions and metric catalogues, improve data quality, document as you go and keep privacy/gdpr front-of-mind. • produce evidence packs and impact summaries for tenders and grants, and automate recurring reporting where possible. • deliver lightweight training and 1-to-1 support to build confident self-serve capability across the enterprise. what you’ll bring to the role • strong power bi (datasets, dax, modelling, rls, performance) and azure / fabric experience. • hands-on sql and pipeline development (e.g. fabric data pipelines / data factory). • git/azure devops, automated testing and environment promotion. • python for wrangling, notebooks, apis and basic ml; advanced m/power query. • sound data architecture knowledge (star schemas, medallion architecture, semantic layers, dictionaries and lineage). • confidence in stakeholder engagement — gathering requirements, writing clear specs, prioritising a shared backlog and communicating simply. • comfortable switching between bi developer, analyst and engineer personas in a fast-moving, mission-led setting. don’t have all the skills or experience? we’re looking for someone with enthusiasm and passion for the role, who can learn quickly and fill in their skill and experience gaps. even if you don’t have all the skills or experience for this role, use your application to tell us how you’ll overcome that. we’re seeking a colleague who wants to transform lives and is motivated to make a tangible difference through effective use of data. view the role profile for full details about this opportunity.",glasgow,Data Analyst,"['azure', 'dashboard', 'data analysis', 'data pipeline', 'elt', 'etl', 'excel', 'power bi', 'python', 'r', 'recommendation', 'sql']","['azure', 'dashboard', 'data analysis', 'data pipeline', 'elt', 'etl', 'excel', 'power bi', 'python', 'r', 'recommendation', 'sql']",£40k a year
postdoctoral data analyst,the university of edinburgh,"full time (35hrs per week) fixed term: available from january 2026 to 31st december 2030 the opportunity: the centre for medical informatics at the usher institute within the university of edinburgh is looking for a postdoctoral data analyst who will use scottish data infrastructure of linked electronic healthcare records to investigate short term and long term outcomes on infants from respiratory syncytial virus (rsv) maternal vaccine the post holder will be working with large amounts of highly complex confidential electronic health data to develop and implement data driven approaches to the above study aims. we will also consider requests for hybrid working (on a non-contractual basis) that combines a mix of remote and regular (weekly) on-campus working. the usher institute expects a minimum of 40%* on campus working. • can be increased depending on business requirements / role. informal enquiries may be directed to dr ting shi, senior lecturer (ting.shi@ed.ac.uk) your skills and attributes for success: • data linkage • electronic health records • infectious disease epidemiology • respiratory syncytial virus vaccine £41,064 to £48,822 per annum grade ue07",edinburgh,Data Analyst,['r'],['r'],"£41,064–£48,822 a year"
sas data analyst,inov8 consulting ltd,"the role inov8 are looking for data analysts / engineers to join our expanding team. you will be designing, developing and supporting a wide range of data solutions. the role will involve extensive programming, data preparation and reporting on a range of projects across various business areas and technologies. we offer one day offsite each month to aid ongoing skills development and certification in your chosen specialities. the majority of our business is on site with clients in edinburgh and we have an office in edinburgh park to provide flexible working. this is an exciting opportunity for a professional to take on a fresh challenge with a rapidly growing and dynamic company. the key responsibilities of a data analyst / engineer include, but are not limited to: • design, develop and support a wide range of data solutions using a range of platforms, software and coding toolsets including, but not limited to sas, python and pyspark. • data preparation, transformation and presentation using sql on a number of different database platforms. • collaborate with others to create and maintain training materials and training plans. • assist with developing both current and new client relationships. • maintain skillset, develop new skills and keep up to date with the evolution of tools and services. • deliver clear handover documentation to the client for any item of work. • communicate daily with stakeholders at all levels. the company inov8 are a professional services company specialising in data analytics, data engineering, cloud engineering and delivery. we have grown from a solid foundation of client delivery across many sectors ensuring that quality and value are at the heart of everything we do. through engaging with our clients, listening to what is important to them and then delivering clear plans to achieve those goals (in the most efficient and cost-effective way) we continue to grow our reputation as a company that delivers on their promises, getting the most out of your data. the benefits the benefits of being a data analyst / engineer are: • excellent salary + opportunities for some flexible working with minimal travel • certification reward scheme • investment in skills and development • private medical care package • ongoing training and career progression the person the key skills and qualities of a data analyst / engineer: • must have excellent sas experience within financial services • sql / python and relevant experience delivering data solutions • solution development experience • experience of financial services • service recommendation knowledge • strong communication skills • ability to manage a team • ability to adapt to new technologies and learn quickly the following skills and qualities are also highly desirable: • sas certifications • microsoft data certifications • machine learning experience • knowledge of physical database design the successful candidate must have the right to work in the uk, now and in the future, be able to attend our office and client offices in edinburgh, and pass background checks (both credit and criminal). if you are keen on joining this exciting, forward thinking company and taking the first step in your data career, then please click the apply now button to find out more. job types: full-time, permanent pay: £40,000.00-£55,000.00 per year benefits: • company events • company pension • on-site parking • private medical insurance • referral programme • work from home application question(s): • please provide details of any work visa you possess or will require in the future. (including post study work, dependant visa, etc) experience: • financial services: 2 years (required) • sas: 3 years (required) • python: 2 years (required) • sql: 3 years (required) work authorisation: • united kingdom (required) work location: hybrid remote in edinburgh, eh12 9dt",edinburgh,Data Analyst,"['cloud', 'data analytics', 'excel', 'machine learning', 'pyspark', 'python', 'r', 'recommendation', 'sas', 'spark', 'sql']","['cloud', 'data analytics', 'excel', 'machine learning', 'pyspark', 'python', 'r', 'recommendation', 'sas', 'spark', 'sql']",£40k–£55k a year
energy & water data analyst,io associates,"energy & water data analyst • location broughton • sector: sustainability • job type: contract • salary: negotiable • contact: diksha narula • contact email: d.narula@ioassociates.co.uk • job ref: bbbh171061_1765211334 energy & water data analyst broughton - onsite 10+ months £40 per hour inside ir35 a highly skilled energy & sustainability data specialist is required to join a leading global aerospace organisation, supporting the uk facilities management & real estate (fmre) energy & sustainability team. this role is critical in managing the full energy and water data lifecycle, ensuring compliance and driving progress toward the company's ambitious 2030 sustainability targets. in this role, you will manage and configure energy management systems (enms) and related data platforms, analysing energy and water data to identify trends, anomalies, and opportunities for improvement. you will produce kpi reports, including water kpis, while ensuring data integrity across all systems. required skills & experience • 5+ years in energy or environmental management. • strong knowledge of energy data lifecycle (bms, metering, ems such as esight). • good understanding of water systems and consumption reduction methods. • experience with energy/water compliance and audits. • excellent communication, analytical, and stakeholder engagement skills. this is an exciting opportunity to contribute to major sustainability and decarbonisation initiatives within a globally recognised aerospace leader.",united kingdom,Data Analyst,"['excel', 'r']","['excel', 'r']",
reference data analyst,balyasny asset management l.p.,"role overview in the role of reference data operations analyst, the employee will be responsible for the following: • driving the highest quality reference data for trading, risk, and operations through the application of data quality techniques • develop and apply expertise in the interpretation of corporate actions impact on the firm and on trading • supporting risk and financial engineers in the modeling of referential data to support analyses • responsible for the creation and ongoing maintenance of security set-ups which include but are not limited to equities, bonds, irs, cds, listed options, forwards, and exotic derivatives (fra’s, fva,s digital options, contingent options, worst of options, etc…) • supporting various firm-wide stakeholders of referential elements (account master, broker master, charge mgmt tool, etc...) that enable trading, settlements, and reporting needs • working with vendors to source data on demand • working with it development teams to build out new capabilities to streamline daily operations • responding to trader queries on positions and pnl showing in firm-wide systems • working with our operations outsourcer on corporate action and security master queries qualifications & requirements: • in order to effectively represent the company and communicate with clients, the employee must be someone who has: • high levels of curiosity and is a self-starter and has proven this by coming up with ideas and enhancements to improve security master processes • high levels of responsibility and organization and has been able to meet daily and ad-hoc initiatives • effective communication skills to ensure proper hand offs and escalation between team members • passion and drive for analyzing, researching, and resolving data issues using various tools • an outgoing personality that has demonstrated to effectively work with people across risk, compliance, technology, front, middle, and back office. education, training & experience: • bachelors’ degree in finance or economics • minimum 3 years of relevant work experience, including specific experience outlined below is mandatory: • heavy data entry experience maintaining static/reference data primarily around security master data • minimum 3 years’ experience maintaining security master data for equity, fixed income, options, credit default swaps, and interest rate swaps. • minimum 3 years’ experience working resolving data quality exceptions and reconciliations • minimum 1 year experience using jira or similar issue tracking system • added technical skills • has used excel for 3 years effectively with financial services data • ability to run vlookups, hlookup, counts, searches and leverage pivot tables to join, organize, and analyze data across large datasets • has used sql editor for 1 year with financial services data • ability to run search, join, order by, and group by queries to analyze data across large db tables • proficient using bloomberg terminal and related functions • ability to use the terminal to research and analyze various bbg data points • ability to create formulas to pull data into excel • understand various mechanisms bbg offers has to help drive automated security master processes",united kingdom,Data Analyst,"['excel', 'r', 'scala', 'sql']","['excel', 'r', 'scala', 'sql']",
senior product data analyst,booksy,"booksy connects beauty, wellness and health professionals with local customers, powering millions of appointments annually. as a booksy employee, you’ll join a team of 800+ around the world building the fastest growing appointments marketplace. regardless of the position you hold, you’ll make a direct impact every day helping our providers grow their businesses and build loyal client relationships in their communities. working in a rapidly growing, ever-changing scale-up comes with its own set of opportunities and challenges. if you prefer a stable environment, with clear processes and structures then, we've got to be honest, you won't always find that here. however, if you enjoy inventively solving problems with others, helping create clarity when things get confusing, and prioritising your own path within ambiguity, then the chances are that you'll love the opportunities available to grow your career at booksy. our vision be the destination for scheduling, empowering service providers to thrive and consumers to discover and book services. our values • people first: we empower and elevate the service provider, the consumer, and their communities. • act like an owner: we take responsibility for our actions and their results. • work as a team: we collaborate and care about the success of our team and others. • shoot for the moon: we have ambitious goals and overcome obstacles with tenacity. interested in joining us? due to our explosive growth and global expansion, we are looking for an experienced senior product data analyst, who has strong numerical and analytical skills, but also an understanding of business metrics and a commercial mindset. at this position, you will analyze millions of transactions and data points recorded on our clients all around the world. you will have the opportunity to work in an agile environment with scrum teams. your responsibilities • drive insights: understand funnels, ecosystems, user behaviors, and long-term trends in the adoption of our products to identify opportunities for step-changes of specific customer segments. • metric definition: define and analyze key metrics that reflect the success of products and allow us to monitor the health of product adoption in the markets we serve. • growth opportunities: identify growth opportunities, remove barriers for product adoption, retention, engagement, and/or monetization by quantitative analysis, data mining, and the presentation of data to see beyond the numbers and understand how our merchants and consumers interact with booksy products. • reporting: prepare, monitor and adjust reports and reporting tools to best contribute to product team results. • collaboration: cooperate with product teams to support them in their product related decisions based on data. • process automation: lead efforts to automate data management and reporting processes, working closely with business intelligence analysts and data engineers to enhance efficiency. • ownership: take ownership of the more complex business challenges, owning them end to end with minimal levels of support. • collaboration with stakeholders: consulting with senior stakeholders across the organisation to understand their business needs and determine the best data / analytics approach delivering value from our data in a clear and actionable manner. • co-responsible for driving the development of the domain and mentoring less experienced analysts. requirements • extensive experience in an analytical role dealing with large volumes of data. • excellent experience working with databases and excellent knowledge of sql (bigquery would be an asset) to create complex queries and analyses. • strong experience with product optimization or growth work i.e. product a/b testing, funnel/website optimization, conversion analysis, events monitoring. • collaboration: ability to collaborate and communicate effectively within the team, with internal stakeholders i.e. product managers, engineers, ux designers, data engineers. • data-driven insights: proven experience in providing insights and recommendations based on data analysis. • communication skills: excellent verbal and written communication skills, with the ability to present complex findings to both technical and non-technical audiences. • data visualization: familiarity with data visualization tools like powerbi, looker, tableau, qlikview, or others. • educational background: a higher degree in statistics, mathematics, or another technical field. • language skills: proficiency in spoken and written english.",anywhere,Data Analyst,"['a/b testing', 'bigquery', 'business intelligence', 'data analysis', 'excel', 'looker', 'r', 'recommendation', 'sql', 'statistics', 'tableau']","['a/b testing', 'bigquery', 'business intelligence', 'data analysis', 'excel', 'looker', 'r', 'recommendation', 'sql', 'statistics', 'tableau']",
investment data analyst,addepar,"who we are addepar is a global technology and data company that helps investment professionals provide the most informed, precise guidance for their clients. hundreds of thousands of users have entrusted addepar to empower smarter investment decisions and better advice over the last decade. with client presence in more than 50 countries, addepar's platform aggregates portfolio, market and client data for over $8 trillion in assets. addepar's open platform integrates with more than 100 software, data and services partners to deliver a complete solution for a wide range of firms and use cases. addepar embraces a global flexible workforce model with offices in new york city, salt lake city, london, edinburgh, pune, dubai, geneva, and são paulo. the role as an investment data analyst, you will partner with clients to integrate and analyse multi-asset class portfolios, performance data, and market data from a wide range of sources. you'll advise on investment workflows, ensuring accuracy and consistency, while collaborating closely with other data solutions consultants and internal teams to deliver seamless client outcomes. this role is ideal for someone who thrives at the intersection of finance and data, is solutions-oriented, and enjoys working directly with clients. applicants must have, and maintain, the right to work in the united kingdom from the first day of employment. please note that visa sponsorship is not available for this role. what you'll do • translate unique client requirements into flexible and scalable investment data solutions • lead data conversion projects to integrate historical portfolio data from legacy systems into addepar • work directly with complex investment datasets, including multi-asset class portfolios, performance data, and market data from various sources • advise clients on investment data workflows, ensuring accuracy, consistency, and scalability • collaborate closely with other data solutions consultants on technical implementations to ensure smooth onboarding and delivery • identify and drive opportunities to improve processes, tools, and data quality standards • communicate proactively and professionally with clients and internal stakeholders who you are • minimum of 2+ years' experience working in technology, finance, or consulting • strong understanding of a wide range of financial instruments, including equities, fixed income, derivatives, and alternative investments • hands-on experience working with complex investment datasets, including multi-asset class portfolios, performance data, and market data from various sources • solutions-oriented mindset and passion for problem-solving • excellent communication, organisational, and time-management skills • strong work ethic; proactive and a high-contributing team-mate • highly organised with great attention to detail, driven to make processes more efficient • independent, adaptable, and able to thrive in a fast-paced environment • strong proficiency with excel (pivot tables, lookups, nested formulas, data cleaning/validation); ability to structure and manipulate complex datasets • experience with python is an advantage but not a requirement our values • act like an owner - think and operate with intention, purpose and care. own outcomes. • build together - collaborate to unlock the best solutions. deliver lasting value. • champion our clients - exceed client expectations. our clients' success is our success. • drive innovation - be bold and unconstrained in problem solving. transform the industry. • embrace learning - engage our community to broaden our perspective. bring a growth mindset. in addition to our core values, addepar is proud to be an equal opportunity employer. we seek to bring together diverse ideas, experiences, skill sets, perspectives, backgrounds and identities to drive innovative solutions. we commit to promoting a welcoming environment where inclusion and belonging are held as a shared responsibility. we will ensure that individuals with disabilities are provided reasonable accommodation to participate in the job application or interview process, to perform essential job functions, and to receive other benefits and privileges of employment. please contact us to request accommodation. phishing scam warning: addepar is among several companies recently made aware of a phishing scam involving con artists posing as hiring managers recruiting via email, text and social media. the imposters are creating misleading email accounts, conducting remote ""interviews,"" and making fake job offers in order to collect personal and financial information from unsuspecting individuals. please be aware that no job offers will be made from addepar without a formal interview process. additionally, addepar will not ask you to purchase equipment or supplies as part of your onboarding process. if you have any questions, please reach out to tainfo@addepar.com.",edinburgh,Data Analyst,"['excel', 'python', 'r', 'scala']","['excel', 'python', 'r', 'scala']",
"python/data engineer - sma solutions, associate",blackrock,"about this role about blackrock sma solutions: at blackrock sma solutions, our strategies are designed to put our clients' and their clients' interests at the center of our investment advice; to minimize costs and taxes; and to incorporate each client's unique values-aligned preferences into their investment portfolio. offering a full suite of tools for our clients, from direct-indexing to active equity and fixed income strategies-with evolution a constant part of our game. as a business, we are fast-growing and our systems need to grow along with it. we are looking for engineers who like to innovate and seek complex problems. we recognize that strength comes from teams with a variety of perspectives, and will embrace your unique skills, curiosity, drive, and passion while giving you the opportunity to grow technically and as an individual. engineers looking to work in the areas of orchestration, data modeling, data pipelines, apis, storage, distribution, distributed computation, consumption and infrastructure are ideal candidates. about this role: we are expanding the team of engineers that owns and operates our extensive data platform underpinning the sma business. as a member of this team, you will build, enhance, and support a wide variety of data-centric applications and tools to generate or consume data feeds from diverse systems; generate reporting artifacts at scale; and expand the data estate in support of our users. you will work with your team and your stakeholders to ensure data arrives in a complete and timely manner, with validated quality. requirements: • ba/bs in computer science or equivalent practical experience • at least 3+ years of post-university experience as a full-stack engineer • solid knowledge of programming fundamentals-algorithms, data structures, design patterns, and paradigms • strong knowledge of python • solid knowledge sql and relational databases • ability to troubleshoot problems in a live environment and provide real time help with critical issues • write code that is easily understood and maintainable by other team members • keep up to date with developments in technologies we are using • ability to communicate and work effectively, supporting various business departments • ability to work in fast-paced interdisciplinary environment desirable additional qualifications: • knowledge of extract-transform-load (etl) and big data analytics tools • experience working in a cloud environment (aws, azure, gcp) • familiarity with popular python data analytics frameworks • familiarity with snowflake or other large analytics engines • experience with data warehousingand working in environments with large scale, complex analytics requirements. • familiarity with dag-based job scheduling tools • cloud-native container orchestration platforms and tools • experience working in the financial services industry for san francisco, ca and sausalito, ca only the salary range for this position is usd$132,500.00 - usd$162,000.00 . additionally, employees are eligible for an annual discretionary bonus, and benefits including healthcare, leave benefits, and retirement benefits. blackrock operates a pay-for-performance compensation philosophy and your total compensation may vary based on role, location, and firm, department and individual performance. for princeton, nj only the salary range for this position is usd$0.00 - usd$0.00 . additionally, employees are eligible for an annual discretionary bonus, and benefits including healthcare, leave benefits, and retirement benefits. blackrock operates a pay-for-performance compensation philosophy and your total compensation may vary based on role, location, and firm, department and individual performance. our benefits to help you stay energized, engaged and inspired, we offer a wide range of benefits including a strong retirement plan, tuition reimbursement, comprehensive healthcare, support for working parents and flexible time off (fto) so you can relax, recharge and be there for the people you care about. our hybrid work model blackrock's hybrid work model is designed to enable a culture of collaboration and apprenticeship that enriches the experience of our employees, while supporting flexibility for all. employees are currently required to work at least 4 days in the office per week, with the flexibility to work from home 1 day a week. some business groups may require more time in the office due to their roles and responsibilities. we remain focused on increasing the impactful moments that arise when we work together in person - aligned with our commitment to performance and innovation. as a new joiner, you can count on this hybrid model to accelerate your learning and onboarding experience here at blackrock. about blackrock at blackrock, we are all connected by one mission: to help more and more people experience financial well-being. our clients, and the people they serve, are saving for retirement, paying for their children's educations, buying homes and starting businesses. their investments also help to strengthen the global economy: support businesses small and large; finance infrastructure projects that connect and power cities; and facilitate innovations that drive progress. this mission would not be possible without our smartest investment - the one we make in our employees. it's why we're dedicated to creating an environment where our colleagues feel welcomed, valued and supported with networks, benefits and development opportunities to help them thrive. for additional information on blackrock, please visit @blackrock | twitter: @blackrock| linkedin:www.linkedin.com/company/blackrock blackrock is proud to be an equal opportunity workplace. we are committed to equal employment opportunity to all applicants and existing employees, and we evaluate qualified applicants without regard to race, creed, color, national origin, sex (including pregnancy and gender identity/expression), sexual orientation, age, ancestry, physical or mental disability, marital status, political affiliation, religion, citizenship status, genetic information, veteran status, or any other basis protected under applicable federal, state, or local law.view theeeoc's know your rights poster and its supplementand thepay transparency statement. blackrock is committed to full inclusion of all qualified individuals and to providing reasonable accommodations or job modifications for individuals with disabilities. if reasonable accommodation/adjustments are needed throughout the employment process, please email disability.assistance@blackrock.com. all requests are treated in line with ourprivacy policy. we recruit, hire, train, promote, pay, and administer all personnel actions without regard to race, color, religion, sex (including pregnancy, childbirth, and medical conditions related to pregnancy, childbirth, or breastfeeding), sex stereotyping (including assumptions about a person's appearance or behavior, gender roles, gender expression, or gender identity), gender, gender identity, gender expression, national origin, age, mental or physical disability, ancestry, medical condition, marital status, military or veteran status, citizenship status, sexual orientation, genetic information, or any other status protected by applicable law. we interpret these protected statuses broadly to include both the actual status and also any perceptions and assumptions made regarding these statuses.blackrock will consider for employment qualified applicants with arrest or conviction records in a manner consistent with the requirements of the law, including any applicable fair chance law.","princeton, nj",Data Engineer,"['aws', 'azure', 'cloud', 'data analytics', 'data pipeline', 'etl', 'gcp', 'python', 'r', 'snowflake', 'sql']","['aws', 'azure', 'cloud', 'data analytics', 'data pipeline', 'etl', 'gcp', 'python', 'r', 'snowflake', 'sql']",
data engineer i,chewy,"our opportunity: chewyʼs ai & data organization is seeking a data engineer i to join the enterprise data systems (eds) team. eds powers the trusted, governed, and intelligent data foundation that fuels analytics, machine learning, and ai across chewy. as a data engineer i, youʼll build your skills as an apprentice data builder—learning how to design, test, and deliver the data pipelines that make insights and ai possible. youʼll work closely with experienced engineers to gain deep exposure to chewyʼs modern data stack, snowflake, dbt cloud, kafka, aws, and tableau, while developing your technical and problem-solving foundation. weʼre looking for teammates who are endlessly curious, passionate about learning, and excited about how data and ai can transform customer and business experiences. what you'll do: • • build and maintain small-scale ingestion and transformation pipelines under mentorship from senior engineers. • write clean, well-documented sql and python for data preparation, validation, and automation. • learn and apply dbt cloud modeling concepts for structured, testable, and auditable data development. • monitor data quality and pipeline performance; assist in triaging and resolving basic issues. • participate in code reviews to understand design principles, version control, and testing best practices. • collaborate with analytics and data science teams to deliver well-structured, reliable datasets. • grow familiarity with kafka streaming, data observability, and cloud infrastructure fundamentals. • maintain documentation for data catalog and objects you own; contribute basic unit tests for pipeline code. • continuously learn about ai, automation, and how they are reshaping data engineering. what you'll need: • 0-2 years of professional experience in data engineering, analytics engineering, or a related technical field. • proficiency in sql and working familiarity with python. • basic understanding of cloud data warehouses (snowflake, redshift, or bigquery preferred). • working familiarity with terraform so you can read modules and safely make small, reviewed plan/apply changes (variables, remote state) toaws/snowflake resources under mentorship. • curiosity about dbt, kafka, and modern orchestration frameworks. • a learning mindset with a passion for ai, data-driven decision-making, and continuous improvement. • clear communication, eagerness to ask questions, and openness to feedback. why you'll love this role: • learn directly from experience data engineers in a collaborative, ai-driven environment. • gain hands-on experience with one of the most modern enterprise data stacks in retail. • contribute to projects that power real business insights and customer experiences. • build a foundation for a long-term career in data, ai, and intelligent systems. the specific salary offered to a candidate may be influenced by a variety of factors including but not limited to the candidate’s relevant experience, education, and work location. in addition, this position is eligible for 401k and a new hire and annual equity grant. we offer different types of insurance and benefits, such as medical/rx, vision, dental, life, disability, hospital indemnity, critical illness, and accident. we offer parental leave, family services benefits, backup dependent care, flexible spending accounts, telemedicine, pet adoption reimbursement, employee assistance program, and many discounts including 10% off pet insurance and 20% off at chewy.com. exempt salary team members have unlimited pto, subject to manager approval. team members will receive six paid holidays per year. team members may be eligible for paid sick and family leave in compliance with applicable state and local regulations. base salary range $96,000—$153,500 usd chewy is an equal opportunity employer. all qualified applicants will receive consideration for employment without regard to race, color, ancestry, national origin, gender, citizenship, marital status, religion, age, disability, gender identity, results of genetic testing, veteran status, as well as any other legally-protected characteristic. if you have a disability under the americans with disabilities act or similar law, and you need an accommodation during the application process or to perform these job requirements, or if you need a religious accommodation, please contact caar@chewy.com. to access chewy's customer privacy policy, please click here. to access chewy's california cpra job applicant privacy policy, please click here.","boston, ma",Data Engineer,"['aws', 'bigquery', 'cloud', 'data pipeline', 'data warehouse', 'dbt', 'kafka', 'machine learning', 'python', 'r', 'redshift', 'snowflake', 'sql', 'tableau']","['aws', 'bigquery', 'cloud', 'data pipeline', 'data warehouse', 'dbt', 'kafka', 'machine learning', 'python', 'r', 'redshift', 'snowflake', 'sql', 'tableau']",
principal data engineer,pepsico,"overview as a member of the data engineering team, you will be the key technical expert developing and overseeing pepsico's data product build & operations and drive a strong vision for how data engineering can proactively create a positive impact on the business. you'll be an empowered member of a team of data engineers who build data pipelines into various source systems, rest data on the pepsico data lake, and enable exploration and access for analytics, visualization, machine learning, and product development efforts across the company. as a member of the data engineering team, you will help lead the development of very large and complex data applications into public cloud environments directly impacting the design, architecture, and implementation of pepsico's flagship data products responsibilities • active contributor to code development in projects and services • manage and scale data pipelines from internal and external data sources to support new product launches and drive data quality across data products • build and own the automation and monitoring frameworks that captures metrics and operational kpis for data pipeline quality and performance • responsible for implementing best practices around systems integration • security, performance and data management • empower the business by creating value through the increased adoption of data • data science and business intelligence landscape • collaborate with internal clients (data science and product teams) to drive solutioning and poc discussions compensation and benefits: • the expected compensation range for this position is between $89,000 - $149,000 • location, confirmed job-related skills, experience, and education will be considered in setting actual starting salary. your recruiter can share more about the specific salary range during the hiring process • bonus based on performance and eligibility target payout is 10% of annual salary paid out annually • paid time off subject to eligibility, including paid parental leave, vacation, sick, and bereavement • in addition to salary, pepsico offers a comprehensive benefits package to support our employees and their families, subject to elections and eligibility: medical, dental, vision, disability, health, and dependent care reimbursement accounts, employee assistance program (eap), insurance (accident, group legal, life), defined contribution retirement plan qualifications • 6+ years of overall technology experience that includes at least 4+ years of hands-on software development, data engineering, and systems architecture. • 4+ years of experience with data lake infrastructure, data warehousing, and data analytics tools. • 4+ years of experience in sql optimization and performance tuning, and development experience in programming languages like python, pyspark, scala etc.). • 2+ years in cloud data engineering experience in azure(azure data factory(adf), adls-2, databricks(lakehouse, workflow sql, unity catalog). • fluent with azure cloud services. azure or databricks certification is a plus. • experience with integration of multi cloud services with on-premises technologies. • experience with data modeling, data warehousing, and building high-volume etl/elt pipelines. • experience with data profiling and data quality tools like apache griffin, deequ, and great expectations. • experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets. • experience with at least one mpp database technology such as redshift, synapse or snowflake. • experience with running and scaling applications on the cloud infrastructure and containerized services like kubernetes. • experience with version control systems like github and deployment & ci tools. • experience with azure data factory, azure databricks and azure machine learning tools. • experience with statistical/ml techniques is a plus. • experience with building solutions in the supply chain space(digital procurement, manufacturing, cost, warehouse, network design) is a plus. • understanding of metadata management, data lineage, and data glossaries is a plus. • working knowledge of agile development, including devops and dataops concepts. • familiarity with business intelligence tools (such as powerbi). eeo statement our company will consider for employment qualified applicants with criminal histories in a manner consistent with the requirements of the fair credit reporting act, and all other applicable laws, including but not limited to, san francisco police code sections 4901-4919, commonly referred to as the san francisco fair chance ordinance; and chapter xvii, article 9 of the los angeles municipal code, commonly referred to as the fair chance initiative for hiring ordinance. all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, or disability status. pepsico is an equal opportunity employer: female / minority / disability / protected veteran / sexual orientation / gender identity if you'd like more information about your eeo rights as an applicant under the law, please download the available eeo is the law & eeo is the law supplement documents. view pepsico eeo policy. please view our pay transparency statement","plano, tx",Data Engineer,"['aws', 'azure', 'business intelligence', 'cloud', 'data analytics', 'data lake', 'data pipeline', 'databricks', 'elt', 'etl', 'machine learning', 'pyspark', 'python', 'r', 'redshift', 'scala', 'snowflake', 'spark', 'sql']","['aws', 'azure', 'business intelligence', 'cloud', 'data analytics', 'data lake', 'data pipeline', 'databricks', 'elt', 'etl', 'machine learning', 'pyspark', 'python', 'r', 'redshift', 'scala', 'snowflake', 'spark', 'sql']",
sr. data engineer,cvs health,"at cvs health, we're building a world of health around every consumer and surrounding ourselves with dedicated colleagues who are passionate about transforming health care. as the nation's leading health solutions company, we reach millions of americans through our local presence, digital channels and more than 300,000 purpose-driven colleagues - caring for people where, when and how they choose in a way that is uniquely more connected, more convenient and more compassionate. and we do it all with heart, each and every day. ​ **position summary** we're seeking a sr. data engineer to design and implement data pipelines that power analytical capabilities. this hands-on role requires an understanding of data engineering best practices and the ability to translate business requirements into technical solutions. you will be part of a dedicated team creating datasets for analytic and data science workloads. key responsibilities: + data pipeline development: design and build etl/elt data pipelines to ingest, process, and transform large datasets from multiple sources. + performance optimization: implement best practices for performance tuning, partitioning, and clustering to optimize data queries and reduce costs. + data quality & governance: establish and enforce data quality standards, data governance frameworks, and security policies for data storage and access. + data modeling & architecture: develop and optimize data models and schemas to support analytics, reporting, and machine learning requirements. + data integration & transformation: collaborate with data scientists and analysts to design data solutions that integrate with bi tools and machine learning models. + documentation & knowledge sharing: create comprehensive documentation for data pipelines, workflows, and processes. share best practices and mentor junior data engineers. + design and architect data infrastructure analytical workloads. • *required qualifications** + 5+ years of applicable work experience + proficiency in python, specifically with etl pipelines. + strong proficiency in sql and experience in developing complex queries. + familiarity with pyspark, dbt, or other similar frameworks. + experience deploying data pipelines in a cloud environment (azure, aws, gcp). + understanding of data warehousing concepts, dimensional modeling, and building data marts. + excellent communication and interpersonal skills, with the ability to collaborate effectively with data scientists, analysts, and product owners. • *preferred qualifications** + knowledge of data governance best practices in a cloud environment. + experience with machine learning flows on gcp. + experience with data design in bigquery + experience working with the epic data model. + experience working with healthcare data (tuva or omop models a plus). • *education and experience** + college degree or certification in related fields • *anticipated weekly hours** 40 • *time type** full time • *pay range** the typical pay range for this role is: $83,430.00 - $222,480.00 this pay range represents the base hourly rate or base annual full-time salary for all positions in the job grade within which this position falls. the actual base salary offer will depend on a variety of factors including experience, education, geography and other relevant factors. this position is eligible for a cvs health bonus, commission or short-term incentive program in addition to the base pay range listed above. our people fuel our future. our teams reflect the customers, patients, members and communities we serve and we are committed to fostering a workplace where every colleague feels valued and that they belong. • *great benefits for great people** we take pride in our comprehensive and competitive mix of pay and benefits - investing in the physical, emotional and financial wellness of our colleagues and their families to help them be the healthiest they can be. in addition to our competitive wages, our great benefits include: + **affordable medical plan options,** a **401(k) plan** (including matching company contributions), and an **employee stock purchase plan** . + **no-cost programs for all colleagues** including wellness screenings, tobacco cessation and weight management programs, confidential counseling and financial coaching. + **benefit solutions that address the different needs and preferences of our colleagues** including paid time off, flexible work schedules, family leave, dependent care resources, colleague assistance programs, tuition assistance, retiree medical access and many other benefits depending on eligibility. for more information, visit anticipate the application window for this opening will close on: 12/19/2025 qualified applicants with arrest or conviction records will be considered for employment in accordance with all federal, state and local laws. we are an equal opportunity and affirmative action employer. we do not discriminate in recruiting, hiring, promotion, or any other personnel action based on race, ethnicity, color, national origin, sex/gender, sexual orientation, gender identity or expression, religion, age, disability, protected veteran status, or any other characteristic protected by applicable federal, state, or local law.","silver springs, nv",Data Engineer,"['aws', 'azure', 'bi tools', 'bigquery', 'cloud', 'clustering', 'data pipeline', 'dbt', 'elt', 'etl', 'excel', 'gcp', 'machine learning', 'pyspark', 'python', 'r', 'spark', 'sql']","['aws', 'azure', 'bi tools', 'bigquery', 'cloud', 'clustering', 'data pipeline', 'dbt', 'elt', 'etl', 'excel', 'gcp', 'machine learning', 'pyspark', 'python', 'r', 'spark', 'sql']",
data engineer - ai/nlp,psi (proteam solutions),"position: ai / nlp data engineer type: contract-to-hire location: 100% remote (onshore required) hours: est preferred overview we are looking for a highly skilled ai/nlp engineer with recent, hands-on experience working with llms and databricks ai capabilities. this individual will partner closely with stakeholders to build iterative, validated nlp solutions that extract structured insights from unstructured documents. this is both a technical and collaborative role requiring excellent communication and problem-solving skills. key responsibilities • build nlp/llm-based solutions using databricks-native ai models. • design, test, and refine prompts to extract accurate structured data from varied document formats. • conduct iterative model validation cycles with stakeholders (multiple review passes expected). • analyze unstructured documents and develop end-to-end extraction pipelines. • present model outputs, obtain feedback, and continuously refine results. • implement checks, balances, and monitoring to ensure long-term model stability. • collaborate closely with technical and non-technical partners to translate vague requests into actionable requirements. required skills • strong hands-on experience with llms, prompt design, and ai model tuning. • proficiency with databricks (sql + python). • ability to evaluate unstructured datasets and build nlp workflows. • strong communication skills and experience working directly with stakeholders. • experience designing iterative validation/testing processes. nice-to-have • healthcare experience a plus. • familiarity with azure cognitive services for ocr (not required). • experience in ambiguous requirement environments. #psi2","dublin, oh",Data Engineer,"['azure', 'databricks', 'excel', 'nlp', 'python', 'r', 'sql']","['azure', 'databricks', 'excel', 'nlp', 'python', 'r', 'sql']",
ai automations data engineer / data scientist,rtx,"date posted: 2025-11-07 country: united states of america location: utct1: corp - ct - remote remote location, remote city, ct, 06101 usa position role type: remote u.s. citizen, u.s. person, or immigration status requirements: this job requires a u.s. person. a u.s. person is a lawful permanent resident as defined in 8 u.s.c. 1101(a)(20) or who is a protected individual as defined by 8 u.s.c. 1324b(a)(3). u.s. citizens, u.s. nationals, u.s. permanent residents, or individuals granted refugee or asylee status in the u.s. are considered u.s. persons. for a complete definition of “u.s. person” go here. https://www.ecfr.gov/current/title-22/chapter-i/subchapter-m/part-120/subpart-c/section-120.62 security clearance: none/not required rtx corporation is an aerospace and defense company that provides advanced systems and services for commercial, military and government customers worldwide. it comprises three industry-leading businesses – collins aerospace systems, pratt & whitney, and raytheon. its 185,000 employees enable the company to operate at the edge of known science as they imagine and deliver solutions that push the boundaries in quantum physics, electric propulsion, directed energy, hypersonics, avionics and cybersecurity. the company, formed in 2020 through the combination of raytheon company and the united technologies corporation aerospace businesses, is headquartered in arlington, va. the following position is to join our rtx enterprise services team: as a data engineer / data scientist at raytheon technologies, you will play a critical role in designing, building, and optimizing data solutions that drive business insights, operational efficiency, and technological innovation. you will be responsible for developing and maintaining robust data pipelines, implementing advanced machine learning models, and delivering actionable business intelligence solutions. this role demands a blend of technical expertise and strategic vision, with opportunities to explore emerging technologies, including generative ai, to support rtx's mission to protect and connect the world. what you will do: • design and implement scalable, reliable, and efficient end-to-end data pipelines and workflows using tools like snowflake and matillion. • collaborate with business owners to identify key data needs, align them with rtx objectives, and translate them into actionable technical solutions. • evaluate industry trends and emerging technologies, including generative ai, to recommend and implement innovative data strategies. • demonstrate expertise in data integration, transformation, and modeling using tools such as snowflake and matillion. • develop impactful dashboards and reports using power bi and tableau to support data-driven decision-making across the organization. • leverage databricks for advanced data processing, machine learning workflows, and big data analytics. • build and deploy machine learning models and statistical analyses to address complex business challenges and deliver measurable outcomes. • stay current with advancements in ai and ml technologies, including generative ai, to identify opportunities for innovation in rtx’s data landscape. • collaborate with cross-functional teams, including architecture and cloud operations, to align data solutions with organizational priorities and policies. • maintain governance standards, ensure compliance with rtx data security protocols, and effectively engage with stakeholders to communicate progress and resolve challenges. qualifications you must have: • bachelor’s degree in computer science, data science, engineering, or a related field, and 5+ years of experience in data engineering, data science, or a related field. • proficiency in snowflake, matillion, bi tools ( power bi, tableau, or similar) and data science platforms (databricks). • strong programming skills in python, sql, or other relevant languages. • hands-on experience in building and deploying machine learning models. • strong analytical and problem-solving skills with a business-oriented mindset. qualifications we prefer: • master’s degree in a relevant field. • knowledge or experience with generative ai frameworks and tools (e.g., llms, rag). • familiarity with cloud platforms such as azure or aws. • strong communication and collaboration skills to work effectively with cross-functional teams. • safe or similar certifications in agile teams or devops what we offer: whether you’re just starting out on your career journey or are an experienced professional, we offer a robust total rewards package with compensation; healthcare, wellness, retirement and work/life benefits; career development and recognition programs. some of the benefits we offer include parental (including paternal) leave, flexible work schedules, achievement awards, educational assistance and child/adult backup care. work location: remote as part of our commitment to maintaining a secure hiring process, candidates may be asked to attend select steps of the interview process in-person at one of our office locations, regardless of whether the role is designated as on-site, hybrid or remote. the salary range for this role is 82,000 usd - 164,000 usd. the salary range provided is a good faith estimate representative of all experience levels. rtx considers several factors when extending an offer, including but not limited to, the role, function and associated responsibilities, a candidate’s work experience, location, education/training, and key skills. hired applicants may be eligible for benefits, including but not limited to, medical, dental, vision, life insurance, short-term disability, long-term disability, 401(k) match, flexible spending accounts, flexible work schedules, employee assistance program, employee scholar program, parental leave, paid time off, and holidays. specific benefits are dependent upon the specific business unit as well as whether or not the position is covered by a collective-bargaining agreement. hired applicants may be eligible for annual short-term and/or long-term incentive compensation programs depending on the level of the position and whether or not it is covered by a collective-bargaining agreement. payments under these annual programs are not guaranteed and are dependent upon a variety of factors including, but not limited to, individual performance, business unit performance, and/or the company’s performance. this role is a u.s.-based role. if the successful candidate resides in a u.s. territory, the appropriate pay structure and benefits will apply. rtx anticipates the application window closing approximately 40 days from the date the notice was posted. however, factors such as candidate flow and business necessity may require rtx to shorten or extend the application window. rtx is an equal opportunity employer. all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability or veteran status, or any other applicable state or federal protected class. rtx provides affirmative action in employment for qualified individuals with a disability and protected veterans in compliance with section 503 of the rehabilitation act and the vietnam era veterans’ readjustment assistance act. privacy policy and terms: click on this link to read the policy and terms","hartford, ct",Data Engineer,"['aws', 'azure', 'bi tools', 'business intelligence', 'cloud', 'dashboard', 'data analytics', 'data pipeline', 'databricks', 'machine learning', 'power bi', 'python', 'r', 'scala', 'snowflake', 'sql', 'tableau']","['aws', 'azure', 'bi tools', 'business intelligence', 'cloud', 'dashboard', 'data analytics', 'data pipeline', 'databricks', 'machine learning', 'power bi', 'python', 'r', 'scala', 'snowflake', 'sql', 'tableau']",
data engineer and power platform specialist,virtualvocations,"a company is looking for an associate director, data engineering and development. key responsibilities : lead the performance team in using black diamond for data aggregation and reporting design, build, and maintain etl processes and sql pipelines for enterprise reporting enhance power bi dashboards and develop low-code automations using power platform required qualifications : bachelor's degree in computer science or a related field 3 to 6 years of experience in sql data engineering or related analytics roles strong proficiency in sql and data modeling; experience with microsoft sql server preferred experience with azure data services or other cloud platforms is preferred working knowledge of power bi and familiarity with apis and data integration patterns","burbank, ca (+1 other)",Data Engineer,"['azure', 'cloud', 'dashboard', 'etl', 'power bi', 'r', 'sql', 'sql server']","['azure', 'cloud', 'dashboard', 'etl', 'power bi', 'r', 'sql', 'sql server']",
staff data engineer - quickbooks capital,intuit,"the quickbooks capital data team is responsible for building and maintaining the core data capabilities and pipelines that drive insights, decisions, and innovation for quickbooks capital. our stakeholders include data scientists, backend engineers, product managers, and business leaders, all of whom rely on the team to deliver seamless data integrations, high-quality pipelines, and optimized data platforms. the data team plays a critical role in empowering quickbooks capital to scale, grow, and provide exceptional experiences to our customers. as a staff data engineer within the team, you will partner closely with cross-functional teams to execute on strategic data initiatives that shape the future of quickbooks capital’s data ecosystem. tasks will include creating scalable data pipelines, designing data products, and optimizing data workflows to support analytics, business intelligence, and ai-driven use cases. the team prioritizes driving impactful outcomes using data, such as improving loan approval processes, enhancing customer insights, and fostering innovation through robust data infrastructure. if you are passionate about solving complex engineering problems, thrive in a collaborative environment, and have a pragmatic approach to building scalable and resilient data systems, this role could be a great fit for you. responsibilities execute and deliver scalable data solutions: collaborate with external technical teams, data scientists, analysts, and stakeholders to understand requirements and translate business challenges into reliable data-driven solutions. own the end-to-end execution of projects by designing, developing, and deploying data pipelines to meet the growing data needs of quickbooks capital. build and optimize data platforms: design, implement, and maintain robust data pipelines that transform and load large datasets across multiple domains. focus on delivering data products with accuracy, reliability, and efficiency to empower actionable insights and data-driven decision-making. experiment and innovate: utilize tools like databricks and advanced big data technologies to build and iterate on proof-of-concepts (pocs). develop innovative solutions, including api integrations, advanced data modeling, and scalable platforms, to support analytics and machine learning use cases. create business impact with data: strategically apply data solutions to address key business challenges and deliver value through actionable insights. uphold best practices in data governance and security while ensuring data accessibility, compliance, and reliability across the organization. collaborate and empower the team: work effectively in a dynamic environment, collaborating with cross-functional teams to drive impactful results. take ownership of monitoring pipeline performance, resolving issues, and mentoring team members to foster excellence and growth within the team. qualifications • bachelor's or master's degree in computer science, data science, engineering, or a related field. • a minimum of 10 years of hands-on experience in data engineering or a similar role focused on big data platforms and solutions. • strong understanding of data engineering and dimensional design fundamentals, front-end analysis / data visualization, and learning new technologies quickly. good understanding of data federation techniques and aggregation of data at scale from multiple source systems. • strong proficiency in programming languages such as python, java, or scala, and experience with sql. • demonstrated experience with big data tools and frameworks such as apache spark, hadoop or databricks. • experience with data modeling, data warehousing, and building etl pipelines. intuit provides a competitive compensation package with a strong pay for performance rewards approach. this position will be eligible for a cash bonus, equity rewards and benefits, in accordance with our applicable plans and programs (see more about our compensation and benefits at intuit®: careers | benefits). pay offered is based on factors such as job-related knowledge, skills, experience, and work location. to drive ongoing fair pay for employees, intuit conducts regular comparisons across categories of ethnicity and gender. the expected base pay range for this position is: $197,000 - $263,670","mountain view, ca",Data Engineer,"['business intelligence', 'data pipeline', 'databricks', 'etl', 'excel', 'hadoop', 'java', 'machine learning', 'python', 'r', 'scala', 'spark', 'sql']","['business intelligence', 'data pipeline', 'databricks', 'etl', 'excel', 'hadoop', 'java', 'machine learning', 'python', 'r', 'scala', 'spark', 'sql']",
senior data engineer - python expert,capital one,"senior data engineer, python do you love building and pioneering in the technology space? do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and iterative delivery environment? at capital one, you'll be part of a big group of makers, breakers, doers and disruptors, who solve real problems and meet real customer needs. we are seeking senior data engineers, who are passionate about marrying data with emerging technologies. as a capital one, senior data engineer, you'll have the opportunity to be on the forefront of driving a major transformation within capital one. what you'll do: • collaborate with and across agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies • work with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems • utilize programming languages like python, java, scala, and open source rdbms and nosql databases and cloud based data warehousing services such as redshift and snowflake • share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community • collaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of americans achieve financial empowerment • perform unit tests and conduct reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance basic qualifications: • bachelor's degree • at least 3 years of experience in application development (internship experience does not apply) • at least 1 year of experience in big data technologies preferred qualifications: • 5+ years of experience in application development including python, sql, scala, or java • 2+ years of experience with a public cloud (aws, microsoft azure, google cloud) • 3+ years experience with distributed data/computing tools (mapreduce, hadoop, hive, emr, kafka, spark, gurobi, or mysql) • 2+ year experience working on real-time data and streaming applications • 2+ years of experience with nosql implementation (mongo, cassandra) • 2+ years of data warehousing experience (redshift or snowflake) • 3+ years of experience with unix/linux including basic commands and shell scripting • 2+ years of experience with agile engineering practices at this time, capital one will not sponsor a new applicant for employment authorization, or offer any immigration related support for this position (i.e. h1b, f-1 opt, f-1 stem opt, f-1 cpt, j-1, tn, e-2, e-3, l-1 and o-1, or any eads or other forms of work authorization that require immigration support from an employer). the minimum and maximum full-time annual salaries for this role are listed below, by location. please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount capital one is willing to pay at the time of this posting. salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked. mclean, va: $158,600 - $181,000 for senior data engineer plano, tx: $144,200 - $164,600 for senior data engineer richmond, va: $144,200 - $164,600 for senior data engineer wilmington, de: $144,200 - $164,600 for senior data engineer candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate's offer letter. this role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (lti). incentives could be discretionary or non discretionary depending on the plan. capital one offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. learn more at the capital one careers website . eligibility varies based on full or part-time status, exempt or non-exempt status, and management level. this role is expected to accept applications for a minimum of 5 business days. no agencies please. capital one is an equal opportunity employer (eoe, including disability/vet) committed to non-discrimination in compliance with applicable federal, state, and local laws. capital one promotes a drug-free workplace. capital one will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, article 23-a of the new york correction law; san francisco, california police code article 49, sections 4901-4920; new york city's fair chance act; philadelphia's fair criminal records screening act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries. if you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact capital one recruiting at 1-800-304-9102 or via email at recruitingaccommodation@ . all information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. for technical support or questions about capital one's recruiting process, please send an email to careers@ capital one does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site. capital one financial is made up of several different entities. please note that any position posted in canada is for capital one canada, any position posted in the united kingdom is for capital one europe and any position posted in the philippines is for capital one philippines service corp. (copssc).","york, pa",Data Engineer,"['aws', 'azure', 'cloud', 'google cloud', 'hadoop', 'java', 'kafka', 'machine learning', 'python', 'r', 'redshift', 'scala', 'snowflake', 'spark', 'sql']","['aws', 'azure', 'cloud', 'google cloud', 'hadoop', 'java', 'kafka', 'machine learning', 'python', 'r', 'redshift', 'scala', 'snowflake', 'spark', 'sql']",
data engineer iii,old national bank,"overview: old national bank has been serving clients and communities since 1834. with over $70 billion in total assets, we are a regional powerhouse deeply rooted in the communities we serve. as a trusted partner, we thrive on helping our clients achieve their goals and dreams, and we are committed to social responsibility and investing in our communities through volunteering and charitable giving. we continually seek highly motivated and talented individuals as our people are critical to our success. in return, we offer competitive compensation with our salary and incentive program, in addition to medical, dental, and vision insurance. 401k, continuing education opportunities and an employee assistance program are also included in our benefit suite. old national also offers a variety of impact network groups led by team members who are passionate about driving engagement, creating awareness of diverse backgrounds and experiences, and building inclusion across the organization. we offer a unique opportunity to join a growing, community and client-focused company that is firmly rooted in its core values. responsibilities: we are seeking a data engineer with expertise in azure, aws, databricks, and python to design, build, and optimize scalable data pipelines and products. this role will focus on developing modern cloud-based data solutions that support analytics, reporting, system integrations, and ai/ml. the ideal candidate will leverage python and sql for automation and data processing while ensuring reliability, security, and performance across platforms. working closely with analysts, data scientists, and business stakeholders, this position plays a key role in delivering high-quality and efficient data solutions. salary range the salary range for this position is $62,300 – $122,430 per year. the base salary indicated for this position reflects the compensation range applicable to all levels of the role across the united states. actual salary offers within this range may vary based on a number of factors, including the specific responsibilities of the position, the candidate’s relevant skills and professional experience, educational qualifications, and geographic location. key accountabilities key competencies for position • ability to work independently and be able to collaborate and guide other team members. • critical thinking and problem-solving skills to ensure the right-sized solution is developed for the task at hand. • technical passion to move the bank towards modern data platform principles. duties/responsibilities • design, development, and maintenance (enhancements and maintenance) of enterprise data products and pipelines. • design and development of repeatable frameworks and functions to be leveraged by other data engineers within the bank. • focus on operational excellence- ensuring we are monitoring, validating, and communicating the health and status of our data products. • collaborate with enterprise architecture, info security and data governance organizations. • confer with relevant team members where necessary. study systems flow, data usage, and work processes. • deliver functional data products that have been thoroughly tested. • follow the development team’s sdlc process. accurately estimate time required to complete projects and tasks. • meet mutually agreed upon deadlines for completion of modules throughout the program development. • work closely with the existing platform owners to design, model, develop, and maintain existing and new objects/products required for all business solutions. skills and qualification • bachelor’s degree required. • over 5 years of experience in data engineering, including developing etl/elt, data pipelines, data lakes, data warehouses, and data integration solutions. • over 3 years of experience on data engineering teams, preferably in a financial services or banking domain. • 3-5 years' experience cloud platforms (aws, azure, gcp, databricks,snowflake) databricks preferred and python programming or advanced sql coding and performance tuning. • experience in a product mindset, using agile frameworks such as scrum or kanban. • expertise in building scalable, reliable, and secure data products and platforms, using metadata driven, event driven, and api driven approaches. • experience in scripting tools such as python. • experience in orchestration tools or job schedulers, such as azure data factory, airflow, or luigi. • experience in data modeling/mastering, data quality, data governance, and data security standards and best practices. • experience in code repositories and version control tools such as azure devops, git, etc. • experience in analytical and data visualization tools such as power bi, tableau, or qlik. • experience in working with salesforce data and digital banking data is a plus. • very good working experience of t-sql. proficiency in complex stored procedures, user defined functions, and query optimization. old national is proud to be an equal opportunity employer focused on fostering an inclusive workplace and committed to hiring a workforce comprised of diverse backgrounds, cultures and thinking styles. as such, all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, protected veteran status, status as a qualified individual with disability, sexual orientation, gender identity or any other characteristic protected by law. we do not accept resumes from external staffing agencies or independent recruiters for any of our openings unless we have an agreement signed by the director of talent acquisition, svp, to fill a specific position. our culture is firmly rooted in our core values. we are optimistic. we are collaborative. we are inclusive. we are agile. we are ethical. we are old national bank. join our team.","lake elmo, mn",Data Engineer,"['airflow', 'aws', 'azure', 'cloud', 'data lake', 'data pipeline', 'data warehouse', 'databricks', 'elt', 'etl', 'excel', 'gcp', 'power bi', 'python', 'r', 'scala', 'snowflake', 'sql', 'tableau']","['airflow', 'aws', 'azure', 'cloud', 'data lake', 'data pipeline', 'data warehouse', 'databricks', 'elt', 'etl', 'excel', 'gcp', 'power bi', 'python', 'r', 'scala', 'snowflake', 'sql', 'tableau']","62,300–122,430 a year"
senior data engineer (intelligent foundations and experiences),capital one,"senior data engineer (intelligent foundations and experiences) do you love building and pioneering in the technology space? do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and iterative delivery environment? at capital one, you'll be part of a big group of makers, breakers, doers and disruptors, who solve real problems and meet real customer needs. we are seeking data engineers who are passionate about marrying data with emerging technologies. as a capital one data engineer, you’ll have the opportunity to be on the forefront of driving a major transformation within capital one. intelligent foundations & experiences (ifx) is a powerful collective of horizontal technology organizations that are driving capital one’s real-time intelligent future. together with our partners in the enterprise and across lines of business, we deliver broad-reaching technical solutions and advance state-of-the-art science to help every capital one associate and our 100+m customers succeed. what you’ll do: • collaborate with and across agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies • work with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems • utilize programming languages like java, scala, python and open source rdbms and nosql databases and cloud based data warehousing services such as redshift and snowflake • share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community • collaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of americans achieve financial empowerment • perform unit tests and conduct reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance basic qualifications: • bachelor’s degree • at least 3 years of experience in application development (internship experience does not apply) • at least 1 year of experience in big data technologies preferred qualifications: • 5+ years of experience in application development including python, sql, scala, or java • 2+ years of experience with a public cloud (aws, microsoft azure, google cloud) • 3+ years experience with distributed data/computing tools (mapreduce, hadoop, hive, emr, kafka, spark, gurobi, or mysql) • 2+ year experience working on real-time data and streaming applications • 2+ years of experience with nosql implementation (mongo, cassandra) • 2+ years of data warehousing experience (redshift or snowflake) • 3+ years of experience with unix/linux including basic commands and shell scripting • 2+ years of experience with agile engineering practices at this time, capital one will not sponsor a new applicant for employment authorization, or offer any immigration related support for this position (i.e. h1b, f-1 opt, f-1 stem opt, f-1 cpt, j-1, tn, e-2, e-3, l-1 and o-1, or any eads or other forms of work authorization that require immigration support from an employer). the minimum and maximum full-time annual salaries for this role are listed below, by location. please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount capital one is willing to pay at the time of this posting. salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked. plano, tx: $144,200 - $164,600 for senior data engineer mclean, va: $158,600 - $181,000 for senior data engineer richmond, va: $144,200 - $164,600 for senior data engineer candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate’s offer letter. this role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (lti). incentives could be discretionary or non discretionary depending on the plan. capital one offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. learn more at the capital one careers website. eligibility varies based on full or part-time status, exempt or non-exempt status, and management level. this role is expected to accept applications for a minimum of 5 business days. no agencies please. capital one is an equal opportunity employer (eoe, including disability/vet) committed to non-discrimination in compliance with applicable federal, state, and local laws. capital one promotes a drug-free workplace. capital one will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, article 23-a of the new york correction law; san francisco, california police code article 49, sections 4901-4920; new york city’s fair chance act; philadelphia’s fair criminal records screening act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries. if you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact capital one recruiting at 1-800-304-9102 or via email at recruitingaccommodation@capitalone.com. all information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. for technical support or questions about capital one's recruiting process, please send an email to careers@capitalone.com capital one does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site. capital one financial is made up of several different entities. please note that any position posted in canada is for capital one canada, any position posted in the united kingdom is for capital one europe and any position posted in the philippines is for capital one philippines service corp. (copssc).","bon air, va",Data Engineer,"['aws', 'azure', 'cloud', 'google cloud', 'hadoop', 'java', 'kafka', 'machine learning', 'python', 'r', 'redshift', 'scala', 'snowflake', 'spark', 'sql']","['aws', 'azure', 'cloud', 'google cloud', 'hadoop', 'java', 'kafka', 'machine learning', 'python', 'r', 'redshift', 'scala', 'snowflake', 'spark', 'sql']",
senior data engineer (remote),parsons corporation,"in a world of possibilities, pursue one with endless opportunities. imagine next! at parsons, you can imagine a career where you thrive, work with exceptional people, and be yourself. guided by our leadership vision of valuing people, embracing agility, and fostering growth, we cultivate an innovative culture that empowers you to achieve your full potential. unleash your talent and redefine what’s possible. job description: parsons is looking for an amazingly talented senior data engineer to join our team! in this role you will get to help shape our modern data architecture and enabling scalable, self-service analytics across the organization. what you'll be doing: • design and implement scalable, efficient data ingestion pipelines using adf, informatica, and parameterized notebooks to support bronze-silver-gold (medallion) architecture. • develop robust etl/elt workflows to ingest data from diverse sources (e.g., sql server, flat files, apis) into parquet/delta formats and model them into semantic layers in snowflake. • build and maintain incremental and cdc-based pipelines to support near-real-time and daily batch processing. • apply best practices for snowflake implementation, including performance tuning, cost optimization, and secure data sharing. • leverage dbt for data transformation and modeling, and implement github-based source control, branching strategies, and ci/cd pipelines for deployment automation. • ensure data quality, reliability, and observability through validation frameworks and self-healing mechanisms. • collaborate with data analysts, data scientists, and business stakeholders to deliver clean, trusted, and accessible data. • mentor junior engineers and contribute to a culture of engineering excellence and continuous improvement. what required skills you'll bring: • strong hands-on experience with t-sql and python. • experience with comprehensive data conversion projects is preferred (erp systems including oracle cloud erp and/or sap s4/hana) • experience with relational database systems • experience with both on-premises / cloud etl toolsets (preferably ssis, adf, synapse, aws) • familiar with multi-dimensional and tabular models • 5+ years of experience in data engineering, data architecture, or data platform development. • proficiency in pyspark and sql notebooks (e.g., microsoft fabric, databricks, synapse, or similar). • experience with azure data factory and/or informatica for building scalable ingestion pipelines. • deep understanding of lakehouse architecture and medallion design patterns. • experience with dbt, github source control, branching strategies, and ci/cd pipelines. • familiarity with data ingestion from apis, sql server, and flat files into parquet/delta formats. • strong problem-solving skills and ability to work independently in a fast-paced environment. • us person what desired skills you'll bring: • experience with data governance, security, and compliance (e.g., sox, hipaa). • snowflake, azure data engineer, dbt, and/or databricks certifications • exposure to real-time data processing and streaming technologies (e.g., kafka, spark streaming). • familiarity with data observability tools and automated testing frameworks for pipelines. • bachelor's or master’s degree in computer science, information systems, or a related field security clearance requirement: none this position is part of our corporate team. for over 80 years, parsons corporation, has shaped the future of the defense, intelligence, and critical infrastructure markets. our employees work in a close-knit team environment to find new, innovative ways to deliver smart solutions that are used and valued by customers around the world. by combining unique technologies with deep domain expertise across cybersecurity, missile defense, space, connected infrastructure, transportation, smart cities, and more, we're providing tomorrow's solutions today. salary range: $100,900.00 - $176,600.00 we value our employees and want our employees to take care of their overall wellbeing, which is why we offer best-in-class benefits such as medical, dental, vision, paid time off, employee stock ownership plan (esop), 401(k), life insurance, flexible work schedules, and holidays to fit your busy lifestyle! this position will be posted for a minimum of 3 days and will continue to be posted for an average of 30 days until a qualified applicant is selected or the position has been cancelled. parsons is an equal opportunity employer, and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, veteran status or any other protected status. we truly invest and care about our employee’s wellbeing and provide endless growth opportunities as the sky is the limit, so aim for the stars! imagine next and join the parsons quest—apply today! parsons is aware of fraudulent recruitment practices. to learn more about recruitment fraud and how to report it, please refer to https://www.parsons.com/fraudulent-recruitment/.",virginia,Data Engineer,"['aws', 'azure', 'cloud', 'databricks', 'dbt', 'elt', 'etl', 'excel', 'kafka', 'pyspark', 'python', 'r', 'scala', 'snowflake', 'spark', 'sql', 'sql server']","['aws', 'azure', 'cloud', 'databricks', 'dbt', 'elt', 'etl', 'excel', 'kafka', 'pyspark', 'python', 'r', 'scala', 'snowflake', 'spark', 'sql', 'sql server']",101K–177K a year
junior data engineer,mod op,"data engineer job description: mod op is a full-service advertising agency with offices across several us locations, panama city, panama, and canada. with continued growth and a dynamic leadership team, we offer a generous time-off package, access to high-quality healthcare options, and a collaborative team dedicated to career and personal development. we believe in teamwork, client collaboration, storytelling, stunning design, and solving complex problems with innovative solutions. we are a 360° agency providing strategy, design, and production across all channels, with clients representing a variety of industries, offering diverse and exciting challenges. we are committed to working smart and enjoying the work we do. about you: as a data engineer with (1-2) years of experience, you will be responsible for designing and implementing robust data pipelines, optimizing data workflows, and supporting analytics initiatives. you will work with aws and gcp cloud services, integrate with crm and marketing platforms, and enable data-driven decision-making through visualization tools like google looker and tableau. key responsibilities: • data pipeline development: design, develop, and maintain scalable etl/elt pipelines in gcp, azure & aws using various services including data flow, composer, azure synapse, aws data pipelines and etc • data integration: work with structured and unstructured data sources, including crm and marketing data platforms. • database management: develop and optimize queries for sql & nosql databases (teradata, bigquery, cassandra, etc.). • data science & ml: implementing models using gcp vertex ai and utilizing python and its data science libraries (pandas, numpy, scikit-learn, etc.) for data analysis and ml model deployment. • data visualization: build and manage dashboards using google looker and tableau to provide business insights. • collaboration: work closely with data analysts, marketing teams, and other stakeholders to understand business needs and implement effective data solutions. • the position operates under a hybrid work model, requiring in-office presence at the grapevine, texas location two days per week, with the remaining days worked remotely. required qualifications: • cloud expertise: gcp or aws (data engineering or ml focus) experience. • programming skills: strong proficiency in python and experience with its data science libraries. • database management: experience with sql (teradata, bigquery, etc.) and nosql databases. • data visualization: hands-on experience with google looker and tableau for reporting and dashboards. • crm & marketing data: experience working with crm, marketing platforms, and analytics tools. • machine learning knowledge: working gcp/azure/aws services with ml workflows, model training, ai models and deployment. • data automation & transformation: knowledge with alteryx for workflow automation and data preparation. preferred qualifications: • experience with data warehousing solutions (snowflake, redshift, etc.). • gcp certified focused on data engineering. • exposure to apache spark, airflow, or other data orchestration tools. • strong understanding of data governance, security, and compliance. • health and life insurance for employees and family, access to vision benefits, telemedicine services, psychology support and others. • on the job training and career growth opportunities. • access to linkedin courses. • fully remote job. • talented team environment, collaborative offices, fun company culture with a great balance of work and play. • vacations are granted by day or weeks according to employee approved request. • salary with yearly review and competitive benefits. • competitive compensation based on experience and skill set. when asked what they love about working at mod op, we hear: • “i feel i can be myself at work and it’s fun!” -mv • “the caliber of the clients/brands we work with, knowing your work is seen by thousands of people, in many cases across the world.” -jc • “we actually create videogames!” -ac • “we have an all-star team, and it’s like playing in the pro-bowl every day!” -mw • “opportunities to always learn from and work with the best and the brightest.” hw • “mentors and opportunities for growth.” -kb mod op, llc provides equal employment opportunities to all employees and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. this policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.",anywhere,Data Engineer,"['airflow', 'aws', 'azure', 'bigquery', 'cloud', 'dashboard', 'data analysis', 'data pipeline', 'elt', 'etl', 'gcp', 'looker', 'machine learning', 'numpy', 'pandas', 'python', 'r', 'redshift', 'scala', 'scikit-learn', 'snowflake', 'spark', 'sql', 'tableau']","['airflow', 'aws', 'azure', 'bigquery', 'cloud', 'dashboard', 'data analysis', 'data pipeline', 'elt', 'etl', 'gcp', 'looker', 'machine learning', 'numpy', 'pandas', 'python', 'r', 'redshift', 'scala', 'scikit-learn', 'snowflake', 'spark', 'sql', 'tableau']",
sr data engineer-martech,h&r block,"our company we care about helping people. our purpose is to provide help and inspire confidence in our clients and communities everywhere. our associates feel a sense of belonging in an inclusive place with an amazing history and a sharp focus on our future. our connected culture is who we are and how we work together to achieve our strategies, accelerate our transformation, and achieve extraordinary results. it’s an exciting time to be a part of h&r block! what you'll do... h&r block is looking for an experienced senior software engineer who is passionate about data and experience in martech skills. as a senior software engineer, you will work independently collaborating with lead, or principal software engineers to design, develop, and maintain software components and products that support h&r block business applications. you will work on all aspects of the design, development, and delivery of data solutions, including problem definition, testing and evaluation, and deploying end-to-end pipelines and solutions in production on azure. the responsibilities include: • design, develop and test enterprise martech platforms for data engineering using sql, azure data engineering skills including azure data factory, databricks/fabric technologies. • proficiency in azure-based cloud technologies to support data needs, along with working in marketing projects (adobe experience platform and/or salesforce marketing cloud platform). day to day, you’ll... • leverage cutting-edge data technologies, programming languages, and industry-standard coding practices to innovate new features and optimize existing product/marketing functionalities. • design, develop, and maintain high-quality software components. • create and execute unit tests, troubleshoot issues, and resolve defects efficiently. • collaborate with product, architects and cross-functional teams to align on requirements and implementation strategies. • translate business and functional requirements into clear technical specifications and product deliverables. • participate in technical design discussions and conduct code reviews to ensure quality and consistency. • document system architecture, design approaches, and development processes for future reference. • develop and maintain unit test plans and alpha test plans to support product validation. • stay current with emerging technologies, tools, and methodologies to continuously improve design, development, and deployment practices. • be part of a high-impact team that powers data-driven marketing strategies for one of the most recognized brands in the industry. our martech squad is agile, collaborative, and committed to innovation and continuous learning. • what you'll bring to the team... • bachelor’s degree or the equivalent in computer science, computer information systems, information technology, or related field. • minimum five years of recent experience in developing, debugging, and maintaining code with below skillsets, • sql, azure data engineering skills including azure data factory, databricks and/or fabric technologies. • deep knowledge in sql server databases, along with expertise in data analysis, data integrations. • proficient with ai technologies to enhance day to day activities. • well-honed troubleshooting skills at various levels of complexity. it would be even better if you also had... • exposure to marketing platforms (adobe aep - cdp, ajo and/or sales force marketing cloud), adtech • adobe experience cloud (analytics, target, launch) experience • hands on experience with python and/or .net • experience in cloud services (azure data services) why work for us since 1955, we have been leaders in tax preparation, financial services, and small business solutions. with 70,000 associates and 9,000 retail tax locations across north america, australia, ireland, and india, we have helped millions of clients and countless communities. if you embrace challenges as opportunities, value winning as a team, and seek to make a meaningful difference, join us on our journey. you’ll reap the rewards of helping others along with competitive compensation and benefits to support your health and well-being. specific benefits may vary based on your role. for detailed eligibility requirements and benefits information, visit blockbenefits.com. equal opportunity employer: h&r block does not tolerate discrimination based on a person’s race, color, religion, ancestry, age, sex/gender (including pregnancy, childbirth, related medical conditions and sex-based stereotypes and transgender status), sexual orientation, gender identity or expression, service in the armed forces, national origin, physical or mental disability, genetic information, citizenship status or any other status protected by law. pay range information the pay range for this position is listed below. local minimum wage laws apply. this information is posted pursuant to local requirements to provide applicants with information about what they might be eligible to receive. individual pay decisions will depend on job-related factors such as experience, education, skill, performance, and geographic location where work will be performed. successful candidates may be able to participate in one or more incentive compensation or short-term incentive plans, which could generate additional earnings in accordance with the terms of each plan. qualifying associates can enroll themselves and/or their eligible dependents in medical and prescription drug coverage; can participate in the h&r block retirement savings plan (401(k) plan), the employee assistance program, (virtual) fitness center programs, and the associate discount program; are automatically enrolled in business travel accident insurance; and receive associate tax prep benefit. pay range $101,200.00 - $161,900.00/yr. sponsored job #li-sh1 #li-remote",missouri,Data Engineer,"['aws', 'azure', 'cloud', 'data analysis', 'databricks', 'python', 'r', 'sql', 'sql server']","['aws', 'azure', 'cloud', 'data analysis', 'databricks', 'python', 'r', 'sql', 'sql server']",
sr. manager- data engineering,adtalem,"company description about adtalem global education adtalem global education is a national leader in post-secondary education and leading provider of professional talent to the healthcare industry. adtalem educates and empowers students with the knowledge and skills to become leaders in their communities and make a lasting impact on public health, well-being and beyond. through equitable access to education, environments that nurture student success, and a focus on expanding and diversifying the talent pipeline in healthcare, adtalem is building a brighter future for communities and the world. adtalem is the parent organization of american university of the caribbean school of medicine, chamberlain university, ross university school of medicine, ross university school of veterinary medicine and walden university. we operate on a hybrid schedule with four in-office days per week (monday–thursday). this approach enhances creativity, innovation, communication, and relationship-building, fostering a dynamic and collaborative work environment. visit adtalem.com for more information, and follow on twitter and linkedin! job description opportunity at a glance adtalem is a data driven organization. the data engineering team builds data solutions that powers strategic and tactical business decisions and supports the analytics and artificial intelligence operations. by implementing data platform, data pipelines and data governance policies this team provides the basis for decision-making in adtalem. adtalem is looking for a senior manager, data engineering who will lead overall technical design, development, modification, and implementation of data solutions using existing and emerging technology platforms. this person will own the development as well as the continual improvement of the data platform for an organization that has the goal to drive best in class student outcomes in the industry. responsibilities • you'll design and build trusted, reliable and timely datasets, metrics and data pipelines that are critical to the direction of the company • build and lead a high-performing data engineering team in a hands-on technical capacity • be responsible for shaping how we acquire, collect and leverage data • you will define and manage sla's for all data sets and processes running in production • work closely with product managers, analysts, data scientists to develop and own data-driven systems • develop data democratization layer for self-served reporting • assist development team in troubleshooting, coding, testing, implementation, and documenting solutions • support, grow, mentor and inspire new and existing team members • be a key leader of the data and analytics team, working to propel the business towards being more data-driven • performs other duties as assigned • complies with all policies and standards qualifications • bachelor's degree computer science, computer engineering, software engineering, or other related technical field • master's degree computer science, computer engineering, software engineering, or other related technical field computer science, computer engineering, software engineering, or other related technical field • 8+ years experience in data engineering solutions such as data platforms, data ingestion, data management, or publication/analytics • 3+ years *leadership experience* building and managing a high performing teams. • 2+ years experience in google cloud with services like bigquery, composer, gcs, datastream, dataflows • progressively responsible experience starting as data engineer and advancement in complexity, and level of responsibility • must be highly analytical having the proven ability to develop and reverse complex engineering solutions. critical thinking and advanced problem-solving skills are core behaviors among the team • excellent oral/written communication and presentation skills which bring clarity and precision to help management understand and visualize complex material to support organizational decision-making. • experience in objectively evaluating current processes for opportunities to optimize inter departmental communication, collaboration, and end-to-end process performance • have proven experience defining a roadmap and managing incremental execution through successful launches. • ability to manage multiple, competing priorities • must thrive in an agile, fast-paced, constantly changing environment • provide mentorship to data engineers • developed technology target state, roadmaps that aligned to short- and long-term business goals. • travel requirements - up to 5% additional information in support of the pay transparency laws enacted across the country, the expected salary range for this position is between $96404.1 and $169021.85. actual pay will be adjusted based on job-related factors permitted by law, such as experience and training; geographic location; licensure and certifications; market factors; departmental budgets; and responsibility. our talent acquisition team will be happy to answer any questions you may have, and we look forward to learning more about your salary requirements. the position qualifies for the below benefits. adtalem offers a robust suite of benefits including: • health, dental, vision, life and disability insurance • 401k retirement program + 6% employer match • participation in adtalem’s flexible time off (fto) policy • 12 paid holidays for more information related to our benefits please visit: https://careers.adtalem.com/benefits. you are also eligible to participate in an annual incentive program, subject to the rules governing the program, whereby an award, if any, depends on various factors, including, without limitation, individual and organizational performance. equal opportunity – minority / female / disability / v / gender identity / sexual orientation","lisle, il",Data Engineer,"['aws', 'bigquery', 'cloud', 'data pipeline', 'excel', 'google cloud', 'r']","['aws', 'bigquery', 'cloud', 'data pipeline', 'excel', 'google cloud', 'r']",
sr. data engineer,visa,"company description visa is a world leader in payments and technology, with over 259 billion payments transactions flowing safely between consumers, merchants, financial institutions, and government entities in more than 200 countries and territories each year. our mission is to connect the world through the most innovative, convenient, reliable, and secure payments network, enabling individuals, businesses, and economies to thrive while driven by a common purpose - to uplift everyone, everywhere by being the best way to pay and be paid. make an impact with a purpose-driven industry leader. join us today and experience life at visa. job description visa u.s.a. inc., a visa inc. company, needs a sr. data engineer (multiple openings) in austin, texas to: design, build, and launch efficient and reliable data pipelines to move data (both large and small amounts) in/out of hadoop data lake. architect, build, and launch new data pipelines and models that provide intuitive analytics to customers. design and develop new systems and tools to enable customers with data analysis and enhanced understanding of their data. create, automate, and scale repeatable analysis and build self service tools for business users. execute data engineering projects ranging from small to large individually and as part of project team. work across multiple teams in high visibility roles and own the solution end-to-end. assist in scoping and designing analytic data assets. build and maintain robust data engineering process to develop and implement self-serve data. perform other tasks on r&d, data governance, system infrastructure, and other cross team functions. position reports to the austin, texas office and may allow for partial telecommuting. qualifications basic qualifications: employer will accept a master's degree in computer science, management information systems, or business analytics and 2 years of experience in information technology or data engineer-related occupation. position requires knowledge or experience in the following:proficient in utilizing hadoop for distributed storage and large-scale data processing, enabling efficient handling of massive datasets. proficient in leveraging apache spark for large-scale data processing and advanced analytics, enabling both near real-time data insights and efficient batch processing. proficient in coding with pyspark, scala, and python for scalable data processing, transformation, and analysis, enabling seamless integration with big data frameworks. knowledge in designing, implementing, and managing both relational (rdbms) and non-relational (nosql) database management systems to ensure efficient data storage, retrieval, and scalability. proficient in crafting and optimizing complex sql queries for efficient data retrieval and manipulation, ensuring high performance and reducing query execution times. proficient in advanced database performance tuning, including indexing strategies and query optimization, to significantly enhance overall system efficiency and responsiveness. skilled in presto, impala, sparksql, and hive for performing sql-like querying on big data, facilitating extensive data analysis and reporting. skilled in data modeling and data warehousing techniques to design and implement efficient, scalable, and robust data storage solutions that support business intelligence initiatives. proficient in utilizing data visualization and business intelligence tools such as tableau and power bi to create insightful, interactive reports and dashboards that drive data-driven decision-making. additional information worksite: austin, texas this is a hybrid position. hybrid employees can alternate time between both remote and office. employees in hybrid roles are expected to work from the office 2-3 set days a week (determined by leadership/site), with a general guidepost of being in the office 50% or more of the time based on business needs. travel requirements: this position does not require travel. mental/physical requirements: this position will be performed in an office setting. the position will require the incumbent to sit and stand at a desk, communicate in person and by telephone, frequently operate standard office equipment, such as telephones and computers. visa is an eeo employer. qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability or protected veteran status. visa will also consider for employment qualified applicants with criminal histories in a manner consistent with eeoc guidelines and applicable local law. u.s. applicants only: the estimated salary range for a new hire into this position is $110,700.00 to $171,800.00 usd per year, which may include potential sales incentive payments (if applicable). salary may vary depending on job-related factors which may include knowledge, skills, experience, and location. in addition, this position may be eligible for bonus and equity. visa has a comprehensive benefits package for which this position may be eligible that includes medical, dental, vision, 401 (k), fsa/hsa, life insurance, paid time off, and wellness program.","westover hills, tx",Data Engineer,"['business intelligence', 'dashboard', 'data analysis', 'data lake', 'data pipeline', 'hadoop', 'power bi', 'pyspark', 'python', 'r', 'scala', 'spark', 'sql', 'tableau']","['business intelligence', 'dashboard', 'data analysis', 'data lake', 'data pipeline', 'hadoop', 'power bi', 'pyspark', 'python', 'r', 'scala', 'spark', 'sql', 'tableau']",
data engineer ii,"astrana health, inc.","data engineer ii department: data - analytics employment type: full time location: 1668 s. garfield ave. 2nd floor, alhambra, ca 91801 compensation: $105,000 - $115,000 / year description we are currently seeking a highly motivated data engineer ii. this role will report to the director - data engineering and work closely with data analysts, data engineers, data scientists, and clinical leaders to produce deliverables for internal and external clients. with over a million managed lives across the country and terabytes of data generated, our teams need to be continuously equipped with the tools and insights to drive strategy and innovation to further our core values of improving patient outcomes and empowering our providers. our values: • put patients first • empower entrepreneurial provider and care teams • operate with integrity & excellence • be innovative • work as one team what you'll do • use data engineering best practices to produce high quality, maximally available data models which are intuitive to data analysts and trusted by stakeholders • develop deep domain knowledge in healthcare operations, tracking regulatory developments related to analytics products you maintain • apply quality measures and other metrics to datasets originating from internal and external clients • build scalable elt pipelines and business intelligence dashboards as needed, embracing automation wherever possible • implement data quality checks which proactively identify data issues and distributional shifts to ensure accuracy of downstream analytical products qualifications • bachelor's degree required in healthcare, analytics, statistics, finance, business, or related field; master’s degree (mba, mph) preferred. • experience with relational databases. • strong understanding of database structures, theories, principles, and practices • working knowledge with programming or scripting languages such as python, spark, and sql. • knowledge of professional software engineering practices and best practices for the full software development life cycle (sdlc), including documentation, coding standards, code reviews, source control management, build processes, testing, and operations. • familiarity with normalized, dimensional, star schema and snowflake schematic models • working experience with databricks preferred • familiarity with business intelligence exploratory or visualization tools (e.g., tableau, powerbi.) preferred • strong written and oral communication skills. • experience with excel. you're a great for this role if: • 2+ years of experience working in the data and analytics landscape • 2+ years of experience using version control to manage code changes • 2+ years of experience in managed care or other healthcare data field preferred • 1+ years’ using cloud-based services from aws, gcp, or azure environmental job requirements and working conditions • this position is remotely based in the u.s. • the total compensation target pay range for this role is: $105,000 - $115,000. the salary range represents our national target range for this role. astrana health is proud to be an equal employment opportunity and affirmative action employer. we do not discriminate based on race, religion, color, national origin, gender (including pregnancy, childbirth, or related medical conditions), sexual orientation, gender identity, gender expression, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics. all employment is decided based on qualifications, merit, and business need. if you require assistance in applying for open positions due to a disability, please email us at humanresourcesdept@astranahealth.com to request an accommodation. additional information: the job description does not constitute an employment agreement between the employer and employee and is subject to change by the employer as the needs of the employer and requirements of the job change. #li-remote",anywhere,Data Engineer,"['aws', 'azure', 'business intelligence', 'cloud', 'dashboard', 'databricks', 'elt', 'excel', 'gcp', 'python', 'r', 'scala', 'snowflake', 'spark', 'sql', 'statistics', 'tableau']","['aws', 'azure', 'business intelligence', 'cloud', 'dashboard', 'databricks', 'elt', 'excel', 'gcp', 'python', 'r', 'scala', 'snowflake', 'spark', 'sql', 'statistics', 'tableau']",105K–115K a year
senior data engineer (remote),carefirst bluecross blueshield,"resp & qualifications purpose: the senior data engineer is responsible for orchestrating, deploying, maintaining and scaling cloud or on-premise infrastructure targeting big data and platform data management (relational and nosql, distributed and converged) with emphasis on reliability, automation and performance. this role will focus on developing solutions and helping transform the company's platforms deliver data-driven, meaningful insights and value to company. this specific opening is for a hedis engineer who can build data solutions to support hedis processing and submissions. this role also requires data analysis to ensure correctness of processing. essential functions • develops and maintains infrastructure systems (e.g., data warehouses, data lakes) including data access apis. prepares and manipulates data using multiple technologies. • interprets data, analyzes results using statistical techniques, and provides ongoing reports. executes quantitative analyses that translate data into actionable insights. provides analytical and data-driven decision-making support for key projects. designs, manages, and conducts quality control procedures for data sets using data from multiple systems. • develops data models by studying existing data warehouse architecture; evaluating alternative logical data models including planning and execution tables; applying metadata and modeling standards, guidelines, conventions, and procedures; planning data classes and sub-classes, indexes, directories, repositories, messages, sharing, replication, back-up, retention, and recovery. • creates data collection frameworks for structured and unstructured data. • improves data delivery engineering job knowledge by attending educational workshops; reviewing professional publications; establishing personal networks; benchmarking state-of-the-art practices; participating in professional societies. • applies data extraction, transformation and loading techniques in order to connect large data sets from a variety of sources. • applies and implements best practices for data auditing, scalability, reliability and application performance. supervisory responsibility position does not have direct reports but is expected to assist in guiding and mentoring less experienced staff. may lead a team of matrixed resources. qualifications education level: bachelor's degree in computer science, information technology or engineering or related field or in lieu of a bachelor's degree, an additional 4 years of relevant work experience is required in addition to the required work experience. experience: 5 years experience with database design and developing modeling tools. experience developing and updating etl/elt scripts. hands-on experience with application development, relational database layout, development, data modeling. preferred qualifications • expert with informatica iics and sql. • knowledge of snowflake db and azure suit preferred or willing to learn. • experience with health care data, preferably hedis experience. knowledge, skills and abilities (ksas) • knowledge and understanding of at least one programming language (i.e., sql, nosql, python). • knowledge and understanding of database design and implementation concepts. • knowledge and understanding of data exchange formats. • knowledge and understanding of data movement concepts. • strong technical and analytical and problem-solving skills to troubleshoot to solve a variety of problems. • requires strong organizational and communication skills, written and verbal, with the ability to handle multiple priorities. • must be able to meet established deadlines and handle multiple customer service demands from internal and external customers, within set expectations for service excellence. must be able to effectively communicate and provide positive customer service to every internal and external customer, including customers who may be demanding or otherwise challenging. salary range: $96,048 - $190,762 salary range disclaimer the disclosed range estimate has not been adjusted for the applicable geographic differential associated with the location at which the work is being performed. this compensation range is specific and considers factors such as (but not limited to) the scope and responsibilities of the position, the candidate's work experience, education/training, internal peer equity, and market and business consideration. it is not typical for an individual to be hired at the top of the range, as compensation decisions depend on each case's facts and circumstances, including but not limited to experience, internal equity, and location. in addition to your compensation, carefirst offers a comprehensive benefits package, various incentive programs/plans, and 401k contribution programs/plans (all benefits/incentives are subject to eligibility requirements). department payment integrity equal employment opportunity carefirst bluecross blueshield is an equal opportunity (eeo) employer. it is the policy of the company to provide equal employment opportunities to all qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, protected veteran or disabled status, or genetic information. where to apply please visit our website to apply: www.carefirst.com/careers federal disc/physical demand note: the incumbent is required to immediately disclose any debarment, exclusion, or other event that makes him/her ineligible to perform work directly or indirectly on federal health care programs. physical demands the associate is primarily seated while performing the duties of the position. occasional walking or standing is required. the hands are regularly used to write, type, key and handle or feel small controls and objects. the associate must frequently talk and hear. weights up to 25 pounds are occasionally lifted. sponsorship in us must be eligible to work in the u.s. without sponsorship","baltimore, md",Data Engineer,"['azure', 'cloud', 'data analysis', 'data lake', 'data warehouse', 'elt', 'etl', 'excel', 'python', 'r', 'sas', 'scala', 'snowflake', 'sql']","['azure', 'cloud', 'data analysis', 'data lake', 'data warehouse', 'elt', 'etl', 'excel', 'python', 'r', 'sas', 'scala', 'snowflake', 'sql']","96,048–190,762 a year"
sr associate data engineer (etl / databricks),mckesson,"mckesson is an impact-driven, fortune 10 company that touches virtually every aspect of healthcare. we are known for delivering insights, products, and services that make quality care more accessible and affordable. here, we focus on the health, happiness, and well-being of you and those we serve – we care. what you do at mckesson matters. we foster a culture where you can grow, make an impact, and are empowered to bring new ideas. together, we thrive as we shape the future of health for patients, our communities, and our people. if you want to be part of tomorrow’s health today, we want to hear from you. rx savings solutions, part of mckesson's covermymeds organization, offers an innovative, patented software system that educates and empowers consumers to make the best healthcare choices at the lowest cost. founded and operated by a team of pharmacists and software engineers, we support a collaborative, cost-saving solution for purchasing prescription drugs. we currently have an opportunity for a sr associate data engineer (etl / databricks) to join our growing business operations support data engineering team! this role will assist in planning, designing, troubleshooting and documenting technical requirements for data flows between disparate operational systems and our data warehouse. our ideal candidate will have databricks experience, etl tool experience (i.e. talend, informatica, ssis or datastage), experience with sql, and general-purpose programming languages such as python or java for data engineering/ingestion work. this individual will assist in end-to-end development of the etl processes, data analytics, review business requirement documents and execute object and data models. • our ideal candidate will reside in the columbus, oh area. the work will primarily be remote from home with occasional in-office work. • at this time, we are not able to offer sponsorship for employment visas. this includes individuals currently on f-1 opt, stem opt, or any other visa status that would require future sponsorship. candidates must be authorized to work in the united states on a permanent basis without the need for current or future sponsorship. responsibilities: • build data pipelines to ingest and automate files provided by customers using etl tool • interface with other technology teams to extract, transform, and load data from a wide variety of data sources using primarily etl tools • explore and learn the latest aws technologies to provide new capabilities and increase efficiency • investigate and resolve data related issues and provide support and troubleshooting expertise • collaborating across teams to understand business needs and propose innovative solutions requirements: • bachelor's degree in computer science or related technical degree, or equivalent experience, and 2+ years of relative experience • 2+ years of experience with databricks and building data pipelines to ingest and automate files provided by customers • 2+ years of experience with sql queries, including various sql commands • experience with python • knowledge of structured and unstructured data • possess understanding of bi concepts and be familiar with relational or multi-dimensional modeling concepts • understanding of rdbms best practices and performance tuning techniques • experience with cloud technologies such as aws services such as s3, cloudwatch, ec2, and passion for a role working in a cloud data warehouse it would be nice if you had... • experience with agile and scrum methodologies • knowledge of java or javascript we are proud to offer a competitive compensation package at mckesson as part of our total rewards. this is determined by several factors, including performance, experience and skills, equity, regular job market evaluations, and geographical markets. the pay range shown below is aligned with mckesson's pay philosophy, and pay will always be compliant with any applicable regulations. in addition to base pay, other compensation, such as an annual bonus or long-term incentive opportunities may be offered. for more information regarding benefits at mckesson, please click here. our base pay range for this position $80,600 - $134,400 mckesson is an equal opportunity employer mckesson provides equal employment opportunities to applicants and employees and is committed to a diverse and inclusive environment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, disability, age or genetic information. for additional information on mckesson’s full equal employment opportunity policies, visit our equal employment opportunity page. join us at mckesson!","hilliard, oh",Data Engineer,"['aws', 'cloud', 'data analytics', 'data pipeline', 'data warehouse', 'databricks', 'etl', 'java', 'python', 'r', 'sas', 'sql']","['aws', 'cloud', 'data analytics', 'data pipeline', 'data warehouse', 'databricks', 'etl', 'java', 'python', 'r', 'sas', 'sql']",
sr. data engineer,concora credit inc.,"overview as a sr. data engineer, you’ll help drive concora credit’s mission to enable customers to do more with credit – every single day. the impact you’ll have at concora credit: we are seeking a sr. data engineer with deep expertise in azure and databricks to lead the design, development, and optimization of scalable data pipelines and platforms. you’ll be responsible for building robust data solutions that power analytics, reporting, and machine learning across the organization using azure cloud services and databricks. we hire people, not positions. that's because, at concora credit, we put people first, including our customers, partners, and team members. concora credit is guided by a single purpose: to help non-prime customers do more with credit. today, we have helped millions of customers access credit. our industry leadership, resilience, and willingness to adapt ensure we can help our partners responsibly say yes to millions more. as a company grounded in entrepreneurship, we're looking to expand our team and are looking for people who foster innovation, strive to make an impact, and want to do more! we’re an established company with over 20 years of experience, but now we’re taking things to the next level. we're seeking someone who wants to impact the business and play a pivotal role in leading the charge for change. responsibilities as our sr. data engineer, you will: • design and develop scalable, efficient data pipelines using azure databricks • build and manage data ingestion, transformation, and storage solutions leveraging azure data factory, azure data lake, and delta lake • implement ci/cd for data workflows using tools like azure devops, git, and terraform • optimize performance and cost efficiency across large-scale distributed data systems • collaborate with analysts, data scientists, and business stakeholders to understand data needs and deliver reliable, reusable datasets • provide guidance and mentor junior engineers and actively contribute to data platform best practices • monitor, troubleshoot, and optimize existing pipelines and infrastructure to ensure reliability and scalability these duties must be performed with or without reasonable accommodation. we know experience comes in many forms and that many skills are transferable. if your experience is close to what we're looking for, consider applying. diversity has made us the entrepreneurial and innovative company that we are today. qualifications requirements: • 5+ years of experience in data engineering, with a strong focus on azure cloud technologies • experience with azure databricks, azure data lake, data factory including pyspark, sql, python and delta lake • strong proficiency in databricks and apache spark • solid understanding of data warehousing, etl/elt, and data modeling best practices • experience with version control, ci/cd pipelines, and infrastructure as code • knowledge of spark performance tuning, partitioning, and job orchestration • excellent problem-solving skills and attention to detail • strong communication and collaboration abilities across technical and non-technical teams • ability to work independently and lead in a fast-paced, agile environment • passion for delivering clean, high-quality, and maintainable code preferred qualifications: • experience with unity catalog, databricks workflows, and delta live tables • familiarity with devops practices or terraform for azure resource provisioning • understanding of data security, rbac, and compliance in cloud environments • experience integrating databricks with power bi or other analytics platforms • exposure to real-time data processing using kafka, event hubs, or structured streaming what’s in it for you: • medical, dental and vision insurance for you and your family • relax and recharge with paid time off (pto) • 6 company-observed paid holidays, plus 3 paid floating holidays • 401k (after 90 days) plus employer match up to 4% • pet insurance for your furry family members • wellness perks including onsite fitness equipment at both locations, eap, and access to the headspace app • we invest in your future through tuition reimbursement • save on taxes with flexible spending accounts • peace of mind with life and ad&d insurance • protect yourself with company-paid long-term disability and voluntary short-term disability concora credit provides equal employment opportunities to all team members and applicants for employment and prohibits discrimination and harassment of any type without regard to race, color, religion, age, sex, national origin, disability status, genetics, protected veteran status, sexual orientation, gender identity or expression, or any other characteristic protected by federal, state or local laws. employment-based visa sponsorship is not available for this role. concora credit is an equal opportunity employer (eeo). please see the concora credit privacy policy for more information on how concora credit processes your personal information during the recruitment process and, if applicable, based on your location, how you can exercise your privacy rights. if you have questions about this privacy notice or need to contact us in connection with your personal data, including any requests to exercise your legal rights referred to at the end of this notice, please contact caprivacynotice@concoracredit.com.","beaverton, or",Data Engineer,"['aws', 'azure', 'cloud', 'data lake', 'data pipeline', 'databricks', 'elt', 'etl', 'excel', 'kafka', 'machine learning', 'power bi', 'pyspark', 'python', 'r', 'scala', 'spark', 'sql']","['aws', 'azure', 'cloud', 'data lake', 'data pipeline', 'databricks', 'elt', 'etl', 'excel', 'kafka', 'machine learning', 'power bi', 'pyspark', 'python', 'r', 'scala', 'spark', 'sql']",
data engineer iii,sam's club,"position summary...the data engineer iii is responsible for designing, building, and optimizing scalable big data pipelines, architectures, and datasets that enable advanced analytics and data-driven decision-making. this role involves developing efficient data transformation and processing frameworks, managing data structures, metadata, dependencies, and workloads, and ensuring the reliability and performance of the data ecosystem. the engineer will also work extensively with unstructured datasets, applying analytical techniques to extract insights and improve data accessibility across the organization. what you'll do... • data modeling: designing and implementing data models to support structured and unstructured datasets, ensuring data integrity and efficiency. • data extraction: developing and optimizing data extraction processes from various sources including databases, apis, and logs. • data cleaning: preprocessing and cleaning data to remove inconsistencies and improve data quality. • data screening: implementing data validation and quality checks to ensure accuracy and completeness of data. • data exploration: conducting exploratory data analysis to understand patterns, trends, and correlations in the data. • data visualization: creating visualizations using tools like tableau, powerbi, or looker to communicate insights and findings effectively. • big data technologies: utilizing tools and frameworks such as spark, spark sql, pyspark, hdfs, and mapreduce for processing large datasets efficiently. • cloud services: leveraging cloud platforms like gcp, azure/aws, databricks, azure hd insights, adf for data storage, processing, and analytics. • data querying: writing advanced sql queries to extract and manipulate data from relational databases and other data stores. • data pipeline development: building and optimizing scalable data pipelines and architectures to move and transform data across systems. • data transformation: developing processes for data transformation, structure, metadata, dependency, and workload management. • enterprise software development: contributing to the development of enterprise-level software products related to data engineering and analytics. what you'll bring: • cross-functional collaboration: working closely with cross-functional teams including data scientists, analysts, and software engineers to achieve common goals. • programming languages: proficiency in at least one scripting language like python or scala for automation, data manipulation, and tool development. • agile environment: collaborating effectively in an agile environment, participating in sprints, and adapting to changing • analytical skills: applying strong analytical skills to work with complex and unstructured datasets, extracting valuable insights and actionable information. project requirements. • big data data stores: implementing and managing highly scalable big data stores to efficiently store and access large volumes of data. • data value extraction: manipulating, processing, and extracting value from large, diverse datasets to drive business decisions and innovation. • big data technologies: experience utilizing tools and frameworks such as spark, spark sql, pyspark, hdfs, and mapreduce for processing large datasets efficiently. • cloud services: experience leveraging cloud platforms like gcp, azure/aws, databricks, azure hd insights, adf for data storage, processing, and analytics. about walmart general/not function specific sam walton opened the first sam's club in 1983 to meet a growing need among customers who wanted to buy merchandise in bulk. since then, sam's club has grown rapidly, opening more than 600 clubs in the u.s. and 100 clubs internationally. by offering affordable, wholesale merchandise to members, sam's club helps make saving simple for families and small business owners. sam's club employs about 110,000 associates in the u.s. the average club is 134,000 square feet and offers bulk groceries and general merchandise. most clubs also have specialty services, such as a pharmacy, an optical department, a photo center, or a tire and battery center. ​​​future ways of working: our company's success can be attributed to our employees. while technology has allowed us to be effective while working remotely, there is no substitute for being in the office together; it helps to shape our culture, collaborate, innovate, build relationships, and move more quickly. we strive to provide flexibility in order to promote a healthy work-life balance but recognize that in-person interactions are important to our culture and shared success. we'll meet in person on a regular and purposeful basis. benefits: benefits: beyond our great compensation package, you can receive incentive awards for your performance. other great perks include 401(k) match, stock purchase plan, paid maternity and parental leave, pto, multiple health plans, and much more. equal opportunity employer: walmart, inc. is an equal opportunity employer – by choice. we believe we are best equipped to help our associates, customers, and the communities we serve live better when we really know them. that means understanding, respecting, and valuing unique styles, experiences, identities, ideas, and opinions – while being inclusive of all people. the above information has been designed to indicate the general nature and level of work performed in the role. it is not designed to contain or be interpreted as a comprehensive inventory of all responsibilities and qualifications required of employees assigned to this job. the full job description can be made available as part of the hiring process. at sam's club, we offer competitive pay as well as performance-based bonus awards and other great benefits for a happier mind, body, and wallet! ‎ - health benefits include medical, vision and dental coverage ‎ - financial benefits include 401(k), stock purchase and company-paid life insurance ‎ - paid time off benefits include pto, parental leave, family care leave, bereavement, jury duty, and voting. you will also receive pto and/or ppto that can be used for vacation, sick leave, holidays, or other purposes. the amount you receive depends on your job classification and length of employment. it will meet or exceed the requirements of paid sick leave laws, where applicable. ‎ for information about pto, see https://one.walmart.com/notices. ‎ - other benefits include short-term and long-term disability, company discounts, military leave pay, adoption and surrogacy expense reimbursement, and more. ‎ live better u is a company paid education benefit program for full-time and part-time associates in walmart and sam's club facilities. programs range from high school completion to bachelor's degrees, including english language learning and short-form certificates. tuition, books, and fees are completely paid for by walmart. ‎ eligibility requirements apply to some benefits and may depend on your job classification and length of employment. benefits are subject to change and may be subject to a specific plan or program terms. ‎ for information about benefits and eligibility, see one.walmart. ‎ the annual salary range for this position is $90,000.00-$180,000.00 ‎ additional compensation includes annual or quarterly performance bonuses. ‎ ‎ ‎ ‎ ‎ minimum qualifications... outlined below are the required minimum qualifications for this position. if none are listed, there are no minimum qualifications. option 1: bachelor’s degree in computer science and 2 years' experience in software engineering or related field. option 2: 4 years’ experience in software engineering or related field. option 3: master's degree in computer science. preferred qualifications... outlined below are the optional preferred qualifications for this position. if none are listed, there are no preferred qualifications. data engineering, database engineering, business intelligence, or business analytics, master’s degree in computer science or related field and 2 years' experience in software engineering or related field, we value candidates with a background in creating inclusive digital experiences, demonstrating knowledge in implementing web content accessibility guidelines (wcag) 2.2 aa standards, assistive technologies, and integrating digital accessibility seamlessly. the ideal candidate would have knowledge of accessibility best practices and join us as we continue to create accessible products and services following walmart’s accessibility standards and guidelines for supporting an inclusive culture. primary location... 2101 se simple savings dr, bentonville, ar 72712-4304, united states of america walmart and its subsidiaries are committed to maintaining a drug-free workplace and has a no tolerance policy regarding the use of illegal drugs and alcohol on the job. this policy applies to all employees and aims to create a safe and productive work environment.","springdale, ar",Data Engineer,"['aws', 'azure', 'business intelligence', 'classification', 'cloud', 'data analysis', 'data pipeline', 'databricks', 'gcp', 'looker', 'pyspark', 'python', 'r', 'scala', 'spark', 'sql', 'tableau']","['aws', 'azure', 'business intelligence', 'classification', 'cloud', 'data analysis', 'data pipeline', 'databricks', 'gcp', 'looker', 'pyspark', 'python', 'r', 'scala', 'spark', 'sql', 'tableau']",90K–180K a year
principal data engineer,warner bros. discovery,"welcome to warner bros. discovery… the stuff dreams are made of. who we are… when we say, “the stuff dreams are made of,” we’re not just referring to the world of wizards, dragons and superheroes, or even to the wonders of planet earth. behind wbd’s vast portfolio of iconic content and beloved brands, are the storytellers bringing our characters to life, the creators bringing them to your living rooms and the dreamers creating what’s next… from brilliant creatives, to technology trailblazers, across the globe, wbd offers career defining opportunities, thoughtfully curated benefits, and the tools to explore and grow into your best selves. here you are supported, here you are celebrated, here you can thrive. your new role: warner bros. discovery seeks a principal data engineer – data for the data & shared services organization. we are looking for a talented principal data engineer to join our data engineering team. the person works across multiple functional and technical groups, educates technical and non-technical stakeholders, effectively manages expectations, and help establish priorities. they must drive project delivery within established timeframes, and scope while delivering business value and simultaneously planning for and managing future requirements. strong analytical skills and passion for data is a must. your role accountabilities: • architect & lead • define and implement the strategic vision for data platforms supporting streaming and ad tech ecosystems. • establish best practices for data engineering, governance, and performance optimization. • platform development • build and optimize large-scale data pipelines using databricks, spark, and flink. • design robust data models and storage solutions leveraging snowflake and aws services. • * streaming & real-time • architect real-time ingestion and processing frameworks using kafka and flink for streaming events and ad delivery data. • integration & interoperability • drive integration with ott platforms, linear networks, cdps, and ad sales systems for campaign targeting and measurement. • reporting enablement • ensure data availability and quality for bi tools such as power bi and tableau to support dashboards and executive reporting. • leadership & mentorship • mentor engineering teams, review designs, and ensure adherence to architectural standards. • innovation • evaluate emerging technologies and recommend solutions to improve scalability, cost efficiency, and data quality. qualifications & experiences: • bachelor’s degree in science, statistics, engineering, business administration or similar field of study • 14+ years of experience building and scaling data platforms. • expert in one or modern data replication and data integration tools (for example informatica, attunity, aws dms, etc.) • expert in one or more modern data lake and data warehouse tools (for example teradata, redshift, snowflake, etc.) • expert in one or more modern advanced analytics tools (for example spark, data bricks, python, aws glue, etc.) • experience in one or more modern advanced business intelligence & visualization tools (for example business objects, tableau, looker, etc.) • experience in software delivery through continuous integration (for example git, bitbucket, jenkins, etc.) • experience in one or more automation and scheduling tools (for example redwood, airflow, etc.) • experience with atlassian suite (jira, confluence) • experience with excel spreadsheets, modeling, and reporting. • knowledge of cloud-based environments (aws is a plus) not required but preferred experience: • public speaking and presentation skills. • experience with data science tools such as numpy, pandas, r, matlab etc. • experience with data processing engines like spark • experience with dbt. • *relocation is not approved for this position. our priority will be consideration of atlanta based candidates) how we get things done… this last bit is probably the most important! here at wbd, our guiding principles are the core values by which we operate and are central to how we get things done. you can find them at www.wbd.com/guiding-principles/ along with some insights from the team on what they mean and how they show up in their day to day. we hope they resonate with you and look forward to discussing them during your interview. championing inclusion at wbd warner bros. discovery embraces the opportunity to build a workforce that reflects a wide array of perspectives, backgrounds and experiences. being an equal opportunity employer means that we take seriously our responsibility to consider qualified candidates on the basis of merit, without regard to race, color, religion, national origin, gender, sexual orientation, gender identity or expression, age, mental or physical disability, and genetic information, marital status, citizenship status, military status, protected veteran status or any other category protected by law. if you’re a qualified candidate with a disability and you require adjustments or accommodations during the job application and/or recruitment process, please visit our accessibility page for instructions to submit your request.","atlanta, ga",Data Engineer,"['airflow', 'aws', 'bi tools', 'business intelligence', 'cloud', 'dashboard', 'data lake', 'data pipeline', 'data warehouse', 'databricks', 'dbt', 'excel', 'kafka', 'looker', 'matlab', 'numpy', 'pandas', 'power bi', 'python', 'r', 'redshift', 'scala', 'snowflake', 'spark', 'statistics', 'tableau']","['airflow', 'aws', 'bi tools', 'business intelligence', 'cloud', 'dashboard', 'data lake', 'data pipeline', 'data warehouse', 'databricks', 'dbt', 'excel', 'kafka', 'looker', 'matlab', 'numpy', 'pandas', 'power bi', 'python', 'r', 'redshift', 'scala', 'snowflake', 'spark', 'statistics', 'tableau']",
"data engineer (on-site columbia, sc)","genesis healthcare, inc","work where balance meets purpose – competitive pay & excellent benefits! join genesis healthcare and be the driving force behind smooth, patient-focused operations. now hiring: data engineer location: genesis healthcare, inc (columbia, sc- onsite) schedule: monday – friday, day shift (8 hours) no weekends – enjoy work-life balance! make a real difference in community healthcare are you looking for a meaningful role where you can truly make an impact? at genesis healthcare, inc., we’re not just another healthcare provider—we’re a nonprofit, community-focused fqhc dedicated to improving lives across the pee dee and low country regions of south carolina. why choose genesis? generous paid time off-holidays, sick leave, cme hours life insurance & employee assistance program– prioritizing your well-being 401(k) match, vision, and more ready to take the next step in your career? apply today and be part of something bigger! position summary the data engineer will work closely with the business intelligence officer and other members of the business intelligence (bi) team to design, develop, and maintain robust data pipelines to ensure high quality and efficient data solutions that meet business needs. the data engineer will assist in creating queries, stored procedures, views, and perform other tasks in our sql environment. primary accountabilities 1. design, build, and maintain data pipelines for analytics, reporting, and machine learning. 2. develop, implement, and optimize data automation and etl (extract, transform, load) processes. 3. maintain and optimize databases, data warehouses, and other data storage solutions. 4. utilize microsoft azure to deploy data pipelines and processing jobs. 5. monitor and troubleshoot data pipeline issues to ensure reliable data flow. 6. assist in designing, developing, and maintaining ai models. 7. continuously evaluate and improve data engineering processes and tools for better performance and efficiency. 8. develop and implement queries, views, tables, stored procedures, and other sql related tasks for reporting needs. 9. collaborate with business stakeholders to design, implement, and maintain bi models. 10. utilize and manage apis for connecting to third party software for data processing, data management, and data extraction. operational excellence 1. ensure and uphold the confidentially requirements of all patient records, and manage all daily tasks and activities consistent with hipaa, state and federal laws and regulations, as well as the clinic’s policies and regulations regarding confidentiality and security. 2. assures that all payments issued are appropriate and documented as ordered and received relationships 3. develop and ensure effective, positive relationships within and among the clinic staff, as well as with patients, vendors, contractors, and related resources. professionalism 4. ensure all actions, job performance, personal conduct and communications represent the organization in a highly professional manner at all times. 5. uphold and ensure compliance with and attention to all corporate policies and procedures, as well as the mission and values of the organization. primary tasks & duties • develop, implement, and maintain data pipelines for bi reporting across the business • utilize ssms to develop queries, tables, views, and stored procedures • automate recurring data processes and improve data accessibility across the organization • creating, managing, and troubleshooting apis for connecting to third party software for data processing and data extraction essential functions/key competencies • 1-5 years (preferred) of experience with microsoft azure, sharepoint, and power platform • 1-5 years (preferred) of experience with sql server management studio (ssms), t-sql query writing, and database development • 1-5 years (preferred) of experience with python and powershell • 1-5 years (preferred) of experience with developing and utilizing apis • advanced analytical and problem-solving skills • strong oral and written communication skills • knowledge of basic computer hardware and windows os position requirements education • batchelor’s degree in information technology, computer science, or related area. must be able to speak, read, write, and understand english. professional • “skilled” business office experience. physical/environmental • ability to interact with computer screen for up to six hours at a time (visual acuity required). • must have manual dexterity for use of keyboard. ability to remain stationary for periods of up to four hours. ability to communicate via phone, mail and in person to resolve disputes, solve problems, etc. • capacity to function in a sometimes stressful, multi-tasking environment","columbia, sc",Data Engineer,"['aws', 'azure', 'business intelligence', 'data pipeline', 'data warehouse', 'etl', 'excel', 'machine learning', 'python', 'r', 'sql', 'sql server']","['aws', 'azure', 'business intelligence', 'data pipeline', 'data warehouse', 'etl', 'excel', 'machine learning', 'python', 'r', 'sql', 'sql server']",
"senior, data engineer",walmart,"position summary... what you'll do...we're looking for a senior data engineer to join our team dedicated to the associate productivity experience apex org within ebs walmart global tech. in this high-impact role, you will design and build next-generation, ai-based data solutions that directly influence the day-to-day operations of walmart across multiple global markets. the ideal candidate has deep expertise in big data and google cloud platform (gcp) technologies, coupled with knowledge of ai engineering principles. you will be challenged with unparalleled complex business problems that spans across multiple countries, large number of users and data. about team: associate productivity and experience (apex) the enterprise people technology team supports the successful deployment and adoption of new people technology across the enterprise. as a fortune #1 company, our work impacts millions of associates globally. we strive to continuously improve people technology and products to help managers and associates so they can focus on what matters most - supporting our customers and members. people technology is one of the major segments of walmart global tech’s enterprise business services, which is invested in building a compact, robust organization that includes service operations and technology solutions for finance, people, and the associate digital experience. enterprise business services (ebs) is invested in building a compact, robust organization that includes service operations and technology solutions for finance, people & associate productivity and experience (apex) early associate experience digital transformation (adet). our associate productivity and experience (apex) organization which supports a large heterogenous enterprise environments and builds software products to enhance associate’s productivity and mobility through innovative ai solutions, seamless experience and continuous improvement. what you'll do: you will use your data engineering experience and technical skills to define, improve and deploy technical mitigation solutions. • data strategy: understands, articulates and applies principles of the defined strategy to routine business problems that involve a single function. • data transformation and integration: extracts data from identified databases. creates data pipelines and transform data to a structure that is relevant to the problem by selecting appropriate techniques. develops knowledge of current analytics trends. • data source identification: supports the understanding of the priority order of requirements and service level agreements. helps identify the most suitable source for data that is fit for purpose. performs initial data quality checks on extracted data. • data modelling: analyses complex data elements, systems, data flows, dependencies, and relationships to contribute to conceptual, physical and logical data models. develops the logical data model and physical data models including data warehouse and data mart designs. defines relational tables, primary and foreign keys and stored procedures to create a data model structure. evaluates existing data models and physical databases for variances and discrepancies. develops efficient data flows. analyses data-related system integration challenges and proposes appropriate solutions. • code development and testing: writes code to develop the required solution and application features by determining the appropriate programming language and leveraging business, technical and data requirements. creates test cases to review and validate the proposed solution design. creates proofs of concept. tests the code using the appropriate testing approach. deploys software to production servers. contributes code documentation, maintains playbooks, and provides timely progress updates. • problem formulation: translates business problems within one's discipline to data related or mathematical solutions. identifies what methods (for example, analytics, big data analytics, automation) would provide a solution for the problem. shares use cases and gives examples to demonstrate how the method would solve the business problem. • applied business acumen: provides recommendations to business stakeholders to solve complex business issues. develops business cases for projects with a projected return on investment or cost savings. translates business requirements into projects, activities, and tasks and aligns to overall business strategy. serves as an interpreter and conduit to connect business needs with tangible solutions and results. recommends new processes and ways of working. • data governance: establishes, modifies, and documents data governance projects and recommendations. implements data governance practices in partnership with business stakeholders and peers. interprets company and regulatory policies on data. educates others on data governance processes, practices, policies, and guidelines. provides recommendations on needed updates or inputs into data governance policies, practices, or guidelines. • demonstrates up-to-date expertise and applies this to the development, execution, and improvement of action plans by providing expert advice and guidance to others. supporting and aligning efforts to meet customer and business needs and building commitment for perspectives and rationales. what you'll bring: • bachelor’s degree in computer science or related technical field. • 8+ years of relevant experience on scala/python (pyspark), distributed databases, kafka with solid hands-on multi-threading, functional programing etc. • a good understanding of cs fundamentals, data structures, algorithms and problem solving. • professional hand-on experience in sql and query optimization. • experience in building frameworks for data ingestions and consumptions patterns. • experience with orchestration tools like airflow ( preferred) , automic , etc. • hand-on experience in data processing and data manipulation. • expertise with gcp cloud and gcp data processing tools, platforms and technologies like gcs, dataproc, dpaas, bigquery, hive etc. • experience with streaming data via kafka or structured streaming etc. • ci/cd automation experience with tools like git, jenkins and looper • exposure to visualization tools for data reporting such as tableau, powerbi, looker etc. • knowledge of llm agent development, prompt engineering, and experience fine-tuning llms (is good to have) • excellent communication skills for collaborating with teams. • appreciate productivity and care deeply about helping others work more effectively and efficiently about walmart global tech imagine working in an environment where one line of code can make life easier for hundreds of millions of people. that’s what we do at walmart global tech. we’re a team of software engineers, data scientists, cybersecurity expert's and service professionals within the world’s leading retailer who make an epic impact and are at the forefront of the next retail disruption. people are why we innovate, and people power our innovations. we are people-led and tech-empowered. we train our team in the skillsets of the future and bring in experts like you to help us grow. we have roles for those chasing their first opportunity as well as those looking for the opportunity that will define their career. here, you can kickstart a great career in tech, gain new skills and experience for virtually every industry, or leverage your expertise to innovate at scale, impact millions and reimagine the future of retail. walmart’s culture is a competitive advantage, and it’s fostered by being together. working together in person allows us to collaborate, align quickly and innovate with greater speed. we use our campuses to create purposeful connection rooted in deepening understanding and investing in the development of our associates. our hubs: walmart is a global company with offices across the united states and around the world. our global headquarters is in bentonville, arkansas, with primary hubs in the san francisco bay area and new york/new jersey. benefits: benefits: beyond our great compensation package, you can receive incentive awards for your performance. other great perks include 401(k) match, stock purchase plan, paid maternity and parental leave, pto, multiple health plans, and much more. equal opportunity employer: walmart, inc. is an equal opportunity employer – by choice. we believe we are best equipped to help our associates, customers, and the communities we serve live better when we really know them. that means understanding, respecting, and valuing unique styles, experiences, identities, ideas, and opinions – while being inclusive of all people. the above information has been designed to indicate the general nature and level of work performed in the role. it is not designed to contain or be interpreted as a comprehensive inventory of all responsibilities and qualifications required of employees assigned to this job. the full job description can be made available as part of the hiring process. at walmart, we offer competitive pay as well as performance-based bonus awards and other great benefits for a happier mind, body, and wallet. health benefits include medical, vision and dental coverage. financial benefits include 401(k), stock purchase and company-paid life insurance. paid time off benefits include pto (including sick leave), parental leave, family care leave, bereavement, jury duty, and voting. other benefits include short-term and long-term disability, company discounts, military leave pay, adoption and surrogacy expense reimbursement, and more. you will also receive pto and/or ppto that can be used for vacation, sick leave, holidays, or other purposes. the amount you receive depends on your job classification and length of employment. it will meet or exceed the requirements of paid sick leave laws, where applicable. for information about pto, see https://one.walmart.com/notices. live better u is a walmart-paid education benefit program for full-time and part-time associates in walmart and sam's club facilities. programs range from high school completion to bachelor's degrees, including english language learning and short-form certificates. tuition, books, and fees are completely paid for by walmart. eligibility requirements apply to some benefits and may depend on your job classification and length of employment. benefits are subject to change and may be subject to a specific plan or program terms. for information about benefits and eligibility, see one.walmart. the annual salary range for this position is $90,000.00 - $180,000.00 additional compensation includes annual or quarterly performance bonuses. additional compensation for certain positions may also include : - stock ㅤ ㅤ ㅤ ㅤ ‎ minimum qualifications... outlined below are the required minimum qualifications for this position. if none are listed, there are no minimum qualifications. option 1: bachelor’s degree in computer science and 3 years' experience in software engineering or related field. option 2: 5 years’ experience in software engineering or related field. option 3: master's degree in computer science and 1 year’s experience in software engineering or related field. 2 years' experience in data engineering, database engineering, business intelligence, or business analytics. preferred qualifications... outlined below are the optional preferred qualifications for this position. if none are listed, there are no preferred qualifications. data engineering, database engineering, business intelligence, or business analytics, etl tools and working with large data sets in the cloud, master’s degree in computer science or related field and 3 years' experience in software engineering, we value candidates with a background in creating inclusive digital experiences, demonstrating knowledge in implementing web content accessibility guidelines (wcag) 2.2 aa standards, assistive technologies, and integrating digital accessibility seamlessly. the ideal candidate would have knowledge of accessibility best practices and join us as we continue to create accessible products and services following walmart’s accessibility standards and guidelines for supporting an inclusive culture. primary location... 508 sw 8th st, bentonville, ar 72712, united states of america walmart and its subsidiaries are committed to maintaining a drug-free workplace and has a no tolerance policy regarding the use of illegal drugs and alcohol on the job. this policy applies to all employees and aims to create a safe and productive work environment.","goshen, ar",Data Engineer,"['airflow', 'aws', 'bigquery', 'business intelligence', 'classification', 'cloud', 'data analytics', 'data pipeline', 'data warehouse', 'etl', 'excel', 'gcp', 'google cloud', 'kafka', 'looker', 'pyspark', 'python', 'r', 'recommendation', 'sas', 'scala', 'spark', 'sql', 'tableau']","['airflow', 'aws', 'bigquery', 'business intelligence', 'classification', 'cloud', 'data analytics', 'data pipeline', 'data warehouse', 'etl', 'excel', 'gcp', 'google cloud', 'kafka', 'looker', 'pyspark', 'python', 'r', 'recommendation', 'sas', 'scala', 'spark', 'sql', 'tableau']",90K–180K a year
distinguished data engineer,capital one,"distinguished data engineer distinguished data engineers are individual contributors who strive to be diverse in thought so we visualize the problem space. at capital one, we believe diversity of thought strengthens our ability to influence, collaborate and provide the most innovative solutions across organizational boundaries. distinguished engineers will significantly impact our trajectory and devise clear roadmaps to deliver next generation technology solutions. deep technical experts and thought leaders that help accelerate adoption of the very best engineering practices, while maintaining knowledge on industry innovations, trends and practices visionaries, collaborating on capital one’s toughest issues, to deliver on business needs that directly impact the lives of our customers and associates role models and mentors, helping to coach and strengthen the technical expertise and know-how of our engineering and product community evangelists, both internally and externally, helping to elevate the distinguished engineering community and establish themselves as a go-to resource on given technologies and technology-enabled capabilities the distinguished data engineering role will be responsible for the architectural design and technical patterns that enable a high-performing, reliable data platform for card authorizations. the focus of the work includes advancing data observably, the spend data product, data standardization, and the core data pipelines that power authorization processing and decisioning. the role is expected to be hands-on, partnering closely with engineering teams and authorization's partners to help drive work forward. responsibilities: build awareness, increase knowledge and drive adoption of modern technologies, sharing consumer and engineering benefits to gain buy-in strike the right balance between lending expertise and providing an inclusive environment where others’ ideas can be heard and championed; leverage expertise to grow skills in the broader capital one team promote a culture of engineering excellence, using opportunities to reuse and innersource solutions where possible effectively communicate with and influence key stakeholders across the enterprise, at all levels of the organization operate as a trusted advisor for a specific technology, platform or capability domain, helping to shape use cases and implementation in an unified manner lead the way in creating next-generation talent for tech, mentoring internal talent and actively recruiting external talent to bolster capital one’s tech talent basic qualifications: bachelor’s degree at least 7 years of experience in data engineering at least 3 years of experience in data architecture at least 2 years of experience building applications in aws preferred qualifications : masters’ degree 9+ years of experience in data engineering 3+ years of data modeling experience 2+ years of experience with ontology standards for defining a domain 2+ years of experience using python, sql or scala 1+ year of experience deploying machine learning models 3+ years of experience implementing big data processing solutions on aws (s3, dynamodb, lambda, glue, flink) 2+ years of experience with orchestration technologies (airflow, step functions) 2+ years of experience with caching and in-memory data stores capital one will consider sponsoring a new qualified applicant for employment authorization for this position the minimum and maximum full-time annual salaries for this role are listed below, by location. please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount capital one is willing to pay at the time of this posting. salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked. chicago, il: $239,900 - $273,800 for distinguished data engineer mclean, va: $263,900 - $301,200 for distinguished data engineer richmond, va: $239,900 - $273,800 for distinguished data engineer candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate’s offer letter. this role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (lti). incentives could be discretionary or non discretionary depending on the plan. capital one offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. learn more at the capital one careers website. eligibility varies based on full or part-time status, exempt or non-exempt status, and management level. this role is expected to accept applications for a minimum of 5 business days.no agencies please. capital one is an equal opportunity employer (eoe, including disability/vet) committed to non-discrimination in compliance with applicable federal, state, and local laws. capital one promotes a drug-free workplace. capital one will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, article 23-a of the new york correction law; san francisco, california police code article 49, sections ; new york city’s fair chance act; philadelphia’s fair criminal records screening act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries. if you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact capital one recruiting at or via email at . all information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. for technical support or questions about capital one's recruiting process, please send an email to capital one does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site. capital one financial is made up of several different entities. please note that any position posted in canada is for capital one canada, any position posted in the united kingdom is for capital one europe and any position posted in the philippines is for capital one philippines service corp. (copssc).","elkridge, md",Data Engineer,"['airflow', 'aws', 'data pipeline', 'excel', 'machine learning', 'python', 'r', 'scala', 'sql']","['airflow', 'aws', 'data pipeline', 'excel', 'machine learning', 'python', 'r', 'scala', 'sql']",
azure data engineer,attainx inc,"job title: azure data engineer location: remote clearance: us citizen w/ active secret clearance we are seeking an experienced azure data engineer with proven ability to evaluate, optimize, and modernize enterprise data warehouse environments. the role combines deep technical skills with practical data integration expertise to deliver secure, scalable, and future-ready solutions in alignment with the government’s evolving data/ai strategies. required skills & experience: • bachelor’s degree in computer science, information systems, data science, or related field (or equivalent experience). • 5+ years of demonstrated experience engineering azure data solutions (etl/elt, adf, synapse, data lake, sql) to optimize performance, automate pipelines, and deliver scalable, future-ready data environments aligned to organizational strategy. • required certifications: • microsoft azure data engineer associate certification. • preferred certifications: • power bi data analyst associate • ms azure solutions architect expert • itil v4 foundation • safe agile certifications (agilist/practitioner) • 5+ years in azure data engineering (adf, synapse, data lake, sql). • background in sql, data modeling, olap, and etl/elt processes. • experience building and optimizing metadata-driven etl pipelines. • support projection planning, must have at a minimum of 1-2 years of execution roadmap and 3-5 years of scalability alignment planning experience. • understanding of programming concepts (python, powershell, java, javascript, pl/sql) as relevant to the customer data environment. • familiarity with bi/reporting platforms (microsoft fabric, power bi) to integrate engineering solutions. • ability to adapt quickly in large-scale enterprise implementation environments. • must be a u.s. citizen with an active us secret clearance. preferred qualifications: • experience with data governance, metadata management, and compliance frameworks. • aws certified data analytics – specialty or google professional data engineer • knowledge of ai/ml integrations in data analytics. • familiarity with government or regulated industry data environments. soft skills: • excellent communication and collaboration skills. • ability to translate technical solutions into business value. • proactive, solution-oriented mindset with flexibility to adapt. key responsibilities: • assess the current state of the enterprise data warehouse, identifying gaps and optimization opportunities. • build and maintain etl/elt pipelines, with emphasis on metadata models and automation scripts to improve performance and ensure smoother transitions. • deliver hands-on engineering solutions across azure data factory, synapse analytics, data lake, and sql/managed instances. • prepare designs for supporting database systems and implement measures to improve performance and scalability. • collaborate with cross-functional teams to align engineering delivery with strategic objectives and roadmap execution. • maintain awareness of industry trends in bi/reporting (microsoft fabric, power bi) and apply them in support of engineering tasks. • support projection planning (must have at a minimum of 1–2 year execution roadmap; 3–5 year scalability alignment planning experience). • develop, enhance, and troubleshoot pipelines, datasets, sql queries, stored procedures, and integrated runtimes in the azure data environment data engineer. • manage connections between the data factory, sql managed instances, power bi, power apps, and external/on-prem environments. • research, engineer, and implement new technologies to enhance data factory operations, migrations, and integrations data engineer. • document all new and current configurations, implementations, and developments to ensure continuity and support. non-essential functions • perform general duty requirements and contribute to organizational objectives as needed. work location: shall perform all functional and technical tasks remotely, hybrid work environment with occasional travel for client engagement, industry events, contract negotiations or at attainx facility. reside within a commutable distance of silver spring, md to work onsite as required. about us: attainx inc. is sba certified 8(a), women owned small business (wosb), economically disadvantaged wosb (edwosb), cmmi level 3, iso 9001:2015 certified qms and silver level safe partner. for more than 12 years, attainx, inc. has delivered emergent technologies, software products, and high-quality services that meet the needs of our federal government customers. the last 4 years have shown significant company growth as we have increased our contracts portfolio and hold the “best in class” contract vehicles, gsa mas and oasis small business and 8(a) pools 1, 2 and 3. in addition, we are prime on several agency specific idiq’s and bpa’s with the national oceanic and atmospheric administration, department of energy, navy, health and human service and the defense intelligence agency. attainx is dedicated to quality and best practices for the services we provide. we understand our people are the key ingredient to ensuring our customers mission and goals are met with excellence. benefits: we are proud to offer competitive compensation and benefits packages to include paid vacation, medical, dental, vision, matching 401k plan, tuition/training reimbursement, and long & short-term disability. eeo commitment: attainx is an equal employment opportunity employer, we are committed to providing a workplace that is free from discrimination based on title vii of the civil rights act, vevraa and section 503, or other status protected by applicable federal, state, local, or international law. these protections also extend to applicants. accommodation: if you are an individual with a disability and would like to request a reasonable workplace accommodation, please send an email to human resources. indicate the specifics of the assistance needed. physical demands: sitting and working on a computer for long, continuous periods each day; effective communications by telephone, email, and face-to-face; standing, walking, and sitting; handling and feeling objects or controls; reaching; talking and hearing; lifting and/or moving up to 10 pounds; and specific vision abilities including close vision, distance vision, color vision, peripheral vision, depth perception, and the ability to adjust and focus. work environment: the noise level in the work environment is usually moderate.","salt lake city, ut",Data Engineer,"['aws', 'azure', 'data analytics', 'data lake', 'data warehouse', 'elt', 'etl', 'excel', 'java', 'power bi', 'python', 'r', 'scala', 'sql']","['aws', 'azure', 'data analytics', 'data lake', 'data warehouse', 'elt', 'etl', 'excel', 'java', 'power bi', 'python', 'r', 'scala', 'sql']",100K–140K a year
"data engineer, senior",pg&e,"requisition id# 169117 job category: information technology job level: individual contributor business unit: information technology work type: hybrid job location: oakland department overview information systems technology services is a unified organization comprised of various departments which collaborate effectively in order to deliver high quality technology solutions. position summary the data analytics and insights team is seeking an experienced and talented senior data engineer to join our growing team of analytics experts. as a key member of our team, you will play an essential role in the design, development, and maintenance of data pipelines, analytic products, which includes data applications, reports, and dashboards. we are looking for a proactive, detail-oriented, and motivated individual who can thrive in a fast-paced environment and help us scale our analytic product development to meet our clients' ever-evolving needs. the data engineer will collaborate with our cross functional team including solution architects, data pipeline engineers, data analysts, and data scientists on mission critical initiatives and will ensure optimal delivery of analytic products. you will have a unique opportunity to be at the forefront of the utility industry and gain a comprehensive view of the nation's most advanced smart grid. it is the perfect role for someone who would like to continue to build upon their professional experience and help advance pg&e's sustainability goals. this position is hybrid, working from your remote office and oakland, ca based on business needs. pg&e is providing the salary range that can reasonably be expected for this position at the time of the job posting. this salary range is specific to the locality of the job. the actual salary paid to an individual will be based on multiple factors, including, but not limited to, internal equity, specific skills, education, licenses or certifications, experience, market value, and geographic location.the decision will be made on a case-by-case basis related to these factors.this job is also eligible to participate in pg&e's discretionary incentive compensation programs. bay area: $122,000 - 173,800 job responsibilities • work closely with subject matter experts (smes) to design and develop data model, data pipelines and front-end applications. • implement data transformations to derive new datasets or create ontology objects necessary for business applications. • design and optimize data workflows within palantir foundry, including ontology modeling, pipeline orchestration, and data lineage tracking. • monitor and debug critical issues such as data staleness or data quality. • perform impact analysis for ontology or schema changes. • improve on performance of data pipelines (latency, resource usage) • implement data visualizations using foundry tools (workshop, quiver and contour). • maintain applications as usage grows and requirements change. • available for 7x24 operational support. qualifications minimum: • bachelors degree in computer science or job-related discipline or equivalent experience • 5 years experience with data engineering/etl ecosystems palantir foundry, informatica, sap bods, obiee or related desired: • knowledge with commercial visualization tools such as tableau or power bi. • databases - familiarity with common relational database models and proprietary instantiations, such as sap, salesforce etc. • strong foundation in data modeling, schema design and data quality best practices, with functional experience working on cloud. • experience working with cloud technologies (aws, azure, or gcp) for data storage, compute, and integration. • git - knowledge of version control / collaboration workflows and best practices. • agile - familiarity with agile and iterative working methodology and rapid user feedback gathering concepts including working on tools like jira. • excellent communication skills and ability to work cross-functionally with technical and non-technical partners. • knowledge of snowflake for data warehousing, including query optimization and cost management. • familiarity with etl tools like informatica (idmc) for integration workflows. • ux design - knowledge of best practices and applications. • data literacy - data analysis and statistical basics to ensure correctness in data aggregation and visualization.","oakland, ca",Data Engineer,"['aws', 'azure', 'cloud', 'dashboard', 'data analysis', 'data analytics', 'data pipeline', 'etl', 'excel', 'gcp', 'power bi', 'r', 'snowflake', 'tableau']","['aws', 'azure', 'cloud', 'dashboard', 'data analysis', 'data analytics', 'data pipeline', 'etl', 'excel', 'gcp', 'power bi', 'r', 'snowflake', 'tableau']",122K–174K a year
principal data and analytics engineer,o'reilly auto parts,"the principal data and analytics engineer holds comprehensive responsibility for the design, implementation, and continuous evolution of the organization's enterprise-wide data infrastructure and analytics capabilities. this is an onsite position located in springfield, mo. this role provides overarching technical vision, establishing architectural standards, and driving the long-term data strategy to facilitate critical business outcomes. operating with a high degree of autonomy, this role influences executive leadership on data innovation, provides thought leadership and mentorship across the entire data and analytics engineering discipline, and champions data engineering maturity, innovation, scalability, security, and governance for all data assets. they are instrumental in translating the most complex and ambiguous business challenges into innovative, high-impact data solutions that fundamentally shape the organization's future. responsibilities and duties: • help define and evolve enterprise data engineering blueprints, including data mesh, medallion architecture, and hybrid cloud data platforms. • set strategic direction for data platforms, tools, and services (e.g., snowflake, gcp bigquery, dbt, kafka, airflow/prefect) in alignment with future-state architecture and business priorities. • architect and design highly scalable, resilient, cost optimal and secure data platforms. • lead the design and implementation of next-generation data platforms, ensuring fault tolerance, high availability, and optimal performance for petabyte-scale data. • establish and enforce organization-wide best practices for data pipeline development, ci/cd for data workflows, automated deployment playbooks, and robust rollback strategies. • lead technology evaluation and adoption, proactively researching, evaluating, and championing the integration of cutting-edge data technologies, frameworks, and methodologies. • define and scale enterprise knowledge management frameworks that ensure consistent documentation, discoverability, and reusability of data assets across domains. • establish and govern standards for metadata management, data lineage, architectural diagrams, and runbooks. • lead the design of federated governance models that empower domain-aligned teams to operate autonomously while conforming to centralized policies, frameworks and playbooks. • collaborate with data governance, compliance, and security teams to operationalize policy-as-code frameworks for data retention, access control, and pii handling. • advocate for and enable self-service knowledge discovery through tightly integrated cataloging tools (e.g., alation, collibra) and automated documentation generators. • ensure robust documentation and versioning standards are embedded in ci/cd workflows for pipeline code, transformation logic, and schema changes. • architect implementation of scalable, automated data quality frameworks that evaluate data at rest and in motion spanning completeness, timeliness, consistency, accuracy, and integrity. • lead integration of data quality rules, metrics, and health indicators directly into orchestration layers (e.g., prefect, airflow) and transformation frameworks (e.g., dbt). • evangelize a culture of data trust and transparency by integrating data quality insights into user-facing dashboards, alerts, and product health reports. • identify and promote enterprise-wide data opportunities through thought leadership, white papers, reference architectures, and innovation labs. • act as technical advisor to senior executives on data modernization, ai readiness, and platform consolidation strategies. • enable intelligent operations and decisioning by translating unstructured business logic into structured knowledge artifacts, such as kpis, rulesets, feature stores, and semantic models used by dashboards or ai agents. • serve as a strategic translator between complex business challenges and modern data architecture by leading domain-level and cross-domain data product strategy engagements. • lead the design of enterprise-grade data products that align with okrs, business transformation goals, and operational needs ensuring value realization across functional areas like supply chain, marketing, store ops, or customer satisfaction. • architect and operationalize a unified enterprise-wide semantic layer, metrics store, and business logic abstraction that powers dashboards, self-service analytics, and machine-readable apis. • lead initiatives to unify kpis, standardize metric definitions, and streamline business logic through reusable models. • design composable data assets and feature stores that enable real-time and offline access patterns for ml models, ai agents, and decision orchestration systems. • lead readiness initiatives for integrating data systems with llm-powered agents and copilots, ensuring robust grounding data, latency optimization, and lineage tracking. • drive innovation in analytics automation, including anomaly detection, agent-triggered insights, • serve as champion for complex analytics transformations, ensuring technical feasibility, business value realization, and adoption. • drive culture change around data stewardship and accountability by embedding governance responsibilities into platform tooling and engineering workflows. • lead internal communities of practice, workshops, and code reviews to disseminate modern data practices. • mentor senior engineers across data and analytics engineering, elevating technical acumen and architectural judgment. • influence hiring and team design decisions, supporting the scaling of high-performing, and collaborative data teams. • represent the organization in external forums (conferences, meetups, technical alliances) and establish credibility as an industry thought leader. required: • proven experience architecting enterprise-scale data platforms and ecosystems, including hybrid and cloud-native environments (e.g., gcp bigquery, snowflake, iceberg, advanced sql, erwin, dbt, kafka, alation, collibra) • deep expertise in designing and scaling highly available, secure, and fault-tolerant batch and streaming pipelines with strong emphasis on cost optimization, observability, and latency control. • advanced proficiency in semantic modeling, reusable data asset design, and cross-functional data product delivery aligned to medallion architecture. • leadership in implementing ci/cd-enabled pipelines, rbac frameworks, schema evolution strategies, and interoperable data exchange using iceberg or equivalent table formats. • ownership of organization-wide metrics store and semantic layers, ensuring consistency, governance, and performance across reporting, ai, and ml use cases. • advanced expertise in programming languages such as python, scala, with the ability to architect complex data solutions. • demonstrated leadership in designing and overseeing the implementation of scalable, idempotent workflows using orchestration frameworks such as airflow and prefect. • demonstrated ability to translate business transformation goals into scalable data solutions and reusable patterns. • deep understanding of business processes, kpis, and capability maps across functions such as supply chain, customer, store ops, and finance. • proven experience in driving cross-functional data product prioritization, influencing senior stakeholders, and quantifying impact of data initiatives. • experience shaping enterprise-wide data strategy by defining the long-term technical vision and architectural evolution roadmap across platforms, domains, and business units driving adoption of scalable, and governed data products. • experience leading platform modernization, tool evaluation, and architecture standardization across business domains. • expert competency in analytical and problem-solving that is crucial for identifying and resolving issues. • expertise in defining and enforcing enterprise-level data governance, metadata standards, and policy-as-code frameworks. • led the design and deployment of automated data quality management systems across ingestion, transformation, and consumption layers. • drive strategic kpi standardization by partnering with stakeholders, data stewards, and product teams to architect reusable semantic layers and metric definitions that enable trustworthy insights and llm agent reasoning. preferred: • proven success enabling iceberg-based, multi-cloud, interoperable data platforms with robust metadata, access control, and lineage frameworks. • experience integrating testing and validation frameworks into ci/cd workflows for dbt transformations, pipeline observability, and ml feature testing. • experience preparing enterprise data platforms for trustworthy insights, ai agentic readiness, including semantic alignment, real-time feature pipelines, and explainability. • communication and presentation skills with the ability to convey complex architectural decisions to both technical and non-technical stakeholders. o’reilly auto parts has a proven track record of growth and stability. o’reilly is full of successful career stories and believes in a strong promote-from-within philosophy, encouraging you to grow your career along with the organization. total compensation package: • competitive wages & paid time off • stock purchase plan & 401k with employer contributions starting day one • medical, dental, & vision insurance with optional flexible spending account (fsa) • team member health/wellbeing programs • tuition educational assistance programs • opportunities for career growth o’reilly auto parts is an equal opportunity employer. the company does not discriminate on the basis of race, religion, color, national origin or ancestry (including immigration status or citizenship), sex, sexual orientation, gender identity, pregnancy (including childbirth, lactation, and related medical conditions,) age (40 and over), veteran status, uniformed service member status, physical or mental disability, genetic information (including testing or characteristics) or another protected status as defined by local, state, or federal law, as applicable. qualified individuals with a disability may be entitled to reasonable accommodation under the americans with disabilities act. if you require a reasonable accommodation during the application or employment process, please send an email to: rar@oreillyauto.com or call (800) 471-7431 option , and provide your requested accommodation, and position details.","springfield, mo",Data Engineer,"['airflow', 'bigquery', 'cloud', 'dashboard', 'data pipeline', 'dbt', 'gcp', 'kafka', 'python', 'r', 'scala', 'snowflake', 'sql']","['airflow', 'bigquery', 'cloud', 'dashboard', 'data pipeline', 'dbt', 'gcp', 'kafka', 'python', 'r', 'scala', 'snowflake', 'sql']",
data engineer,tricom technical services,"data engineer summary the data engineer will support the design, development, and optimization of a modern data ecosystem including on-premise and cloud platforms. this role will build and improve data pipelines, maintain reliable data warehouse structures, and help shape architectural standards for scalable analytics. this data engineer will partner closely with technology and business teams to deliver high-quality data that supports reporting, insights, and strategic decisions. responsibilities • build and maintain etl pipelines that support data warehousing, analytics, and reporting. • develop scalable data solutions using cloud data platforms and modern engineering practices. • optimize sql queries, data models, and cloud warehouse performance. • support data migrations including moving on-premise data sources to cloud platforms. • collaborate with analytics teams to deliver accurate, reliable datasets and bi outputs. • apply dimensional modeling and data warehousing best practices. • use scripting languages to automate processes and streamline data operations. • assist in defining development standards, architectural patterns, and data engineering roadmap. requirements • two or more years of data engineering experience. • strong sql skills with experience in platforms including sql server or oracle. • experience working with cloud data environments (snowflake, databricks, or similar). • proficiency in python or other scripting languages. • background using bi and reporting tools (sigma, power bi, tableau, alteryx, or similar). • solid understanding of data warehousing concepts, dimensional modeling, and data lake architecture. • familiarity with data integration tools, orchestration frameworks, and automation workflows. this is a 6-month contract-to-hire opportunity with our topeka, ks client. employee benefits include medical/dental benefits, paid time off, paid holidays, and 401(k). h1-b visa sponsorship is not available for this position. no third-parties, please.","shawnee, ks",Data Engineer,"['cloud', 'data lake', 'data pipeline', 'data warehouse', 'databricks', 'etl', 'power bi', 'python', 'r', 'scala', 'snowflake', 'sql', 'sql server', 'tableau']","['cloud', 'data lake', 'data pipeline', 'data warehouse', 'databricks', 'etl', 'power bi', 'python', 'r', 'scala', 'snowflake', 'sql', 'sql server', 'tableau']",
"data engineer, ground network engineering (gateway)",spacex,"about the position responsibilities • own end-to-end workflow deployment and optimization of starlink ground network processes • develop and automate systems to optimize antenna, optical hardware, and critical spare part flow through inventory and logistics accounting for inventory stock levels, telemetry data, and predictive failure models • build and maintain forecasting engines to predict hardware demand from deployment planning, team and program targets • architect backend services, apis, and workflows to integrate planning, logistics, and inventory systems (warpdrive erp/mrp, 3rd party logistics service provider's apis) • automate supply chain processes, including inventory syncs, order creation, kitting, shipment validation, and spares replenishment • refactor and scale data ingestion and delivery systems to handle increased deployment volume and geographic expansion • create tools and dashboards to track real-time metrics on shipments, inventory, deployment timelines, and reliability • identify and resolve inefficiencies in material flow and logistics, developing standardized processes and key performance indicators • develop and monitor performance metrics for build, deployment, and ongoing maintenance/reliability of the ground network hardware fleet (antennas, network/optical devices) • collaborate with cross-functional teams to translate engineering needs into system requirements and drive adoption • support emergency deployments and incident response by managing critical operational and data workflows requirements • bachelor's degree in engineering, computer science, or related field • experience in python or sql nice-to-haves • experience with inventory systems, logistics workflows, and/or supply chain tooling • proficient in c#, c++, or angular • basic understanding of kubernetes concepts, including pods, services, deployments, and namespaces • prior success in delivering strong data visualizations using tools such as powerbi, grafana, metabase or other real-time metrics systems • experience building backend services using apis (celery, fastapi, django, or similar frameworks) and system-to-system data exchange in logistics and supply chain environments • proven track record developing custom forecasting tools and planning logic • knowledge of mrp/erp, wms, or other supply chain systems • strong problem-solving skills and comfort working in ambiguous environments with evolving requirements • excellent communication and cross-functional program management skills benefits • pay range: gateway site develop engineer/level i: \$122,500.00 - \$145,000.00/per year • gateway site develop engineer/level ii: \$140,000.00 - \$170,000.00/per year • long-term incentives, in the form of company stock, stock options, or long-term cash awards • potential discretionary bonuses • ability to purchase additional stock at a discount through an employee stock purchase plan • comprehensive medical, vision, and dental coverage • access to a 401(k)-retirement plan • short and long-term disability insurance • life insurance • paid parental leave • various other discounts and perks • 3 weeks of paid vacation • 10 or more paid holidays per year • 5 days of sick leave per year • company shuttles offered for round trip travel from select seattle locations to the spacex redmond office monday to friday",anywhere,Data Engineer,"['c#', 'c++', 'dashboard', 'excel', 'python', 'r', 'sql']","['c#', 'c++', 'dashboard', 'excel', 'python', 'r', 'sql']",
"staff product manager, data engineering",liftoff,"liftoff is a leading ai-powered performance marketing platform for the mobile app economy. our end-to-end technology stack helps app marketers acquire and retain high-value users, while enabling publishers to maximize revenue across programmatic and direct demand. liftoff’s solutions, including accelerate, direct, monetize, intelligence, and vungle exchange, support over 6,600 mobile businesses across 74 countries in sectors such as gaming, social, finance, ecommerce, and entertainment. founded in 2012 and headquartered in redwood city, ca, liftoff has a diverse, global presence. liftoff is seeking a senior product manager, data engineering to lead the roadmap for our data infrastructure and engineering systems that power machine learning and analytics across the organization. this is a highly technical product role requiring a background as a data engineer and a strong ability to communicate across both technical and business teams. the ideal candidate is proactive, technically fluent, and an excellent communicator who can translate complex requirements from analysts and ml engineers into structured, actionable engineering specifications. this position partners closely with data engineering, machine learning, data science, and analytics teams across both u.s. and beijing time zones, requiring strong coordination, documentation, and clarity in asynchronous communication. key responsibilities: • define and execute the product vision and roadmap for liftoff’s data infrastructure, ensuring scalability, reliability, and performance. • translate complex machine learning and analytics requirements into detailed technical documentation and product specifications for engineering teams. • partner with cross-functional teams across the u.s. and beijing to design, prioritize, and deliver high-impact data pipelines, apis, and storage architectures. • establish and monitor data quality, latency, and reliability kpis, driving continuous improvement and standardization. • collaborate with ml and analytics stakeholders to ensure the data platform enables rapid experimentation and model development. • anticipate data and infrastructure needs, proactively shaping architecture and tooling strategy. • advocate for data best practices, including governance, lineage, and documentation across global teams. requirements • 5–8 years of total experience in data-intensive environments, including: • minimum 2 years as a data engineer, building or maintaining large-scale pipelines and distributed data systems. • minimum 3 years in product management or technical program management, owning data or infrastructure products. • deep understanding of data engineering concepts: etl frameworks, data modeling, distributed processing, and streaming systems (e.g., spark, airflow, kafka, flink). • strong familiarity with cloud data platforms (snowflake, bigquery, redshift) and data lake architectures. • proficient in sql; comfortable understanding python or similar scripting languages for technical discussions. • demonstrated ability to write clear technical documentation and product specifications. • excellent verbal and written communication, capable of bridging technical and non-technical stakeholders across global teams. • highly proactive and self-directed, able to manage complex cross-time-zone projects with minimal supervision. preferred experience: • background in ad tech, mobile marketing, or large-scale programmatic data systems. • experience with model training pipelines, feature stores, or data observability tooling. • knowledge of data governance, schema evolution, and metadata management practices. • experience collaborating with distributed teams across u.s. and asia time zones. why join liftoff? • innovative environment: be part of a company at the forefront of mobile app marketing, utilizing cutting-edge technologies to drive results.​ • professional growth: opportunities for continuous learning and career advancement in a dynamic industry.​ • collaborative culture: work with a diverse team of talented professionals who are passionate about delivering excellence. • comprehensive benefits: competitive salary, health benefits, and other perks to support your well-being. working at liftoff is fast-paced, fun, and challenging, and we thrive on innovation. come join the rocket ship and help shape the future of the mobile app ecosystem with us! location: this role is eligible for full-time remote work in california or near one of our us hubs in redwood city or los angeles. this position is located in the pacific time zone. travel expectations: we offer several opportunities for in-person team gatherings, including but not limited to project meetings, regional meetups, and company-wide events. we expect our employees to attend these gatherings at least once per quarter. these gatherings provide essential opportunities for collaboration, communication, and team building. this position will require travel at least once per quarter to work with the beijing team. compensation: liftoff offers all employees a full compensation package that includes equity and health/vision/dental benefits associated with your country of residence. base compensation will vary based on candidate's location and experience. the following are our base salary ranges for this role: • sf bay area/los angeles/orange county/seattle: $210,000 to $235,000 • all other california and washington state locations: $193,200 to $216,200 • all other locations in our approved states: $180,600 to $202,100 #li-vm1 #li-remote we use covey as part of our hiring and / or promotional process for jobs in nyc and certain features may qualify it as an aedt. as part of the evaluation process we provide covey with job requirements and candidate submitted applications. we began using covey scout for inbound on january 22, 2024. please see the independent bias audit report covering our use of covey here. liftoff offers a fast-paced, collaborative, and innovative work environment where employees are empowered to grow and make an impact. we’re shaping the future of the mobile app ecosystem—join us and help accelerate what’s next. liftoff’s compensation strategy includes competitive salaries, equity, and benefits designed to support employee well-being and performance. we benchmark compensation based on role, level, and location to ensure fairness and market alignment. benefits may include medical coverage, wellness stipends, and additional perks based on your country of residence. liftoff is an equal opportunity employer. we are committed to creating an inclusive environment for all employees and applicants regardless of race, ethnicity, national origin, age, marital status, disability, sexual orientation, gender identity, religion, veteran status, or any other characteristic protected by applicable law. agency and third party recruiter notice: liftoff does not accept unsolicited resumes from individual recruiters or third-party recruiting agencies in response to job postings. no fee will be paid to third parties who submit unsolicited candidates directly to our hiring managers or recruiting team. all candidates must be submitted via our applicant tracking system by approved liftoff vendors who have been expressly requested to make a submission by our recruiting team for a specific job opening. no placement fees will be paid to any firm unless such a request has been made by the liftoff recruiting team and such a candidate was submitted to the liftoff recruiting team via our applicant tracking system.","redwood city, ca",Data Engineer,"['airflow', 'bigquery', 'cloud', 'data lake', 'data pipeline', 'etl', 'excel', 'experimentation', 'kafka', 'machine learning', 'python', 'r', 'redshift', 'scala', 'snowflake', 'spark', 'sql']","['airflow', 'bigquery', 'cloud', 'data lake', 'data pipeline', 'etl', 'excel', 'experimentation', 'kafka', 'machine learning', 'python', 'r', 'redshift', 'scala', 'snowflake', 'spark', 'sql']",181K–235K a year
"data engineer - hybrid/bellevue, wa",brook inc,"description about brook health brook health delivers care beyond the walls of the doctor’s office. brook provides people living with chronic conditions a highly personalized experience enhanced by ai and powered by mobile apps, connected devices, and a team of health coaches and clinicians. we help people achieve their long-term health goals by supporting smart, daily decisions and partnering with their primary care physicians. our product suite includes continuous remote monitoring, population health management tools, and a cdc-approved diabetes prevention program. brook has an intentional, user-centric culture with high expectations for delivering better health outcomes for patients, providers, and health systems. job overview the data engineer is responsible for designing, building, and maintaining brook’s data pipelines, warehouses, and architectures. this role transforms raw data into trusted, consumable formats that power analytics, dashboards, ai systems, and operational decision-making across the organization. you will work closely with product, engineering, and analytics teams to ensure our data infrastructure is secure, scalable, and optimized for performance—enabling our mission to deliver better health outcomes through data-driven insights. this role is based in bellevue, wa (hybrid), and some travel may be required. all members of the engineering team are based in the pacific time zone to support real-time collaboration and agile development. requirements key responsibilities • data pipeline development: build, maintain, and optimize etl/elt workflows (batch and real-time) using python or java and orchestration tools such as airflow or prefect. • data warehouse & modeling: design and evolve schemas in snowflake (or similar) to ensure efficient storage, query performance, and maintainability. • dbt modeling & quality: develop and maintain dbt models and tests to standardize data transformations and enforce data quality. • infrastructure & scalability: plan and monitor capacity, and performance-tune data stores (postgresql, mongodb, mysql). • architecture & solution design: partner with cross-functional teams to design scalable solutions and document data architecture standards. • data quality & governance: implement validation, reconciliation, and lineage tracking to ensure data integrity, security, and compliance with hipaa and soc 2. • cross-functional collaboration: work with analysts, product managers, and engineers to translate business needs into technical solutions. • monitoring & troubleshooting: diagnose and resolve data-related issues to maintain high availability and reliability. • continuous improvement: champion best practices for ci/cd, version control, and agile/scrum data delivery. knowledge, skills, & abilities • advanced proficiency in python and/or java for data pipeline development. • deep expertise in sql and relational database design (postgresql). • strong understanding of data warehousing principles, etl/elt patterns, and performance tuning. • hands-on experience with snowflake (or equivalent cloud data warehouse), mongodb, and aws infrastructure. • excellent communication skills—able to explain technical concepts to non-technical stakeholders. • collaborative, analytical, and detail-oriented problem solver. • passion for building scalable data systems that drive patient care and business outcomes. preferred experience • minimum 5 years of hands-on data engineering experience. • bachelor’s degree in computer science, engineering, or related field (master’s a plus). • experience in healthcare, digital health, or regulated environments. familiarity with dbt, airflow, or prefect for orchestration and transformation. • exposure to streaming technologies (kafka, kinesis) and message queues (redis). understanding of healthcare data standards such as fhir. • certifications in data engineering or cloud platforms (aws, gcp) are a plus. working at brook • fast-paced environment – brook operates in two of the fastest changing industries in america – healthcare and technology. we move quickly to design tools and protocols based on customer and industry feedback. thriving in an environment of change and continuous improvement is a core competency for all members of our team. • dynamic roles - we are a small and tight-knit team enthusiastically tackling difficult problems in an entrenched industry. all team members are expected to contribute to company protocols, provide product feedback and to generally think critically about our processes and care model. • high expectations - we have big goals for the future. we expect dedication and positive collaboration from all our team to achieve them. this position is not eligible for relocation or visa sponsorship. candidates must live within a commuting distance from the office. this is a hybrid role, onsite in the office required weekly along with remote work. brook is as focused on our employees’ health as we are on that of our patients. our benefits program reflects that. we recognize that health does not just mean physical health, but mental and financial health as well. we make every effort to cover all those areas in our plan offerings. benefits at brook health in addition to meaningful work in a mission-driven company, brook offers a comprehensive benefits package designed to support the medical, financial and mental health wellbeing of our employees and their families. healthcare coverage • employee & child(ren): brook pays 100% of premiums for full-time employees and their child or children for medical, dental, and vision coverage. this means there are no paycheck deductions for you or your child(ren). • spouse/domestic partner: brook contributes 50% of premiums for coverage of a spouse or domestic partner. • hsa contribution: employees who enroll in our hsa-eligible medical plan receive a brook-funded contribution to help cover medical expenses such as deductibles, prescriptions, and office visits. • medical concierge: brook provides a concierge service to help employees and their families manage healthcare needs like claims, referrals, and care coordination. mental health & wellbeing • mental health support: brook supplements the mental health coverage included in our medical plan with additional resources. employees have access to free therapy sessions through spring health, providing confidential, professional support when it’s needed most. • flexible pto: our pto program is truly flexible — no accruals and no preset limits. you and your manager decide what’s reasonable, so you can take the time you need to recharge. in addition, we provide dedicated sick time to support your health and well-being, and a generous holiday schedule that ensures time to rest and celebrate with family and friends. financial wellness & security • income protection: brook provides short-term and long-term disability insurance to all full-time employees, helping replace income during an illness or injury. short-term disability works alongside any state or insurance benefits and brook coverage to provide added financial support while you’re away from work. • life insurance: brook provides company-paid life insurance equal to one times salary, up to a set maximum. • 401(k) retirement savings: all employees (full-time and part-time) are automatically enrolled in our 401(k) plan. brook provides a company match to help employees grow their retirement savings. • emergency savings account (esa): brook helps employees build financial resilience by supporting contributions to an emergency savings account. brook matches a portion of employee contributions, helping the fund grow faster. the account is completely flexible — you decide what qualifies as an emergency and how to use the funds. recognition & community • employee referral bonus: great people know great people. when you refer a candidate who is hired, you’ll receive a referral bonus. brook inc is an equal opportunity employer. we are committed to building an inclusive and diverse workforce. brook does not discriminate on the basis of race, religion, color, sex, gender identity, sexual orientation, marital status, age, non-disqualifying physical or mental disability, national origin or ethnic origin, military service status, citizenship or any other protected characteristic covered by appropriate law. all employment is decided on the basis of qualifications, merit, and business need.","bellevue, wa",Data Engineer,"['airflow', 'aws', 'cloud', 'dashboard', 'data pipeline', 'data warehouse', 'dbt', 'elt', 'etl', 'excel', 'gcp', 'java', 'kafka', 'python', 'r', 'scala', 'snowflake', 'sql']","['airflow', 'aws', 'cloud', 'dashboard', 'data pipeline', 'data warehouse', 'dbt', 'elt', 'etl', 'excel', 'gcp', 'java', 'kafka', 'python', 'r', 'scala', 'snowflake', 'sql']",130K–160K a year
lead data engineer - data control,first citizens bank,"overview this is a remote role that may only be hired in the following locations: nc, wa we are seeking a highly skilled and experienced lead data engineer to lead and drive data governance, control frameworks, and advanced reconciliation processes. the ideal candidate will have a strong technical background in data engineering, regulatory compliance, and automation, with expertise in modern data platforms, tools, and frameworks. this role requires a lead with hands-on experience in designing and implement robust data control solutions while ensuring compliance with regulatory standards such as 2052a, fr370, ccpa, and gdpr. responsibilities • ensure adherence to regulatory requirements, including 2052a, fr370, ccpa, and gdpr. • build and maintain data privacy control frameworks to safeguard sensitive information. • design and implement data governance & control frameworks to ensure data integrity, security, and compliance. • design and implement data control frameworks for advanced reconciliation processes, data movement controls, and data quality checks. • establish and enforce data lineage and metadata management practices to enhance traceability and transparency and create preventive automation activities for support. • hands-on experience in designing, implementing, and maintaining data platform that supports data integrations for enterprise data warehouse, operational data store or data marts etc. with appropriate data access, data security, data privacy and data governance. • establish enterprise-scale data integration procedures, data pipelines and frameworks across the data development life cycle. suggest and implement appropriate technologies to deliver resilient, scalable, and future-proof data solutions. • create data ingestion pipelines in data warehouses and other large-scale data platforms. • develop apis and automation scripts to streamline data processes and enhance operational efficiency. • optimize data storage, processing, and retrieval systems for performance and scalability. • collaborate with stakeholders to deliver actionable insights through intuitive dashboards and reports. • mentor and guide cross-functional teams in data governance and engineering best practices. • collaborate with business and technical stakeholders to align data strategies with organizational goals. • ensure adherence to regulatory requirements, including 2052a, fr370, ccpa, and gdpr. • build and maintain data privacy control frameworks to safeguard sensitive information. • design and implement data governance & control frameworks to ensure data integrity, security, and compliance. • design and implement data control frameworks for advanced reconciliation processes, data movement controls, and data quality checks. • establish and enforce data lineage and metadata management practices to enhance traceability and transparency and create preventive automation activities for support. • hands-on experience in designing, implementing, and maintaining data platform that supports data integrations for enterprise data warehouse, operational data store or data marts etc. with appropriate data access, data security, data privacy and data governance. • establish enterprise-scale data integration procedures, data pipelines and frameworks across the data development life cycle. suggest and implement appropriate technologies to deliver resilient, scalable, and future-proof data solutions. • create data ingestion pipelines in data warehouses and other large-scale data platforms. • develop apis and automation scripts to streamline data processes and enhance operational efficiency. • optimize data storage, processing, and retrieval systems for performance and scalability. • collaborate with stakeholders to deliver actionable insights through intuitive dashboards and reports. • mentor and guide cross-functional teams in data governance and engineering best practices. • collaborate with business and technical stakeholders to align data strategies with organizational goals. qualifications bachelor's degree and 6 years of experience in advanced data engineering, enterprise architecture, project leadership or high school diploma or ged and 10 years of experience in advanced data engineering, enterprise architecture, project leadership preferred functional skills: • team player: support peers, team, and department management. • communication: excellent verbal, written, and interpersonal communication skills. • problem solving: excellent problem-solving skills, incident management, root cause analysis, and proactive solutions to improve quality. • partnership and collaboration: develop and maintain partnership with business and it stakeholders • attention to detail: ensure accuracy and thoroughness in all tasks. technical/ business skills: • proficiency in python for data engineering frameworks and automation tasks. • strong experience with advanced reconciliation frameworks and data movement controls. • automate data integration, transformation, and quality checks using tools like denodo, dbt, and qlik. • technical expertise working in large scale data warehousing applications and databases such as snowflake, oracle, postgres, netezza, teradata, and sql server. • expertise in collibra data lineage, metadata management, and data quality control frameworks for regulatory and compliance. • semantic data modelling experience and advanced reporting solution using power bi, tableau & qlik and splunk for monitoring • setting up role-based access controls (rbac) in snowflake • experience with public cloud-based data platforms especially aws. • experience in data control, observability frameworks, tools/ technologies • experience working in financial industry • strong leadership and communication skills to influence and guide teams. • knowledge of modern data architecture and cloud platforms. this role is ideal for a lead data engineer who thrives in a dynamic, fast-paced environment and is passionate about building robust, compliant, and scalable data solutions. if hired in wa, the base pay for this position is generally between $125000 and $165000. if actual starting base pay will be determined based on skills, experience, location, and other non-discriminatory factors permitted by law. for some roles, total compensation may also include variable incentives, bonuses, benefits, and/or other awards as outlined in the offer of employment. benefits are an integral part of total rewards and first citizens bank is committed to providing a competitive, thoughtfully designed and quality benefits program to meet the needs of our associates. more information can be found at https://jobs.firstcitizens.com/benefits.","seattle, wa",Data Engineer,"['aws', 'cloud', 'dashboard', 'data pipeline', 'data warehouse', 'dbt', 'excel', 'power bi', 'python', 'r', 'scala', 'snowflake', 'sql', 'sql server', 'tableau']","['aws', 'cloud', 'dashboard', 'data pipeline', 'data warehouse', 'dbt', 'excel', 'power bi', 'python', 'r', 'scala', 'snowflake', 'sql', 'sql server', 'tableau']",
"software engineering manager - data, burger king",restaurant brands international,"ready to make your next big professional move? join us on our journey to achieve our big dream of building the most loved restaurant brands in the world. restaurant brands international inc. is one of the world's largest quick service restaurant companies with nearly $45 billion in annual system-wide sales and over 32,000 restaurants in more than 120 countries and territories. rbi owns four of the world's most prominent and iconic quick service restaurant brands – tim hortons®, burger king®, popeyes®, and firehouse subs®. these independently operated brands have been serving their respective guests, franchisees and communities for decades. through its restaurant brands for good framework, rbi is improving sustainable outcomes related to its food, the planet, and people and communities. rbi is committed to growing the tim hortons®, burger king®, popeyes® and firehouse subs® brands by leveraging their respective core values, employee and franchisee relationships, and long track records of community support. each brand benefits from the global scale and shared best practices that come from ownership by restaurant brands international inc. the data engineering manager is responsible for leading a team of data engineers to design, build, and maintain robust data pipelines and infrastructure for the burger king company in the us. this role oversees the development and optimization of scalable data solutions that handle millions of transactions per day, enabling actionable insights and supporting strategic business initiatives. the ideal candidate will have extensive technical expertise, leadership experience, and a passion for innovation in data engineering. this position is based in miami, fl and is in the office 5 days a week. role & responsibilities: • lead and mentor a team of data engineers, fostering professional growth and knowledge sharing. • oversee day-to-day activities of the data engineering team, ensuring timely delivery of projects. • collaborate with cross-functional teams, including data scientists, analysts, product managers, and it, to develop and implement data solutions that meet business needs. • design, build, and maintain scalable and efficient data pipelines and etl processes. • implement best practices for data integration, modeling, quality, and governance. • ensure data reliability, integrity, and availability for critical analytics workflows. • drive the development of the overall data engineering strategy and optimize processes to ensure efficiency and scalability. • plan and execute data engineering projects, ensuring alignment with organizational objectives. • gather and document requirements for projects, create data migration plans, and communicate progress with stakeholders. • manage relationships with technology vendors and oversee service-level agreements. • continuously evaluate emerging technologies and methodologies to improve data engineering capabilities. • encourage a culture of innovation and continuous improvement within the team. qualifications & skills: • 5+ years of progressive experience in data engineering, • 2+ years leading people and engineering teams. • bachelor’s or master’s degree in computer science, engineering, information technology, or a related field. • strong programming skills in python and sql. • extensive experience with aws services (e.g., s3, glue, athena, emr, sns, sqs, kms). • experience with data warehousing solutions like snowflake or redshift. • proficiency with workflow orchestration tools (e.g., apache airflow, dagster). • familiarity with reporting tools (e.g., tableau, microstrategy). • experience with infrastructure as code (e.g., terraform). • knowledge of nosql databases like dynamodb. • experience with retool development and ticketing systems such as jira. • demonstrated ability to lead and manage multiple complex projects in a fast-paced environment. • excellent verbal and written communication skills, with the ability to translate technical concepts for non-technical stakeholders. • strong analytical and problem-solving abilities. • ability to build and maintain cross-functional relationships while managing competing priorities. • self-motivated and driven to continuously improve processes and outcomes. #burgerking benefits at all of our global offices are focused on physical, mental and financial wellness. we offer unique and progressive benefits, including a comprehensive global paid parental leave program that supports employees as they expand their families, free telemedicine and mental wellness support. restaurant brands international and all of its affiliated companies (collectively, rbi) are equal opportunity and affirmative action employers that do not discriminate on the basis of race, national origin, religion, age, color, sex, sexual orientation, gender identity, disability, or veteran status, or any other characteristic protected by local, state, provincial or federal laws, rules, or regulations. rbi's policy applies to all terms and conditions of employment. accommodation is available for applicants with disabilities upon request.","miami, fl",Data Engineer,"['airflow', 'aws', 'data pipeline', 'etl', 'excel', 'python', 'r', 'redshift', 'scala', 'snowflake', 'sql', 'tableau']","['airflow', 'aws', 'data pipeline', 'etl', 'excel', 'python', 'r', 'redshift', 'scala', 'snowflake', 'sql', 'tableau']",
data engineer,consultnet,"data engineer (onsite) utah county (onsite) 6 month c2h or direct hire pay: $50.00-$65.00/hr job description: our client is looking for a data engineer who will play a key role in developing and optimizing the data infrastructure supporting the company brand. this position ensures the reliability of data pipelines, enabling seamless data flow and delivering actionable insights that drive strategic decisions and empower analytics teams. you'll collaborate with analysts, business leaders, and cross-functional teams to build robust, scalable, and secure data systems—shaping and executing the data strategy across both brands. responsibilities: • data collection: gather data from a variety of sources, including internal systems, apis, and streaming platforms. • data transformation: clean, structure, and prepare raw data for analysis. • data storage: design and manage scalable storage solutions, such as data warehouses. • data quality: ensure data accuracy and integrity through monitoring and validation processes. • data pipelines: develop and maintain secure, efficient pipelines using apis, sftp, and other integration methods. • data architecture: build and refine data infrastructure to meet evolving business requirements. • data testing: conduct rigorous testing to identify and resolve data issues and system errors. • data visualization: collaborate with the analytics team to develop dashboards, reports, and visualizations that drive data-driven insights. required skills: • 5+ years working with python, sql, json, and data visualization/exploration tools. • proven expertise in building and maintaining robust etl pipelines. • strong hands-on experience with snowflake and aws services. technical & analytical skills: • skilled in interpreting and solving complex problems with minimal standardization. • proficient in basic mathematics, including percentages, ratios, and graph interpretation. • able to follow and adapt to various instruction formats. communication: • capable of explaining complex technical concepts to non-technical audiences. • excellent written communication skills; able to craft clear reports, memos, and documentation using correct grammar and structure. • strong verbal communication with clear articulation, correct pronunciation, and proper use of tense and emphasis. collaboration: • effective in fast-paced, research-driven environments managing multiple concurrent projects. • works well within cross-functional teams and adapts quickly to changing priorities. welcome to consultnet and the family of companies, tekne, saltclick, techbridge, and omnimedia. as a premier national provider of technology talent and solutions, our expertise spans across project services, contract-to-hire, direct placement, and managed services both onshore and nearshore. celebrating more than 25 years of partnership with a diverse client base, we've crafted rewarding opportunities for our consultants, fostering high-performing teams that deliver impactful results. over the last few years thousands of consultants have found their calling with us in roles that have made a meaningful impact on their lives, enhanced their career, challenged them, and propelled them towards achieving their personal and professional goals. at the consultnet family of companies, we believe effective communication is crucial in aligning the right job with your unique skills and professional aspirations. to us, it's all about the personal approach we take and the values we uphold. our comprehensive service offerings cover a wide range of technology positions across key markets nationwide. client more at www.consultnet.com . we champion equality and inclusivity, proudly supporting an equal opportunity employer policy. we welcome applicants regardless of race, color, religion, sex, sexual orientation, gender identity, national origin, age, genetic information, disability, protected veteran status, or any other status protected by law. about the company: consultnet","pleasant grove, ut",Data Engineer,"['aws', 'dashboard', 'data pipeline', 'data warehouse', 'etl', 'excel', 'python', 'r', 'scala', 'snowflake', 'sql']","['aws', 'dashboard', 'data pipeline', 'data warehouse', 'etl', 'excel', 'python', 'r', 'scala', 'snowflake', 'sql']",
senior data engineering manager,mogi i/o : ott/podcast/short video apps for you,"skills: azure databricks, apache spark/pyspark, azure data factory, azure functions, delta lake, adls gen2, sql, etl/elt pipelines, data lakehouse architecture, data modeling, location: alpharetta, georgia experience required: 3 – 15 years compensation: usd 82000 - 123000 visa status: us citizens, green card holder about the role we are seeking a senior/lead data engineer with 8+ years of experience building scalable, high-performance data platforms on azure & databricks. this role involves end-to-end data engineering, architecture, and leadership across modern data lakehouse environments. key responsibilities • architect and implement scalable data platforms and pipelines on azure and databricks. • build and optimize batch & real-time data ingestion and transformation (spark/pyspark). • develop robust etl/elt workflows, ensuring reliability and performance. • work with adls, delta lake, and spark for large-scale processing. • design data models (conceptual, logical, physical) for analytics and operational use. • collaborate with cross-functional teams to convert requirements into technical solutions. • troubleshoot performance issues and support production systems. • ensure best practices for data governance, security, quality, and architecture. • mentor junior engineers and contribute to engineering standards. required skills • 8+ years of enterprise data engineering experience. • strong hands-on experience with: • azure databricks, azure functions, adf • apache spark (pyspark) • sql & performance tuning • delta lake (schema evolution, acid, optimization) • expertise in etl/elt, distributed systems, and cloud-native architecture. • experience with data modelling (dimensional, normalized, lakehouse). • exposure to streaming frameworks (kafka, event hub). • strong problem-solving and technical leadership skills. preferred skills • ci/cd using azure devops, git. • iac tools (terraform/arm). • azure purview or other data governance tools. • experience supporting ml or bi workloads on databricks. benefits • medical, dental, vision insurance • paid vacation, sick leave, holidays • employer-matched 401(k) • life, ad&d, short-term & long-term disability • eligibility varies by employment type and state law","marietta, ga",Data Engineer,"['azure', 'cloud', 'data lake', 'databricks', 'elt', 'etl', 'kafka', 'pyspark', 'r', 'scala', 'spark', 'sql']","['azure', 'cloud', 'data lake', 'databricks', 'elt', 'etl', 'kafka', 'pyspark', 'r', 'scala', 'spark', 'sql']",
junior data engineer,enhance it,"job title: jr. data engineer(urgent hiring) location: lansing, mi (onsite) employment type: fulltime duration: 2 years exp: 1-2 years(entry level) educational qualification: bachelor s/master s degree in computer science, information systems, electrical engineering, mathematics, or a related quantitative field. are you excited about taking your technical career to new heights with a full-time, w-2 role as a consultant in a dynamic and rapidly growing company? if you are, let's get in touch your interest is the first step to starting the conversation. what this role requires: • 1-4 years of programming experience after your degree • must have coding experience in both python and sql • it is preferred that you have experience in at least one of the following additional languages: java, c#, c++, scala • familiarity with big data technology in cloud and on-premises environments: hadoop, hdfs, spark, nosql databases, hive, mongodb, airflow, kafka, aws, azure, dockers or snowflake • good understanding of object-oriented programming (oop) principles & concepts • familiarity with advanced sql techniques • familiarity with data visualization tools such as tableau or power bi • familiarity with apache flink or apache storm • understanding of devops practices and tools for (ci/cd) pipelines. • awareness of data security best practices and compliance requirements (e.g., gdpr, hipa). to qualify: • you should be willing to relocate anywhere in the us on a client project-to-project basis, as this is an onsite, in-office position. • strong english communication skills, both written and verbal. • bachelor s degree in computer science, information systems, electrical engineering, mathematics, or a related quantitative field. what s in it for you? • gain valuable, career-enhancing experience working with our fortune 1,000 clients. • receive relocation support for training and project assignments, as required. • enjoy comprehensive w2 employee benefits. • access full coverage medical, dental, and vision insurance. • qualify for 401k eligibility after one year of employment. • benefit from basic life/ad&d and dependent disability (short/long term) coverage. who are we? • we are a premier it consulting firm specializing in delivering top-tier data science solutions to companies across various sectors such as finance, energy, e-commerce, logistics, travel, retail, entertainment, automotive, and healthcare. our clientele includes industry giants like microsoft, google, johnson & johnson, fannie mae, walmart, paypal, t-mobile, mcdonald's, cvs, verizon, charter, nike, dell, wells fargo, capital one, and charles schwab, among many others. as a consultant, joining our team means you'll also have the opportunity to work with these renowned and leading companies and gain valuable, career-accelerating experience. company highlights: • our specialization: providing it consulting services. • experience: over 25 years of combined domestic and international expertise in it consulting serving hundreds of fortune 1,000 and innovative startup clients. interested in accelerating your tech career through career-enhancing it consulting experience? do you meet the required qualifications? if so, apply today! we look forward to hearing from you.","lansing, mi",Data Engineer,"['airflow', 'aws', 'azure', 'c#', 'c++', 'cloud', 'hadoop', 'java', 'kafka', 'power bi', 'python', 'r', 'scala', 'snowflake', 'spark', 'sql', 'tableau']","['airflow', 'aws', 'azure', 'c#', 'c++', 'cloud', 'hadoop', 'java', 'kafka', 'power bi', 'python', 'r', 'scala', 'snowflake', 'spark', 'sql', 'tableau']",30–35 an hour
clinical data engineer,xealth,this job listing in king - wa has been recently added. tallo will add a summary here for this job shortly.,washington,Data Engineer,['r'],['r'],
principal data engineer,shutterfly,"at shutterfly, we make life's experiences unforgettable. we believe there is extraordinary power in the self-expression. that's why our family of brands helps customers create products and capture moments that reflect who they uniquely are. we're seeking a principal data engineer/systems architect with 7-10 years of industry experience working in high-transaction, high-volume e-commerce environments. this role requires deep systems thinking to design, build, and fully own highly available, resilient distributed data architecture. the focus is on connecting our core data warehouse with critical marketing and customer engagement platforms using fault-tolerant pipelines that handle massive scale. with your technical expertise, you will play a critical role in bringing in architectural efficiencies, establishing system reliability, and optimizing resources to deliver cost-saving architectural changes within our martech stack. what you'll do here: • architect, design, and lead the implementation of scalable, resilient, and high-performance data pipelines using modern distributed systems. • develop and maintain etl/elt pipelines in python and sql, specifically ensuring data consistency, idempotency, and high availability under heavy load. • integrate and manage data synchronization with key marketing systems like cdps, esps, and campaign management tools. • implement bi-directional data flows that feed customer and campaign data into engagement platforms and capture performance/behavior data back into internal systems. • establish and enforce best practices for data pipeline monitoring, alerting, and automated failure recovery (i.e., observability and resiliency). • evaluate and make strategic decisions on data infrastructure components, including distributed messaging queues and workflow orchestration systems. • partner with marketing, analytics, and crm teams to translate requirements into robust technical solutions. • rapidly gain an understanding of the entire stack, operationalize it, and thoroughly document architectural decisions and system design for long-term maintainability. • ensure compliance with data governance, security, and privacy requirements. the skills you'll bring: • 7-10 years of professional experience as a data engineer, ideally at high-volume e-commerce or transactional platforms. • expert level in python (data scripting, automation, api integrations). • strong sql skills and experience with modern dw (e.g., redshift, snowflake, bigquery, etc.). • deep expertise (3+ years hands-on) with distributed message queues (e.g., apache kafka, aws kinesis, rabbitmq) for event-driven architecture and high-volume data streaming. • expert understanding of system resiliency, fault tolerance, and disaster recovery in a data pipeline context (e.g., dead letter queues, retry mechanisms, exactly-once processing). • proven experience designing and implementing complex, scalable, and cost-efficient aws cloud data architectures. • expert hands-on experience with workflow orchestration tools (e.g., airflow, dbt) and internal/third-party api integrations. • strong problem-solving skills and ability to drive technical consensus and effectively communicate complex architectural trade-offs. • bs in computer science or equivalent. location: this is a fully remote position based in us. the position requires coming onsite to the nearest hub location for first day onboarding. supporting a diverse and inclusive workforce is important to shutterfly not only because it directly reflects our value of embracing our differences, but also because it's the right thing to do for our business and for our people. we welcome all applicants and evaluate them based on their qualifications, without regard to age, race, creed, color, national origin, ancestry, marital status, affectional or sexual orientation, gender identity or expression, disability, nationality, sex, or other characteristic covered by law. learn more about our commitment to diversity, equity, and inclusion on our career site. this position will accept applications on an ongoing basis until filled. the compensation package for this role is based on multiple factors, such as job level, responsibilities, location, and candidate experience. the base pay ranges included below are specific to the locations listed, and may not be applicable to other locations. california : [$134,750-190,750] connecticut and new york: [$134,750-174,750 colorado, illinois, minnesota and washington: [$134,750-161,750] nevada: [$126,750-174,750] maryland and new jersey: [$145,500-174,750] hawaii : [$126,750-142,250] this position may be eligible for a bonus incentive, health benefits, a 401k program, and other employee perks. more details about our company benefits can be found at https://shutterflyinc.com/benefits/. this opportunity can be remote, but candidates must reside in a state in which shutterfly is registered to do business. this includes all us states except district of columbia, north dakota, mississippi, rhode island, vermont, and wyoming. this position will accept applications on an ongoing basis until filled. #sflytechnology","fort mill, sc",Data Engineer,"['airflow', 'aws', 'bigquery', 'cloud', 'data pipeline', 'data warehouse', 'dbt', 'elt', 'etl', 'kafka', 'python', 'r', 'redshift', 'sas', 'scala', 'snowflake', 'sql']","['airflow', 'aws', 'bigquery', 'cloud', 'data pipeline', 'data warehouse', 'dbt', 'elt', 'etl', 'kafka', 'python', 'r', 'redshift', 'sas', 'scala', 'snowflake', 'sql']",
"associate, data engineering",bank ozk,"why bank ozk founded on a legacy of more than 120 years in banking, bank ozk is much more than just a company. we're nationally recognized as an industry leader in financial services. that means we combine exceptional service with innovative technologies to deliver smart solutions to our clients across the country. we're investing in small businesses, fueling economies in local communities and changing skylines in the largest cities across america. here, we're not simply filling roles. we're fostering even greater careers. the foundation for a great career starts with an exceptional team and a comprehensive benefits package. we believe in providing our dedicated team members with the best resources to support their physical, mental and financial wellbeing, including generous pto, 401(k) matching, health, dental, vision (and pet!) insurance as well as special perks and discounts. learn more about bank ozk benefits (https://careers.ozk.com/benefits) . job purpose & scope as associate, data engineering you will be responsible for developing and maintaining data solutions which support bank data strategy. in this role, you will act as a problem-solver who can understand business requirements while developing and maintaining data models, etl or elt processes, and reporting solutions. as a member of a data engineering team, you will perform a variety of tasks to develop and maintain data solutions. you will demonstrate exceptional customer service, solutioning, planning, teamwork, and organization skills. you will work independently, yet collaboratively, to fulfill assigned objectives. essential job functions + assist in the development and maintenance of data model-based solutions as specified by engineering standards. + perform testing of data solutions to ensure developed product meets business requirements. + resolve bugs and data issues as identified by product, project management, or governance functions. + provide work estimates while communicating with data engineering management when assignments deviate from initial estimates. + adhere to data development standards. + work in a coordinated manner with the data engineering team and management. + support the design, development, and production of data applications. + apply knowledge of data modeling, elt/etl methodologies, and database architecture in work processes and deliverables. + demonstrate an understanding of ozk enterprise data architecture, data platforms, and data operations. + work with data assurance and deployment team to ensure smooth and well-documented releases. + actively engage in and learn the business processes of the supported lines of business. + support change control and version management. + keep apprised of technological innovation and constantly improve through training. + enthusiastically embrace, support, and model the bank's values and mission. + ensure compliance with internal/external regulatory policies, procedures, and knowledge, skills & abilities + knowledge of data development skills sets to create and maintain data solutions. + ability to maintain focus for sustained time periods to deliver on individual or multiple assigned tasks. + ability to perform in challenging environments. + ability to troubleshoot issues on standard, sensitive, and/or complex accounts. + ability to communicate effectively both verbally and in writing and interact with all levels of employees. + ability to work independently under general supervision. + skill in using computer and microsoft office, including word, excel, and outlook. basic qualifications + bachelor's degree in a job-related field, or commensurate work experience, required. + 3+ years of experience in database development, data warehouse development, and/or data science experience, required. + 3+ years of experience in reporting technologies such as power bi, ssrs, tableau or similar platform, preferred. + experience in jira, azure devops, or similar development management environment, preferred. + experience with snowflake cloud data platform, preferred. + data development certification, such as snow pro certification or microsoft sql server certification, preferred. job expectations job expectations: o perate customary equipment and technology used in a business environment, with or without accommodation. note: this description is not an exhaustive list of all job functions, duties, skills, and job standards required. other job functions, duties, skills, and standards may be added. management reserves the right to add or change the job requirements at any time. #li-kc1 eeo statement bank ozk is an equal opportunity employer and gives consideration for employment to qualified applicants without regard to race, color, religion, sex, national origin, age, sexual orientation, gender identity, disability status, protected veteran status, or any other characteristic protected by federal, state, and local law. member fdic.",texas,Data Engineer,"['azure', 'cloud', 'data warehouse', 'elt', 'etl', 'excel', 'power bi', 'r', 'snowflake', 'sql', 'sql server', 'tableau']","['azure', 'cloud', 'data warehouse', 'elt', 'etl', 'excel', 'power bi', 'r', 'snowflake', 'sql', 'sql server', 'tableau']",70K–140K a year
sr. data engineer,cvs health,"at cvs health, we're building a world of health around every consumer and surrounding ourselves with dedicated colleagues who are passionate about transforming health care.as the nation's leading health solutions company, we reach millions of americans through our local presence, digital channels and more than 300,000 purpose-driven colleagues - caring for people where, when and how they choose in a way that is uniquely more connected, more convenient and more compassionate. and we do it all with heart, each and every day.position summarywe're seeking a sr. data engineer to design and implement data pipelines that power analytical capabilities. this hands-on role requires an understanding of data engineering best practices and the ability to translate business requirements into technical solutions.you will be part of a dedicated team creating datasets for analytic and data science workloads. key responsibilities:data pipeline development: design and build etl/elt data pipelines to ingest, process, and transform large datasets from multiple sources.performance optimization: implement best practices for performance tuning, partitioning, and clustering to optimize data queries and reduce costs.data quality & governance: establish and enforce data quality standards, data governance frameworks, and security policies for data storage and access.data modeling & architecture: develop and optimize data models and schemas to support analytics, reporting, and machine learning requirements.data integration & transformation: collaborate with data scientists and analysts to design data solutions that integrate with bi tools and machine learning models.documentation & knowledge sharing: create comprehensive documentation for data pipelines, workflows, and processes. share best practices and mentor junior data engineers.design and architect data infrastructure analytical workloads.required qualifications5+ years of applicable work experienceproficiency in python, specifically with etl pipelines.strong proficiency in sql and experience in developing complex queries.familiarity with pyspark, dbt, or other similar frameworks.experience deploying data pipelines in a cloud environment (azure, aws, gcp).understanding of data warehousing concepts, dimensional modeling, and building data marts.excellent communication and interpersonal skills, with the ability to collaborate effectively with data scientists, analysts, and product owners.preferred qualificationsknowledge of data governance best practices in a cloud environment.experience with machine learning flows on gcp.experience with data design in bigqueryexperience working with the epic data model.experience working with healthcare data (tuva or omop models a plus).education and experiencecollege degree or certification in related fieldsanticipated weekly hours40time typefull timepay rangethe typical pay range for this role is:$83,430.00 - $222,480.00this pay range represents the base hourly rate or base annual full-time salary for all positions in the job grade within which this position falls. the actual base salary offer will depend on a variety of factors including experience, education, geography and other relevant factors. this position is eligible for a cvs health bonus, commission or short-term incentive program in addition to the base pay range listed above. our people fuel our future. our teams reflect the customers, patients, members and communities we serve and we are committed to fostering a workplace where every colleague feels valued and that they belong.great benefits for great peoplewe take pride in our comprehensive and competitive mix of pay and benefits - investing in the physical, emotional and financial wellness of our colleagues and their families to help them be the healthiest they can be. in addition to our competitive wages, our great benefits include:affordable medical plan options, a 401(k) plan (including matching company contributions), and an employee stock purchase plan.no-cost programs for all colleagues including wellness screenings, tobacco cessation and weight management programs, confidential counseling and financial coaching.benefit solutions that address the different needs and preferences of our colleagues including paid time off, flexible work schedules, family leave, dependent care resources, colleague assistance programs, tuition assistance, retiree medical access and many other benefits depending on eligibility.for more information, visit https://jobs.cvshealth.com/us/en/benefitswe anticipate the application window for this opening will close on: 12/19/2025qualified applicants with arrest or conviction records will be considered for employment in accordance with all federal, state and local laws.","colorado, tx",Data Engineer,"['aws', 'azure', 'bi tools', 'bigquery', 'cloud', 'clustering', 'data pipeline', 'dbt', 'elt', 'etl', 'excel', 'gcp', 'machine learning', 'pyspark', 'python', 'r', 'spark', 'sql']","['aws', 'azure', 'bi tools', 'bigquery', 'cloud', 'clustering', 'data pipeline', 'dbt', 'elt', 'etl', 'excel', 'gcp', 'machine learning', 'pyspark', 'python', 'r', 'spark', 'sql']","83,430–222,480 a year"
data engineer,akkodis,"akkodis is seeking a data engineer (sap ewm implementations) for a contract job with a client in allen park, mi. ""the ideal candidate should have experience with sap ewm implementations.""rate range: $65/hour to $68/hour; the rate may be negotiable based on experience, education, geographic location, and other factors.data engineer job responsibilities include:design, build, and configure sap ewm solutions aligned with business processes and requirements.lead end-to-end sap ewm implementations for automotive warehouses and manufacturing sites.collaborate with customers to define processes, create and validate functional specs, and deliver complex solutions.provide user training and on-site hypercare support to ensure smooth, timely, and cost-effective go-lives.desired qualifications:bachelor's degree in computer science or a related field.7+ years of combined experience in data engineering.sap ewm implementations experience.advanced business application programming (abap)if you are interested in this role, then please click apply now. for other opportunities available at akkodis, or any questions, feel free to contact me at 610-227-6372 or kshitij.ahlawat@akkodisgroup.comequal opportunity employer/veterans/disabledbenefit offerings available for our associates include medical, dental, vision, life insurance, short-term disability, additional voluntary benefits, an eap program, commuter benefits, and a 401k plan. our benefit offerings provide employees the flexibility to choose the type of coverage that meets their individual needs. in addition, our associates may be eligible for paid leave including paid sick leave or any other paid leave required by federal, state, or local law, as well as holiday pay where applicable. disclaimer: these benefit offerings do not apply to client-recruited jobs and jobs that are direct hires to a client.to read our candidate privacy information statement, which explains how we will use your information, please visit https://www.akkodis.com/en/privacy-policy.the company will consider qualified applicants with arrest and conviction records in accordance with federal, state, and local laws and/or security clearance requirements, including, as applicable:• the california fair chance act• los angeles city fair chance ordinance• los angeles county fair chance ordinance for employers• san francisco fair chance ordinance","wyandotte, mi",Data Engineer,"['aws', 'r']","['aws', 'r']",
clinical data engineer,manpowergroup,"clinical data engineer 100% remote 1+ year rate $55-70/hr based on experience objective / purpose: clinical data engineering team, provides strategic planning, integrating, execution, build and oversight of clinical trial deliverables. cde leads the integration, design, development, and execution of data pipelines for the ingestion of clinical data from all sources at an enterprise level at the study level. the cde is an enterprise level role and is primarily responsible for ensuring smooth end to end processes for data collection/ingestion from all data collection sources, providing an output into a data lake that is fit for use by downstream end users. accountabilities: serve as a technical expert in building data pipelines for the ingestion and delivery of clinical data at the study level, supporting study start-up, conduct, and close-out activities. lead the planning, execution, and delivery of data pipeline initiatives across clinical trials. develop robust data pipelines for integrating heterogeneous data sources. identify, design, and implement scalable data delivery solutions, automating manual processes whenever possible. provide technical leadership in all aspects of clinical data flow, including defining, building, and validating application programming interfaces (apis), data streams, and data staging for extraction and integration across systems. manage, maintain, and troubleshoot pipelines within data lakes or data warehouses, ensuring ongoing reliability and quality. develop and implement comprehensive data integrity and quality checks throughout the data ingestion process. prepare functional areas for submission readiness and represent the clinical data engineering group in formal inspections or audits. represent client interactions with key external partners. design and build infrastructure for optimal data extraction, transformation, and loading (etl/elt) using cloud platforms such as aws or azure. collaborate with downstream users-including statistical programmers, sdtm programmers, analytics, and clinical data programmers-to ensure deliverables meet end-user requirements. appropriately escalate issues to cde leadership as needed. job description education & competencies (technical and behavioral): bs/ba required in a health-related, life science area or technology-related fields. minimum of 5 years' technology experience. may lead study level negotiation and agreement for data transfer or integration. should be able to function collaboratively (with some guidance) with all levels of employees critical thinking technical/functional (line) expertise proficient with python, sql, and nosql databases. hands-on cloud experience with aws/azure/gcp. hands-on experience with big data processing frameworks like apache spark familiarity with any one of gitlab, github, and jenkins for version control and ci/cd. proven expertise in deploying data pipelines in cloud environments and scheduling it using airflow. skilled in setting up and managing data warehouses and data lakes (e.g., snowflake, amazon redshift). efficient in designing, developing, and maintaining scalable data pipelines for large datasets. strong understanding of database concepts, with working knowledge of xml, json, and api integrations. solid experience applying system development life cycle (sdlc) manpowergroup is committed to providing equal employment opportunities in a professional, high quality work environment. it is the policy of manpowergroup and all of its subsidiaries to recruit, train, promote, transfer, pay and take all employment actions without regard to an employee's race, color, national origin, ancestry, sex, sexual orientation, gender identity, genetic information, religion, age, disability, protected veteran status, or any other basis protected by applicable law.","milwaukee, wi",Data Engineer,"['airflow', 'aws', 'azure', 'cloud', 'data lake', 'data pipeline', 'data warehouse', 'elt', 'etl', 'gcp', 'python', 'r', 'redshift', 'scala', 'snowflake', 'spark', 'sql']","['airflow', 'aws', 'azure', 'cloud', 'data lake', 'data pipeline', 'data warehouse', 'elt', 'etl', 'gcp', 'python', 'r', 'redshift', 'scala', 'snowflake', 'spark', 'sql']",
principal data engineer,coca cola,"job description: general purpose the position involves managing and overseeing the tasks of data engineers who create and execute etl pipelines and workflows in our cloud infrastructure. the principal data engineer is responsible for maintaining the accuracy and consistency of the data, as well as optimizing pipeline efficiency and cost within our azure setup. duties and responsibilities: • effective management of the data engineering team's daily tasks and development work ensures alignment with objectives and deliverables • assumes responsibility for the ongoing evaluation and optimization of the etl pipelines delivered by the data engineers, striving to enhance efficiency and value • develop a comprehensive quality control framework that includes automated reporting and alerts, as well as transparent communication with key stakeholders • develop and implement an agile cloud infrastructure roadmap that adapts to changing business needs and technology advancements • verify that technical harmony with the data engineering crew at the headquarters is established. qualifications: • a bachelor's degree in computer science, software engineering, information technology, or a comparable subject is mandatory for this role. • the possession of an advanced degree can be a valuable asset. • over three years of data engineering exposure has equipped me with significant expertise in building and sustaining data pipelines, as well as infrastructure management. • as a leader and manager of data engineers for over three years, i have established a reputation for excellence in guiding and directing my team. my strengths include defining objectives, providing clear instructions, and regularly assessing performance • thorough comprehension of data engineering principles, including the amalgamation of data, etl (extract, transform, load) strategies, data warehousing, and data modeling. • an individual should possess a strong understanding of programming languages that are frequently utilized in data engineering, including scala, java, sql, or python. • experienced in utilizing state-of-the-art big data technologies and platforms, such as hadoop, spark, apache kafka, and distributed data processing frameworks. • well-versed in handling different types of databases, both relational ones like sql server and postgresql, and nosql ones such as mongodb and cassandra, among others. • to become highly proficient in modern technology, it is essential to have a thorough understanding of cloud platforms such as azure and aws for tasks like data storage and processing. • possessing the competence to oversee and supervise a team of data engineers, providing mentorship, and assessing their achievements. • 30% travel projected coca-cola southwest beverages llc strictly prohibits any form of discrimination, whether intentional or not, in all aspects of employment, including but not limited to recruitment, hiring, promotions, compensation, and termination.","ennis, tx",Data Engineer,"['aws', 'azure', 'cloud', 'data pipeline', 'etl', 'excel', 'hadoop', 'java', 'kafka', 'python', 'r', 'scala', 'spark', 'sql', 'sql server']","['aws', 'azure', 'cloud', 'data pipeline', 'etl', 'excel', 'hadoop', 'java', 'kafka', 'python', 'r', 'scala', 'spark', 'sql', 'sql server']",
"data engineer, product analytics",meta,"about the position as a data engineer at meta, you will play a crucial role in shaping the future of our people-facing and business-facing products across various applications, including facebook, instagram, messenger, whatsapp, reality labs, and threads. your technical skills and analytical mindset will be essential in designing and building extensive data sets that enhance user experiences for billions of people and hundreds of millions of businesses worldwide. you will collaborate with cross-functional teams to create scalable data solutions that optimize growth, strategy, and user experience, while also addressing complex analytical challenges at an unmatched scale. this position offers the opportunity to be part of a world-class data engineering community focused on skill development and career growth. responsibilities • manage and execute data warehouse plans for a product or a group of products to solve well-scoped problems • identify the data needed for a business problem and implement logging required to ensure availability of data • collaborate with engineers, product managers, and data scientists to understand data needs and represent key data insights meaningfully • build data expertise and leverage data controls to ensure privacy, security, compliance, data quality, and operations for allocated areas of ownership • design, build, and launch new data models and visualizations in production, leveraging common development toolkits • independently design, build, and launch new data extraction, transformation, and loading processes in production, mentoring others around efficient queries • support existing processes running in production and implement optimized solutions with limited guidance • define and manage service level agreements for data sets in allocated areas of ownership requirements • bachelor's degree in computer science, computer engineering, relevant technical field, or equivalent • 2+ years of experience where the primary responsibility involves working with data, such as data analyst, data scientist, data engineer, or similar roles • 2+ years of experience with sql, etl, data modeling, and at least one programming language (e.g., python, c++, c#, scala or others) nice-to-haves • master's or ph.d degree in a stem field benefits • bonus • equity • health benefits • flexible working hours • career development opportunities","nashville, tn",Data Engineer,"['c#', 'c++', 'data warehouse', 'etl', 'python', 'r', 'scala', 'sql']","['c#', 'c++', 'data warehouse', 'etl', 'python', 'r', 'scala', 'sql']",
lead data engineer (intelligent foundations and experiences) in harrisonburg,energy jobline zr,"energy jobline is the largest and fastest growing global energy job board and energy hub. we have an audience reach of over 7 million energy professionals, 400,000+ monthly advertised global energy and engineering jobs, and work with the leading energy companies worldwide. we focus on the oil & gas, renewables, engineering, power, and nuclear markets as well as emerging technologies in ev, battery, and fusion. we are committed to ensuring that we offer the most exciting career opportunities from around the world for our jobseekers. job description lead data engineer (intelligent foundations and experiences) do you love building and pioneering in the technology space? do you enjoy solving complex business problems in a fast-paced, collaborative, inclusive, and iterative delivery environment? at capital one, you'll be part of a big group of makers, breakers, doers and disruptors, who solve real problems and meet real customer needs. we are seeking data engineers who are passionate about marrying data with emerging technologies. as a capital one lead data engineer, you’ll have the opportunity to be on the forefront of driving a major transformation within capital one. intelligent foundations & experiences (ifx) is a powerful collective of horizontal technology organizations that are driving capital one’s real-time intelligent future. together with our partners in the enterprise and across lines of business, we deliver broad-reaching technical solutions and advance state-of-the-art science to help every capital one associate and our 100+m customers succeed. what you’ll do: • collaborate with and across agile teams to design, develop, test, implement, and support technical solutions in full-stack development tools and technologies • work with a team of developers with deep experience in machine learning, distributed microservices, and full stack systems • utilize programming like java, scala, python and open source rdbms and nosql databases and cloud based data warehousing services such as redshift and snowflake • share your passion for staying on top of tech trends, experimenting with and learning new technologies, participating in internal & external technology communities, and mentoring other members of the engineering community • collaborate with digital product managers, and deliver robust cloud-based solutions that drive powerful experiences to help millions of americans achieve financial empowerment • perform unit tests and conduct reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance basic qualifications: • bachelor’s degree • at least 4 years of experience in application development ( experience does not apply) • at least 2 years of experience in big data technologies • at least 1 year experience with cloud computing (aws, microsoft azure, google cloud) qualifications: • 7+ years of experience in application development including python, sql, scala, or java • 4+ years of experience with a public cloud (aws, microsoft azure, google cloud) • 4+ years experience with distributed data/computing tools (mapreduce, hadoop, hive, emr, kafka, spark, gurobi, or mysql) • 4+ year experience working on real-time data and streaming applications • 4+ years of experience with nosql implementation (mongo, cassandra) • 4+ years of data warehousing experience (redshift or snowflake) • 4+ years of experience with unix/linux including basic commands and shell scripting • 2+ years of experience with agile engineering practices at this time, capital one will not sponsor a new applicant for employment authorization, or offer any immigration related support for this position (i.e. h1b, f-1 opt, f-1 stem opt, f-1 cpt, j-1, tn, e-2, e-3, l-1 and o-1, or any eads or other forms of work authorization that require immigration support from an employer). the minimum and maximum full-time annual salaries for this role are listed below, by location. please note that this salary information is solely for candidates hired to perform work within one of these locations, and refers to the amount capital one is willing to pay at the time of this posting. salaries for part-time roles will be prorated based upon the agreed upon number of hours to be regularly worked. mclean, va: $193,400 - $220,700 for lead data engineer plano, tx: $175,800 - $200,700 for lead data engineer richmond, va: $175,800 - $200,700 for lead data engineer candidates hired to work in other locations will be subject to the pay range associated with that location, and the actual annualized salary amount offered to any candidate at the time of hire will be reflected solely in the candidate’s offer letter. this role is also eligible to earn performance based incentive compensation, which may include cash bonus(es) and/or long term incentives (lti). incentives could be discretionary or non discretionary depending on the plan. capital one offers a comprehensive, competitive, and inclusive set of health, financial and other benefits that support your total well-being. learn more at the capital one careers website. eligibility varies based on full or part-time status, exempt or non-exempt status, and management level. this role is expected to accept applications for a minimum of 5 business days. no agencies please. capital one is an equal opportunity employer (eoe, including disability/vet) committed to non-discrimination in compliance with applicable federal, state, and local laws. capital one promotes a drug-free workplace. capital one will consider for employment qualified applicants with a criminal history in a manner consistent with the requirements of applicable laws regarding criminal background inquiries, including, to the extent applicable, article 23-a of the new york correction law; san francisco, california police code article 49, sections 4901-4920; new york city’s fair chance act; philadelphia’s fair criminal records screening act; and other applicable federal, state, and local laws and regulations regarding criminal background inquiries. if you have visited our website in search of information on employment opportunities or to apply for a position, and you require an accommodation, please contact capital one recruiting at 1-800-304-9102 or via email at recruitingaccommodation@capitalone.com. all information you provide will be kept confidential and will be used only to the extent required to provide needed reasonable accommodations. for technical support or questions about capital one's recruiting process, please send an email to careers@capitalone.com capital one does not provide, endorse nor guarantee and is not liable for third-party products, services, educational tools or other information available through this site. capital one financial is made up of several different entities. please note that any position posted in canada is for capital one canada, any position posted in the united kingdom is for capital one europe and any position posted in the philippines is for capital one philippines service corp. (copssc). if you are interested in applying for this job please press the apply button and follow the application process. energy jobline wishes you the very best of luck in your next career move.","harrisonburg, va",Data Engineer,"['aws', 'azure', 'cloud', 'google cloud', 'hadoop', 'java', 'kafka', 'machine learning', 'python', 'r', 'redshift', 'scala', 'snowflake', 'spark', 'sql']","['aws', 'azure', 'cloud', 'google cloud', 'hadoop', 'java', 'kafka', 'machine learning', 'python', 'r', 'redshift', 'scala', 'snowflake', 'spark', 'sql']",
data engineer and power platform specialist,virtualvocations,"a company is looking for an associate director, data engineering and development. key responsibilities : lead the performance team in using black diamond for data aggregation and reporting design, build, and maintain etl processes and sql pipelines for enterprise reporting enhance power bi dashboards and develop low-code automations using power platform required qualifications : bachelor's degree in computer science or a related field 3 to 6 years of experience in sql data engineering or related analytics roles strong proficiency in sql and data modeling; experience with microsoft sql server preferred experience with azure data services or other cloud platforms is preferred working knowledge of power bi and familiarity with apis and data integration patterns","st. louis, mo (+1 other)",Data Engineer,"['azure', 'cloud', 'dashboard', 'etl', 'power bi', 'r', 'sql', 'sql server']","['azure', 'cloud', 'dashboard', 'etl', 'power bi', 'r', 'sql', 'sql server']",
principal data engineer,oracle,"**job description** we are looking for a hands-on engineering expert with deep experience in cloud data ecosystems (oci, aws, azure) and the ability to deliver secure, high-quality, and scalable data solutions. as a principal data engineer within oracle's consumer global industries unit, you will architect and build foundational components of a next-generation data platform supporting oracle industry applications. you will be responsible for designing and implementing scalable, high-performance data pipelines, and contribute significantly to data integration, transformation, and delivery processes. you will collaborate with data architects on best practices for data modeling and architecture (including medallion architecture), champion modern engineering methods, and ensure compliance with security, data quality, and governance requirements. your expertise in cloud-based data services, etl/elt, and real-time/batch data processing will directly enable high-value, ai-enriched solutions across multiple industries. required experience: - bachelor's or master's degree in computer science, information systems, engineering, or a related discipline. - 7+ years of experience in data engineering with demonstrated expertise designing, building, and maintaining large-scale enterprise data platforms. - proven hands-on experience with cloud ecosystem data services (oci, aws, azure), their data engineering tools, and ai/ml service integration. - advanced skills in building reliable etl/elt pipelines, data integration, transformation, validation, and orchestration frameworks. - strong programming skills (e.g., python, kafka, java, scala) with experience in distributed data processing technologies (e.g. spark, databricks, apache beam). - in-depth knowledge of data governance, quality, security, and compliance best practices within cloud-first or hybrid environments. - proficiency with industry-standard tools for workflow orchestration (e.g., airflow), data cataloging, and monitoring. - experience in regulated industries and familiarity with relevant compliance standards is a plus. - excellent problem-solving, communication, and collaboration skills within global and cross-functional teams. • *responsibilities** key responsibilities: - design and develop robust, scalable, and secure data pipelines (batch and real-time) to ingest, process, and deliver data from multiple sources to oracle industry applications. - implement and optimize data architectures in cloud environments (oci, aws, azure) including data lakes, data warehouses, etl/elt, and orchestration frameworks. - collaborate with data architects to implement best practices for data modeling (conceptual, logical, and physical) and support platform-wide standards such as medallion architecture (bronze/silver/gold). - design and establish monitoring, alerting, and data quality frameworks to ensure reliability and accuracy of data pipelines. - develop and enforce best practices for data security, lineage, metadata management, and compliance requirements. - work closely with analysts, ml engineers, business stakeholders, and cross-functional teams to enable industry-specific data use cases and analytics for oracle industry applications. - evaluate, recommend, and onboard new data technologies to continuously improve platform performance and capabilities. - lead troubleshooting and root cause analysis for data issues, ensuring continuous platform availability and reliability. - mentor and guide junior data engineers and drive technical excellence and innovation across engineering efforts. disclaimer: • *certain us customer or client-facing roles may be required to comply with applicable requirements, such as immunization and occupational health mandates.** • *range and benefit information provided in this posting are specific to the stated locations only** us: hiring range in usd from: $96,800 to $223,400 per annum. may be eligible for bonus and equity. oracle maintains broad salary ranges for its roles in order to account for variations in knowledge, skills, experience, market conditions and locations, as well as reflect oracle's differing products, industries and lines of business. candidates are typically placed into the range based on the preceding factors as well as internal peer equity. oracle us offers a comprehensive benefits package which includes the following: 1. medical, dental, and vision insurance, including expert medical opinion 2. short term disability and long term disability 3. life insurance and ad&d 4. supplemental life insurance (employee/spouse/child) 5. health care and dependent care flexible spending accounts 6. pre-tax commuter and parking benefits 7. 401(k) savings and investment plan with company match 8. paid time off: flexible vacation is provided to all eligible employees assigned to a salaried (non-overtime eligible) position. accrued vacation is provided to all other employees eligible for vacation benefits. for employees working at least 35 hours per week, the vacation accrual rate is 13 days annually for the first three years of employment and 18 days annually for subsequent years of employment. vacation accrual is prorated for employees working between 20 and 34 hours per week. employees working fewer than 20 hours per week are not eligible for vacation. 9. 11 paid holidays 10. paid sick leave: 72 hours of paid sick leave upon date of hire. refreshes each calendar year. unused balance will carry over each year up to a maximum cap of 112 hours. 11. paid parental leave 12. adoption assistance 13. employee stock purchase plan 14. financial planning and group legal 15. voluntary benefits including auto, homeowner and pet insurance the role will generally accept applications for at least three calendar days from the posting date or as long as the job remains posted. career level - ic4 • *about us** as a world leader in cloud solutions, oracle uses tomorrow's technology to tackle today's challenges. we've partnered with industry-leaders in almost every sector-and continue to thrive after 40+ years of change by operating with integrity. we know that true innovation starts when everyone is empowered to contribute. that's why we're committed to growing an inclusive workforce that promotes opportunities for all. oracle careers open the door to global opportunities where work-life balance flourishes. we offer competitive benefits based on parity and consistency and support our people with flexible medical, life insurance, and retirement options. we also encourage employees to give back to their communities through our volunteer programs. we're committed to including people with disabilities at all stages of the employment process. if you require accessibility assistance or accommodation for a disability at any point, let us know by emailing accommodation-request_mb@oracle.com or by calling +1 888 404 2494 in the united states. oracle is an equal employment opportunity employer. all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability and protected veterans' status, or any other characteristic protected by law. oracle will consider for employment qualified applicants with arrest and conviction records pursuant to applicable law.","lansing, mi",Data Engineer,"['airflow', 'aws', 'azure', 'cloud', 'data lake', 'data pipeline', 'data warehouse', 'databricks', 'elt', 'etl', 'excel', 'java', 'kafka', 'python', 'r', 'scala', 'spark']","['airflow', 'aws', 'azure', 'cloud', 'data lake', 'data pipeline', 'data warehouse', 'databricks', 'elt', 'etl', 'excel', 'java', 'kafka', 'python', 'r', 'scala', 'spark']",
"manager, data operations, data engineer",kpmg,"known for being a great place to work and build a career, kpmg provides audit, tax and advisory services for organizations in today's most important industries. our growth is driven by delivering real results for our clients. it's also enabled by our culture, which encourages individual development, embraces an inclusive environment, rewards innovative excellence and supports our communities. with qualities like those, it's no wonder we're consistently ranked among the best companies to work for by fortune magazine, consulting magazine, seramount, fair360 and others. if you're as passionate about your future as we are, join our team. kpmg is currently seeking a manager of data engineering to join our digital nexus technology organization. this is a hybrid work opportunity. responsibilities: • lead a team of azure data lake and business intelligence engineers in designing and delivering adl pipelines, notebooks and interactive power bi dashboards that clearly communicate actionable insights to stakeholders; contribute strategic thought leadership to shape the firm's business intelligence vision and standards • design and maintain scalable data pipelines using azure data factory and databricks to ingest, transform, and deliver data across medallion architecture layers; develop production-grade etl/elt solutions using pyspark and sql to produce analytics-ready delta lake datasets aligned with enterprise standards • apply critical thinking and creativity to design innovative, non-standard bi solutions that address complex and evolving business challenges; design, build, and optimize data models to support analytics, ensuring accuracy, reliability, and efficiency • stay ahead of emerging technologies including generative ai and ai agents to identify novel opportunities that improve analytics, automation, and decision-making across the enterprise • manage and provide technical expertise and strategic guidance to counselees (direct reports), department peers, and cross-functional team members; set goals, participate in strategic initiatives for the team, and foster the development of high-performance teams • act with integrity, professionalism, and personal responsibility to uphold kpmg's respectful and courteous work environment qualifications: • minimum seven years of recent experience designing and building adl pipelines and data bricks notebooks and interactive dashboards using modern business intelligence tools (preferably power bi); minimum two years of recent experience designing scalable data pipelines using azure data factory and azure databricks to support ingestion, transformation, and delivery of data across medallion architecture layers • bachelor's degree from an accredited college or university is preferred; minimum of a high school diploma or ged is required • demonstrated analytical and problem-solving abilities, with a creative and methodical approach to addressing complex challenges • advanced knowledge of sql, dax, and data modeling concepts; proven track record in defining, managing, and delivering bi projects; ability to participate in the development of resource plans and influence organizational priorities • excellent written and verbal communication skills, including the ability to effectively present proposals and vision to executive leadership • applicants must be authorized to work in the u.s. without the need for employment-based visa sponsorship now or in the future; kpmg llp will not sponsor applicants for u.s. work visa status for this opportunity (no sponsorship is available for h-1b, l-1, tn, o-1, e-3, h-1b1, f-1, j-1, opt, cpt or any other employment-based visa) kpmg llp and its affiliates and subsidiaries (“kpmg”) complies with all local/state regulations regarding displaying salary ranges. if required, the ranges displayed below or via the url below are specifically for those potential hires who will work in the location(s) listed. any offered salary is determined based on relevant factors such as applicant's skills, job responsibilities, prior relevant experience, certain degrees and certifications and market considerations. in addition, kpmg is proud to offer a comprehensive, competitive benefits package, with options designed to help you make the best decisions for yourself, your family, and your lifestyle. available benefits are based on eligibility. our total rewards package includes a variety of medical and dental plans, vision coverage, disability and life insurance, 401(k) plans, and a robust suite of personal well-being benefits to support your mental health. depending on job classification, standard work hours, and years of service, kpmg provides personal time off per fiscal year. additionally, each year kpmg publishes a calendar of holidays to be observed during the year and provides eligible employees two breaks each year where employees will not be required to use personal time off; one is at year end and the other is around the july 4th holiday. additional details about our benefits can be found towards the bottom of our kpmg us careers site at benefits & how we work. follow this link to obtain salary ranges by city outside of ca: https://kpmg.com/us/en/how-we-work/pay-transparency.html/?id=7567_9_25 kpmg offers a comprehensive compensation and benefits package. kpmg is an equal opportunity employer. kpmg complies with all applicable federal, state and local laws regarding recruitment and hiring. all qualified applicants are considered for employment without regard to race, color, religion, age, sex, sexual orientation, gender identity, national origin, citizenship status, disability, protected veteran status, or any other category protected by applicable federal, state or local laws. the attached link contains further information regarding kpmg's compliance with federal, state and local recruitment and hiring laws. no phone calls or agencies please. kpmg recruits on a rolling basis. candidates are considered as they apply, until the opportunity is filled. candidates are encouraged to apply expeditiously to any role(s) for which they are qualified that is also of interest to them. los angeles county applicants: material job duties for this position are listed above. criminal history may have a direct, adverse, and negative relationship with some of the material job duties of this position. these include the duties and responsibilities listed above, as well as the abilities to adhere to company policies, exercise sound judgment, effectively manage stress and work safely and respectfully with others, exhibit trustworthiness, and safeguard business operations and company reputation. pursuant to the california fair chance act, los angeles county fair chance ordinance for employers, fair chance initiative for hiring ordinance, and san francisco fair chance ordinance, we will consider for employment qualified applicants with arrest and conviction records.","harrisburg, pa",Data Engineer,"['aws', 'azure', 'business intelligence', 'classification', 'dashboard', 'data lake', 'data pipeline', 'databricks', 'elt', 'etl', 'excel', 'power bi', 'pyspark', 'r', 'scala', 'spark', 'sql']","['aws', 'azure', 'business intelligence', 'classification', 'dashboard', 'data lake', 'data pipeline', 'databricks', 'elt', 'etl', 'excel', 'power bi', 'pyspark', 'r', 'scala', 'spark', 'sql']",
"data engineer, loyalty data & analytics","canadian tire corporation, limited","what you'll do we are seeking a motivated data engineer to join the ctc triangle program - loyalty data product engineering team, reporting to the engineering manager. this team leverages a broad range of data technologies and analytical techniques to design, develop, and automate data pipelines and processes consumed across the organization both analytically and operationally. made up of data engineers and analysts, the team is focused on creating a portfolio of best-in-class products and services that fuel customer-focused analytics and enhance the customer and brand experience. in this role, you are expected to: follow development best practices under the guidance of senior team members, combining automation and ci/cd with a culture of review and collaboration assist in implementing and testing data pipelines to monitor and validate model assumptions and performance adhering to coding standards learn and understand business and operational processes to translate stakeholders' requirements into technical requirements to support work processes and strategic business objectives support it and business stakeholders by assisting in decision-making processes with data and data products identify and troubleshoot technical issues as they arise, providing scalable resolutions with follow-through under supervision contribute to ad-hoc projects that leverage multiple internal and external sources of data such as point-of-sales, loyalty issuance, product hierarchy, customer data etc. what you bring bachelor’s degree or higher in computer science, math, engineering, or a related field 1+ years of internship/professional experience in data engineering or data analytics hands-on experience in pyspark and exposure to pipeline orchestration tools such as airflow knowledge of database management systems (e.g., hadoop & hive, mysql, databricks etc.) self-sufficient in interpreting, decoupling and testing of complicated queries, joins, subqueries in sql basic understanding of delta tables, etl pipeline concepts, and change data capture familiarity with git for version control understanding of object-oriented programming (oop) principles basic knowledge of bash scripting and cloud sdks for automation strong collaboration and communication skills; proven ability to work effectively in a team environment experience working in the retail sector, especially with pos (point-of-sale) data preferred familiarity with databricks, microsoft azure, or google cloud preferred familiarity with network security protocols (especially relevant for the senior data engineer role) preferred experience with key management and authentication mechanisms preferred about us canadian tire corporation, limited (“ctc”) is one of canada’s most admired and trusted companies. with more than 90 owned brands, 1,700 retail locations, financial services, exemplary e-commerce capabilities, and exciting market-leading merchandising strategies. we dream big and work as one to innovate with purpose for our customers at every level of our business, investing in new technologies and products, and doubling down on top talent to drive the company forward. we offer competitive salaries and wages to ctc employees, as well as store discounts, supported learning through our triangle learning academy, canadian tire profit sharing, and retirement and savings programs for eligible employees. as part of our enhanced flex benefits program, we offer mental health benefits in the amount of $5,000 per year for benefits-eligible employees and their families, including total well-being, and mental health tools and resources for all employees. join us in helping to make life in canada better through living and working our core values: we are innovators and entrepreneurs at our core, outcomes drive us, inclusion is a must, we are stronger together and we take personal responsibility. it is an especially exciting time to join ctc and its family of companies where career opportunities are wide-ranging! join us, where there's a place for you here. our commitment to diversity, inclusion and belonging we are committed to fostering an environment where belonging thrives, and diversity, inclusion and equity are infused into everything we do. we believe in building an organizational culture where people are consistently treated with dignity while respecting individual religion, nationality, gender, race, age, perceived ability, spoken language, sexual orientation, and identification. we are united in our purpose of being here to help make life in canada better. accommodations we stand firm in our core value that inclusion is a must. we welcome and encourage candidates from equity-seeking groups such as people who identify as racialized, indigenous, 2slgbtqia+, women, people with disabilities, and beyond. should you require any accommodation in applying for this role, or throughout the interview process, please make them known when contacted and we will work with you to help meet your needs. we are one of canada’s most admired and trusted companies. with world-class owned brands and exciting market-leading merchandising strategies, we are continually innovating with purpose: to excite and serve canadian customers from coast-to-coast. we are connected to communities, big and small, from coast-to-coast, offering products and services that reflect the diverse nature of every one of them. from sports to outdoors, automobiles to homes, we know and understand life in canada like no other retailer can. we are always on the lookout for curious, creative people who are able to navigate and excel in a rapidly evolving retail environment. if you’re ready to take on new challenges – be it in digital, it, marketing, data & analytics, merchandising, or one of the many other roles we have – there is a place for you here, so apply today.",canada,Data Engineer,"['airflow', 'azure', 'cloud', 'data analytics', 'data pipeline', 'databricks', 'elt', 'etl', 'excel', 'google cloud', 'hadoop', 'pyspark', 'r', 'scala', 'spark', 'sql']","['airflow', 'azure', 'cloud', 'data analytics', 'data pipeline', 'databricks', 'elt', 'etl', 'excel', 'google cloud', 'hadoop', 'pyspark', 'r', 'scala', 'spark', 'sql']",
"senior data engineer ( snowflake, dbt )",yochana,"position name – senior data engineer type of hiring – fulltime/subcon location – markham, on (need to visit 3 days a week in office mandatorily) vacancy status: we are looking to hire a senior data engineer immediately who has experience in snowflake, dbt and aws – glue. job description: as a senior data engineer, you will play a pivotal role in driving the design, development, and delivery of scalable data solutions across the enterprise. you will: • advocate for engineering standards and process efficiencies to ensure high-quality, timely delivery. • lead the development of complex, high-performance data pipelines using tools like dbt core/cloud. • help design, and review data models (conceptual, logical, physical) that meet business and technical requirements. • own and maintain robust, reusable code in sql, python, shell, and terraform scripts. • develop detailed low-level engineering solution design documents to guide implementation and ensure alignment with technical standards. • create and review data test plans to ensure alignment with solution requirements. • advocate importance of data catalogs, data governance and data quality practices. • conduct root cause analysis and implement effective solutions for technical data issues. • lead scrum ceremonies and foster an agile mindset across the team. • mentor and support data engineers and associate data engineers, elevating engineering standards and practices. • collaborate cross-functionally to deliver high-quality data products with strong customer-centric focus. • drive technical presentations and provide constructive feedback on data designs and processes. • plan and execute data release activities to ensure smooth and high-performance delivery. • support talent acquisition by designing interview challenges, conducting interviews, and contributing to hiring decisions. what you'll bring • 8+ years of professional experience in data engineering, with a proven track record of delivering 3+ high-impact data projects from inception to warranty. • deep expertise in relational databases (snowflake, postgresql, amazon aurora), big data platforms (hadoop), and nosql databases (mongodb). • advanced proficiency in data visualization tools such as snow sight, streamlit, qlik, and sap business objects. • expert level coding skills in sql, python, glue, dbt, shell, and terraform, with a focus on maintainability and performance. • experience with data orchestration and pipeline tools, including zena and aws managed airflow. • high resilience and adaptability in ambiguous or high-pressure environments. • a customer-first mindset, using data-driven insights to deliver impactful solutions. • strong collaboration and communication skills, with the ability to lead teams and influence stakeholders. • insurance knowledge is an asset -ability to foundationally understand complex business processes driving technical systems. • prior experience in ai/ml model operationalization is an asset. any or all of the following certifications will be nice to have: • snowpro core • snowpro advanced: data engineer (dea-c01 or dea-c02) • dbt developer • aws cloud practitioner",canada,Data Engineer,"['airflow', 'aws', 'cloud', 'data pipeline', 'dbt', 'hadoop', 'python', 'r', 'scala', 'snowflake', 'sql']","['airflow', 'aws', 'cloud', 'data pipeline', 'dbt', 'hadoop', 'python', 'r', 'scala', 'snowflake', 'sql']",
cloud data integration engineer,altitude technology solutions inc,"urgent role cloud data integration engineer job location- remote client: ust note from the client- data integration engineer with infrastructure and dev ops experience to support and enhance our cloud-based data integration platforms. this role will focus on managing and optimizing infrastructure for tools such as qlik replicate, informatica cdir, alteryx, and dbt (saas) and would also need to be a data ingestion and integration subject matter expert helping to enable self-service data engineering patterns and best practices. this role is more of a data integration engineer. overview: we’re seeking a seasoned cloud data integration engineer with infrastructure and dev ops experience to support and enhance our cloud-based data integration platforms. this role will focus on managing and optimizing infrastructure for tools such as qlik replicate, informatica cdir, alteryx, and dbt (saas) and would also need to be a data ingestion and integration subject matter expert helping to enable self-service data engineering patterns and best practices. key responsibilities: • design, deploy, and manage cloud infrastructure using terraform across multi-regional aws environments. • administer and automate windows server configurations, os updates and application patching/updates through terraform to support these integration platforms. • serve as a subject matter expert for data replication, ingestion, and integration tools, ensuring seamless connectivity and performance. • develop and manage ci/cd pipelines for integration tools and infrastructure updates. • implement monitoring, logging, and alerting solutions to ensure system reliability. required skills: • strong experience with terraform and infrastructure-as-code principles. • experience with aws and devops tools (e.g., github actions, jenkins, azure devops). • solid background in windows server administration and scripting • expertise with amazon web services, including ec2, s3, api gateway, lambda, glue, step functions. cloud watch, and airflow. • hands-on experience with data integration and ingestion tools like qlik replicate, informatica, alteryx, and dbt and analytics platforms such as databricks and snowflake. • excellent problem-solving and communication skills. • experience with python and api development, change data capture, and etl/elt. regards ravi sharma r.sharma@atsitinc.com job type: fixed term contract contract length: 6 months pay: $42.24-$60.00 per hour",canada,Data Engineer,"['airflow', 'aws', 'azure', 'cloud', 'databricks', 'dbt', 'elt', 'etl', 'excel', 'python', 'r', 'snowflake']","['airflow', 'aws', 'azure', 'cloud', 'databricks', 'dbt', 'elt', 'etl', 'excel', 'python', 'r', 'snowflake']",$42.24–$60.00 an hour
"senior data engineer, platform",jobber,"does working with data motivate and excite you? then jobber might be the place for you! we’re looking for a senior data engineer to be part of our data platform team. jobber exists to help people in small businesses be successful. we work with small home service businesses, like your local plumbers, painters, and landscapers, to transform the way service is delivered through technology. with jobber they can quote, schedule, invoice, and collect payments from their customers, while providing an easy and professional customer experience. running a small business today isn’t like it used to be—the way we consume and deliver service is changing rapidly, technology is evolving, and customers expect more. that’s why we put the power and flexibility in their hands to run their businesses how, where, and when they want! our culture of transparency, inclusivity, collaboration, and innovation has been recognized by great place to work, canada’s most admired corporate cultures, and more. jobber has also been named on the globe and mail’s canada’s top growing companies list, and deloitte canada’s technology fast 50™, enterprise fast 15, and technology fast 500™ lists. with an executive team that has over thirty years of industry experience of leading the way, we’ve come a long way from our first customer in 2011—but we’ve just scratched the surface of what we want to accomplish for our customers. we help employees grow professionally; we have a ton of onboarding resources, tutorials, hackathons and buddies to support learnings and provide opportunities to innovate. we have a range of experience levels on teams which allows for mentor/mentee opportunities. leaders at jobber work with empathy and support employees to build healthy work-life harmony. bring your dedication and passion to this job to fulfill your goals the data platform team at jobber: as a senior data engineer, reporting to the manager of data engineering, you’ll be a key contributor on our data platform team. our team builds jobber’s data infrastructure and systems, driving improved operational outcomes, enhancing workflow efficiencies, and generating critical business insights. we empower teams across the organization to fully leverage data, tools, and technology to achieve their goals. by researching, developing, and maintaining data systems, we provide essential operational and analytical support to ensure our internal teams are set up for success. we're seeking individuals who are ready for their next challenge—those who want to use their expertise to influence people, processes, and decisions that have a direct impact on the company's trajectory. the senior data engineer will: • build scalable data solutions: design, develop, and maintain batch and real-time data pipelines within cloud infrastructure (preferably aws). leverage python, sql, and aws technologies (glue, lambda, ecs fargate) to ensure smooth data operations. build scripts, serverless applications, and automated workflows. • empower internal teams: develop tools and frameworks that automate manual processes, set up alerting/monitoring systems, and help teams run data-driven experiments and analyze results. work closely with cross-functional teams to support their needs and ensure data accessibility. • accelerate business growth: collaborate with data analysts, scientists, and product teams to extract actionable insights from data. utilize tools like airflow and dbt to streamline etl/elt pipelines and ensure the seamless flow of data. • strategic planning and innovation: lead initiatives to research and propose new technologies and tooling for our data stack, with an emphasis on performance and scalability. participate in design and code reviews, continuously learning from and mentoring your peers. • data integrity: own the integrity of our data and maintain a high level of trust across the organization. • on-call rotation: members of the data platform team participate in an on-call rotation, covering one week at a time. when an incident occurs outside of regular working hours, we provide time off in lieu to support healthy balance and recovery. from time to time, major maintenance work may require team members to serve as primary or secondary on-call support over a weekend. jobber is committed to maintaining a fair, transparent, and humane on-call experience. your interview team will be happy to walk you through how we manage on-call responsibilities in practice. to be successful, you should have: • leadership skills: proven ability to lead and collaborate in team environments, fostering a culture of shared success. • technical proficiency: • strong coding skills in python and sql. • expertise in building and maintaining etl pipelines using tools like airflow and dbt. • experience working with aws data infrastructure, particularly redshift, glue, lambda, and ecs fargate. • familiarity with handling large datasets using tools like spark or similar (e.g., trino). • experience with terraform for infrastructure management. • data expertise: • experience with dimensional modelling, star schemas, and data warehousing in a cloud environment (preferably aws redshift). • knowledge of ci/cd processes, data ingestion, and optimizing data flow across systems. • proficient in working with high-volume, scalable data infrastructure. • strong communication: ability to collaborate effectively with both technical and non-technical teams, explaining complex data concepts in a clear and concise manner. it would be really great (but not a deal-breaker) if you had: • experience with aws tools like emr and sagemaker. • familiarity with kafka for stream processing and message queuing. • knowledge of nosql databases. • hands-on experience with integration and apis. work environment: at jobber you will choose where you do your most impactful work from! you can work from home across canada, or in a hybrid setting from one of our offices in edmonton (hq) or toronto. compensation: at jobber, we also believe that compensation should be transparent, fair, and supportive of your experience and growth. this role has a minimum annual salary of $125,800 cad, a midpoint of $147,900 cad, and a maximum salary of $170,100 cad, designed to reflect the progression from learning the ropes to truly excelling. we design our compensation to reflect each new hire's skills, experience, and the complexity of the role, ensuring a fair and competitive salary. our range is intentionally broad to support growth and long-term impact, with fully established hires typically starting around the midpoint. the higher end of the range is reserved for those who have demonstrated deep expertise and lasting contributions, while offers below the midpoint reflect strong potential with room to develop. this approach ensures that compensation aligns with both an individual's current capabilities and their opportunity for future growth. base salary is just one part of a total compensation package that will include equity rewards, annual stipends for health and wellness, retirement savings matching, and an extended health package with fully paid premiums for body and mind. your professional growth matters to us too! you'll have access to a dedicated talent development program that includes career coaching and opportunities for career development. we believe in transparency and open conversations about compensation. if you have any questions about our approach, we're happy to discuss them throughout the hiring process! what you can expect from jobber: • a total compensation package that includes an extended health benefits package with fully paid premiums for both body and mind, matching in rrsp, tfsa or fhsa, and stock options. • a dedicated talent development team and access to coaching, learning, and leadership programs to help you grow your career, reach your goals, and unlock your full potential. • a unique opportunity to build, grow, and leave your impact on a $400-billion industry that has no dominant player...yet. • to work with a group of people who are humble, supportive, and give a sh*t about our customers. we believe that diverse teams perform better and that fostering an inclusive work environment is a key part of growing a successful team. we welcome people of diverse backgrounds, experiences, and perspectives. we are an equal opportunity employer, and we are committed to working with applicants requesting accommodation at any stage of the hiring process. a bit more about us: job by job, we’re transforming the way service is delivered. your lawn care provider, home cleaning service, plumber or painter could use jobber to better connect with their customers, save time in the office, invoice faster, and get paid! we’re bringing tens of thousands of people together with technology to deliver billions of dollars a year in services to happy customers. jobber exists to help make these small businesses successful, and when they’re successful we all win!",canada,Data Engineer,"['airflow', 'aws', 'cloud', 'data pipeline', 'dbt', 'elt', 'etl', 'excel', 'kafka', 'python', 'r', 'redshift', 'scala', 'spark', 'sql']","['airflow', 'aws', 'cloud', 'data pipeline', 'dbt', 'elt', 'etl', 'excel', 'kafka', 'python', 'r', 'redshift', 'scala', 'spark', 'sql']",
senior data & ai engineer: build alpha-generating ai for global banks,prospect 33,"location: canada (preferably toronto) | full-time | in-house leadership role we are expanding our elite data, ai/ml engineering practice and looking for an outstanding senior/lead data & ai engineer to join the prospect 33 core team. this role is based in canada with a strong preference for greater toronto. exceptional candidates from vancouver, montreal, or other canadian cities will also be considered. you will be a pivotal member of p33, leading transformative projects for many of the world’s largest and most sophisticated banks and asset managers. this is real ownership inside our firm — you’ll architect solutions end-to-end, set technical direction, and deliver ai systems that directly influence global investment research, trading, sales, and risk. what you’ll do • build and productionize next-gen data products and alpha-generating ai signals using traditional + alternative data at massive scale. • own end-to-end data & ml pipelines (ingestion → feature stores → training/tuning → low-latency inference) that power live trading and research platforms. • lead distributed-systems optimization: profile spark/databricks workloads, crush latency and cost inefficiencies, and establish firm-wide best practices. • champion modern mlops and generative ai adoption (ci/cd, iac, mlflow, llm orchestration, agents, rag). • mentor rising talent and represent prospect 33 technically to c-suite stakeholders at top-tier global financial institutions. must-have • expert-level python, sql, pyspark, mlflow in production at scale • master's degree in computer science, engineering, or equivalent • deep mastery of algorithms, data structures, and distributed/parallel computing • proven history of shipping complete, battle-tested data & ml pipelines from zero to production strong advantages • phd • azure databricks, snowflake, kubernetes, terraform • hands-on generative ai experience (llms, rag, agents, mcp servers, prompt engineering) • capital markets domain exposure (research, trading, risk, alternative data) why prospect 33 – p33.ai • permanent, high-impact role on our core team — not a client bench • collaborate seamlessly with our toronto, new york, london, and dublin hubs • ranked one of the top employers in financial technology (check glassdoor and beyond) • comprehensive canadian benefits: top-tier health & dental, generous rrsp matching, pto, learning budget, commuter support, wellness perks, and more • great salary range (depending on experience and location) + performance bonus + equity in prospect 33 if you thrive on large-scale, complex domain problems and love watching your models move real markets, we want you leading the charge with us in canada. send your resume + a quick note about the most ambitious data/ai system you’ve built to careers@p33.ai with subject “data & ai engineer – canada (toronto preferred)”. prospect 33 – elite engineering. capital markets focus. canadian momentum. come build the future of finance with us.",canada,Data Engineer,"['azure', 'databricks', 'pyspark', 'python', 'r', 'snowflake', 'spark', 'sql']","['azure', 'databricks', 'pyspark', 'python', 'r', 'snowflake', 'spark', 'sql']",
senior reliability engineer - data / cloud,accellor,"at accellor, we are a trusted digital transformation partner that uses best-of-breed cloud technology to deliver superior customer engagement and business effectiveness for clients. we’ve created an atmosphere that encourages curiosity, constant learning, and persistence. we encourage our employees to grow and explore their interests. we cultivate an environment of collaboration, autonomy, and delegation – we know our people have a strong work ethic and a sense of pride and ownership over their work. they are passionate, eager, and motivated – focused on building the perfect solution but never losing sight of the bigger picture. senior reliability engineer – data & cloud position overview we are seeking a senior reliability engineer to support and enhance the reliability, performance, and stability of our retail data and cloud platforms. this role will work closely with data engineering, analytics, and retail operations teams to ensure that store, inventory, and supply chain are accurate, reliable, and available in real time. you will play a key role in monitoring and maintaining azure-based data pipelines, optimizing fabric lakehouse workloads, and improving automation across our retail technology ecosystem. key responsibilities: • support the reliability, performance, and uptime of data pipelines that power retail operations (pos data, supply chain feeds, inventory updates, etc.). • monitor production workloads using azure monitor, log analytics, and custom dashboards. • respond to incidents, troubleshoot failures, perform root-cause analysis, and implement prevention measures. • optimize etl/elt workflows using azure data factory (adf) for retail datasets. • automate system/power bi integrations (e.g., pos, erp, loyalty systems) using azure logic apps. • implement quality checks, data validations, and alerting to ensure data freshness and accuracy. • write and optimize sql queries and stored procedures supporting operational data stores. • work with azure services including storage accounts, key vault, azure functions, event grid, and app insights. • support ci/cd pipelines for data integration and fabric workloads (azure devops). • develop and maintain fabric lakehouse pipelines to support reporting, forecasting, and analytics. • use fabric notebooks and pyspark for data transformations, batch processing, and scaling large retail data workloads. • collaborate with bi teams to ensure data is ready and reliable for dashboards and real-time insights. • identify opportunities to automate manual processes and improve reliability across retail systems. • 7+ years of experience in reliability engineering, data engineering, cloud engineering, or similar roles. • strong hands-on experience with: • sql (debugging, tuning, modeling) • azure data factory (adf) • azure logic apps • microsoft azure services (storage, functions, key vault, monitor) • solid understanding of monitoring, observability, and incident management. • strong analytical, problem-solving, and communication skills. • ability to work in fast-paced environments with frequent data updates (common in retail). preferred qualifications • experience in the retail industry (pos systems, inventory, supply chain, merchandising, or loyalty data). • familiarity with power bi or other visualization tools. • experience with git, azure devops, and ci/cd workflows. • practical experience with microsoft fabric, such as working with lakehouse, pipelines, or dataflows through notebooks and pyspark original job senior reliability engineer - data / cloud posted on grabjobs ©. to flag any issues with this job please use the report job button on grabjobs.",canada,Data Engineer,"['azure', 'cloud', 'dashboard', 'data pipeline', 'elt', 'etl', 'power bi', 'pyspark', 'r', 'spark', 'sql']","['azure', 'cloud', 'dashboard', 'data pipeline', 'elt', 'etl', 'power bi', 'pyspark', 'r', 'spark', 'sql']",
"senior data engineer i, data infrastructure (12 months fixed-term)",khan academy,"about khan academy khan academy is a nonprofit with the mission to deliver a free, world-class education to anyone, anywhere. our proven learning platform offers free, high-quality supplemental learning content and practice that cover pre-k - 12th grade and early college core academic subjects, focusing on math and science. we have over 181 million registered learners globally and are committed to improving learning outcomes for students worldwide, focusing on learners in historically under-resourced communities. our community our students, teachers, and parents come from all walks of life, and so do we. our team includes people from academia, traditional/non-traditional education, big tech companies, and tiny startups. we hire great people from diverse backgrounds and experiences because it makes our company stronger. we value diversity, equity, inclusion, and belonging as necessary to achieve our mission and impact the communities we serve. we know that transforming education starts in-house with learning about ourselves and our colleagues. we strive to be world-class in investing in our people and commit to developing you as a professional. the role khan academy is partnering with safeinsights to provide researchers access to student learning data in a secure containerized system that strongly protects student privacy. we are looking for a senior data engineer for a 12 month fixed-term to implement creation of the safeinsight container, etl processes, and tools. we are looking for a curious, passionate individual who can apply their strong technical skills to this area and work collaboratively with our data architect, the data infrastructure team, and our data insights group to drive the technical success of the implementation effort. the role • collaborate with our data insights group to understand the desired end data product and the overall projects goals • collaborate with our data architect and data infrastructure team to understand available data source and tooling • propose tooling and processes that will work with extant khan academy systems to meet the projects goals • build test and refine these tools and processes • deliver the safeinsights container and associated project deliverables • provide documentation and training required to maintain and enhance the above what you bring • 5+ years of experience in data engineering within a production environment • experience with google infrastructure services and data services particularly gcs & big query • advanced knowledge of python • strong sql skills • demonstrated proficiency with docker and kubernetes • experience with airflow • background in applied math, statistics, economics, or a related technical field • motivated by the khan academy mission “to provide a free world-class education for anyone, anywhere."" • proven cross-cultural competency skills demonstrating self-awareness, awareness of other, and the ability to adopt inclusive perspectives, attitudes, and behaviors to drive inclusion and belonging throughout the organization. • knowledge of go is a strong plus • experience in a data science research capacity is a plus perks and benefits we may be a non-profit, but we reward our talented team extremely well! we offer: • competitive salaries • ample paid time off as needed – your well-being is a priority. • remote-first culture - that caters to your time zone, with open flexibility as needed, at times • generous parental leave • an exceptional team that trusts you and gives you the freedom to do your best • the chance to put your talents towards a deeply meaningful mission and the opportunity to work on high-impact products that are already defining the future of education • opportunities to connect through affinity, ally, and social groups • and we offer all those other typical benefits as well: 401(k) + 4% matching & comprehensive insurance, including medical, dental, vision, and life at khan academy we are committed to fair and equitable compensation practices, the well-being of our employees, and our khan community. this belief is why we have built out a robust total rewards package that includes competitive base salaries, and extensive benefits and perks to support physical, mental, and financial well-being. the target salary range for this position is $137,871 - $172,339 usd / $186,306 - $232,883 cad. the pay range for this position is a general guideline only. the salary offered will depend on internal pay equity and the candidate’s relevant skills, experience, qualifications, and job market data. additional incentives are provided as part of the complete total rewards package, in addition to comprehensive medical and other benefits. more about us • sal’s ted talk from 2011 • sal’s ted talk from 2015 • sal's ted talk from 2023 • our team: http://www.khanacademy.org/about/the-team our company values live & breathe learners we deeply understand and empathize with our users. we leverage user insights, research, and experience to build content, products, services, and experiences that our users trust and love. our success is defined by the success of our learners and educators. take a stand as a company, we have conviction in our aspirational point of view of how education will evolve. the work we do is in service to moving towards that point of view. however, we also listen, learn and flex in the face of new data, and commit to evolving this point of view as the industry and our users evolve. embrace diverse perspectives we are a diverse community. we seek out and embrace a diversity of voices, perspectives and life experiences leading to stronger, more inclusive teams and better outcomes. as individuals, we are committed to bringing up tough topics and leaning into different points of view with curiosity. we actively listen, learn and collaborate to gain a shared understanding. when a decision is made, we commit to moving forward as a united team. work responsibly and sustainably we understand that achieving our audacious mission is a marathon, so we set realistic timelines and we focus on delivery that also links to the bigger picture. as a non-profit, we are supported by the generosity of donors as well as strategic partners, and understand our responsibility to our finite resources. we spend every dollar as though it were our own. we are responsible for the impact we have on the world and to each other. we ensure our team and company stay healthy and financially sustainable. bring out the joy we are committed to making learning a joyful process. this informs what we build for our users and the culture we co-create with our teammates, partners and donors. cultivate learning mindset we believe in the power of growth for learners and for ourselves. we constantly learn and teach to improve our offerings, ourselves, and our organization. we learn from our mistakes and aren’t afraid to fail. we don't let past failures or successes stop us from taking future bold action and achieving our goals. deliver wow we insist on high standards and deliver delightful, effective end-to-end experiences that our users can rely on. we choose to focus on fewer things — each of which aligns to our ambitious vision — so we can deliver high-quality experiences that accelerate positive measurable learning with our strategic partners. we are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, gender, gender identity or expression, national origin, sexual orientation, age, citizenship, marital status, disability, or veteran status. we value diversity, equity, and inclusion, and we encourage candidates from historically underrepresented groups to apply. as part of this commitment, khan academy will ensure that persons with disabilities are provided reasonable accommodations for the hiring process. if reasonable accommodation is needed, please contact careers@khanacademy.org",canada (+4 others),Data Engineer,"['airflow', 'etl', 'python', 'r', 'sql', 'statistics']","['airflow', 'etl', 'python', 'r', 'sql', 'statistics']","US$137,871–US$172,339 a year"
data integration engineer,hexaware technologies,"overview we are seeking a data engineer to build and maintain a modern enterprise data warehouse (edwh) with a focus on python-based transformations, sql server development, and airflow orchestration. key responsibilities develop and maintain data transformation pipelines using python. design and manage edwh models and queries in sql server. build, schedule, and troubleshoot airflow dags with proper dependencies. implement and support medallion architecture (bronze/silver/gold layers). work with aws services for cloud-based data workflows. manage access using ad security groups. support reporting needs with basic power bi knowledge. troubleshoot pipeline, data quality, and performance issues. required skills strong python, sql, and sql server dwh experience. hands-on experience with airflow orchestration. understanding of medallion architecture. familiarity with aws services. knowledge of ad security groups and role-based access. basic power bi experience. experience with git and standard data engineering practices",canada,Data Engineer,"['airflow', 'aws', 'cloud', 'data warehouse', 'power bi', 'python', 'r', 'sql', 'sql server']","['airflow', 'aws', 'cloud', 'data warehouse', 'power bi', 'python', 'r', 'sql', 'sql server']",
data engineer with master data management,synechron,"we are at synechron, we believe in the power of digital to transform businesses for the better. our global consulting firm combines creativity and innovative technology to deliver industry-leading digital solutions. synechron’s progressive technologies and optimization strategies span end-to-end artificial intelligence, consulting, digital, cloud & devops, data, and software engineering, servicing an array of noteworthy financial services and technology firms. through research and development initiatives in our finlabs we develop solutions for modernization, from artificial intelligence and blockchain to data science models, digital underwriting, mobile-first applications and more. over the last 20+ years, our company has been honored with multiple employer awards, recognizing our commitment to our talented teams. with top clients to boast about, synechron has a global workforce of 14,500+, and has 58 offices in 21 countries within key global markets. our challenge we are seeking a seasoned senior data engineer with extensive experience in mdm implementations, preferably within the financial services or investment banking sector. the ideal candidate will have a proven track record of architecting end-to-end mdm solutions, evaluate various platforms, and ensuring data integrity across complex reference data domains. additional information* the base salary for this position will vary based on geography and other factors. in accordance with law, the base salary for this role if filled within mississauga, on is cad $115k – cad $130k/year & benefits (see below). the role responsibilities: • lead the design, development, and implementation of end-to-end master data management (mdm) solutions to meet organizational needs. • present and articulate architecture designs for mdm solutions to technical teams and stakeholders. • conduct comparative analysis of different mdm platforms, providing strategic recommendations aligned with business objectives. • collaborate closely with data governance, data quality, and business teams to define reference data standards. • develop and maintain scalable etl/elt workflows utilizing python, data bricks, azure data factory (adf), apache airflow, and other tools. • integrate mdm solutions with data lakes, warehouses, and other analytical platforms like snowflake. • design and implement data pipelines leveraging kafka for real-time data integration. • ensure adherence to best practices in ci/cd pipelines for seamless deployment and version control. • proactively troubleshoot, optimize, and document data workflows. • maintain strong communication with technical teams, stakeholders, and project managers to ensure alignment and clarity. requirements: • 8+ years of experience (several implementations) in mdm projects, with preferable exposure in the financial/investment banking sector. • strong ability to architect and present comprehensive mdm solutions. • deep understanding of reference data management and governance. • expertise with mdm platform comparisons (e.g., informatica, reltio, talend, or similar). • proficiency in python, data bricks, azure data factory, sql, kafka, snowflake, and apache airflow. • familiarity with ci/cd tools and practices (azure devops, jenkins, etc.). • excellent problem-solving and analytical skills. • strong communication and stakeholder management skills. preferred, but not required: • bachelor’s or master’s degree in computer science, data engineering, or related field. • prior experience working in financial services, investment banking, or related domains. • certifications related to cloud platforms (azure, aws), data management, or mdm solutions. we offer: • a multinational organization with 58 offices in 21 countries and the possibility to work abroad. • 15 days (3 weeks) of paid annual leave plus an additional 10 days of personal leave(floating days and sick days). • a comprehensive insurance plan including medical, dental, vision, life insurance, and long-term disability. • flexible hybrid policy. • rrsp with employer’s contribution up to 4%. • a higher education certification policy. • on-demand udemy for business for all synechron employees with free access to more than 5000 curated courses. • coaching opportunities with experienced colleagues from our financial innovation labs (finlabs) and center of excellences (coe) groups. • cutting edge projects at the world’s leading tier-one banks, financial institutions and insurance firms. • a truly diverse, fun-loving and global work culture. s​ynechron’s diversity & inclusion statement diversity & inclusion are fundamental to our culture, and synechron is proud to be an equal opportunity workplace and is an affirmative action employer. our diversity, equity, and inclusion (dei) initiative ‘same difference’ is committed to fostering an inclusive culture – promoting equality, diversity and an environment that is respectful to all. we strongly believe that a diverse workforce helps build stronger, successful businesses as a global company. we encourage applicants from across diverse backgrounds, race, ethnicities, religion, age, marital status, gender, sexual orientations, or disabilities to apply. we empower our global workforce by offering flexible workplace arrangements, mentoring, internal mobility, learning and development programs, and more. all employment decisions at synechron are based on business needs, job requirements and individual qualifications, without regard to the applicant’s gender, gender identity, sexual orientation, race, ethnicity, disabled or veteran status, or any other characteristic protected by law. candidate application notice",canada,Data Engineer,"['airflow', 'aws', 'azure', 'cloud', 'data lake', 'data pipeline', 'elt', 'etl', 'excel', 'kafka', 'python', 'r', 'recommendation', 'scala', 'snowflake', 'sql']","['airflow', 'aws', 'azure', 'cloud', 'data lake', 'data pipeline', 'elt', 'etl', 'excel', 'kafka', 'python', 'r', 'recommendation', 'scala', 'snowflake', 'sql']",
"manager, data engineer (ontario canada)",wavestone north america,"be part of a global consulting powerhouse, partnering with clients on their most critical strategic transformations. we are wavestone. energetic, solution-driven experts who focus as much on people as on performance and growth. hand in hand, we share a deep desire to make a positive impact. we are an ambitious firm with a worldwide reach and an ever-expanding portfolio of clients, topics, and projects. in north america, wavestone operates from hubs in new york city, pittsburgh, dallas and toronto. we work closely with ceos and technology leaders to optimize it strategy, sourcing models, and business processes and are committed to building lasting partnerships with our clients. are you a true team player, living strong values? are you a passionate learner, aiming to grow every day? are you a driven go-getter, tackling challenges head-on? then we could be the right fit for you. join wavestone and thrive in an environment that’s empowering, collaborative, and full of opportunities to turn today’s challenges into tomorrow’s solutions – contributing to one or more of our core 4 capabilities: business consulting | business strategy & transformation, organizational effectiveness & change management, operating model design & agility, program leadership & project management, marketing, innovation, & customer experience technology consulting | it strategy & cto advisory, technology delivery, data & artificial intelligence, software & application: development & integration, sap consulting, cybersecurity | cyber transformation remediation, cyber defense & recovery, digital identity, audit & incident response, product & industrial cybersecurity sourcing & service optimization | global services strategy, it & business process services outsourcing, global in-house center support, services optimization, sourcing program management read more at www.wavestone.com job description as a manager, data engineer at wavestone, you will be expected to help address strategic as well as detailed client needs, specifically serving as a trusted advisor to c-level executives and be comfortable supporting and leading hands‑on data projects with technical teams. in this role you would be leading or supporting high‑impact data transformation, data modernization and data initiatives to accelerate and enable ai solutions, bridging business strategy and technical execution. you will architect and deliver robust, scalable data solutions, while mentoring teams and helping to shape the firm’s data consulting offerings and skills. this role requires a unique blend of strategic vision, technical depth, and consulting leadership. key responsibilities • lead complex client engagements in data engineering, analytics, and digital transformation, from strategy through hands‑on implementation. • advise c‑level and senior stakeholders on data strategy, architecture, governance, and technology adoption to drive measurable business value. • architect and implement enterprise‑scale data platforms, pipelines, and cloud‑native solutions (azure, aws, snowflake, databricks, etc.). • oversee and optimize etl/elt processes, data integration, and data quality frameworks for large, complex organizations. • translate business objectives into actionable technical road maps, balancing innovation, scalability, and operational excellence. • mentor and develop consultants and client teams, fostering a culture of technical excellence, continuous learning, and high performance. • drive business development by shaping proposals, leading client pitches, and contributing to thought leadership and market offerings. • stay at the forefront of emerging technologies and industry trends in data engineering, ai/ml, and cloud platforms. • strategic data leadership: proven ability to set and execute data strategy, governance, and architecture at the enterprise level. • advanced data engineering: deep hands‑on experience designing, building, and optimizing data pipelines and architectures (python, sql, spark, databricks, snowflake, azure, aws, etc.). • designing data models: experience creating conceptual, logical, and physical data models that leverage different data modeling concepts and methodologies (normalization/denormalization, dimensional typing, data vault methodology, partitioning/embedding strategies, etc.) to meet solution requirements. • cloud data platforms: expertise in architecting and deploying solutions on leading cloud platforms (azure, aws, gcp, snowflake). • data governance & quality: mastery of data management, mdm, data quality, and regulatory compliance (e.g., ifrs17, gdpr). • analytics & ai enablement: experience enabling advanced analytics, bi, and ai/ml initiatives in complex environments. • executive stakeholder management: ability to communicate and influence at the c‑suite and senior leadership level. • project & team leadership: demonstrated success managing project delivery, budgets, and cross‑functional teams in a consulting context. • continuous learning & innovation: commitment to staying ahead of industry trends and fostering innovation within teams. qualifications education • bachelor’s or master’s degree in computer science, engineering, data science, or related field. required experience • 8+ years of experience in data engineering, data architecture, or analytics consulting, with at least 2 years in a leadership or management role. • demonstrated success in client‑facing roles, ideally within a consulting or professional services environment. • advanced proficiency in python, sql, and modern data engineering tools (e.g., spark, databricks, airflow). • experience with cloud data platforms (azure, aws, gcp, snowflake). • relevant certifications (e.g., aws certified data analytics, azure data engineer, databricks, snowflake) are a strong plus. • exceptional problem‑solving, analytical, and communication skills. • ability to travel based. • industry exposure: deep experience in insurance, pharma, or financial services. our commitment: wavestone’s positive way at wavestone, we believe our employees are our greatest ambassadors. by embodying our shared values, vision, mission, and corporate brand, you'll become a powerful force for positive change. we are united by a shared commitment to making a positive impact, no matter where we are. this is better defined by our value base, ""the positive way,"" which serves as the glue that binds us together: • energetic – a positive attitude gives energy to lead projects to success. while we may not control the circumstances, we can always choose how we respond to them. • responsible – we act with integrity and take ownership of our decisions and actions, considering their impact around us. • together – we want to be a great team, not a team of greats. the team’s strength is each individual member, each member’s strength is the team. we are energetic, responsible and together! • participation in corporate bonus program – base 15% for achieving baseline targets, paid at the end of june. • matching employee contributions to retirement plan (401k or rrsp) to a maximum of 4% of employee’s base salary • 3 weeks paid vacation (ac/c) or 4 weeks paid vacation (sc and above) per calendar year (or pro rata for employees starting after january 1st) • vacation should be used within the calendar year with limited options to roll over remaining days into the first 3 months of the following year • 10 days off for public holidays • 5 sick days per year • 5 training days available for personal advancement • company laptop and phone (transitioning from company provided iphone to byod program that incorporates a monthly usage payment) • allowance to promote ergonomic and effective work from home infrastructure – dependent on individual needs • enrollment in ohip supplemental plan that seeks to align benefit options (where it makes sense and where economically practical) with us team e.g. dental, vision, ltd • for employees who are required to work in a location with a different tax authority, wavestone will support the filing and advance payment of any tax obligations – any tax refunds directly attributable from wavestone payments should returned to wavestone when received by the employee. travel and location this full‑time position is based in the province of ontario, canada. travel requirements tend to fluctuate depending on project and client needs. additional information wavestone is an equal opportunity employer. we embrace diversity as a core component of our culture. our collective success depends heavily on the recruitment and inclusion of qualified professionals, regardless of individual characteristics such as race, ancestry, religion, color, sex, age, national origin, sexual orientation, gender identity, disability, veteran’s status, or any characteristic protected by law.",canada,Data Engineer,"['airflow', 'aws', 'azure', 'cloud', 'data analytics', 'data pipeline', 'databricks', 'elt', 'etl', 'excel', 'gcp', 'python', 'r', 'scala', 'snowflake', 'spark', 'sql']","['airflow', 'aws', 'azure', 'cloud', 'data analytics', 'data pipeline', 'databricks', 'elt', 'etl', 'excel', 'gcp', 'python', 'r', 'scala', 'snowflake', 'spark', 'sql']",$100K–$130K a year
"co-op student , data engineer",ht0001 home trust company,"the data engineering team is responsible for the overall development and support of home trust data lake house initiatives to improve information integrity and trust. using governed data and best-in-class technologies to determine market and business trends to increase profit and efficiency. the coop student (data engineer), will help our teamwork through various tasks and technical challenges to sustain a high level of customer satisfaction using industry-leading technologies. key responsibilities (minimum weighting = 10%) assist with various documentation tasks including meta data in databricks lake house. using ai to assist with tuning / testing of databricks business consumer sql queries. assist team with testing data pipeline development jobs in databricks /harp assist with maintaining relevant documentation in compliance with regulatory guidelines and audit requirements perform all tasks and duties under the direction of management to ensure compliance with regulatory and policy standards education requirement: enrolled and successfully completed 1st year of post-secondary education computer science/math/engineering/data science/statistics program preferred years of experience: financial services experience is an asset additional qualifications, certifications, skills: takes initiative to work effectively/independently within established guidelines strong organizational, time-management and listening skills analytical and detail oriented effective written/verbal communication adaptable within a fast-paced, deadline-driven environment knowledge in ms word and strong knowledge in ms excel knowledge in python (must have) knowledge in sql we are proud to be recognized as great place to work® canada for 2025 and montreal's top employers 2025 by canada’s top 100 employers! learn more: https://www.fairstone.ca/en/about/canadian-lender follow us on linkedin: https://www.linkedin.com/company/fairstone/mycompany/ if you have a preferred language for communication, please kindly inform us whether you prefer french or english on your application. fairstone is an equal opportunity employer. accordingly, we will make reasonable accommodations to respond to the needs of people with disabilities. individuals who view themselves as aboriginals, members of visible minorities, and disabled are encouraged to apply in confidence. time type: full time job type: intern (fixed term) (trainee) recognized as one of montreal's top employers (2025) fairstone bank of canada and its subsidiaries, including fairstone financial inc. and home trust company, deliver innovative, accessible and reliable financial solutions that enable canadians to reach their financial goals. collectively, we offer residential and commercial mortgages, consumer deposits and gics, retail and automobile financing, credit cards and digital lending, in addition to unsecured and secured personal loans online and at more than 255 branches coast to coast. with a long-established history, we are proud to be canada's leading alternative lending bank. more at fairstonebank.ca",canada,Data Engineer,"['data lake', 'data pipeline', 'databricks', 'excel', 'python', 'r', 'sql', 'statistics']","['data lake', 'data pipeline', 'databricks', 'excel', 'python', 'r', 'sql', 'statistics']",
big data engineer (remote)- contract,hirevouch,"senior data engineer - contract our client is leading global software consulting company. they are looking for a senior data engineer to join their team. in this role, the engineer will design and implement modern data pipelines, integrate advanced machine learning workflows, and enable scalable solutions across snowflake and aws environments. the successful candidate will collaborate with cross‑functional teams to deliver high‑impact projects, including initiatives that leverage large language models (llms) for “talk to data” applications. data pipeline development — build and optimize etl/elt pipelines to move and transform data across snowflake and aws. llm integration — partner with ml engineers to design workflows that connect structured data with llm applications. cloud deployment — architect and manage scalable data solutions using aws services (glue, lambda, s3, redshift, etc.). python engineering — write clean, modular python code for data ingestion, transformation, and orchestration. data governance — ensure data quality, lineage, and security across all pipelines and integrations. collaboration — work closely with client stakeholders, ml engineers, and product teams to align technical solutions with business needs. innovation — explore emerging tools and frameworks to enhance “talk to data” capabilities and improve efficiency. strong proficiency in snowflake for data engineering and analytics. ~ hands‑on experience with python for data workflows and automation. ~ proven expertise in aws cloud services for data engineering. ~ exposure to llm development and integration into data pipelines. ~5+ years of experience in data engineering or related roles. ~ ability to operate independently and lead technical initiatives in client environments. prior experience building llm‑powered applications , especially “talk to data” solutions. familiarity with ml frameworks (pytorch, tensorflow, hugging face). consulting or client‑facing experience in data engineering projects.",canada,Data Engineer,"['aws', 'cloud', 'data pipeline', 'elt', 'etl', 'machine learning', 'python', 'pytorch', 'r', 'redshift', 'scala', 'snowflake', 'tensorflow']","['aws', 'cloud', 'data pipeline', 'elt', 'etl', 'machine learning', 'python', 'pytorch', 'r', 'redshift', 'scala', 'snowflake', 'tensorflow']",
sr data engineer,hays,"role: sr. data engineer type: 6-month contract (possibility of extension) location: remote in canada (est hrs) your new company join a major canadian retailer that operates a wide range of grocery, pharmacy, health and beauty, apparel, and financial services across the country. your new role: as a sr. data engineer, you will impact business by fueling the best in class data analytic and reporting platform that will empower our business users in making data driven decisions and strategies in their everyday work. growing bi team to better serve the enterprise data strategy and want you to be a part of it. you’ll work with users across the organization to understand how data helps them and translate that into solutions that will bring data from various systems into our data warehouse. we’re looking to turn our enviable wealth of data into actionable insights and meaningful recommendations that drive growth and improved engagement across all of our lines of business, and a customer-focused mindset is a must-have. responsibilities: ● work closely with business stakeholders to understand business needs, perspectives and use cases; translate them into data solutions ● build automated and scalable reporting solutions to enable business users to efficiently consume data insights ● build robust reporting data models and semantic layer using looker and bigquery ● develop pipelines using python, dbt, airflow, and other gcp technologies ● understand company data and actively support and enforce data processes and standards what you’ll need to succeed ● 5+ years of experience working in a bi developer role ● strong sql skills and ability to optimize queries ● knowledge of bi concepts and data modelling ● hands on experience with advanced features in looker ● experience in data processing with dbt and python ● working experience with git ● exposure to cloud technology (gcp, aws) ● understanding of common business kpis ● analytical thinking and open mindset are must-haves ● strong business communication and stakeholder management skills #li-dni #1122339 - shivangi gupta",canada,Data Engineer,"['airflow', 'aws', 'bigquery', 'cloud', 'data warehouse', 'dbt', 'gcp', 'looker', 'python', 'r', 'recommendation', 'scala', 'sql']","['airflow', 'aws', 'bigquery', 'cloud', 'data warehouse', 'dbt', 'gcp', 'looker', 'python', 'r', 'recommendation', 'scala', 'sql']",
bi data engineer,insight,"requisition number: 103088 bi data engineer location: this is a remote work opportunity salary: $140,000 annually insight at a glance • 14,000+ engaged teammates globally • #20 on fortune’s world's best workplaces™ list • $9.2 billion in revenue • received 35+ industry and partner awards in the past year • $1.4m+ total charitable contributions in 2023 by insight globally now is the time to bring your expertise to insight. we are not just a tech company; we are a people-first company. we believe that by unlocking the power of people and technology, we can accelerate transformation and achieve extraordinary results. as a fortune 500 solutions integrator with deep expertise in cloud, data, ai, cybersecurity, and intelligent edge, we guide organisations through complex digital decisions. about the role as a data engineer at sada, you will work collaboratively with key business leads, architects, analysts, and data scientists to understand the business domain and how data can empower them. • you will engage with fellow engineers to develop and optimize data platforms, focusing on business intelligence and data engineering to ensure seamless data flows and insightful reporting. • you will have the opportunity to propose, define, and develop metrics for business reporting and product/process performance monitoring to drive decision-making for our customers. engagements vary from consultative to hands-on work and cover a range of domain areas, including bi tool migrations, data pipeline engineering, and big data analytics solutions. • you will be expected to run point on whole projects, end-to-end, and to mentor less experienced data engineers. • you will be recognized as an expert within the team and will build a reputation with google and our customers. • you will demonstrate repeated delivery of project architectures and critical components that other engineers demur to you for lack of expertise. • you will also participate in early-stage opportunity qualification calls, as well as guide client-facing technical discussions for established projects. be ambitious: this opportunity is not just about what you do today but also about where you can go tomorrow. when you bring your hunger, heart, and harmony to insight, your potential will be met with continuous opportunities to upskill, earn promotions, and elevate your career. what we’re looking for • 4+ year of experience in a business intelligence role, including data warehousing, business intelligence tools, data visualization tools, and data engineering technologies • experience with upgrading and migrating legacy bi tools to data visualization tools like tableau, looker, etc. • experience gathering business requirements, using business intelligence tools to extract data, formulate metrics, and build reports • experience in designing and delivering cross-functional custom reporting solutions • proficiency with enterprise bi tools like looker (required), microstrategy, tableau, powerbi, obiee, domo, superset etc • experience in cloud-based data engineering, including building and managing data pipelines using gcp services or similar (e.g., databricks, dbt) • experience embedding bi reports into customer portals • experience developing automated functionality by interacting with bi tools programmatically using sdks what you can expect we’re legendary for taking care of you, your family and to help you engage with your local community. we want you to enjoy a full, meaningful life and own your career at insight. some of our benefits include: join us today, your ambitious journey starts here. insight is an equal opportunity employer, and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, sexual orientation or any other characteristic protected by law. when you apply, please tell us the pronouns you use and any reasonable adjustments you may need during the interview process. at insight, we celebrate diversity of skills and experience so even if you don’t feel like your skills are a perfect match - we still want to hear from you! the position described above provides a summary of some the job duties required and what it would be like to work at insight. for a comprehensive list of physical demands and work environment for this position, click here. insight is an equal opportunity employer, and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status, sexual orientation or any other characteristic protected by law. posting notes: remote || california (us-ca) || united states (us) || it infrastructure & support || none || us - los angeles, ca ||",canada,Data Engineer,"['bi tools', 'business intelligence', 'cloud', 'data analytics', 'data pipeline', 'databricks', 'dbt', 'gcp', 'looker', 'r', 'superset', 'tableau']","['bi tools', 'business intelligence', 'cloud', 'data analytics', 'data pipeline', 'databricks', 'dbt', 'gcp', 'looker', 'r', 'superset', 'tableau']",
"senior data engineer - £80,000 - hybrid in watford",energy jobline zr,"energy jobline is the largest and fastest growing global energy job board and energy hub. we have an audience reach of over 7 million energy professionals, 400,000+ monthly advertised global energy and engineering jobs, and work with the leading energy companies worldwide. we focus on the oil & gas, renewables, engineering, power, and nuclear markets as well as emerging technologies in ev, battery, and fusion. we are committed to ensuring that we offer the most exciting career opportunities from around the world for our jobseekers. job descriptionsenior data engineer - £85,000 - hybrid about the role we are seeking a skilled senior data engineer to help shape and deliver our data and mi reporting strategy. you'll work closely with the cto, data & reporting manager, and a team of four engineers to build, optimise, and support high-quality data models, pipelines, and reports across the business. key responsibilities • define and implement short- and long-term data & mi reporting strategies. • work with product owners, developers, designers, devops, and business stakeholders. • develop and maintain mi/bi reports and dashboards (ideally in quicksight). • build, test, and optimise etl/elt processes using aws glue, python, and sql. • analyse complex reporting requirements and deliver scalable solutions. • review and validate report accuracy and data integrity. • recommend improvements to reporting processes and standards. • mentor junior team members and support the resolution of data/reporting issues. requirements • 4+ years' experience as a data engineer or similar role. • strong sql skills; extensive experience with amazon redshift (and ideally mysql). • experience with data visualisation tools (preferably amazon quicksight). • proficient in aws glue and python for etl. • strong data modelling and dashboard development skills. • experience working in an agile environment. • good communication, mentoring ability, and a proactive, collaborative approach. • familiarity with jira and confluence. • curious, self-motivated, and comfortable learning new technologies. • nice to have: knowledge of sap business objects. to apply for this role please submit your cv or contact dillon blackburn on 0191 255 1428 or at d.blackburn@tenthrevolution.com. tenth revolution group are the go-to recruiter for data & ai roles in the uk offering more opportunities across the country than any other recruitment agency. we're the proud sponsor and supporter of sqlbits, power platform world tour, and the london fabric user group. we are the global leaders in data & ai recruitment. if you are interested in applying for this job please press the apply button and follow the application process. energy jobline wishes you the very best of luck in your next career move.",canada,Data Engineer,"['aws', 'dashboard', 'elt', 'etl', 'python', 'r', 'redshift', 'scala', 'sql']","['aws', 'dashboard', 'elt', 'etl', 'python', 'r', 'redshift', 'scala', 'sql']",
senior data engineering lead,receptive,"description the company receptive is a modern telehealth platform for emotional wellness and mental clarity. since our founding, we’ve connected over a million individuals with the right mental health support at the right time. licensed in all 50 states and serving over 100,000 customers each year, we partner with pettable for esa evaluations and adhd advisor for comprehensive adhd care. grounded in clinical excellence and a people-first mission, we empower licensed professionals with the autonomy and tools they need to make mental health care more accessible and impactful. the position we’re looking for an analytics engineer, you’ll turn messy source data into trustworthy, well-modeled datasets and metrics that power dashboards, product decisions, and ops. we run a lean, warehouse-centric elt stack and care about reliability, clarity, and speed. key responsibilities • own the warehouse modeling layer (dbt or sql): build clean, tested marts (star/snowflake schemas), reusable macros, and clear docs. • run elt: manage connectors (e.g., fivetran/airbyte), handle schema changes, backfills, and historical re-syncs. • define and enforce data quality: dbt tests, anomaly checks; triage and fix issues at the source or in transforms. • create a governed metrics layer: standardize kpis (definitions, owners, acceptance tests) for analytics and product teams. • orchestrate & ship: set up ci/cd for transforms (e.g., github actions) and light orchestration (prefect/dagster/airflow optional). • partner with stakeholders: translate questions into durable datasets; enable analysts to move faster with less ad-hoc wrangling. • tune for reliability & cost: materializations, indexes/cluster keys, caching, and sensible scheduling to keep queries fast and bills sane. • document everything: data contracts, lineage, and playbooks for incidents, backfills, and onboarding. skills, knowledge and expertise • 5+ years relevant industry experience in a data engineering role and graduate degree in computer science, statistics, informatics, information systems or another quantitative field. • proficiency in writing sql queries and knowledge of cloud-based databases like snowflake, redshift, bigquery or other big data solutions. • experience in data modelling, etl processes, and data warehousing. • experience with python and data pipeline tools such as airflow. • experience with version control systems like github / gitlab. compensation range: $130k - $140k",canada,Data Engineer,"['airflow', 'bigquery', 'cloud', 'dashboard', 'data pipeline', 'dbt', 'elt', 'etl', 'excel', 'python', 'r', 'redshift', 'snowflake', 'sql', 'statistics']","['airflow', 'bigquery', 'cloud', 'dashboard', 'data pipeline', 'dbt', 'elt', 'etl', 'excel', 'python', 'r', 'redshift', 'snowflake', 'sql', 'statistics']",$130K–$140K a year
staff data engineer,jobber,"does working with data motivate and excite you? do you want to make a difference cross functionally? then jobber might be the place for you! we’re looking for a new staff data engineer to join our data platform team. jobber exists to help people in small businesses be successful. we work with small home service businesses, like your local plumbers, painters, and landscapers, to transform the way service is delivered through technology. with jobber they can quote, schedule, invoice, and collect payments from their customers, while providing an easy and professional customer experience. running a small business today isn’t like it used to be—the way we consume and deliver service is changing rapidly, technology is evolving, and customers expect more. that’s why we put the power and flexibility in their hands to run their businesses how, where, and when they want! our culture of transparency, inclusivity, collaboration, and innovation has been recognized by great place to work, canada’s most admired corporate cultures, and more. jobber has also been named on the globe and mail’s canada’s top growing companies list, and deloitte canada’s technology fast 50™, enterprise fast 15, and technology fast 500™ lists. with an executive team that has over thirty years of industry experience of leading the way, we’ve come a long way from our first customer in 2011—but we’ve just scratched the surface of what we want to accomplish for our customers. we help employees grow professionally; we have a ton of onboarding resources, tutorials, hackathons and buddies to support learnings and provide opportunities to innovate. we have a range of experience levels on teams which allows for mentor/mentee opportunities. leaders at jobber work with empathy and support employees to build healthy work-life harmony. bring your dedication and passion to this job to fulfill your goals. the team: the data platform team is responsible for managing the critical data stores and systems in which enable orchestration and transformation of data to power; analytics, machine learning and reporting at jobber. data platform empowers teams across the organization to fully leverage data, tools, and technology to achieve their goals while ensuring that we uphold high data quality standards and governance. you’ll research, develop and maintain data systems and provide essential operational and analytical support to ensure jobber’s internal teams are set up for success. the role: as a staff data engineer at jobber, you will play a critical role in shaping the future of our data platform. as a technical champion and force multiplier, you’ll lead and mentor a team of exceptional data engineers while solving complex technical challenges. your expertise will span architecture, technical leadership, design, and hands-on coding, enabling you to significantly influence the direction of data at jobber. beyond day-to-day delivery, you will dedicate time to work acceleration, cross-team initiatives, exploration of emerging technologies, addressing technical debt, and investing in the future of engineering at jobber. this is a highly strategic and hands-on position where you’ll combine deep technical expertise with leadership to design resilient systems, empower teams across the organization to self-serve with confidence, and ensure our data remains a trusted asset that accelerates business growth. as a staff data engineer, you will: • shape foundational data components: design, build, and maintain scalable batch and real-time data pipelines, while also looking across systems and ahead to anticipate future needs. • demonstrate technical mastery: deliver high-quality solutions through deep expertise in modern data tools and technologies. champion technical excellence within the team by setting best practices, raising the bar for engineering quality, and mentoring team members at all levels to support their growth and career development. • drive reliability & resilience: establish testing and reliability standards, slas (uptime, rto, rpo), and disaster recovery playbooks. lead major reliability initiatives to minimize downtime and protect critical business data. • advance observability & governance: build frameworks for monitoring, logging, lineage, and auditing to ensure visibility, compliance, and trust in data. define governance policies that enforce data integrity, availability, and reliability across the platform. • accelerate and empower data access: develop self-service tools, frameworks, and automation that reduce manual effort, improve efficiency, and enable teams across engineering, analytics, and data science to work effectively with data while minimizing dependency on the data platform team. • contribute to strategic planning: partner with technical program managers to define and refine strategic roadmaps, ensuring that data engineering priorities align with business objectives. • drive cross-team collaboration: collaborate with staff engineers and technical leaders across domains to identify friction points, and work collectively to design solutions that improve system reliability, consistency, and scalability. • accelerate business growth: work closely with data analysts, scientists, and product teams to enable fast, seamless exploration, analysis, modeling, and reporting. build automation and infrastructure that reduce friction and accelerate decision-making. • safeguard data integrity: own the integrity and reliability of data, ensuring stakeholders across the organization maintain trust in the insights and decisions driven by it. to be successful, you should have: • core data engineering expertise: hands-on experience with batch and real-time data processing frameworks, lakehouse/warehouse management, large-scale data transformation, data serialization, workflow orchestration and dimensional modeling (star/snow-flake schemas) • scalable systems development: proven ability to design and deliver highly scalable, maintainable, and high-performance solutions across multiple layers of the technology stack, leveraging containerization, ci/cd, and api development. • aws cloud proficiency: strong understanding of aws services relevant to the data domain, with hands-on experience leveraging them to design and implement data solutions. • technical leadership & engineering excellence: demonstrated success guiding teams through complex, high-impact projects while providing architectural direction and serving as a trusted technical lead. exceptional proficiency in software design, system architecture, and coding, with a focus on long-term maintainability, performance, and resilience. • reliability & devops practices: strong background in infrastructure-as-code (terraform, cloudformation), observability (logging, monitoring, tracing), and system reliability. • collaboration & adaptability: exceptional communication skills, self-motivation, and resourcefulness, with the ability to navigate ambiguity, prioritize effectively, and deliver results in fast-paced environments. it would be really great (but not a deal-breaker) if you had: • machine learning platform experience: exposure to ml platforms and distributed compute frameworks (e.g., ray, tensorflow, pytorch). experience collaborating with data scientists to operationalize models, implement drift detection, or scale ml workloads. • cross-domain engineering experience: hands-on exposure to non–data engineering codebases, such as web application frameworks (ruby on rails) and modern front-end stacks (typescript/react). • api & integration knowledge: familiarity with graphql, api layer design, and performance optimization. • platform building experience: prior work on developer tooling or shared platforms that supported multiple engineering domains. • governance & compliance awareness: knowledge of data privacy, security, and compliance in cloud-based data environments. • knowledge of data privacy, security, and compliance considerations in a cloud-based data environment. compensation: at jobber, we also believe that compensation should be transparent, fair, and supportive of your experience and growth. this role has a minimum annual salary of $145,900 cad, a midpoint of $171,600 cad and a maximum salary of $197,400 cad, designed to reflect the progression from learning the ropes to truly excelling. we design our compensation to reflect each new hire's skills, experience, and the complexity of the role, ensuring a fair and competitive salary. our range is intentionally broad to support growth and long-term impact, with fully established hires typically starting around the midpoint. the higher end of the range is reserved for those who have demonstrated deep expertise and lasting contributions, while offers below the midpoint reflect strong potential with room to develop. this approach ensures that compensation aligns with both an individual's current capabilities and their opportunity for future growth. base salary is just one part of a total compensation package that will include equity rewards, annual stipends for health and wellness, retirement savings matching, and an extended health package with fully paid premiums for body and mind. your professional growth matters to us too! you'll have access to a dedicated talent development program that includes career coaching and opportunities for career development. we believe in transparency and open conversations about compensation. if you have any questions about our approach, we're happy to discuss them throughout the hiring process! what you can expect from jobber: • a total compensation package that includes an extended health benefits package with fully paid premiums for both body and mind, retirement savings plan matching, and stock options. • a dedicated talent development team and access to coaching, learning, and leadership programs to help you grow your career, reach your goals, and unlock your full potential. • support for all your breaks: from vacation to rest and recharge, your birthday off to celebrate, health days to support your physical and mental health, and parental leave top-ups to support your growing family. • a unique opportunity to build, grow, and leave your impact on a $400-billion industry that has no dominant player...yet. • to work with a group of people who are humble, supportive, and give a sh*t about our customers. we believe that diverse teams perform better and that fostering an inclusive work environment is a key part of growing a successful team. we welcome people of diverse backgrounds, experiences, and perspectives. we are an equal opportunity employer, and we are committed to working with applicants requesting accommodation at any stage of the hiring process. a bit more about us: job by job, we’re transforming the way service is delivered. your lawn care provider, home cleaning service, plumber or painter could use jobber to better connect with their customers, save time in the office, invoice faster, and get paid! we’re bringing tens of thousands of people together with technology to deliver billions of dollars a year in services to happy customers. jobber exists to help make these small businesses successful, and when they’re successful we all win!",canada,Data Engineer,"['aws', 'cloud', 'data pipeline', 'excel', 'machine learning', 'pytorch', 'r', 'sas', 'scala', 'tensorflow']","['aws', 'cloud', 'data pipeline', 'excel', 'machine learning', 'pytorch', 'r', 'sas', 'scala', 'tensorflow']",
data & ai engineer - sr data engineer,capgemini engineering,"about the job you’re considering name of the position: sr .data engineer(python & spark) reports to: team lead/delivery manager department/project: engineering as a senior data engineer, you will build distributed data processing solution and highly loaded database solutions for various businesses cases including reporting, product analytics, marketing optimization and financial reporting. contribute as part of self-organized team of experienced data engineers working in a challenging, innovative environment for our client, creating the foundation for decision-making at a company dealing with billions of events per day. investigate, create, and implement the solutions for existing technical challenges. provide guidance, instruction, direction, leadership to a development team with the purpose of achieving project goals. your role • obtains tasks from the project lead or team lead (tl), prepares functional and design specifications, approves them with all stakeholders. • ensures that assigned area/areas are delivered within set deadlines and required quality objectives. • provides estimations, agrees task duration with the manager and contributes to project plan of assigned area. • analyzes scope of alternative solutions and makes decision about area implementation based on his/her experience and technical expertise. • leads functional and architectural design of assigned areas. makes sure design decisions on the project meet architectural and design requirements. • addresses area-level risks, provides and implements mitigation plan. • reports about area readiness/quality, and raises red flags in crisis situations which are beyond his/her aor. • responsible for resolving crisis situations within his/her aor. • design, develop and implement large scale, high volume, high performance data models and pipelines for data lake and data warehouse. • develop and implement data quality checks, conduct qa and implement monitoring routines improve the reliability and scalability of our etl processes • initiates and conducts code reviews, creates code standards, conventions and guidelines. • suggests technical and functional improvements to add value to the product; • constantly improves his/her professional level. • collaborates with other teams. your skills and experience • university degree in computer related sciences or similar. • 5+ years experience working in data engineering, business intelligence, or a similar role proficiency in programming languages python. • expert in database fundamentals, advanced proficiency in complex sql and distributed computing • 5+ years of experience in etl orchestration and workflow management tools like airflow, flink, oozie and azkaban using aws/gcp • experience in spark, snowflake & databricks. • must have ability to debug spark jobs. • experience working with snowflake, redshift, postgresql and/or other dbms platforms. • excellent communication skills and experience working with technical and non-technical teams. • able to clear hacker rank code test. • experience in aws (ec2/s2/iam). • experience working with technical and non-technical teams knowledge of reporting tools such as tableau, superset and looker. • strong python coder with expert data migration experience . life at capgemini capgemini supports all aspects of your well-being throughout the changing stages of your life and career. for eligible employees, we offer: • flexible work • healthcare including dental, vision, mental health, and well-being programs • financial well-being programs such as 401(k) and employee share ownership plan • paid time off and paid holidays • paid parental leave • family building benefits like adoption assistance, surrogacy, and cryopreservation • social well-being benefits like subsidized back-up child/elder care and tutoring • mentoring, coaching and learning programs • employee resource groups • disaster relief about capgemini capgemini is a global business and technology transformation partner, helping organizations to accelerate their dual transition to a digital and sustainable world, while creating tangible impact for enterprises and society. it is a responsible and diverse group of 340,000 team members in more than 50 countries. with its strong over 55-year heritage, capgemini is trusted by its clients to unlock the value of technology to address the entire breadth of their business needs. it delivers end-to-end services and solutions leveraging strengths from strategy and design to engineering, all fueled by its market leading capabilities in ai, generative ai, cloud and data, combined with its deep industry expertise and partner ecosystem. the group reported 2024 global revenues of €22.1 billion. get the future you want | www.capgemini.com disclaimer capgemini is an equal opportunity employer encouraging inclusion in the workplace. all qualified applicants will receive consideration for employment without regard to race, national origin, gender identity/expression, age, religion, disability, sexual orientation, genetics, veteran status, marital status or any other characteristic protected by law. this is a general description of the duties, responsibilities and qualifications required for this position. physical, mental, sensory or environmental demands may be referenced in an attempt to communicate the manner in which this position traditionally is performed. whenever necessary to provide individuals with disabilities an equal employment opportunity, capgemini will consider reasonable accommodations that might involve varying job requirements and/or changing the way this job is performed, provided that such accommodations do not pose an undue hardship. capgemini is committed to providing reasonable accommodations during our recruitment process. if you need assistance or accommodation, please reach out to your recruiting contact. please be aware that capgemini may capture your image (video or screenshot) during the interview process and that image may be used for verification, including during the hiring and onboarding process. click the following link for more information on your rights as an applicant http://www.capgemini.com/resources/equal-employment-opportunity-is-the-law applicants for employment in the us must have valid work authorization that does not now and/or will not in the future require sponsorship of a visa for employment authorization in the us by capgemini.",canada,Data Engineer,"['airflow', 'aws', 'business intelligence', 'cloud', 'data lake', 'data warehouse', 'databricks', 'etl', 'excel', 'gcp', 'looker', 'python', 'r', 'redshift', 'sas', 'scala', 'snowflake', 'spark', 'sql', 'superset', 'tableau']","['airflow', 'aws', 'business intelligence', 'cloud', 'data lake', 'data warehouse', 'databricks', 'etl', 'excel', 'gcp', 'looker', 'python', 'r', 'redshift', 'sas', 'scala', 'snowflake', 'spark', 'sql', 'superset', 'tableau']",
data engineer job at i-cube software llc in remote nationwide,i-cube software llc,"job description job description hybrid hadoop engineer and hadoop infrastructure administrator to build and maintain a scalable and resilient big data framework to support data scientists. as an administrator, your responsibility will be to deploy and maintain hadoop clusters, add and remove nodes using cluster management and monitoring tools like cloudera manager, and support performance and scalability requirements, in support of our data scientist's needs. some relational database administrator experience will also be desirable to support the general administration of relational databases. design, build, and maintain big data workflows/pipelines to process a continuous stream of data with experience in end-to-end design and build process of near-real-time and batch data pipelines. demonstrated work experience in the following with big data and distributed programming models and technologies knowledge of database structures, theories, principles, and practices (both sql and nosql). active development of etl processes using spark or other highly parallel technologies, and implementing etl/data pipelines experience with data technologies and big data tools, like spark, kafka, hive understanding of map reduce and other data query and processing and aggregation models understanding of challenges of transforming data across distributed clustered environment experience with techniques for consuming, holding, and aging out continuous data streams ability to provide quick ingestion tools and corresponding access apis for continuously changing data schema, working closely with data engineers around specific transformation and access needs preferred: experience as a database administrator (dba) will be responsible for keeping critical tools database up and running building and managing high-availability environments for databases and hdfs systems familiarity with transaction recovery techniques and db backup skills and attributes: ability to have effective working relationships with all functional units of the organization excellent written, verbal, and presentation skills excellent interpersonal skills ability to work as part of a cross-cultural team self-starter and self-motivated ability to work without lots of supervision - works under pressure and is able to manage competing priorities. hide",canada,Data Engineer,"['cloud', 'data pipeline', 'etl', 'excel', 'hadoop', 'kafka', 'r', 'scala', 'spark', 'sql']","['cloud', 'data pipeline', 'etl', 'excel', 'hadoop', 'kafka', 'r', 'scala', 'spark', 'sql']",
ey - gds consulting - ai and data - snowflake data engineer - manager,ey,"at ey, we’re all in to shape your future with confidence. we’ll help you succeed in a globally connected powerhouse of diverse teams and take your career wherever you want it to go. join ey and help to build a better working world. job title: manager – data engineer - snowflake as part of our ey-gds d&a (data and analytics) team, we help our clients solve complex business challenges with the help of data and technology. we dive deep into data to extract the greatest value and discover opportunities in key business and functions like banking, insurance, manufacturing, healthcare, retail, manufacturing and auto, supply chain, and finance. roles and responsibilities: • snowflake development: build and optimize snowflake-based data warehouses, including snowprocs and snowsql scripts. • data pipeline management: design and maintain etl/elt workflows for large-scale data ingestion and transformation. • schema design & optimization: architect efficient schemas and manage database performance tuning. • programming & automation: use python and sql for data engineering tasks and automation. • real-time & batch processing: implement solutions for streaming and batch data pipelines. • team leadership: manage and mentor a team of data engineers, ensuring adherence to best practices. • collaboration: partner with analytics and business teams to deliver high-quality data solutions. required skills: • snowflake expertise: advanced knowledge of snowflake dw, snowprocs, snowsql, and performance optimization. • programming: strong proficiency in python and sql. • data processing frameworks: experience with tools like airflow, spark, or similar. • database management: strong understanding of schema design and relational database principles. • pipeline design: expertise in real-time and batch data processing systems. • leadership: proven experience managing data engineering teams and projects. experience : • should have 8+ years of relevant experience. what we look for • a team of people with commercial acumen, technical experience and enthusiasm to learn new things in this fast-moving environment what working at ey offers at ey, we’re dedicated to helping our clients, from startups to fortune 500 companies — and the work we do with them is as varied as they are. you get to work with inspiring and meaningful projects. our focus is education and coaching alongside practical experience to ensure your personal development. we value our employees, and you will be able to control your own development with an individual progression plan. you will quickly grow into a responsible role with challenging and stimulating assignments. moreover, you will be part of an interdisciplinary environment that emphasizes high quality and knowledge exchange. plus, we offer: • support, coaching and feedback from some of the most engaging colleagues around • opportunities to develop new skills and progress your career • the freedom and flexibility to handle your role in a way that’s right for you ey | building a better working world ey is building a better working world by creating new value for clients, people, society and the planet, while building trust in capital markets. enabled by data, ai and advanced technology, ey teams help clients shape the future with confidence and develop answers for the most pressing issues of today and tomorrow. ey teams work across a full spectrum of services in assurance, consulting, tax, strategy and transactions. fueled by sector insights, a globally connected, multi-disciplinary network and diverse ecosystem partners, ey teams can provide services in more than 150 countries and territories.",canada,Data Engineer,"['airflow', 'data pipeline', 'data warehouse', 'elt', 'etl', 'python', 'r', 'snowflake', 'spark', 'sql']","['airflow', 'data pipeline', 'data warehouse', 'elt', 'etl', 'python', 'r', 'snowflake', 'spark', 'sql']",
azure data engineer with postgresql,freelancejobs,"we are seeking an experienced azure data engineer for a 6-month contract to enhance our data management and analytical capabilities. the ideal candidate will have strong expertise in postgresql and cosmos db, with a proven ability to design, implement, and maintain data solutions on azure. must have experience migrating onprem databases like oracle to azure postgresql. you will collaborate with our team to optimize data workflows and ensure seamless data integration and accessibility. if you are passionate about leveraging cloud technologies to drive insights, we want to hear from you contract duration of more than 6 months. with 40 hours per week. mandatory skills: sql, postgresql, microsoft azure, etl pipeline",canada,Data Engineer,"['azure', 'cloud', 'etl', 'r', 'sql']","['azure', 'cloud', 'etl', 'r', 'sql']",US$65–US$75 an hour
data engineer/developer (intermediate–senior) – canada (remote),info-tech research group inc.,"info-tech research group delivers impartial, highly pertinent it research, enabling cios and it leaders to make well-informed, strategic decisions. we are currently serving over 30,000 professionals and collaborate closely with it teams, equipping them with actionable tools and expert guidance to drive measurable results and enhance technology initiatives and organizational processes. why join us? we pride ourselves on consistent year-on-year growth, with double-digit growth even during a global pandemic. we are in the top 3 on linkedin's top companies list in canada. info-tech fosters a growth-focused, entrepreneurial culture with unlimited opportunities for professional growth and development. we provide financial support for professional development and training. about the role we're looking for seasoned intermediate or senior data engineer. experience with ms-fabric and powerbi is mandatory. additional experience with the following is preferred: azure/openai, ruby on rails, python, mysql, js, ms-fabric, powerbi and azure/openai services. your responsibilities as an intermediate-senior data engineer/developer: create and maintain data structures and etl in ms-fabric produce queries and data output from ms-fabric using sql and power bi produce chart output from power bi analyze requirements and decide the best ways to meet or even rework them; unique and innovative ideas are welcome be proactive by finding and addressing areas of technical concern mentor your peers and contribute to code reviews, and be mentored if something is new to you collaborate across teams to continually improve our processes promote the use of design patterns and best practices contribute to agile ceremonies for prioritizing, understanding, and estimating support the growth of our agile practices through retrospectives why you should join us work with like-minded individuals in a work-hard/play-hard culture continued learning is a major part of our best-practice-driven environment. we support our teams with a substantial budget for training and career development work-life balance is important to us: after-hours and weekend work are rare to non-existent you will work in a highly collaborative team that functions efficiently even in a remote work setting. you will have flexibility to work from home, at one of our unique offices in london (ontario) or toronto, or in a hybrid mode we offer great competitive salaries, benefits plan, and rrsp matching plans minimum qualifications: minimum 3 years of experience in a professional software development role, with at least 1 year focused on data engineering comfortable developing in an agile product development framework must have skills: experience with ms-fabric and powerbi ability to design, develop, and deploy data-related solutions, with seamless integration into existing architecture, and to support those solutions after deployment experience with sql authoring and optimization familiarity with agile development methodologies and version control systems strong problem-solving and analytical skills with a focus on continuous improvement ability to communicate technical details and build functional requirements experience with source control software ability to maintain focus on priority, and deliver solutions appropriate to requirements nice to have: minimum 5 years of experience in a professional software development role, with at least 3 years focused on data engineering experience building and maintaining applications in ruby on rails, or with a similar mvc framework such as laravel or netmvc experience building systems or services in python experience building user interfaces in html, css, js, and js frameworks familiarity with agile development methodologies experience with salesforce integration exposure to aws tools and services we encourage you to apply even if you don't tick all the boxes! info-tech research group of companies is an equal opportunity employer committed to diversity and inclusion and does not discriminate on the basis of any legally protected status or characteristic including minority/female/sexual orientation/gender identity/disability/veteran and are pleased to consider all qualified applicants. to that end, upon request, itrg will ensure, to the extent possible, that accommodation be made available to applicants throughout the recruitment and hiring process.",canada,Data Engineer,"['aws', 'azure', 'etl', 'power bi', 'python', 'r', 'sql']","['aws', 'azure', 'etl', 'power bi', 'python', 'r', 'sql']",
data engineer (contract),acto technologies,"engineeringfull timecanadaremote job type: contract - 6-12 months job location: remote; ability to work on eastern standard time about us: acto is an intelligent field excellence (ife) platform built for life sciences that improves field and hcp interactions with unified agentic ai. acto helps sales, marketing, and medical teams improve customer engagement and brand performance by turning field professionals into “masters of the message” that engage hcps and their support teams with authority and impact. acto partners with biopharma companies to ensure field professionals are always competent, confident, and credible, delivering the right message to hcps, while providing senior leaders and frontline managers with the insight they need to drive continuous field force effectiveness. as a validated platform compliant with fda 21 cfr part 11 and soc 2 type ii certified, acto is the trusted partner for intelligent field excellence in the life sciences industry. for more information, visit www.acto.com.role summary: we are seeking a skilled data engineer to design, build, and maintain robust data pipelines and a scalable data lakehouse architecture. the ideal candidate will integrate various data sources and tools, such as snowflake, databricks, crm, ensuring seamless data flow across systems. this role also involves supporting the data architect in implementing efficient, secure, and reliable data infrastructure solutions. in this role, you will be responsible for: • build and maintain data processing pipeline and tools using state-of-the-art technologies. • work with python on spark-based data pipelines. • develop algorithms to build complex data relationships. • build analytical data structures to support reporting. • build and maintain data quality processes. • collaborate with product team to adapt our reference data to changing demands in the market. to be successful in this role, you’ll need: • 4+ years of experience developing data pipelines using cloud-managed spark clusters (e.g. aws emr, databricks) • must have experience with aws athena, glue architecture • fluent in python and spark (3+ years of experience) • previous experience building tools and libraries to automate and streamline data processing workflows. • proficient with sql / sparksql • hands-on experience working with a data lakehouse. • good verbal and written communication in english • proven experience of working and delivering in an agile environment. bonus points if you have: • experience running data workflows through devops pipelines • develop data pipelines with orchestration tools (e.g. airflow) • previous experience in the life sciences sector • experience working at a startup what you’ll enjoy about acto: • industry leading, multiple award-winning technology • competitive compensation package • being part of a mission driven organization with the ability to drive solutions that focus on improving patient outcomes • results-driven and collaborative culture • remote work at acto we believe diverse and inclusive teams perform better. we are an equal opportunity employer and are committed to working with applicants requesting accommodations during our interview process. we may use ai-powered tools during parts of our hiring process to help review applications and support candidate communication. these tools are designed to assist our team, but all final hiring decisions are made by human recruiters and hiring managers. if you have any questions or concerns about this process, please let us know. we thank everyone for their interest in acto, only those applicants that have been selected for an interview will be contacted.",canada,Data Engineer,"['airflow', 'aws', 'cloud', 'data lake', 'data pipeline', 'databricks', 'excel', 'python', 'r', 'scala', 'snowflake', 'spark', 'sql']","['airflow', 'aws', 'cloud', 'data lake', 'data pipeline', 'databricks', 'excel', 'python', 'r', 'scala', 'snowflake', 'spark', 'sql']",
sr. snowflake data engineer,micro1,"**job title:** sr. snowflake data engineer • *job type:** contract • *location:** remote • *job summary** join our customer's team as a sr. snowflake data engineer, where you'll design and optimize cutting-edge data solutions in a fully remote environment. you will leverage your deep snowflake and etl expertise to drive impactful data strategies, ensuring robust, scalable, and efficient data pipelines. this is a unique opportunity to shape the data infrastructure of a forward-thinking organization that values clear and proactive communication. • *key responsibilities** - architect, develop, and maintain complex etl pipelines utilizing snowflake's advanced features. - collaborate with cross-functional teams to translate business requirements into robust data models and scalable solutions. - optimize and tune snowflake data warehouses for performance, security, and cost-effectiveness. - implement best practices for data governance, quality, and integrity across the data platform. - monitor, troubleshoot, and resolve data pipeline and platform issues to ensure seamless operations. - document data workflows, architecture, and processes with precision, emphasizing effective written and verbal communication. - mentor and guide junior engineers, fostering a culture of continuous learning and innovation within the customer’s team. • *required skills and qualifications** - 5-10 years of professional experience in data engineering, with significant expertise in snowflake and etl development. - demonstrated proficiency in designing and optimizing large-scale data warehouses using snowflake. - strong understanding of data warehousing concepts, data modeling, and performance tuning. - exceptional written and verbal communication skills; able to clearly convey technical concepts to diverse audiences. - hands-on experience with data integration tools and scripting languages relevant to etl processes. - proven ability to work autonomously in a remote, collaborative environment. - track record of delivering high-quality solutions and documentation under tight deadlines. • *preferred qualifications** - snowpro certification or equivalent validation of snowflake expertise. - experience mentoring colleagues and contributing to technical knowledge sharing. - background in fostering effective cross-team communication and alignment.",canada,Data Engineer,"['data pipeline', 'data warehouse', 'etl', 'r', 'scala', 'snowflake']","['data pipeline', 'data warehouse', 'etl', 'r', 'scala', 'snowflake']",
principal data engineer: real-time data & ai leader,pointclickcare,a leading health tech company in mississauga seeks a principal data engineer to design scalable data pipelines and optimize real-time data solutions. candidates should have over 10 years in data engineering and strong expertise in streaming technologies like apache kafka and spark. the role emphasizes collaboration across product and analytics teams to enhance data governance and quality. competitive salary offered. #j-18808-ljbffr,canada,Data Engineer,"['data pipeline', 'kafka', 'r', 'scala', 'spark']","['data pipeline', 'kafka', 'r', 'scala', 'spark']",
lead data engineer: sap to bigquery on gcp,insight global,"a technology services organization in canada is seeking an experienced data engineer to modernize enterprise data ecosystems by migrating from sap business warehouse to a unified data platform on google cloud. the ideal candidate has over 8 years of data engineering experience, strong skills in python and sql, and leadership capabilities in managing engineering teams. this is a contract role focused on delivering high-quality solutions for data analytics and reporting. #j-18808-ljbffr",canada,Data Engineer,"['cloud', 'data analytics', 'google cloud', 'python', 'r', 'sql']","['cloud', 'data analytics', 'google cloud', 'python', 'r', 'sql']",
architect - data engineering - mississauga - canada,photon career site,"key responsibilities: • architect and design large-scale, distributed big data solutions using java and big data technologies to handle high-volume data processing and analytics. • optimize and tune spark applications for better performance on large-scale data sets. • work with the cloudera hadoop ecosystem (e.g., hdfs, hive, impala, hbase, kafka) to build data pipelines and storage solutions. • collaborate with data scientists, business analysts, and other developers to understand data requirements and deliver solutions. • design and implement high-performance data processing and analytics solutions. • ensure data integrity, accuracy, and security across all processing tasks. • troubleshoot and resolve performance issues in spark, cloudera, and related technologies. • implement version control and ci/cd pipelines for spark applications. required skills & experience: • minimum 15+ years of experience in application development. • strong hands on experience in apache spark, scala, and spark sql for distributed data processing. • hands-on experience with cloudera hadoop (cdh) components such as hdfs, hive, impala, hbase, kafka, and sqoop. • familiarity with other big data technologies, including apache kafka, flume, oozie, and nifi. • experience building and optimizing etl pipelines using spark and working with structured and unstructured data. • experience with sql and nosql databases such as hbase, hive, and postgresql. • knowledge of data warehousing concepts, dimensional modeling, and data lakes. • ability to troubleshoot and optimize spark and cloudera platform performance. • familiarity with version control tools like git and ci/cd tools (e.g., jenkins, gitlab).",canada,Data Engineer,"['cloud', 'data lake', 'data pipeline', 'etl', 'hadoop', 'java', 'kafka', 'r', 'scala', 'spark', 'sql']","['cloud', 'data lake', 'data pipeline', 'etl', 'hadoop', 'java', 'kafka', 'r', 'scala', 'spark', 'sql']",
data engineer (guru apps),universe group,"привіт! 👋 guruapps шукає data engineer’а, який(а) підсилить нашу команду аналітики, допоможе масштабувати інфраструктуру даних і зробити її ще надійнішою, зручнішою та ефективнішою — відповідно до високих амбіцій нашого продукту. guruapps — бізнес universe group, що створює екосистему мобільних застосунків для ios . серед найвідоміших — scan guru, cleaner guru та visify . • нам довіряють 82+ мільйони користувачів зі 180+ країн світу. • у жовтні 2024 року cleaner guru очолив рейтинг app store у сша, канаді та австралії, випередивши instagram, temu, chatgpt та tiktok. • ми — частина швидкозростаючої компанії з продуктовим фокусом, потужною аналітикою і глибокою експертизою в мобільному growth-напрямку. якщо хочеш побачити, як ми запускаємо апки за тиждень, тестуємо сотні ідей і масштабуємо продукти, які вже у кишенях світу — просто подивись відео про guru apps. наш стек: • python, • sql, • aws (s3, athena), • airflow, • dbt твої майбутні задачі: • розібратись у поточних пайплайнах, інфраструктурі, інструментах. • підтримка та оптимізація etl/elt-процесів (s3, athena, airflow, dbt, python). • автоматизація збору даних із нових каналів та api. • валідація, моніторинг, контроль якості даних. • документування процесів, інтеграцій, архітектурних рішень. що тебе чекає: • станеш другим дата-інженером у команді і драйвером нових змін. • отримаєш вплив на архітектуру , інфраструктуру, вибір інструментів і якість даних. • працюватимеш із різними джерелами даних — зокрема app store, appsflyer, amplitude, сторонніми api. • станеш частиною технічно сильної команди аналітики та інженерії. що потрібно, щоб приєднатись до нас: • практичний досвід з aws (зокрема s3, athena). • досвід роботи з airflow або подібними оркестраторами. • глибокі знання python та sql . • практику з dbt . • розуміння архітектури data lake / dwh . • досвід із моніторингом, перевіркою якості та валідацією даних. • вміння працювати з raw-даними і трансформувати їх у структуровані data marts. буде плюсом: • досвід з snowflake . • розуміння devops-практик (docker, kubernetes). • участь у побудові або перебудові архітектури дата-систем. будує tech-бізнеси, перетворюючи ідеї на глобальні продукти. до групи входять три компанії: guru apps , та . їхні продукти об’єднують понад 200 мільйонів користувачів зі 180 країн світу, спрощуючи буденність і створюючи нові можливості для розвитку. також universe group розвиває власний r&d-центр, який фокусується на дослідженні нових ідей, впровадженні інноваційних рішень та запуску бізнесів, що будуть частиною глобального технологічного ринку. що ми пропонуємо: • 🔝розвиток та навчання – твій ріст визначає успіх команди. внутрішні тренінги та кращі експерти з україни та світу допоможуть швидко прокачати навички. • ↗️кар’єрне зростання – у нас культура швидкого розвитку: до 10 менеджерів щороку отримують підвищення. все залежить від твого бажання та результатів. • гібридний формат роботи — ми цінуємо нашу офісну культуру та робимо все для того, щоб кожному було зручно працювати в офісі на подолі в києві. співробітники можуть обирати зручний комбінований формат роботи (з офісу та ремоут). • 📍all-inclusive офіс у києві — у нас є все для твоєї комфортної роботи, а саме: сніданки, обіди, доріжки для ходіння, silent room для фокуса уваги — це далеко не все, що чекає на тебе в нашому спейсі. • 🧳релокаційний пакет – комфортний переїзд до києва з фінансовою підтримкою, допомогою рієлторів та адаптацією в новому місті. • 🤜🏻🤛🏻один із кращих соцпакетів – винагорода відповідає запитам ринку, 20 днів оплачуваного відпочинку, співпраця через фоп або дія.city, оплачувані лікарняні, медстрахування та забезпечуємо харчуванням в офісі у києві. • 💛well-being program – ми турбуємося про ментальне здоровʼя команди, тому компенсуємо роботу з психологом, а також проводимо класи з йоги та медитації в офісі. • 🇺🇦підтримка під час війни – безпека та стабільність для тебе та твоїх рідних. забезпечуємо всім необхідним для безперебійної роботи та долучаємось до ініціатив із відновлення україни. надсилай своє резюме та ставай частиною нашого всесвіту.",canada,Data Engineer,"['airflow', 'aws', 'data lake', 'dbt', 'elt', 'etl', 'python', 'r', 'snowflake', 'sql']","['airflow', 'aws', 'data lake', 'dbt', 'elt', 'etl', 'python', 'r', 'snowflake', 'sql']",
power bi data engineer,hso,"become a key player as a power bi data engineer at hso! at hso, we are committed to driving digital transformation through innovative data solutions. as a power bi data engineer, you will play a pivotal role in helping our clients harness the power of their data to make informed business decisions. your expertise in power bi and data engineering will empower organizations to visualize and analyze their data effectively. key responsibilities: • design, build, and maintain efficient power bi reports and dashboards, ensuring they meet user requirements and deliver valuable insights. • develop and optimize data models and etl processes to enhance data quality and access, using tools such as azure data factory and sql. • collaborate with stakeholders to gather business requirements and translate them into technical specifications. • monitor and analyze data performance and troubleshoot issues to ensure uninterrupted data flows and optimal system performance. • implement data governance techniques to ensure data integrity and security. • work with cross-functional teams to streamline data processes and improve overall data strategy. • provide training and support to end-users to empower them in leveraging power bi effectively. requirements what you bring: • bachelor’s degree in computer science, information technology, or a related field. • proven experience as a data engineer or in a similar role, specifically with power bi and azure services. • strong proficiency in sql, power bi, and data modeling techniques. • experience with etl processes and tools, particularly azure data factory. • solid knowledge of data warehouse concepts and data integration processes. • strong problem-solving skills and attention to detail. • excellent communication skills, with the ability to articulate technical information to non-technical stakeholders. • experience with d365 is a plus • experience in a collaborative and agile work environment is a plus. benefits if you would like to work with a market-leading business applications suite, and a forward-thinking and positive team, we encourage you to apply for this position. in return, hso provide a target driven earnings bonus and a generous benefits package. we offer flexible working hours, and the opportunity to join our learning academy to help keep your skills current, with exposure to the latest and greatest microsoft products and innovations. position is fully remote with the occasional need to visit a client site. we offer a structured career development plan to help you grow your career with hso. we include a benefits program, competitive compensation plans, paid vacation, and a dynamic environment.",canada,Data Engineer,"['azure', 'dashboard', 'data warehouse', 'etl', 'excel', 'power bi', 'r', 'sql']","['azure', 'dashboard', 'data warehouse', 'etl', 'excel', 'power bi', 'r', 'sql']",
head of pt data engineering,roche,"at roche you can show up as yourself, embraced for the unique qualities you bring. our culture encourages personal expression, open dialogue, and genuine connections, where you are valued, accepted and respected for who you are, allowing you to thrive both personally and professionally. this is how we aim to prevent, stop and cure diseases and ensure everyone has access to healthcare today and for generations to come. join roche, where every voice matters. the position the pharma technical operations (pt) department is establishing the one pt data office to serve as the strategic center for data governance, strategy, and enablement across the entire global pt network. this team is at the heart of our digital transformation, responsible for architecting and leading a central data office to unlock the full potential of pt's data assets. the head of pt data engineering will be instrumental in building the robust data backbone that powers pt's digital transformation and data driven decision making. reporting into the one pt data office, this critical role is accountable for leading a cutting-edge internal and external global data engineering team. you will define the strategy, evolve the data platforms and processes, and oversee the delivery of scalable, high-quality data products to enable advanced analytics, ai initiatives, and critical business processes across pharma technical operations. you will lead a critical team of internal and external data engineers, fostering a culture of technical excellence, innovation, and continuous delivery. this pivotal role requires a visionary leader to build and manage the foundational data infrastructure, pipelines, and platforms that enable the seamless flow of high-quality, fair data from diverse sources to data consumers, ensuring compliance, scalability, and future readiness for pt's ambitious digital agenda. the opportunity + provide strategic leadership and vision for pt's global data engineering capabilities, defining the roadmap for data ingestion, transformation, storage, and consumption architectures. + accountable for the design, development, and evolution of scalable, robust, and cost-effective data platforms (e.g., data lakes, data warehouses, streaming platforms) that support pt's advanced analytics, ai/ml, and data product needs. + define and implement best practices, standards, and guidelines for data modeling, etl/elt processes, data quality, and data pipeline orchestration across the pt landscape. + actively monitor and integrate cutting-edge industry trends, emerging data engineering technologies, and cloud-native solutions to continually optimize pt's data infrastructure in close collaboration with it. + build, mentor, mobilize, and empower a high-performing, global team of internal and external data engineers, fostering a culture of technical excellence, innovation, and agile delivery. + accountable for the end-to-end delivery and operational excellence of critical data pipelines, ensuring timely, accurate, and reliable data availability for pt's business processes and analytical use cases. + ensure data infrastructure and pipelines adhere to strict quality, security, and compliance standards (e.g., gxp, data integrity, data privacy), collaborating closely with data governance and cybersecurity teams. + drive the automation and optimization of data engineering workflows to enhance efficiency, reduce manual effort, and improve data freshness. who you are + 12+ years of progressive experience in data engineering, data platform architecture, or related roles within a complex, global enterprise, preferably in life sciences/pharma and 7+ years of senior leadership experience, specifically building, developing, and leading large, global teams of data engineers. + proven track record of successfully designing, implementing, and scaling robust data pipelines and cloud-based data platforms (aws, azure, gcp data services) for advanced analytics and ai/ml. + expert-level knowledge of modern data architectures, etl/elt, data orchestration, and data quality management. + strong understanding of gxp, data integrity, and data privacy regulations in a manufacturing context. + exceptional strategic thinking, communication, and influencing skills to lead and align diverse stakeholders globally. + bachelor's degree in a relevant technical field required; master's or advanced certifications are highly advantageous. ready for the next step? we look forward to hearing from you. apply now to discover this exciting opportunity! who we are a healthier future drives us to innovate. together, more than 100'000 employees across the globe are dedicated to advance science, ensuring everyone has access to healthcare today and for generations to come. our efforts result in more than 26 million people treated with our medicines and over 30 billion tests conducted using our diagnostics products. we empower each other to explore new possibilities, foster creativity, and keep our ambitions high, so we can deliver life-changing healthcare solutions that make a global impact. let's build a healthier future, together. roche is an equal opportunity employer.",canada,Data Engineer,"['aws', 'azure', 'cloud', 'data lake', 'data pipeline', 'data warehouse', 'elt', 'etl', 'excel', 'gcp', 'r', 'scala']","['aws', 'azure', 'cloud', 'data lake', 'data pipeline', 'data warehouse', 'elt', 'etl', 'excel', 'gcp', 'r', 'scala']",
analytics insights engineer ii,meloche monnex inc.,"work location: toronto, ontario, canada hours: 35 line of business: analytics, insights, & artificial intelligence pay details: $74,500 - $111,700 cad td is committed to providing fair and equitable compensation opportunities to all colleagues. growth opportunities and skill development are defining features of the colleague experience at td. our compensation policies and practices have been designed to allow colleagues to progress through the salary range over time as they progress in their role. the base pay actually offered may vary based upon the candidate's skills and experience, job-related knowledge, geographic location, and other specific business and organizational needs. as a candidate, you are encouraged to ask compensation related questions and have an open dialogue with your recruiter who can provide you more specific details for this role. job description: summary the analytics engineer is responsible for designing, building, and maintaining robust data pipelines and analytics solutions—including creating reports and dashboards—that enable business stakeholders to make data-driven decisions. this role covers the full spectrum of analytics engineering, from hands-on technical implementation to solution architecture and technical leadership. key responsibilities perform data analysis and assess data management requirements for platforms and projects. build and optimize data pipelines in azure databricks (pyspark/sql), aligned with medallion architecture. orchestrate data ingestion, transformation, and validation processes. maintain production-grade code with ci/cd, version control, and devops/github practices. design and deliver high-performance semantic models, data marts, and datasets optimized for executive-level insurance and operational reporting. develop advanced power bi dashboards using complex dax, calculation groups, row-level security, and best-practice modeling patterns. implement and maintain claim operations semantic layer ensuring consistent business definitions, metrics, and kpis. own the documentation of pipelines, data models, definitions, and lineage. continuously optimize pipelines, sql queries, dax measures, and cloud spend. partner with bi & data management teams to enforce governance standards, access controls, and data quality frameworks. provide coaching and guidance to team members on databricks, power bi, and analytics best practices. collaborate with business and technology partners to elicit, analyze, and understand data requirements. ensure data compliance with enterprise standards, policies, and guidelines. support partners and stakeholders in interpreting and analyzing data. build effective working relationships across teams to encourage collaboration. skills & qualifications 5-8+ years of experience in business intelligence, data engineering, or analytics engineering roles. expert sql and strong python. expert-level power bi + dax: complex measures, optimization, semantic modeling, rls, tabular editor, calculation groups. knowledge in azure, databricks (unity catalog, delta, medallion architecture), and ci/cd. demonstrated ability to design and implement a robust semantic layer using dbt, ensuring consistent business definitions, metrics, and kpis across the analytics ecosystem. proven experience building dashboards and delivering insurance analytics solutions. excellent analytical, problem-solving, and communication skills. ability to work collaboratively in cross-functional teams and mentor others. who we are: as part of td bank group, one of canada's largest financial institutions, at td insurance, we care for canadian families, making it easy to get the best advice, protection, and support in their moments of need, always. td insurance offers a wide range of products, including general insurance and life and health. more than four million customers count on us. as the largest direct to consumer insurer in canada, we are always innovating and providing exciting and rewarding career opportunities for our canada-wide workforce. our total rewards package our total rewards package reflects the investments we make in our colleagues to help them and their families achieve their financial, physical, and mental well-being goals. total rewards at td includes a base salary, variable compensation, and several other key plans such as health and well-being benefits, savings and retirement programs, paid time off, banking benefits and discounts, career development, and reward and recognition programs. learn more additional information: we’re delighted that you’re considering building a career with td. through regular development conversations, training programs, and a competitive benefits plan, we’re committed to providing the support our colleagues need to thrive both at work and at home. please be advised that this job opportunity is subject to provincial regulation for employment purposes. it is imperative to acknowledge that each province or territory within the jurisdiction of canada may have its own set of regulations, requirements. colleague development if you’re interested in a specific career path or are looking to build certain skills, we want to help you succeed. you’ll have regular career, development, and performance conversations with your manager, as well as access to an online learning platform and a variety of mentoring programs to help you unlock future opportunities. whether you have a passion for helping customers and want to expand your experience, or you want to coach and inspire your colleagues, there are many different career paths within our organization at td – and we’re committed to helping you identify opportunities that support your goals. training & onboarding we will provide training and onboarding sessions to ensure that you’ve got everything you need to succeed in your new role. interview process we’ll reach out to candidates of interest to schedule an interview. we do our best to communicate outcomes to all applicants by email or phone call. accommodation your accessibility is important to us. please let us know if you’d like accommodations (including accessible meeting rooms, captioning for virtual interviews, etc.) to help us remove barriers so that you can participate throughout the interview process. we look forward to hearing from you! language requirement (quebec only): sans objet us labor & employment posters | california privacy | accessibility | faq our values at td we’re guided by our purpose to enrich the lives of our customers, communities and colleagues, and share a set of values that shape our culture and guide our behavior. in exchange for how our colleagues show up to help td succeed, we are committed to delivering a colleague experience grounded in impact, growth and a culture of care. no matter where you work across td, we empower you to make an impact at work and in your community, explore and grow your career and be part of our caring and inclusive culture. our commitment to diversity, equity, and inclusion at td, we’re committed to fostering an environment where all colleagues are encouraged to bring their authentic selves to work, experience equitable opportunities, and feel respected and supported. we’re dedicated to building an inclusive workforce that reflects the diversity of the customers and the communities in which we live and serve. helping to make an impact in communities – td ready commitment td has a long-standing commitment to help drive progress towards a more inclusive and sustainable future. that’s why we launched the td ready commitment in 2018, now a multi-year north american initiative. under the td ready commitment, we are targeting a total of c$1 billion by 2030 in community giving across four key, interconnected drivers of change: financial security, vibrant planet, connected communities, and better health. it’s our goal to help support change, nurture progress, and contribute to making the world a better, more inclusive place for our customers, colleagues, and communities. learn more: canada | us | europe & asia pacific",canada,Data Engineer,"['azure', 'business intelligence', 'cloud', 'dashboard', 'data analysis', 'data pipeline', 'databricks', 'dbt', 'elt', 'excel', 'power bi', 'pyspark', 'python', 'r', 'spark', 'sql']","['azure', 'business intelligence', 'cloud', 'dashboard', 'data analysis', 'data pipeline', 'databricks', 'dbt', 'elt', 'excel', 'power bi', 'pyspark', 'python', 'r', 'spark', 'sql']",
gen ai data engineer,expedite technology solutions llc,"job description must have • design, build, and scale genai-driven systems that power research digitization, banking workflows, global markets, and monetization pipelines • strong backend development skills in python, fastapi, and async programming • solid hands-on experience with kubernetes, docker, and api deployment at scale • deep understanding of databricks, *** lake, pyspark, and distributed data workflows • proven experience building or integrating with llm-based applications, including prompt routing or semantic matching • client is doing significant checks on random forest and decision tree capabilities. • excellent debugging, profiling, and optimization skills in high-throughput environments • comfort working with cloud platforms, especially azure nice to have • familiarity with model orchestration frameworks (langchain, llamaindex, or similar) • experience designing or contributing to mcp-style architectures (multi-modal, intent-aware, tool-executing systems) • working knowledge of mlflow, airflow, or snowflake • exposure to alternative data sources (web, satellite, social, geospatial) and their ai use cases • understanding of enterprise ci/cd, secrets management, and secure api gateways",canada,Data Engineer,"['airflow', 'azure', 'cloud', 'databricks', 'excel', 'pyspark', 'python', 'r', 'snowflake', 'spark']","['airflow', 'azure', 'cloud', 'databricks', 'excel', 'pyspark', 'python', 'r', 'snowflake', 'spark']",
"senior data engineer - python, spark, scalable data",capgemini,"a leading global technology consulting firm is seeking a senior data engineer with expertise in python and spark. you will design and implement data processing solutions for high-volume projects, collaborating in a self-organized team. the role requires 5+ years of experience in data engineering and proficiency in etl processes, databases, and cloud platforms. this position offers a flexible work environment and a comprehensive benefits package including healthcare and financial well-being programs. #j-18808-ljbffr",canada,Data Engineer,"['cloud', 'etl', 'python', 'r', 'spark']","['cloud', 'etl', 'python', 'r', 'spark']",
test data engineer ( informatica idmc (tdm) ),yochana,"role: test data engineer (tdm) location: canada (remote) hire type: fulltime job description position summary: test data management (tdm) plays a key role in sourcing the right data with every changing needs to make sure software testing is done effectively. test data administration is the way of creating non-production data that reliably mimic an organization’s actual data so that systems can be rigorously tested. it is a measure of quality and quantity of data that is available in non-prod environments. this role will be accountable for understanding test data requirements, perform data discovery, protect sensitive data using masking techniques, create test data warehouse and enable test data availability to users in kroger. essential job functions: • define, implement, and adopt tdm strategy, technical solution, processes & methodology, delivery model, standards & governance. • establish test data supply chain (from prod to non-prod) applying masking, sub-setting to build gold copy test data set. • good understanding of the application architecture/domains and data flows across systems. • implement test data management function with nosql & relational data sources. • proficient in tdm tools (informatica idmc, ca, delphix or others) for masking, sub setting, synthetic data generation & gold copy. • support, operate and maintain tdm’s environment, platform, and services. • integrate tdm solution with test automation framework, automate test data refresh and creation process, integrate with ci / cd pipelines. • service test data requests, perform data identification / analysis across data sources for required scenarios. • analyze business requirements, work closely with project team members, technical leads, and business partners to arrive at an optimal solution design. • conduct feasibility analysis, create high level design document, and review it with key identified stakeholders. • perform proof of concept (pocs) on agreed approach and provide presentations to key stakeholders. • excellent in writing complex sql queries. • capable of handling multiple programs simultaneously and work under offshore and onsite model. • provide support to identify test data required to test specific business scenarios. • coordinate with application dbas to copy data from one environment to another while protecting customer sensitive information. document data quality issue findings and recommendations. • demonstrated ability to provide high volumes of test data with referential integrity required to support test execution. • perform data analysis when needed to analyze issues in pre-production and/or production environments in cloud and/or on-premises infrastructure. • prepare test data reports/metrics for stakeholders. • experience in defect management process, and defect triage calls. • mentor and groom team members on product/application features/functionality and domain. • knowledge of any of database systems like sql server, oracle or db2 etc • working knowledge of test management tools like hp alm/ quality center/ jira/qmetry • excellent verbal and written communication skills. minimum positions qualifications/education requirements: • 6+ years of software product development, testing and delivery experience. • 5+ years of tdm/sql experience. • 3+ years of informatica power center, idq, iics/idmc experience. • 2+ years of experience in some of the azure cloud tools: adf, pyspark, databricks notebooks • 1+ experience with python and libraries like panads, numpy & matplotlib. • familiar with latest cloud technologies. mandate key skills: • expertise in tdm strategy and implementation: including defining and driving tdm strategy, governance, and delivery model as a core function. • strong proficiency in sql for data identification, extraction, transformation, and analysis: sql, nonsql, relational database. • experience with tdm tools & automation: hands-on experience with tdm tools like informatica idmc, ca, delphix • experience with azure cloud tools: azure data factory, pyspark, databricks notebooks. • experience with test management tools like: hp alm, jira, qmetry",canada,Data Engineer,"['azure', 'cloud', 'data analysis', 'data warehouse', 'databricks', 'excel', 'matplotlib', 'numpy', 'pyspark', 'python', 'r', 'recommendation', 'spark', 'sql', 'sql server']","['azure', 'cloud', 'data analysis', 'data warehouse', 'databricks', 'excel', 'matplotlib', 'numpy', 'pyspark', 'python', 'r', 'recommendation', 'spark', 'sql', 'sql server']",
principal data engineer /pacific time zone preferred/,docker,"at docker, we make app development easier so developers can focus on what matters. our remote-first team spans the globe, united by a passion for innovation and great developer experiences. with over 20 million monthly users and 20 billion image pulls, docker is the #1 tool for building, sharing, and running apps—trusted by startups and fortune 100s alike. we’re growing fast and just getting started. come join us for a whale of a ride! docker is seeking an exceptional principal data engineer to lead the technical vision and architecture of our data organization within our infrastructure group. this role will address critical data strategy challenges, building and launching scalable software systems and processes to unlock data at docker. you will drive technical leadership across data infrastructure, analytics platforms, and revenue-enabling data products while mentoring senior engineers and collaborating with stakeholders across the company. as a principal engineer, you will be responsible for solving docker's most complex data engineering challenges at scale, architecting systems that support millions of developers and containers, and establishing data governance frameworks that enable rapid business growth and decision-making. responsibilities technical leadership & architecture • define and drive the long-term technical strategy for docker's data platform, addressing current fragmentation across disparate data sources. • architect scalable, reliable data infrastructure supporting docker's growing customer base and container ecosystem • lead cross-functional technical discussions to align on data architecture decisions • establish technical standards and best practices across data engineering, analytics engineering, and data science teams • design and implement data governance frameworks that ensure quality, security, and compliance (soc-2, privacy regulations) systems design & implementation • own the design of mission-critical data pipelines supporting customer usage measurement, billing systems, and revenue operations • build robust etl/elt frameworks capable of processing docker's container telemetry, user analytics, and business metrics at scale • architect customer 360 data models that unify user behavior, account information, and product usage across docker's platform • design monitoring, alerting, and observability systems for data infrastructure health and reliability • lead integration efforts with third-party tools (crm, erp, analytics platforms) and internal docker services strategic impact • partner with business stakeholders to translate complex business requirements into scalable technical solutions • drive data-driven decision making by establishing clear metrics, dashboards, and kpis aligned with business objectives • lead strategic initiatives that unlock new revenue streams through improved data capabilities and insights • establish processes that reduce time-to-insights from months to weeks for critical business questions • create data architecture that enables docker's expansion into new markets and customer segments leadership & mentorship • provide technical mentorship to senior data engineers, analytics engineers, and data scientists • lead architectural reviews, code reviews, and technical decision-making processes • drive hiring and technical interview processes for senior data team members • foster a culture of operational excellence, data quality, and technical innovation • collaborate with engineering leadership on team roadmaps, prioritization, and resource allocation cross-functional collaboration • partner with product, sales, marketing, and customer success teams to understand and address their data needs • work closely with security and compliance teams to ensure data handling meets enterprise requirements • collaborate with platform engineering teams on shared infrastructure and tooling • engage with finance and legal teams on data governance, retention, and privacy requirements required qualifications experience & background • 8+ years of hands-on experience in data engineering, analytics engineering, or related technical roles • 3+ years in senior technical leadership positions (staff engineer, principal engineer, or equivalent) • experience designing and scaling data systems for companies with 100m+ users or equivalent scale • experience guiding technical teams and facilitating cross-functional engineering initiatives technical expertise • expert-level proficiency in modern data stack technologies (dbt, snowflake/bigquery/databricks, apache airflow/prefect) • strong programming skills in python, sql, and at least one additional language (scala, java, go, or rust) • deep understanding of distributed systems, data modeling, and database optimization techniques • experience with cloud platforms (aws, gcp, azure) and infrastructure-as-code (terraform, cloudformation) • knowledge of streaming data technologies (kafka, kinesis, pub/sub) and real-time analytics leadership & communication • demonstrated ability to influence technical decisions across multiple engineering teams • strong written and verbal communication skills, with ability to explain complex technical concepts to non-technical stakeholders • experience mentoring and developing senior engineering talent • track record of successfully delivering large-scale, multi-quarter technical initiatives business acumen • understanding of saas business models, customer analytics, and revenue operations • experience with product analytics, user behavior analysis, and a/b testing frameworks • knowledge of data privacy regulations (gdpr, ccpa) and enterprise compliance requirements preferred qualifications • experience at high-growth technology companies or developer-focused platforms • background in containerization technologies, docker ecosystem, or developer tools • experience with machine learning infrastructure and mlops practices • previous experience in marketplace, multi-tenant, or developer platform environments • advanced degree in computer science, data science, or related technical field • industry certifications (aws solutions architect, dbt, snowflake pro, etc.) we use covey as part of our hiring and / or promotional process for jobs in nyc and certain features may qualify it as an aedt. as part of the evaluation process we provide covey with job requirements and candidate submitted applications. we began using covey scout for inbound on april 13, 2024. please see the independent bias audit report covering our use of covey here. perks • freedom & flexibility; fit your work around your life • designated quarterly whaleness days • home office setup; we want you comfortable while you work • 16 weeks of paid parental leave • technology stipend equivalent to $100 net/month • pto plan that encourages you to take time to do the things you enjoy • quarterly, company-wide hackathons • training stipend for conferences, courses and classes • equity; we are a growing start-up and want all employees to have a share in the success of the company • docker swag • medical benefits, retirement and holidays vary by country docker embraces diversity and equal opportunity. we are committed to building a team that represents a variety of backgrounds, perspectives, and skills. the more inclusive we are, the better our company will be. due to the remote nature of this role, we are unable to provide visa sponsorship. #li-remote original job principal data engineer /pacific time zone preferred/ posted on grabjobs ©. to flag any issues with this job please use the report job button on grabjobs.",canada,Data Engineer,"['a/b testing', 'airflow', 'aws', 'azure', 'bigquery', 'cloud', 'dashboard', 'data pipeline', 'databricks', 'dbt', 'elt', 'etl', 'excel', 'gcp', 'java', 'kafka', 'machine learning', 'python', 'r', 'scala', 'snowflake', 'sql']","['a/b testing', 'airflow', 'aws', 'azure', 'bigquery', 'cloud', 'dashboard', 'data pipeline', 'databricks', 'dbt', 'elt', 'etl', 'excel', 'gcp', 'java', 'kafka', 'machine learning', 'python', 'r', 'scala', 'snowflake', 'sql']",$208K–$286K a year
"avp, data engineering",aviva canada inc,"individually we are people, but together we are aviva. individually these are just words, but together they are our values – care, commitment, community, and confidence. we are seeking a positive and collaborative avp for our fast-paced data engineering area. you will lead data engineering teams, mentor talent, and drive successful data platform transformation to help aviva build out our advanced analytics and data-driven business capabilities. what you'll do strategic leadership: shape the delivery of our technology roadmap for aviva’s enterprise data platforms, enabling advanced analytics and business intelligence. lead implementation of solutions working across business, data governance, ai/ml and other technology teams to partner and deliver on business outcomes define, implement and maintain data engineering standards, controls and automation, including ci/cd, devops, infosec, and cloud-first approaches to deliver stable and secure data pipelines and workflow operations. technology & platform ownership: recommend and implement technology solutions leveraging snowflake, aws rds, s3, airflow, dbt, informatica powercentre, and cloudera. oversee modernization and migration of legacy data systems to cloud and distributed platforms. ensure robust data pipeline development, orchestration, and automation using industry-leading tools. governance & quality: build in security best practices and ensure compliance with aviva and customer data protection standards. champion data quality, lineage, and metadata management throughout the delivery cycle. expedite and support devops and automation journeys—ci/cd pipelines, zero downtime deployments, and environment decoupling. collaboration & communication: foster strong partnerships with business stakeholders, it, and global aviva technology teams. inspire, motivate, and communicate with technology teams to ensure alignment and shared vision. operational excellence: lead disaster recovery planning, essential maintenance, and infrastructure upgrades. address technical debt, capacity, performance, and resiliency planning for data platforms. what you'll bring bachelor’s degree in engineering, computer science, information systems, or equivalent experience. experience & capabilities 15+ years of deep technical expertise, including 8+ years leading data engineering delivery. proven track record in building and delivering implementations of modern enterprise data platforms using technologies such as snowflake, aws rds, s3, airflow, dbt, informatica powercentre, and cloudera. outstanding experience with all aspects of it and data delivery, including architecture, design, and modernization. ability to manage multiple stakeholders, and experience engaging with technology and business teams to resolve delivery challenges and drive delivery outcomes. deep appreciation for platform performance, stability, and business appetite for change delivery and service. knowledge, skills & abilities solid understanding of data engineering, cloud platforms, data governance, and automation. strategic thinker with strong impact and influence—able to provide direction on key decisions using a technical lens. resourceful, creative troubleshooting skills and ability to challenge the status quo. commitment to quality and advocacy for end-user and customer needs. excellent leadership skills and experience leading multi-disciplined teams. innovative, passionate about change and modernization—a true change agent. key technologies data platforms: snowflake, aws rds, s3, cloudera, informatica powercentre orchestration & automation: airflow, dbt, ci/cd pipelines cloud & infrastructure: aws, hybrid cloud architectures programming & scripting: python, sql, shell scripting devops & monitoring: jenkins, git, sonarqube, containerization strategies what you’ll get compelling rewards package including base compensation, eligibility for annual bonus, retirement savings, share plan, health benefits, personal wellness, and volunteer opportunities. outstanding career development opportunities. we’ll support your professional development education. competitive vacation package with the option to purchase 5 extra days off per year. employee driven programs focused on gender, lgbtq+, origins, diversity, and inclusion. corporate wellness programs to support our employees’ physical and mental health. hybrid flexible work model. please note that we may use ai tools to help us through the recruitment process. this is an existing position which has been posted both internally & externally. aviva canada has an accommodation process in place to provide accommodations for employees with disabilities. if upon commencement of employment you require a specific accommodation because of a disability, please contact your talent acquisition partner so that an appropriate accommodation can be arranged. this process applies throughout your career with aviva canada. #li-ps1 #li-hybrid we help our 19.5 million customers to save for the future and manage the risks of everyday life. to give these customers the best possible products and service we know we must make aviva the most attractive choice for talented, entrepreneurial people with diverse backgrounds and an evolving range of expertise and insight. so, we’re passionate about helping our 23,000 people to do the best work of their lives, to enable them to make a positive difference to the lives of our customers.",canada,Data Engineer,"['airflow', 'aws', 'business intelligence', 'cloud', 'data pipeline', 'dbt', 'excel', 'python', 'r', 'sas', 'snowflake', 'sql']","['airflow', 'aws', 'business intelligence', 'cloud', 'data pipeline', 'dbt', 'excel', 'python', 'r', 'sas', 'snowflake', 'sql']",
senior data engineer (data & ai engineering),zoominfo technologies llc,"zoominfo is where careers accelerate. we move fast, think boldly, and empower you to do the best work of your life. you'll be surrounded by teammates who care deeply, challenge each other, and celebrate wins. with tools that amplify your impact and a culture that backs your ambition, you won't just contribute. you'll make things happen–fast. we are looking for a highly skilled senior data engineer based in ontario, canada to become part of our core data & ai engineering team. in this pivotal role, you will be responsible for designing and expanding enterprise-level data infrastructure that enables zoominfo's internal teams to interact with data comprehensively—extracting, exploring, analyzing, and generating insights—through various platforms using zi's internal chat agent the ideal candidate has a strong background in big data processing, pipeline orchestration, and data modeling, with a proven track record of delivering scalable and high-quality data solutions in fast-paced, data-centric product environments. given the dynamic nature of emerging technologies, this role requires an individual who excels at exploration and embraces continuous learning as core responsibilities. you'll constantly research and implement innovative solutions while integrating vast, diverse data sources into our ai applications, including our industry-leading llm-powered systems what you'll do: • design, develop, and maintain high-performance, product-centric data pipelines using airflow, dbt, and python. • architect and optimize the massive-scale data warehouse and lakehouse that serves as our single source of truth for all customer data, primarily using snowflake. • lead the integration of diverse structured and unstructured data sources (e.g., web data, third-party apis) into our data ecosystem, ensuring high-quality and reliable ingestion. • implement and enforce model context protocol (mcp) or similar architectures to feed accurate and contextual data into our llm-powered products for applications like retrieval augmented generation (rag) and advanced search. • collaborate with ml engineers, data scientists, and product managers to translate business needs into scalable data solutions that directly enhance customer value. • define, monitor, and enforce data quality slas across all pipelines and products, ensuring data accuracy and lineage are a top priority. • mentor and coach junior engineers, promoting best practices in code quality, data architecture, and operational excellence. • participate in architectural decisions and long-term strategy planning for our enterprise-wide data infrastructure, with a focus on cost, performance, and reliability. what you bring: • expert-level sql for building performant, scalable queries and transformations on massive datasets. • strong python programming skills with a focus on distributed computing, data manipulation, and building robust apis. • production-level experience for large-scale batch and streaming data processing. • hands-on experience with dbt (data build tool) for advanced data modeling and transformations in a modern data stack. • deep knowledge of snowflake data warehouse design, optimization, and cost modeling. • experience implementing model context protocol (mcp) or similar architectures to feed structured and unstructured data into llm-powered systems. • strong understanding of data architecture concepts including data lakes, event-driven architectures (e.g., kafka), etl/elt, and data mesh. • proficiency with cloud platforms (gcp and/or aws) and infrastructure as code (e.g., terraform). nice to have • familiarity with llmops, langchain, or rag (retrieval augmented generation) pipelines. • experience with building embedding models or pipelines for named entity recognition (ner). • knowledge of data cataloging tools (e.g., openlineage etc) and lineage tracking. • familiarity with other distributed systems and databases (e.g., dynamodb, flink). required non-technical skills • excellent communication skills – ability to explain complex technical concepts to both engineering teams and non-technical stakeholders. • strategic & product-oriented thinking – can translate business objectives and customer needs into scalable, high-impact data solutions. • leadership & mentorship – experience guiding and uplifting engineering teams to achieve their full potential. • stakeholder management – able to collaborate effectively across departments (product, engineering, sales, compliance). • agility & adaptability – thrives in ambiguous, evolving environments and can rapidly prototype and iterate on solutions. • strong documentation habits and ability to evangelize best practices across the organization. qualifications • bachelor's or master's degree in computer science, engineering, or a related field. • 8+ years of progressive experience in data engineering, with a track record of leadership and impact. • demonstrated experience in implementing or scaling data infrastructure for a data-centric product company. location: toronto, ca (remote/hybrid). #li-vc1 #li-hybrid about us: zoominfo (nasdaq: gtm) is the go-to-market intelligence platform that empowers businesses to grow faster with ai-ready insights, trusted data, and advanced automation. its solutions provide more than 35,000 companies worldwide with a complete view of their customers, making every seller their best seller. zoominfo is committed to protecting your privacy when you apply for jobs with us. please review our job applicant privacy notice for more details on how we handle your personal information. zoominfo may use a software-based assessment as part of the recruitment process. more information about this tool, including the results of the most recent bias audit, is available here. zoominfo is proud to be an equal opportunity employer, hiring based on qualifications, merit, and business needs, and does not discriminate based on protected status. we welcome all applicants and are committed to providing equal employment opportunities regardless of sex, race, age, color, national origin, sexual orientation, gender identity, marital status, disability status, religion, protected military or veteran status, medical condition, or any other characteristic protected by applicable law. we also consider qualified candidates with criminal histories in accordance with legal requirements. for massachusetts applicants: it is unlawful in massachusetts to require or administer a lie detector test as a condition of employment or continued employment. an employer who violates this law shall be subject to criminal penalties and civil liability. zoominfo does not administer lie detector tests to applicants in any location.",canada,Data Engineer,"['airflow', 'aws', 'cloud', 'data lake', 'data pipeline', 'data warehouse', 'dbt', 'elt', 'etl', 'excel', 'gcp', 'kafka', 'python', 'r', 'scala', 'snowflake', 'sql']","['airflow', 'aws', 'cloud', 'data lake', 'data pipeline', 'data warehouse', 'dbt', 'elt', 'etl', 'excel', 'gcp', 'kafka', 'python', 'r', 'scala', 'snowflake', 'sql']",
jr client data engineer,acosta,"description the jr client data engineer position will assist in developing and managing the ingestion, storage, modeling, and consumption of data for mosaic and our clients. this role involves transforming raw data into usable formats, ensuring efficient data storage and processing, and supporting the back end of our data environment. the jr client data engineer will work closely with senior engineers and other teams to create and monitor data solutions, resolve incidents, optimize workloads, and explore emerging technologies. responsibilities • work with the broader team and client service to understand and implement solutions based on high level designs and requirements. • help create datasets for analytics by designing logical data models and turning them into physical data structures. • support the integration of data from disparate sources and systems, including databases, apis, files, and streaming platforms. • assist in ensuring analytics-ready data is of sufficient quality to make critical operational, tactical, and business decisions. • participate in the software development lifecycle, including requirements gathering, design, development, testing, deployment, and maintenance of data solutions. • develop resources which aid data platform users such as data catalogues, lineage documentation, and user guides. • support the development and maintenance of infrastructure on google cloud platform and/or azure. • develop understanding of all areas of mosaic business to find new opportunities for optimization. • stay current with technology trends to continually develop ingestion, etl/elt, and general data/software skills, and evangelize within the team. • ensure data security and compliance with company policies. qualifications minimum qualifications: • a bachelor’s degree in computer science, engineering, information technology, or equivalent. • proficiency in sql and familiarity with programming languages such as python. • experience with software version control with git and git management platforms such as github or gitlab. • basic understanding of cloud computing platforms such as gcp/aws/azure and their services. • understanding of data warehousing and data modeling concepts. • familiarity with data orchestration and workflow management tools such as airflow. • ability to handle data processing tasks and the support the development of scalable data applications. preferred qualifications: • excellent communication skills, with the ability to interact comfortably with peers and leadership both internally and externally. • hands-on experience with data manipulation libraries and frameworks like polars, pandas, spark, or similar tool. • familiarity with data warehousing concepts and technologies, such as google bigquery, amazon redshift, or snowflake, and basic experience in writing and optimizing sql queries for these platforms. • knowledge of etl/elt processes and tools, with basic understanding of how to build and manage data pipelines. • exposure to containerized tools like docker or kubernetes in a plus. • basic knowledge or experience with infrastructure as code (iac) tools like terraform or pulumi. • some experience with data visualization tools such as tableau, power bi or looker. • proven ability to manage and complete projects within deadlines, even in an ambiguous or rapidly changing environment.",canada,Data Engineer,"['airflow', 'aws', 'azure', 'bigquery', 'cloud', 'data pipeline', 'elt', 'etl', 'excel', 'gcp', 'google cloud', 'looker', 'pandas', 'power bi', 'python', 'r', 'redshift', 'scala', 'snowflake', 'spark', 'sql', 'tableau']","['airflow', 'aws', 'azure', 'bigquery', 'cloud', 'data pipeline', 'elt', 'etl', 'excel', 'gcp', 'google cloud', 'looker', 'pandas', 'power bi', 'python', 'r', 'redshift', 'scala', 'snowflake', 'spark', 'sql', 'tableau']",
data engineer- aws cloud,j&m group,"join to apply for the data engineer - aws cloud role at j&m group . have strong knowledge of aws and its related services. have hands-on experience with python, spark, aws glue, and redshift. have strong knowledge of airflow scheduling and aws monitoring tools like cloudwatch and kibana. have strong knowledge of aws emr services. experience in migrating workloads from on-premises to cloud environments. knowledge of incident, problem, and change management processes. strong problem-solving skills to troubleshoot and optimize automated processes. ability to analyze business processes and identify automation opportunities. skilled in data interpretation and making data-driven decisions. excellent communication skills to interact with stakeholders and team members. ability to work collaboratively in a team environment. strong organizational skills to manage multiple tasks and priorities. seniority level • entry level employment type • contract job function • information technology industries • it services and it consulting #j-18808-ljbffr",canada,Data Engineer,"['airflow', 'aws', 'cloud', 'excel', 'python', 'r', 'redshift', 'spark']","['airflow', 'aws', 'cloud', 'excel', 'python', 'r', 'redshift', 'spark']",
cloud data engineer,alphapoint,"about us alphapoint’s ai labs’ team of engineers and ai scientists is solving complex business problems by bridging the gap between transformative breakthroughs in ai technology and increasingly competitive markets. our team is developing and applying the latest generative ai, data and knowledge modeling technologies to large scale problems, right at the edge of what is possible. alphapoint is a financial technology company powering digital asset exchanges and brokerages worldwide. the role • build a scalable and highly performant infrastructure to process batch and real-time workloads • work with the ai engineering team and external engineering teams to monitor and extract data from a vast array of data sources • implement etl data pipelines • architect backend data solutions to support various microservices • develop third-party integrations with large-scale legacy systems you • bachelor’s degree in computer science or similar discipline • 7-10 years experience in software development • proficient in python, node.js, and/or java • familiarity with the basic principles of distributed computing and data modeling • experience building etl pipelines using apache airflow and spark, databricks, or other pipeline orchestration tools • experience with nosql databases such as mongodb, cassandra, dynamodb, or cosmosdb • experience with real-time stream processing systems like kafka, aws kinesis, gcp data flow • experience with redis, elasticsearch, solr • experience with messaging systems like rabbitmq, aws sqs, gcp cloud tasks • ability to find creative ways to harvest data in unstructured formats by scraping, modeling, and ingesting data into semantic databases and graphs • familiarity with delta lake and parquet files • familiarity with one or more cloud providers: aws, gcp, or azure • proficiency with test driven development (tdd) • proficiency with git using services such as github or bitbucket preferred qualifications • experience in a production environment with large-scale knowledge systems. • great written and verbal communication skills • team player hungry to learn from and teach fellow team members benefits • 100% remote work environment • competitive compensation • equity or stock options (if applicable) • a culture of autonomy, experimentation, and learning • opportunity to make a real impact on company trajectory fog8n0tf7y",canada,Data Engineer,"['airflow', 'aws', 'azure', 'cloud', 'data pipeline', 'databricks', 'elt', 'etl', 'experimentation', 'gcp', 'java', 'kafka', 'python', 'r', 'scala', 'spark', 'sql']","['airflow', 'aws', 'azure', 'cloud', 'data pipeline', 'databricks', 'elt', 'etl', 'experimentation', 'gcp', 'java', 'kafka', 'python', 'r', 'scala', 'spark', 'sql']",
lead data engineer - private equity & portfolio support - toronto (hybrid role),saragossa,"if you're an experienced data engineering professional with a buy side financial background who's ready for their next challenge, consider this greenfielf contract-to-hire opportunity. my client is a small-mid sized private equity firm headquarted in toronto with about $5b aum, and currently oversee 15 active portfolio companies. they take a gradual investment approach by investing in 1-2 companies yearly, having kept this pace for the past 5 or so years and have continually grown their portfolio. their internal technology staff is lean, consisting of rougly 30 data orientated professionals. much of the work they undertake is directly involved with their portfolio companies as the majority of them don't have the resources in place to build out modernized data functions on their own. they're looking for a few talented data engineering professionals who can start to taking on a vast majority of the work their internal team has been shouldering over the years, and ultimately becoming one of the data execs/leaders for some of their portco's. most of their portfolio companies utilize the same technology stack, as they've strategized having consistency across the board as they continue to grow. they're in need of engineers with technology experience/skills with databricks, dbt, bi, and python/sql, and have worked in dynamic investment environments previuously (any buy side finance company would suffice). if you're interested to throw your hat in the ring and learn more about this unique opportunity in the private equity space, we encourage you to apply ahead of the new year as interviews will start in january.",canada,Data Engineer,"['databricks', 'dbt', 'python', 'r', 'sql']","['databricks', 'dbt', 'python', 'r', 'sql']",US$100–US$110 an hour
azure data engineer lead,methodhub,"job title: lead data engineer location: canada (hybrid toronto) 12 + months contract position about the role: we are seeking an experienced lead data engineer to spearhead the design, development, and implementation of our enterprise data platform. this role will be instrumental in shaping and driving our data engineering roadmap, ensuring scalability, performance, and innovation. the ideal candidate combines deep technical expertise with strong leadership skills and a solid understanding of financial services data domains. key responsibilities: • lead the data architecture and engineering strategy , ensuring alignment with business and technology roadmaps. • design and implement data models, pipelines, and data integration frameworks across multiple platforms. • partner with stakeholders to translate business requirements into scalable data solutions . • performance optimization: optimize data pipelines for performance, scalability, and reliability, including query tuning and resource management within snowflake. • drive adoption of best practices in data engineering design patterns and modern cloud architectures. • data quality assurance: implement and monitor data validation procedures to ensure data accuracy and consistency across systems. • collaboration and communication: work closely with project managers, data architects, and business analysts to align project milestones and deliverables with business goals. • mentor and guide data engineering teams (onsite & offshore) to deliver high-quality outcomes. • ensure compliance with data governance, security, and privacy standards. • documentation: create and maintain detailed documentation of data pipelines, data flow diagrams, and transformation logic. • issue resolution: troubleshoot and resolve issues related to data pipelines, including job failures and performance bottlenecks. required qualifications: • bachelor's degree in computer science, information technology, or a related field. • 8+ years of experience in data engineering with a strong focus on data architecture , data modelling (conceptual, logical, physical), elt processes and data pipeline development. • hands-on experience with snowflake cloud data platform, including data sharing, secure views, and performance optimization. • proficiency in sql and familiarity with data integration and etl/elt tools. • azure data services and other cloud technologies (glue, emr) • data bricks hands on experience • python for data engineering workflows • strong understanding of data engineering design patterns • strong problem-solving skills and the ability to work independently to meet deadlines. • excellent communication skills for effectively interacting with technical and non-technical stakeholders. preferred qualifications: • certifications in snowflake or relevant data technologies. • experience in the financial services sector, especially asset management / alternative investments, with an understanding of data security and compliance requirements. • familiarity with cloud platforms (e.g., aws, azure) and data orchestration tools (e.g., apache airflow). • knowledge of data visualization tools (e.g., tableau, power bi).",canada,Data Engineer,"['airflow', 'aws', 'azure', 'cloud', 'data pipeline', 'elt', 'etl', 'excel', 'power bi', 'python', 'r', 'scala', 'snowflake', 'sql', 'tableau']","['airflow', 'aws', 'azure', 'cloud', 'data pipeline', 'elt', 'etl', 'excel', 'power bi', 'python', 'r', 'scala', 'snowflake', 'sql', 'tableau']",
senior data engineer (databricks focused),bdo canada llp,"putting people first, every day bdo is a firm built on a foundation of positive relationships with our people and our clients. each day, our professionals provide exceptional service, helping clients with advice and insight they can trust. in turn, we offer an award-winning environment that fosters a people-first culture with a high priority on your personal and professional growth. your opportunity bdo digital is seeking an experienced and technically proficient senior data engineer (databricks focused) to join our technology advisory services practice. this role blends technical leadership with hands-on data engineering, focusing on delivering scalable, secure, and production-grade data pipelines using azure and databricks. the successful candidate will also lead client engagements, aligning data initiatives with business objectives to create real-world impact. key responsibilities data engineering and platform development design, build, and optimize robust data pipelines using databricks, delta lake, and apache spark. develop scalable etl processes for structured and unstructured data, ensuring data quality and reliability. implement data integration solutions across cloud and on-premises sources, leveraging databricks and azure data lake. automate data ingestion, transformation, and validation workflows. monitor, troubleshoot, and enhance data pipeline performance and reliability. ensure compliance, security, and operational excellence across all data workflows. client and team leadership serve as a primary technical lead for client engagements focused on data engineering and analytics solution delivery. translate business requirements into scalable data architectures and operational plans. collaborate with cross-functional teams to integrate data solutions into production environments. mentor junior team members and promote adoption of best practices in data engineering and cloud analytics. data application convert business requirements and analytics prototypes into robust, scalable data solutions. apply appropriate data modeling and transformation techniques to support analytics and machine learning initiatives. document data pipelines and contribute to internal knowledge sharing. qualifications required educational background: a bachelor’s or master’s degree in computer science, data engineering, information systems, or a closely related discipline. strong foundation in data structures, algorithms, and distributed systems. professional experience: a minimum of 5 years of hands-on experience in data engineering, including at least 3 years of direct experience with databricks, apache spark, and cloud data platforms. proven success in deploying and maintaining large-scale data solutions in production environments. in-depth knowledge of the microsoft azure ecosystem, with demonstrated experience using services such as azure data lake, azure databricks, azure synapse, and azure devops. very strong proficiency with databricks, including hands-on work with delta lake, apache spark, and data pipeline orchestration. experience optimizing performance and reliability in production. advanced programming skills in python and/or scala, with practical experience using data engineering libraries and frameworks. solid understanding of ci/cd practices, with experience designing and maintaining pipelines using tools like github actions, azure devops, or jenkins. familiarity with infrastructure-as-code tools such as terraform or arm templates for automating environment provisioning and deployment. excellent written and verbal communication skills, with the ability to effectively engage with a range of stakeholders, including data scientists, engineers, business partners, and executive leadership. proven ability to explain technical concepts to non-technical audiences and influence decision-making. preferred certifications: professional certifications such as databricks certified data engineer or microsoft certified, e.g. azure data engineer associate. experience with data governance, data security, and compliance frameworks. consulting experience and excellent communication and client management skills. why bdo? our people-first approach to talent has earned us a spot among canada’s top 100 employers for 2025. this recognition is a milestone we’re thrilled to add to our collection of awards for both experienced and student talent experiences. our firm is committed to providing an environment where you can be successful in the following ways: we enable you to engage with how we change and evolve, being a key contributor to the success and growth of bdo in canada. we help you become a better professional within our services, industries, and markets with extensive opportunities for learning and development. we support your achievement of personal goals outside of the office and making an impact on your community. giving back adds up: where company meets community. bdo is actively involved in our communities by supporting local charity initiatives. we support staff with local and national events where you will be given the opportunity to contribute to your community. total rewards that matter: we pay for performance with competitive total cash compensation that recognizes and rewards your contribution. we provide flexible benefits from day one, and a market leading personal time off policy. we are committed to supporting your overall wellness beyond working hours and provide reimbursement for wellness initiatives that fit your lifestyle. everyone counts: we believe every employee should have the opportunity to participate and succeed. through leadership by our diversity, equity and inclusion leader, we are committed to a workplace culture of respect, inclusion, and diversity. we recognize and celebrate the valuable differences among each of us, including race, religious beliefs, physical or mental disabilities, age, place of origin, marital status, family status, gender or gender identity and sexual orientation. if you require accommodation to complete the application process, please contact us. flexibility: all bdo personnel are expected to spend some of their time working in the office, at the client site, and virtually unless accommodations or alternative work arrangements are in place. our model is a blended approach designed to support the flexible needs of our people, the firm and our clients. it’s about creating work experiences that meet everyone’s needs and providing flexibility to adjust when, where and how we work to meet the expectations of our role. code of conduct: our code of conduct sets clear standards for how we conduct business. it reflects our shared values and commitments and includes guiding principles to help us make ethical decisions and maintain trust with each other, our clients, and the public. with your consent, bdo canada may use ai technology (microsoft copilot) to transcribe during preliminary conversations, solely for the purpose of note-taking and not for other purposes, such as resume review, evaluation or selection of candidates. more information on bdo canada’s privacy policy can be found here: privacy policy | bdo canada ready to make your mark at bdo? click “apply now” to send your up-to-date resume to one of our talent acquisition specialists. to explore other opportunities at bdo, check out our careers page. #li-sa3 when it comes to our people, we believe in helping you unlock possibilities, build your passions, and grow your competitive edge. from our expansive client base to growing talent pool, our people are the core of bdo. our multi-award winning talent experience continues to make us proud. our culture is open and collaborative. we foster inclusive ways to thinking and celebrate diversity among all contributors. we respect each other, do all things with integrity, and support our people to be their most authentic selves.",canada,Data Engineer,"['azure', 'cloud', 'data lake', 'data pipeline', 'databricks', 'elt', 'etl', 'excel', 'machine learning', 'python', 'r', 'scala', 'spark']","['azure', 'cloud', 'data lake', 'data pipeline', 'databricks', 'elt', 'etl', 'excel', 'machine learning', 'python', 'r', 'scala', 'spark']",
senior data engineer – snowflake,recrute action,"senior data engineer – snowflake / dbt / aws glue strategic role in the insurance industry for a skilled data expert. leverage snowflake, python, sql, aws, and dbt to build scalable data ecosystems and enable ai capabilities. enjoy a hybrid environment and influence enterprise-level data architecture and engineering standards across canada. what is in it for you: • salaried: $ per hour. • incorporated business rate: $90-100 per hour. • 9-month contract. • full-time position: 37.50 hours per week. • hybrid model: 3 days per week on-site, subject to change. responsibilities: • advocate for engineering standards and process efficiencies to ensure high-quality, timely delivery. • lead the development of complex, high-performance data pipelines using tools like dbt core/cloud. • design and review conceptual, logical, and physical data models based on business and technical requirements. • own and maintain robust, reusable code in sql, python, shell, and terraform. • develop detailed low-level engineering solution design documents aligned with technical standards. • create and review data test plans to ensure solution quality. • promote the use of data catalogs, data governance, and data quality practices. • conduct root cause analysis and implement effective solutions for technical data issues. • lead scrum ceremonies and foster an agile mindset across the team. • mentor and support junior data engineers to elevate engineering practices. • collaborate across functions to deliver customer-centric data products. • drive technical presentations and offer constructive feedback on data designs and processes. • plan and execute data release activities for smooth, high-performance delivery. • support talent acquisition through interview design, participation, and hiring decisions. what you will need to succeed: • bachelor's degree or higher in computer science, engineering, or related field. • 8+ years of professional experience in data engineering with a track record of delivering 3+ full-cycle high-impact data projects. • certification(s) in any of the following are considered assets: • snowpro core • snowpro advanced: data engineer (dea-c01 or dea-c02) • dbt developer • aws cloud practitioner • expert-level coding in sql, python, glue, dbt, shell, and terraform with focus on maintainability and performance. • deep expertise in relational (snowflake, postgresql, amazon aurora), big data (hadoop), and nosql (mongodb) platforms. • proficiency with data visualization tools: snow sight, streamlit, qlik, sap business objects. • experience with data orchestration and pipeline tools such as zena and aws managed airflow. • high resilience and adaptability in ambiguous or high-pressure environments. • strong collaboration and communication skills with ability to influence stakeholders and lead teams. • a customer-first mindset driven by data insights. • insurance industry knowledge is an asset. • experience with ai/ml model operationalization is an asset. why recruit action? recruit action (agency permit: ap provides recruitment services through quality support and a personalized approach to job seekers and businesses. only candidates who match hiring criteria will be contacted. # avicjp",canada,Data Engineer,"['airflow', 'aws', 'cloud', 'data pipeline', 'dbt', 'hadoop', 'python', 'r', 'scala', 'snowflake', 'sql']","['airflow', 'aws', 'cloud', 'data pipeline', 'dbt', 'hadoop', 'python', 'r', 'scala', 'snowflake', 'sql']",
data engineering specialist - healthcare,mariner innovations,"who we are: mariner innovates always and everywhere, turning novel ideas into practical solutions. from products and services to venture creation, we build better futures powered by people. our purpose has always been to be a great, everlasting technology company for those who make an impact - elevating people, data, and technology in the best way to solve hard problems. we partner with people embracing change, leveraging data, cybersecurity, cloud, and change management to drive successful business outcomes that matter to our customers. mariners build for change. like shift energy, our energy management and decarbonization company. or east valley, created to scale social economic impact in our communities through investment and mentorship of emerging change-makers and their companies. mariner’s purpose has always been to be a great, everlasting technology company and the core values that guide us every day are: our values • we care for our team. • we care about the impact we have on our community. • we serve by solving complex problems. • we grow because growth fuels opportunities. • we innovate always and everywhere. • we build a business that lasts. overview of the role as a data engineer, you will be responsible for designing, develop, test, and deploy azure-based data pipelines and architecture supporting the adoption of fhir r4 and secure, standards-based data exchange. you will collaborate closely with cross-functional teams to ensure efficient data processing, storage, and accessibility for analytical and operational purposes. what you’ll be doing in the role • work closely with project team members and subject matter experts to capture metadata from various data sources feeding the azure environment. understand existing scripts, tools, and processes used for data extraction in current business intelligence systems. • perform data profiling and generate descriptive metrics to assess the characteristics and quality of source system data. • design and implement robust data pipelines to move data from on-premises systems to the azure landing zone, and between data zones within the azure environment. this includes developing real-time data movement pipelines and ensuring all processes are well-documented. • create comprehensive documentation describing each pipeline, including file structures, formats (e.g., csv, json), metadata, file sizes, attributes, and codes for verification. • assemble data into secure, well-structured packages for transport and storage in recognized formats suitable for parsing and extraction. • contribute to the development of testing strategies, plans, and scripts to validate data engineering work. execute tests and report to ensure data accuracy and quality standards are met. • collaborate with the technical and business stakeholders to create detailed schedules and work plans for all data engineering activities. • provide written progress reports highlighting completed activities, potential risks, and outstanding issues impacting project timelines. • deliver high-quality outputs including data extract and transport design documents, implemented and tested data engineering code, metadata and data profiling documentation, data quality dashboards, and comprehensive test plans and reports. what skills / qualifications you’ll need to do the job • minimum of five years of experience in technology-related roles. • at least three years of experience in data and analytics, software engineering, or software architecture, preferably in business intelligence (bi) and data analytics environments. • more than two years of recent experience with cloud-native tools and data strategies on the microsoft data platform, including microsoft fabric, data factory, data lake storage, databricks, synapse, azure sql, and azure analysis services. • proven experience profiling and extracting data, developing data pipelines, and integrating data across multiple systems. • demonstrated ability to model data for bi and analytics use cases such as enterprise data warehouses and data marts. • strong knowledge of data security models and encryption practices related to the transmission, use, and storage of sensitive or large-scale (“big data”) information. • experience working in agile or devops environments on complex data projects. • proven ability to collaborate effectively with multiple stakeholder groups across leadership levels, departments, and organizations. • experience in the health sector and/or canadian public sector within a bi or analytics context is preferred. • experience with the fhir r4 standard and health-sector master data (e.g., clients/patients, providers) is an asset. • experience working with primary care electronic medical record (emr) data is considered an asset. …and you are... • always improving your skills and knowledge: you want to be the best at what you do • curious and creative; comfortable taking on new problems and challenges • a self-starter with the ability to recommend priorities to the project leader • comfortable working through problems and figuring things out with minimal supervision • great problem-solving skills: thorough and reliable • able to work in a team: share knowledge and assist other team members • a good communicator: able to explain your ideas and recommendations • well organized and dependable under pressure: you manage your time effectively • energized by our company values! mariner offers you • a values-driven workplace where people really matter • flexible work location, support for remote work and some on-site work may be required. • competitive salary, retirement savings program, and rewards program • comprehensive health, dental, vision, life and disability insurance plans and access to e-health care • paid vacation, maternity/parental leave, paid sick leave and paid mariner mylife days • unlimited training why mariner? we believe in making a positive impact in the communities where we live and work! not only are we passionate about nurturing a thriving technology sector here in atlantic canada, but we are also committed to solving technology challenges that will make an impact worldwide. technology is everlasting, and we take pride in connecting world class talent to world class projects and business solutions. if you are an it professional, software professional, or management consultant, that is passionate about career development, data, and problem solving, we want to hear from you! if this sounds like someone you know, share this with them so they do not miss the opportunity to experience mariner! to achieve this, we know we need to be all in on training and development of our team so we can continue to move the needle in the technology sector together. of course, we offer competitive compensation and benefits packages to our team, but our competitive edge is the development and training that our team experiences every day. be part of our vibrant community that values diversity and embraces inclusion at every level. we are committed to fostering a culture where individuals from all backgrounds and walks of life feel empowered to contribute their unique perspectives. our collaborative environment celebrates the richness of ideas and experiences, and we believe that by embracing diversity, we can achieve greater innovation and excellence together. come and help us create a workplace that reflects the beautiful tapestry of our global community. join our growing team!",canada,Data Engineer,"['azure', 'business intelligence', 'cloud', 'dashboard', 'data analytics', 'data lake', 'data pipeline', 'data warehouse', 'databricks', 'excel', 'r', 'recommendation', 'sql']","['azure', 'business intelligence', 'cloud', 'dashboard', 'data analytics', 'data lake', 'data pipeline', 'data warehouse', 'databricks', 'excel', 'r', 'recommendation', 'sql']",
data engineer (m/f/x),makersite gmbh,"data engineer (m/f/x) location: eu (remote) who you’ll work for : at makersite, we're pioneering the future of sustainable product development and digital collaboration. as a leading platform for product lifecycle management (plm), we empower companies to make smarter, more sustainable decisions across their entire supply chain. our cutting-edge software enables teams to design, prototype, and manufacture with transparency, efficiency, and responsibility—reducing environmental impact while optimizing performance. we're a fast-growing, innovative company that thrives on creativity, collaboration, and continuous learning. if you're passionate about technology, sustainability, and creating meaningful impact, we’d love to hear from you. join us and be a part of shaping the future of manufacturing and product innovation. who we're looking for: as part of our growing team, you will play a key role in shaping the future of sustainable product development. we’re looking for innovative, driven data engineers who are passionate about technology and sustainability to join us in building tools that enable companies to make smarter, more responsible decisions. in this role, you’ll have the opportunity to collaborate with talented professionals, contribute to cutting-edge projects, and help drive the digital transformation of industries worldwide. if you're ready to make an impact and contribute to meaningful change, we want to hear from you! this role is a fixed, permanent position. all successful applicants will receive a permanent employment contract regardless of location. the role: • design, build, and maintain scalable data pipelines and infrastructure to support analytics and machine learning workflows. • develop and optimize data storage solutions (e.g., data lakes, warehouses, databases) for performance, reliability, and scalability. • ensure data quality, consistency, and accessibility through robust validation, cleaning, and transformation processes. • collaborate with data scientists, analysts, and other stakeholders to understand data needs and translate them into technical solutions. • integrate data from multiple internal and external sources, ensuring security and compliance with data governance policies. • support machine learning initiatives by preparing, structuring, and serving data for model training, testing, and deployment. • monitor, troubleshoot, and improve data systems for performance, cost-efficiency, and reliability. • document processes, data flows, and best practices to ensure maintainability and knowledge sharing across the team. • stay up to date with emerging data engineering tools, technologies, and best practices to continuously improve our data infrastructure. • proven experience in ai/ml concepts and applying them in real-world data workflows. • 5+ years of full-time commercial experience with sql and python (minimum 3 years considered). • hands-on experience with data orchestration tools such as airflow, metaflow, or prefect . • strong knowledge of cloud platforms , with expertise in aws (azure is a plus). • solid understanding of data compliance and security best practices , particularly when working with sensitive client data. • exposure to tracing and monitoring frameworks such as grafana, sentry, or opentelemetry . • experience handling both structured data (with defined schemas and rules) and unstructured data (requiring consolidation and organization). • residing in and legally permitted to work in the eu. what we offer: • competitive salary – we reward your skills and experience with a compensation package that reflects your value. • 30 days paid time off – take the time you need to recharge and maintain a healthy work-life balance. • remote-first flexibility – work from anywhere in the eu, with the option to collaborate in person at our offices in stuttgart, berlin (role dependent). • generous learning & development budget – we invest in your growth, providing ample resources for personal and professional development. • choose your ideal work equipment – whether you prefer apple or microsoft, we’ll equip you with the tools you need to excel. the experience: • purpose-driven work – build something meaningful. here, you’re not just creating another app; you’re contributing to a transformative vision that you can be proud of. • great colleagues – join a team that’s open, transparent, and international. we value inclusivity and foster a culture of collaboration and mutual respect. • work-life balance – as a remote-first company, we trust you to do your best work, your way. you have the flexibility to work when it suits you, ensuring a healthy balance between personal and professional life. • pride in impact – we're already making a significant difference with global, well-known customers who share our vision. be part of something that matters. • stability with ambition – with realistic growth plans, we’re committed to changing the way things are made—without sacrificing our ambitious goals for innovation and impact. if you want to find out more about what it’s like to work at makersite, check out our careers page diversity and inclusion if you have a medical condition or an individual need for an adjustment to our process, and you believe this may affect your ability to be at your best – please let us know so we can talk about how we can best support you and make any adjustments that may be needed. at makersite, we are dedicated to fostering an environment that champions diversity, equity, inclusion, and belonging. we believe that diverse teams drive innovation and success, and we are proud to be an equal opportunity employer. we welcome applicants from all backgrounds and will consider all applications regardless of age, disability, gender identity, marital status, pregnancy or maternity, race, nationality, religion, sex, sexual orientation, or any other status protected by applicable law. rest assured, all applications will be handled with the utmost confidentiality.",canada,Data Engineer,"['airflow', 'aws', 'azure', 'cloud', 'data lake', 'data pipeline', 'excel', 'machine learning', 'python', 'r', 'scala', 'sql']","['airflow', 'aws', 'azure', 'cloud', 'data lake', 'data pipeline', 'excel', 'machine learning', 'python', 'r', 'scala', 'sql']",
software engineer - data,n3xt,"liberating money data software engineer location: remote (can) job type: full time, remote position summary we are seeking a talented data engineer to build, scale, and own the data backbone of our platform. you will be responsible for designing and implementing robust etl pipelines, managing our data lakes, and creating the libraries that power our analytics, compliance, and product features. this role is critical to our success, as you will ensure that high-quality data is available, reliable, and accessible to drive business decisions and power our core services. you will work with a modern tech stack on google cloud platform, including dataflow and bigquery, and will have the opportunity to solve complex challenges in the fintech and web3 space. key responsibilities • design and build data pipelines: architect, develop, and maintain scalable and reliable etl/elt pipelines using google cloud dataflow, python, and bigquery to process large volumes of structured and semi-structured data. • backend & library development: contribute to the development of backend services and data-centric libraries in python and typescript/node.js, ensuring they are well-tested, performant, and maintainable. • platform integration: ingest and process data from critical third-party services, including persona (kyc/kyb), sardine (fraud/compliance), and stytch (authentication), ensuring data integrity and availability. • data architecture & strategy: contribute to the technical direction of our data domain, including event cataloging, schema design and evolution, and data governance practices. • system scalability & reliability: gain a deep understanding of our cloud architecture to ensure the high availability and scalability of our apis, data processing reactors, and ledger systems. • mentorship & collaboration: act as a technical mentor for junior engineers and a subject-matter expert for business stakeholders, helping them effectively consume and interpret platform data. • feature delivery: consistently deliver high-quality features and associated tests in alignment with our product roadmap. what you'll bring (required qualifications) • 3+ years of professional software engineering experience, with a significant focus on data engineering or backend systems. • strong proficiency in python for data processing, scripting, and etl development. • hands-on experience with google cloud platform (gcp), specifically with dataflow, bigquery, and cloud storage. • experience building or contributing to backend services and apis using typescript and node.js. • solid understanding of data modeling, database design (sql and nosql), and data warehousing concepts. • experience with infrastructure as code (iac) tools, preferably terraform. • a strong foundation in software development best practices, including version control (git), automated testing, and ci/cd. • excellent problem-solving skills and the ability to work independently in a fast-paced environment. nice to haves (preferred qualifications) • familiarity with the fintech, blockchain, or web3 ecosystems and concepts like smart contracts. • direct experience integrating with apis for identity verification (e.g., persona), fraud detection (e.g., sardine), or authentication (e.g., stytch). • experience with other gcp services such as pub/sub, cloud functions, and composer. • knowledge of containerization and orchestration technologies like docker and kubernetes. • previous experience mentoring junior engineers or acting as a tech lead on projects. • a bachelor's or master's degree in computer science, engineering, or a related field. why join us? • be part of a high-impact team • work with cutting-edge technology and regulatory frameworks • work with a diverse, global team in a remote-friendly environment. • competitive salary, benefits and professional development support. the pay range for this role is: 140,000 - 170,000 cad per year(remote (canada))",canada,Data Engineer,"['bigquery', 'cloud', 'data lake', 'data pipeline', 'elt', 'etl', 'excel', 'gcp', 'google cloud', 'python', 'r', 'scala', 'sql']","['bigquery', 'cloud', 'data lake', 'data pipeline', 'elt', 'etl', 'excel', 'gcp', 'google cloud', 'python', 'r', 'scala', 'sql']",$140K–$170K a year
intermediate data engineer,agentnoon,"we are seeking an intermediate data engineer to help build the world's best organizational transformation product for complex workforce planning, m&a, and organizational modeling. our customers are making multi-hundred million dollar plans and decisions on our product with game-like ux. if you have built something that solves complex data problems or sophisticated functionality that solves difficult problems in a clever way, we'd love to talk. user experience, performance, and velocity are our top priorities. we are obsessed with them. key responsibilities • work on data engineering, backend, and security layer • engage customers in solving urgent and important problems qualifications • ability to design and implement a performant and secure backend system • motivated to make everything fast and secure • end-to-end performance optimizations. • nodejs experience. • (bonus) experience with a complicated single-page web app. • (bonus) vuejs experience • postgres and query optimization experience. • (bonus) experience with expressjs, firebase auth, and firestore • previous startup experience. • have built a complex project that can be demonstrated • great communicator • obsessed with great ui/ux/performance • sense of urgency to get things done • works hard, learns fast. and want to grow quickly • love to code, and understand that shipped code is better than perfect code expectations • have a great sense of what a great product looks like and can build a delightful product. • great and concise writer. everyone writes a lot to minimize the number of meetings to focus on building the product. • get things done fast with a high-quality bar. • end-to-end ownership of projects. projects are typically defined in collaboration with customers, product, design, and other engineers. • operates autonomously and seeks help to get things done. • actively involved in product planning. • proactively seeks additional work. able to be self-sufficient and contribute to team goals. • seeks to identify project risks and autonomously keeps projects on track. keeps the team updated. • contributes outside of their core projects, including code reviews, interviewing, and establishing company/engineering culture.",canada,Data Engineer,['r'],['r'],
lead data engineer,fusemachines,"about fusemachines fusemachines is a leading ai strategy, talent, and education services provider. founded by sameer maskey ph.d., adjunct associate professor at columbia university, fusemachines has a core mission of democratizing ai. with a presence in 4 countries (nepal, united states, canada, and dominican republic and more than 400 full-time employees). fusemachines seeks to bring its global expertise in ai to transform companies around the world. about the role location: remote (full-time) this is a full-time position responsible for designing, building, maintaining, and optimizing the infrastructure required for data integration (batch and real-time), storage (including databases and data modeling), processing, and analytics (bi, visualization, and advanced analytics) using microsoft azure in the public sector. the lead data engineer works closely with cross-functional teams supporting business objectives and serves as an azure cloud solutions subject matter expert (sme), collaborating with the delivery team on solution design and implementation. this role is split approximately 50/50 between hands-on data engineering (building pipelines, modeling data, troubleshooting) and solution architecture (designing the platform, setting standards, and leading technical direction). qualifications & experience • bachelor’s degree in computer science, information systems, or a related field. • minimum 5+ years as a data & analytics solution architect or senior/lead data engineer with strong hands-on experience in microsoft azure environments. • 5+ years working with azure cloud platform and azure devops (exposure to aws/gcp is a plus). • proven experience delivering end-to-end data and analytics projects, including hands-on development as a senior or lead data engineer. • deep and recent experience with microsoft fabric, azure databricks, azure functions, and pyspark for real-time and low-latency data pipeline design. • expertise in data modeling, integration (etl/elt), warehousing, and bi using services such as data factory, data lake, stream analytics, and azure sql database. • experience designing and implementing graphql and restful apis within azure ecosystems • preferred certifications • microsoft certified: azure fundamentals • microsoft certified: azure data engineer associate • microsoft certified: azure solutions architect expert • microsoft certified: azure ai engineer associat required skills & competencies • deep experience designing and implementing end-to-end data and analytics solutions using microsoft azure services such as microsoft fabric, azure databricks, azure data factory, azure data lake, azure stream analytics, azure sql database, and power bi. • proven hands-on experience building and optimizing real-time, low-latency pipelines using pyspark, azure functions, event hubs, and stream analytics. • strong understanding of ci/cd, infrastructure as code (terraform or arm templates), configuration management, automated testing, and cost optimization principles. • expertise in data modeling, data integration (etl/elt), warehousing, bi, and advanced analytics (ml/ai) within azure environments, ensuring scalable and resilient designs. • proficiency in python and sql, with ability to diagnose performance issues, optimize pipelines, and ensure system reliability. • experience designing data governance, cataloging, data quality, and lineage frameworks; implementing azure security best practices (iam, encryption, compliance); and managing monitoring tools such as azure monitor, application insights, or datadog. • familiarity with kubernetes and docker for scalable architecture, including design and configuration of containerized services. • strong ability to work cross-functionally with architects, engineers, and data scientists, and to clearly document architecture and deployment configurations. • committed to staying updated with emerging azure technologies, ai/ml integrations, and data architecture best practices to drive innovation and efficiency equal opportunity employer: fusemachines is committed to fostering a diverse and inclusive workplace. we welcome applications from all qualified individuals regardless of race, color, religion, sex, sexual orientation, gender identity, national origin, age, genetic information, disability, protected veteran status, or any other legally protected status. important: immigration sponsorship policy fusemachines is unable to proceed with candidates who require any form of work authorization or immigration support from the company. tysodovkrz",canada,Data Engineer,"['aws', 'azure', 'cloud', 'data lake', 'data pipeline', 'databricks', 'elt', 'etl', 'gcp', 'power bi', 'pyspark', 'python', 'r', 'scala', 'spark', 'sql']","['aws', 'azure', 'cloud', 'data lake', 'data pipeline', 'databricks', 'elt', 'etl', 'gcp', 'power bi', 'pyspark', 'python', 'r', 'scala', 'spark', 'sql']",
data engineer - central government,talent international uk ltd,job description: data engineer. central government. inside ir35. hybrid - manchester £480 per day - duration - 12 months our central government client is looking to bring in experienced data engineers with extensive power platforms and full life cycle experience in agile digital (ddat) environments click apply for full job details,manchester,Data Engineer,['r'],['r'],
azure data engineer,tenth revolution group,"role: senior data engineerlocation - newcastle upon tyne/hybrid (2-3 days per week in office)contract - 6 monthsdaily rate - £450 (outside ir35)active sc clearance is a must have key responsibilities design, build, and optimise data pipelines and solutions on the azure platform.implement best practices for data architecture, security, and performance.collaborate with stakeholders to deliver robust and scalable data solutions.troubleshoot and resolve complex data engineering challenges. required skills & experience proven experience as a senior data engineer with strong azure expertise.strong hands-on experience with databricks.proficiency in python, sql, and data modelling.strong understanding of etl processes, data warehousing, and cloud architecture.excellent problem-solving skills and ability to work independently visa sponsorship is not available for this role.",newcastle upon tyne,Data Engineer,"['azure', 'cloud', 'data pipeline', 'databricks', 'etl', 'excel', 'python', 'r', 'scala', 'sql']","['azure', 'cloud', 'data pipeline', 'databricks', 'etl', 'excel', 'python', 'r', 'scala', 'sql']",£400–£450 a day
data engineer,wrk digital,"data engineer contract length: 6 months ir35 status: inside ir35 day rate: £610/day location: york/remote (twice a month in the office) overview wrk digital is excited to partner with a well-known and trusted uk brand to recruit a skilled data engineer for a new contract opportunity. this is a fantastic chance to join a forward-thinking technology team focused on scalability, automation, and cloud infrastructure innovation. the successful candidate will play a key role in designing, building, and optimising data pipelines and analytical workflows, with a strong focus on aws cloud services and alteryx data automation. key responsibilities • design, build, and maintain scalable etl/elt pipelines across aws environments. • develop, optimise, and automate workflows in alteryx to support data transformation and analytics. • collaborate with data analysts, data scientists, and business stakeholders to understand requirements and deliver high-quality data solutions. • implement best practices for data quality, governance, and security within cloud and automation platforms. • monitor and troubleshoot data pipelines and alteryx workflows to ensure reliability and performance. • contribute to ongoing enhancements in cloud architecture, tooling, and data engineering standards. essential skills & experience • strong hands-on expertise with aws, ideally including: • s3, lambda, glue, step functions • redshift or other cloud data warehousing solutions • iam, cloudwatch, and related services • advanced proficiency with alteryx designer and/or alteryx server. • solid sql skills and experience working with relational and cloud-based databases. • familiarity with python or similar scripting languages for automation and data processing. • excellent problem-solving skills and the ability to work independently in a fast-paced environment.",york,Data Engineer,"['aws', 'cloud', 'data pipeline', 'elt', 'etl', 'excel', 'python', 'r', 'redshift', 'scala', 'sql']","['aws', 'cloud', 'data pipeline', 'elt', 'etl', 'excel', 'python', 'r', 'redshift', 'scala', 'sql']",£75–£78 an hour
junior data engineer - 32482,environment agency,"the environment agency are fully committed to having an inclusive workforce to reflect the communities we serve. we don't just talk about diversity; we seek it, embrace it, and live it, for the benefit of our staff, our communities, and our environment. the environment agency is undergoing modernisation of systems and it infrastructure and moving towards microsoft azure cloud computing and microsoft analytics tooling and technology such as synapse analytics, fabric and powerbi. this represents a new and exciting opportunity for the water quality digital services team to become a pioneering group delivering new ways to do reporting and analytics. to prepare for this challenge we are recruiting a junior analytics engineer to support the development and deployment of data products and analytics tools to enable the water quality user community to efficiently leverage data from various sources and, in turn, better inform business insights and drive effective policy changes. the team this is an exciting opportunity to join the digital services team within national evidence and business (e&b). we focus on building innovative products that improve origination efficiency and enhance our ability to transform data into meaningful insights for effective decision-making. we're a dispersed team and work with teams across the country. we promote a positive, inclusive, and supportive culture where everyone feels valued. we use evidence, expertise, engagement, and innovation to enhance delivery., you'll have an inclusive incident management objective in your development plan. we'll help you find a role to suit your needs. appropriate training will be given. you'll have an ea office base location, as a national role the working location is flexible / hybrid. we use smart tools to stay connected and reduce travel some travel and overnights may be required. please read the candidate / additional information pack for information. any queries, contact andrew.chiverton@ applications are blind assessed purely using your answers to the competency questions. interviews will be held via ms teams within four weeks of the closing date. if you consent to being held on a reserve list, we'll hold your details for 6 months and may offer you an alternative post. competence 1 focuses on efficiency, innovation and quality, if a large number of applications are received, an initial sift using this lead capability may be conducted. successful candidates will then proceed to a full sift or directly to assessment/interview. describe a time when you implemented a new innovative approach to satisfy a specific data reporting requirement. describe the approach and what you did to ensure the process was resilient, repeatable and quality assured. competence 2 takes decisions and solves problems, give an example of when you have had to solve a complex data analysis problem that involved diverse data sources. what data did you use? what was the problem and what decisions you took to solve it? competence 3 data and information management, provide an example of a complex data management task you have undertaken. explain how you approached this and provide details on the data, tools and techniques used. if you are applying from the civil service please note that the environment agency is not a part of hm civil service and you would not be a crown servant in the event of being appointed. therefore, you will not be eligible for continuous service. for applicants who currently work in local government or other bodies listed in the redundancy payments (continuity of employment in local government etc) (modification) order 1999, you may be eligible for continuous service for the purpose of calculating any future redundancy payment. if you are unsure of your status then you should contact your own hr team. we are fully committed to having a diverse and inclusive workforce to reflect the communities we serve. we welcome flexible working patterns for all our vacancies, including job share, so please include clearly any information regarding your preferred working arrangements on your application. we also have a guaranteed interview policy to support those with a disability who are seeking employment. we have committed to guaranteeing an interview to anyone with a disability whose application meets the minimum criteria for the post. the environment agency, as a non-departmental public body, is committed to providing value for money and utilises central government frameworks and contracts for all external recruitment needs. for this reason, we are unable to engage with the market directly through post, email or phone calls . should you wish to become a support supplier on one of these frameworks or contracts please visit for more information. artificial intelligence can be a useful tool to support your application, however, all examples and statements provided must be truthful, factually accurate and taken directly from your own experience. where plagiarism has been identified (presenting the ideas and experiences of others, or generated by artificial intelligence, as your own) applications may be withdrawn and internal candidates may be subject to disciplinary action. although the environment agency is a non-departmental public body sponsored by defra, we subscribe to and align with the candidate guidance on the use of artificial intelligence found on the civil service careers website . please review for more information on appropriate and inappropriate use. applicant must have: + extensive experience managing and analysing large datasets from multiple sources to support varied reporting and analytics needs. + experience in organising data in star schema data models that effectively represent business entities and relationships and support efficient querying and analysis from relational data warehouses. + experience building and maintaining reliable etl pipelines to integrate data from diverse sources. + strong python expertise, including development of clean, reusable functions and object-oriented code for production-quality solutions. + solid knowledge of sql server, database management, and data warehousing best practices. + proficiency with analytical tools and programming languages to transform raw data into actionable insights. + solve complex problems and formulate efficient and innovative solutions using sound data management and data analysis practices. + ability to work effectively with people from diverse backgrounds while promoting inclusion and collaboration.",nottingham,Data Engineer,"['azure', 'cloud', 'data analysis', 'data warehouse', 'etl', 'python', 'r', 'sql', 'sql server']","['azure', 'cloud', 'data analysis', 'data warehouse', 'etl', 'python', 'r', 'sql', 'sql server']",
lead data engineer,hireful,"fancy working for a certified b corp? are you a senior azure data engineer looking to shape financial inclusion by building ethical, purpose-driven lending solutions for homeowners across the uk? as a lead data engineer, you will help shape the next generation of our data ecosystem. based in manchester city centre, you’ll lead the architecture, development and delivery of modern azure-based data platforms that power smarter decisions and better outcomes for our customers. in this pivotal role, you’ll design and own scalable data systems, build robust pipelines, mentor a talented team and champion engineering best practice. you’ll work closely with engineering, risk, operations and bi teams to turn complex data into meaningful insights, ensuring our products and processes are fast, secure and future proof. role: lead data engineer, azure data engineer, senior data engineer, data engineering lead, data engineering, principle data engineer, data platform engineer, cloud data engineer salary: £80k - £90k base salary + great benefits and career progression. location: manchester city centre – hybrid working is in place you’ll bring deep experience across sql server, azure data services, data modelling and cloud-native engineering, along with the curiosity to keep improving and the clarity to communicate technical thinking to any audience. this is your opportunity to work with cutting-edge azure fabric tools, shape our architecture roadmap and drive a data-driven culture within a company that invests in innovation and values real craftsmanship. if you want to lead, innovate and build data solutions that genuinely change lives, click apply and send through a copy of a cv.",greater manchester,Data Engineer,"['azure', 'cloud', 'r', 'scala', 'sql', 'sql server']","['azure', 'cloud', 'r', 'scala', 'sql', 'sql server']",£80k–£90k a year
data engineer (national security),sanderson plc,"the role as a data engineer, you'll be responsible for designing, building, and maintaining robust data pipelines and architectures. you will work closely with stakeholders to understand complex data challenges, transform raw data into meaningful insights, and support analytics and reporting. this includes working with batch, streaming, real-time, and unstructured data, applying distributed compute techniques to handle large datasets efficiently. key responsibilities • develop and maintain data ingestion pipelines and orchestration workflows • design database schemas and data models • integrate and enrich data from multiple sources, ensuring consistency and quality • design and implement etl/elt processes (e.g., using apache nifi) • produce reusable, maintainable code with a test-driven approach • maintain and enhance existing data platforms and services • investigate and resolve operational issues in integrated datasets • implement data security measures to protect sensitive information • support agile delivery, breaking down user requirements into actionable tasks • monitor and optimise system performance for reliability and efficiency required skills • apache kafka • apache nifi • sql and nosql databases (e.g., mongodb) • etl/elt development with groovy, python, or java about the employer with over 60 years of experience supporting government and defence programmes, this employer delivers deep technical expertise in sensors, communications, cyber, and advanced analytics. the organisation applies innovation, technology, and data to help clients make informed decisions and protect critical systems and infrastructure. clearances due to the nature of this role, we require you to be eligible to achieve the highest level of security clearance. benefits & culture • work at the cutting edge of technology in defence and national security • opportunity to spend time on innovative r&d projects and concept creation • collaborative, geeky, and creative environment that celebrates technical brilliance • competitive bonus scheme up to £3,000 / 6% of salary • generous holiday: 30 days + bank holidays, 3.5 days over christmas, option to buy/sell extra leave • supportive and engaging culture, focused on growth and innovation • hybrid working: 3 days in the office, 2 days from home, flexible to work fully on-site if needed reasonable adjustments: respect and equality are core values to us. we are proud of the diverse and inclusive community we have built, and we welcome applications from people of all backgrounds and perspectives. our success is driven by our people, united by the spirit of partnership to deliver the best resourcing solutions for our clients. if you need any help or adjustments during the recruitment process for any reason, please let us know when you apply or talk to the recruiters directly so we can support you.",gloucester,Data Engineer,"['data pipeline', 'elt', 'etl', 'java', 'kafka', 'python', 'r', 'sql']","['data pipeline', 'elt', 'etl', 'java', 'kafka', 'python', 'r', 'sql']",£50k–£65k a year
data engineer,robert walters,"data engineer bolton- hybrid £45,000 – £55,000 (dependent on experience) we’re looking for a data engineer experienced in generative ai to join our growing international and cross functional client. in this role, you’ll be responsible for evaluating, building, and maintaining high quality data sets that support their internal stakeholders across the business. you’ll play a key part in ensuring data pipelines are secure, reliable, and scalable. data engineer duties: • design, develop, and maintain robust data pipelines and architectures. • evaluate, build, and optimise data sets for internal customers. • ensure data systems comply with governance and security standards. • collaborate with internal teams to understand data needs and deliver effective solutions. • stay ahead of emerging data and ai technologies, contributing to our innovation roadmap. • support the integration of generative ai capabilities into existing systems. as a data engineer, you will have: • strong experience with sql (e.g., ms sql, oracle). • knowledge of nosql technologies (e.g., mongodb, influxdb, neo4j). • experience with data exchange and processing (etl, esb, apis). • proficiency in python or similar programming languages. • familiarity with big data frameworks (e.g., hadoop ecosystem). desirable skills: • understanding of nlp (natural language processing) and ocr (optical character recognition). • exposure to generative ai concepts and tools. • experience with containerisation (e.g., docker). • background in the industrial or defence sectors. role benefits: • company bonus: (dependent on company performance). • amazing pension scheme • flexi leave • flexible working options available • enhanced parental leave • amazing onsite amenities robert walters operations limited is an employment business and employment agency and welcomes applications from all candidates",bolton,Data Engineer,"['data pipeline', 'etl', 'hadoop', 'natural language processing', 'nlp', 'python', 'r', 'scala', 'sql']","['data pipeline', 'etl', 'hadoop', 'natural language processing', 'nlp', 'python', 'r', 'scala', 'sql']",£45k–£55k a year
analytics engineer,tameside council,"contract type: permanent salary: grade i. scale 35 - £46,142 to scale 37 - £48,226 hours per week: 36 hours base location: tameside one assessment date: 26th january 2026 the role we are seeking an experienced analytics engineer to help establish a data engineering and bi development function here at tameside. the role will support our drive towards data-driven decision making and enhanced public services. you will play a key role in designing, building, and maintaining scalable data pipelines within microsoft’s cutting-edge fabric data analytics platform, integrating disparate datasets, and enabling cross-cut analytics and reporting for various local government functions via power bi. the job role covers the full range of data relevant to local government, from social care case management systems, geospatial information and publicly available datasets and more. it will also play a key part in developing our wider data strategy and maximising the investments being made across our digital infrastructure. the role will sit within a newly established data integration team as part of the council’s business intelligence service, alongside our systems and analytics functions. close links with our ict and information governance teams will also be required. this is an exciting opportunity to join tameside council at the start of its journey to adopting a modern data infrastructure. as much of the work will be ‘green field’, you will be empowered to make significant progress and impact in a short space of time and build your skills and knowledge exponentially in the process. about you you will be an experienced data professional, with a solid grounding in data engineering or bi development or have worked as a data analyst with a knack for the technical side of the role. if you’ve got knowledge and experience of standardising and automating data flows and reports, building dashboards that turn complex data into actionable intelligence and using data-centric coding languages like sql and/or python, that’s great. if you’ve done those things in a local government setting or in a modern paas or saas data platform, that’s even better. you will have good analytical and problem-solving skills, strong technical knowledge, organisational skills and an ability to plan and deliver timely outcomes whilst dealing with multiple enquiries and tasks, excellent written and verbal communication skills, good numerical skills and ability to conduct data and systems analysis and be capable of building relationships with a wide variety of stakeholders. about us at tameside, we are committed to ensuring all our citizens lead long, fulfilling and healthy lives. we are committed to supporting economic growth, providing high quality health and care services, protecting our most vulnerable and creating strong and supportive, self-sufficient communities. we recognise that our people drive our success and enable the organisation to deliver on its vision, purpose and priorities, and that our people are our most important resource and asset. our values underpin our practice and behaviours, and are at the heart of everything that we do; and we understand that how we do things is just as important as what we do. we pride ourselves on living by our strive values of support, trust, respect, integrity, value diversity and engage. our values underpin our practice and behaviours and are at the heart of everything that we do. how we do things are just as important as what we do. our employees’ skills, experience and knowledge are essential to our success along with their happiness, wellbeing, commitment, enthusiasm and motivation to be the best they can be. unfortunately, at this time we are not able to offer sponsorship. we are committed to developing a culture which respects individuals, appreciates difference and allows everyone regardless of background to reach their full potential. we are proud to be an accredited disability confident employer. reasonable adjustments will be considered in accordance with the equality act 2010 in relation to the job role criteria. our rewards and benefits package can be viewed here for further information about this role please contact the recruiting manager dan hodgson on 0161 342 3546 or dan.hodgson@tameside.gov.uk supporting documents ↗ analytics engineer job description and person specification calculate your take home pay. use this service to estimate how much income tax and national insurance you should pay for the current tax year. employment could affect your benefits, find out more here. turn2us benefits calculator.",nan,Data Engineer,"['business intelligence', 'dashboard', 'data analytics', 'data pipeline', 'excel', 'power bi', 'python', 'r', 'scala', 'sql']","['business intelligence', 'dashboard', 'data analytics', 'data pipeline', 'excel', 'power bi', 'python', 'r', 'scala', 'sql']",
principal data engineer,viqu it,"role: principal data engineer salary: £85,000 - £95,000 per annum location: manchester (remote/ once a month) viqu have partnered with a national organisation going through an exciting transformation in their data infrastructure and so are hiring a principal data engineer to lead the design of their platform within the google cloud platform (gcp). the role will involve an even split of technical engineering, architecture and leadership/people management. requirements for the principal data engineer: • experience as a lead or principal data engineer. • prior experience designing data platform(s) within gcp, working hands on with; airflow, big query, data flow, data fusion, and data stream. • deep understanding of data mesh/ decentralised design and data lake/warehouse solutions. • previously led teams of data engineers. • hands on skills across the gcp tech stack, sql and python. • ability to lead cultural change across organisations, and manage senior stakeholders. • ability to work across multiple contexts and teams. job duties of the principal data engineer: • lead the architecture, best practise and engineering strategy of data squads. • hands on data engineering work, utilising both python and sql. • mentor and lead teams of engineers, checking and reviewing code, and setting standards. • ensure all data platform processes; including ingestion, quality, transformation, security, batch management, monitoring, alerting, and cost control are efficient. • design and help build the data platform – ensuring data is processed through semantic layers and can be modelled effectively. • suggest improvements for automation and cost savings. • lead changes across the organisation, adopting a decentralised design. role: principal data engineer salary: £85,000 - £95,000 per annum location: manchester (remote/hybrid) apply now to speak with viqu it in confidence. or reach out to jack mcmanus via the (url removed) do you know someone great? we’ll thank you with up to £1,000 if your referral is successful (terms apply). for more exciting roles and opportunities like this, please follow us on linkedin @viqu it recruitment",manchester,Data Engineer,"['airflow', 'cloud', 'data lake', 'gcp', 'google cloud', 'python', 'r', 'sql']","['airflow', 'cloud', 'data lake', 'gcp', 'google cloud', 'python', 'r', 'sql']",£85k–£95k a year
data engineer,head resourcing,"mid-level data engineer (azure / databricks) no visa requirements location: glasgow (3+ days) reports to: head of it my client is undergoing a major transformation of their entire data landscape-migrating from legacy systems and manual reporting into a modern azure + databricks lakehouse. they are building a secure, automated, enterprise-grade platform powered by lakeflow declarative pipelines, unity catalog and azure data factory. they are looking for a mid-level data engineer to help deliver high-quality pipelines and curated datasets used across finance, operations, sales, customer care and logistics. what you'll do lakehouse engineering (azure + databricks) build and maintain scalable elt pipelines using lakeflow declarative pipelines, pyspark and spark sql. work within a medallion architecture (bronze ? silver ? gold) to deliver reliable, high-quality datasets. ingest data from multiple sources including chargebee, legacy operational files, sharepoint, sftp, sql, rest and graphql apis using azure data factory and metadata-driven patterns. apply data quality and validation rules using lakeflow declarative pipelines expectations. curated layers & data modelling develop clean and conforming silver & gold layers aligned to enterprise subject areas. contribute to dimensional modelling (star schemas), harmonisation logic, scds and business marts powering power bi datasets. apply governance, lineage and permissioning through unity catalog. orchestration & observability use lakeflow workflows and adf to orchestrate and optimise ingestion, transformation and scheduled jobs. help implement monitoring, alerting, slas/slis and runbooks to support production reliability. assist in performance tuning and cost optimisation. devops & platform engineering contribute to ci/cd pipelines in azure devops to automate deployment of notebooks, lakeflow declarative pipelines, sql models and adf assets. support secure deployment patterns using private endpoints, managed identities and key vault. participate in code reviews and help improve engineering practices. collaboration & delivery work with bi and analytics teams to deliver curated datasets that power dashboards across the business. contribute to architectural discussions and the ongoing data platform roadmap. tech you'll use databricks: lakeflow declarative pipelines, lakeflow workflows, unity catalog, delta lake azure: adls gen2, data factory, event hubs (optional), key vault, private endpoints languages: pyspark, spark sql, python, git devops: azure devops repos & pipelines, ci/cd analytics: power bi, fabric what we're looking for experience commercial and proven data engineering experience. hands-on experience delivering solutions on azure + databricks. strong pyspark and spark sql skills within distributed compute environments. experience working in a lakehouse/medallion architecture with delta lake. understanding of dimensional modelling (kimball), including scd type 1/2. exposure to operational concepts such as monitoring, retries, idempotency and backfills. mindset keen to grow within a modern azure data platform environment. comfortable with git, ci/cd and modern engineering workflows. able to communicate technical concepts clearly to non-technical stakeholders. quality-driven, collaborative and proactive. nice to have databricks certified data engineer associate. experience with streaming ingestion (auto loader, event streams, watermarking). subscription/entitlement modelling (e.g., chargebee). unity catalog advanced security (rls, pii governance). terraform or bicep for iac. fabric semantic models or direct lake optimisation experience. why join? opportunity to shape and build a modern enterprise lakehouse platform. hands-on work with azure, databricks and leading-edge engineering practices. real progression opportunities within a growing data function. direct impact across multiple business domains",strathaven,Data Engineer,"['azure', 'dashboard', 'databricks', 'elt', 'power bi', 'pyspark', 'python', 'r', 'scala', 'spark', 'sql']","['azure', 'dashboard', 'databricks', 'elt', 'power bi', 'pyspark', 'python', 'r', 'scala', 'spark', 'sql']",
lead data engineer (azure),opensource,"about the company we are partnering with a fast-growing saas company to hire a lead data engineer who will help shape and scale their next-generation data and analytics platform. about the role this role sits within a modern data function where engineering, analytics, and product work closely together. the lead data engineer will guide a small team, own the full data lifecycle, and deliver trusted, high-performance data products that support customers and internal stakeholders. it’s a hands-on leadership role with strong technical influence — ideal for someone who enjoys building scalable pipelines, improving data quality, and shaping the direction of a growing platform. responsibilities • leading a small team of data engineers and analysts to design, build, and maintain scalable data solutions. • owning the end-to-end data lifecycle — from ingestion and transformation through to analytics and data product delivery. • architecting and operating pipelines using databricks, spark, and delta lake, ensuring performance, reliability, and cost-efficiency. • working closely with bi developers and analysts to deliver dashboards, extracts, datasets, and apis that power customer insights. • shaping platform architecture and setting technical direction for data engineering best practices. • driving improvements in data quality, lineage, governance, and observability. • playing a key role in data devops, ci/cd, testing, and cloud operations. • partnering with product and engineering teams to align work with the platform roadmap. • overseeing operational monitoring and support for the data platform. • promoting a learning culture in the team and encouraging experimentation with new tools and approaches. • mentoring team members and supporting their development. qualifications • experience leading or mentoring data engineering teams within a saas or product-led environment. • deep hands-on knowledge of databricks, apache spark, and delta lake, including large-scale or near real-time workloads. • strong proficiency in python, sql, and cloud data services (azure preferred, but any major cloud is fine). • experience designing and operating end-to-end data and analytics architectures. • good understanding of bi tooling (e.g. power bi, tableau) and analytics modelling. • strong grasp of etl/elt orchestration, data quality frameworks, and observability tooling. • familiarity with governance practices including lineage, cataloguing, and data integrity standards. • awareness of data security, access controls, and compliance considerations. • experience with ci/cd, infrastructure-as-code, and cost-optimised cloud engineering. • confident communicator, comfortable working with both technical and non-technical teams. • naturally curious and motivated by delivering new insights and data products using modern tooling. why this role? • chance to lead and grow a talented team while remaining hands-on technically. • ownership of a modern data platform with strong influence on architecture and future direction. • opportunity to deliver customer-facing data products with real business impact. • collaborative environment with the freedom to innovate and use emerging technologies.",stockport,Data Engineer,"['azure', 'cloud', 'dashboard', 'databricks', 'elt', 'etl', 'experimentation', 'power bi', 'python', 'r', 'scala', 'spark', 'sql', 'tableau']","['azure', 'cloud', 'dashboard', 'databricks', 'elt', 'etl', 'experimentation', 'power bi', 'python', 'r', 'scala', 'spark', 'sql', 'tableau']",
cloud data platform engineer,dfs,"what you’ll be doingas our cloud data platform engineer, you will play a key role in designing, developing and deploying cloud based data platforms that are secure, scalable and cost efficient. working to industry best practice and aligned to our internal standards, you will help build and maintain the core data lakehouse architecture that underpins analytics and reporting across the group.you will work closely with the principal data architect, cloud dba and data engineers to deliver robust, high performing data solutions. alongside this, you will automate infrastructure deployment, ensure strong security governance, and proactively monitor and optimise performance across the platform.you will be:supporting the definition, design and deployment of end to end data lakehouse architecture in line with industry best practice.developing, maintaining and versioning terraform configurations to provision and manage gcp infrastructure.working with the group technology cloud infrastructure team to standardise deployment methods and contribute to shared modules and tooling.integrating security best practice by managing fine grained access control (iam) and ensuring data compliance and segregation across bigquery and cloud storage.continuously monitoring and tuning performance, including managing slot capacity and optimising partitioning and clustering to deliver fast and cost efficient queries.creating and maintaining ci/cd pipelines to automate terraform deployments and cloud composer dag releases.establishing monitoring, logging and alerting across all data platform components to detect issues early and prevent failures.providing third line support through deep dive troubleshooting, root cause analysis and resolution of complex gcp data platform issues.creating and updating documentation including architectural diagrams, runbooks and terraform module guides.it is a highly technical and collaborative role where you will have real ownership of the cloud data platform and play a key part in shaping the future of data engineering across the group.this role is for you if…you enjoy solving complex technical challenges, building secure and scalable cloud solutions and working with modern data engineering tools and platforms. you will thrive in an environment where automation, best practice and continuous improvement are central to success.you will be a great fit if you have:two or more years experience designing, developing or implementing cloud solutions.hands on experience with at least one major cloud provider, ideally google cloud platform.strong cloud infrastructure knowledge, including vpcs, networking, routing, firewall rules and secure connectivity.experience using terraform to build and manage cloud infrastructure.knowledge of ci/cd tools such as github actions or cloud build.the ability to integrate security and compliance requirements into cloud solutions, including iam, encryption and vulnerability scanning.experience setting up cloud native monitoring and logging tools, such as google cloud operations or prometheus.proficiency in a scripting language such as python or bash to support automation and tooling.experience designing and implementing big data solutions.cloud certifications in engineering, security or architecture (desirable).clear communication skills and the ability to work effectively with developers, operational teams and leadership.a proactive, self starting approach and confidence working independently.if you enjoy shaping modern data platforms, building automation and ensuring cloud environments are secure, reliable and high performing, this could be the ideal next step for you.about dfswe’re home to dfs, sofology, and the sofa delivery company - three distinctive brands with a shared mission to bring great design and comfort into every home, affordably, responsibly, and sustainably.at dfs group, everyone plays a part. whether you’re supporting our brands or serving our customers, you’re helping shape the future of furniture retail. each brand has its own identity and creative direction, but we’re united by a culture that puts people and purpose at the heart of everything we do.everyone welcomeacross our group, we're committed to a culture where everyone feels welcome, valued, and can thrive at work. we celebrate diverse voices, champion inclusion, and support each other through our growing colleague networks. we invest in partnerships that drive change, from supporting doncaster pride and introducing the hidden disabilities sunflower, to partnering with carers uk and developing leaders with diversity in retail. we're proud to be an equal opportunities employer, committed to building a workplace that reflects the communities we serve. so, if your experience doesn't quite match the specification, we'd still really love to hear from youdfs benefitspotential annual bonus scheme: contribute to our success and get rewarded for itgrowth and training: learn new skills and develop your career with usleave: enjoy a great holiday allowance, with the option to buy 5 extra days. take advantage of our enhanced leave for maternity, paternity, shared parental, and adoption, plus a paid volunteering day each yeardiscounts: get 30% off dfs and sofology products for yourself, plus discounts for friends and family - and savings at big brands like sainsbury’s, asos, and ikeawellbeing perks: access healthcare services, an employee assistance programme, and discounted gym membershipspension and savings: join our group pension and sharesave schemeslife assurance & sick pay: peace of mind with life assurance and company sick pay",doncaster,Data Engineer,"['bigquery', 'cloud', 'clustering', 'data lake', 'gcp', 'google cloud', 'python', 'r', 'scala']","['bigquery', 'cloud', 'clustering', 'data lake', 'gcp', 'google cloud', 'python', 'r', 'scala']",
data and insights engineer,pwc uk,"integrated services delivers professional and joined up services to enable the firm's growth; connecting people, streamlining processes and using technology to simplify the user experience and make it easier for all our people to focus their energy on what is important. this is a key role within our data & insights team, within the integrated services function. we are looking for an engineer to work across our integrated services teams' data to drive efficiencies, remove duplication and develop new tools to drive greater insights. this is an exciting time to join an evolving team to drive the data analytics transformation, particularly in the people data domain. what your days will look like this role requires data engineering capability, with most work focused on back-end data pipeline development and downstream analytics delivery. it will also require considerable relationship building skills and some stakeholder management. • partner with stakeholder teams to understand reporting and analytics needs, identifying opportunities to streamline data flows and enhance insight quality. • design, build, and maintain scalable and reusable data pipelines in databricks using python (pyspark) and sql, transforming raw data from multiple sources into trusted, analytics-ready datasets. • continuously enhance the platform by automating, standardising, and optimising existing data processes. • identify and implement process improvements across multiple reporting and analytics functions. • transition existing reporting solutions into sustainable, automated frameworks within databricks and power bi. this role is for you if • you have practical experience in data engineering or analytics, ideally within a professional or enterprise environment. • you have considerable working knowledge of databricks or other data integration and transformation platforms —developing and optimising data pipelines using python (pyspark) and/or sql. • you have experience with data transformation and visualisation tools (e.g. power bi, alteryx) to deliver high-quality insights. • you have ample problem-solving and analytical skills, with the ability to diagnose and resolve complex data issues. • you demonstrate excellent communication skills, particularly when troubleshooting complex problems or explaining data concepts. • you have experience or an understanding of deploying modern ai/ml solutions into production (preferred not essential). • you have experience of coaching/mentoring others what you'll receive from us no matter where you may be in your career or personal life, our benefits are designed to add value and support, recognising and rewarding you fairly for your contributions. we offer a range of benefits including empowered flexibility and a working week split between office, home and client site; private medical cover and 24/7 access to a qualified virtual gp; six volunteering days a year and much more.",sunderland,Data Engineer,"['data analytics', 'data pipeline', 'databricks', 'excel', 'power bi', 'pyspark', 'python', 'r', 'scala', 'spark', 'sql']","['data analytics', 'data pipeline', 'databricks', 'excel', 'power bi', 'pyspark', 'python', 'r', 'scala', 'spark', 'sql']",
senior/data engineer,mwh treatment,"we are looking to strengthen our information technology/data team with a senior data engineer or data engineer with hybrid working available. you will be instrumental in designing, building, and maintaining the data pipelines and systems that power our analytics and decision-making. working under the guidance of the head of data, you will ensure that data is accessible, reliable, and secure, enabling the organization to harness its full potential. this is an opportunity to work in a ""greenfield"" environment and contribute to the creation of a cutting-edge data ecosystem from the ground up. key responsibilities: • data pipeline development: design, develop, and optimize etl/elt pipelines to collect, transform, and load data from various sources into our data infrastructure. • data modelling: implement robust data models and schemas to support analytics, reporting, and other business needs. • infrastructure management: assist in building and maintaining scalable, secure, and efficient data platform architectures (e.g., data warehouses, data lakes). • data quality assurance: implement processes and tools to monitor and ensure data quality, integrity, and consistency across systems. • collaboration: work closely with the head of data, software engineers, and business stakeholders to understand data requirements and deliver effective solutions. • automation: automate repetitive tasks and workflows to streamline data operations. • technology integration: evaluate and integrate new tools and technologies into the data stack to improve performance and scalability. why join us? • impact: contribute to the creation of a transformative data function from the ground up. • innovation: work in a forward-thinking company that values creativity and cutting-edge solutions. • growth opportunities: develop your skills and career as the data function scales. • collaboration: be part of a supportive and driven team under the guidance of an experienced data leader. • culture: be part of a company that fosters a collaborative and inclusive environment. how to apply: if you’re excited by the challenge of creating a data function from the ground up and making a lasting impact, we want to hear from you!",hyde,Data Engineer,"['data lake', 'data pipeline', 'data warehouse', 'elt', 'etl', 'r', 'scala']","['data lake', 'data pipeline', 'data warehouse', 'elt', 'etl', 'r', 'scala']",
data engineer - management services,miller insurance,"we have a new exciting opportunity for a data engineer to join our technology, data and innovation team at miller.",united kingdom,Data Engineer,['r'],['r'],
python data engineer / software engineer with leading energy company,eaglecliff recruitment,"exciting contract opportunity for an experienced python data engineer / software engineer to join a leading energy company and help build scalable, cloud-native data solutions. this will be a long term project with scope for extensions and possible transition to permanent staff. core skills needed: • strong python development • pyspark and sql expertise • solid understanding of azure infrastructure • knowledge of containers, microservices, and functional design patterns • experience working in agile environments • terraform for iac • unit testing best practices (pytest preferred) • html/css • react • typescript • fastapi framework competitive paye rates available. with a focus within energy trading, oil & gas, financial markets and commodities, we offer a transparent recruitment service that has proven to be reliable and effective for over 40 years. we are iso accredited and proud of our excellent trustpilot reviews. your search for a new contract assignment or for a new permanent job will be in safe hands with eaglecliff recruitment. please telephone for an immediate response or email your cv for a quick response. eaglecliff ltd is acting in the capacity of an employment agency for permanent recruitment and an employment business for contractor resourcing.",united kingdom,Data Engineer,"['azure', 'cloud', 'excel', 'pyspark', 'python', 'r', 'scala', 'spark', 'sql']","['azure', 'cloud', 'excel', 'pyspark', 'python', 'r', 'scala', 'spark', 'sql']",
staff data engineer: build scalable pipelines & dashboards,sava,"a leading health technology company in the uk is seeking a staff data engineer to join their growing data engineering team. you will be responsible for building reliable data pipelines and storage solutions while collaborating with a top-tier team across engineering and science. the ideal candidate has 10+ years of experience in data engineering, proficiency in python and sql, and is skilled in modern data processing frameworks. this position offers the opportunity to shape the company’s data architecture in a cutting-edge environment. #j-18808-ljbffr",united kingdom,Data Engineer,"['data pipeline', 'python', 'r', 'sql']","['data pipeline', 'python', 'r', 'sql']",
data engineer (databricks and aws),cgi,"at cgi, we’re helping to transform the future of healthcare through the power of data. as a data engineer, you’ll play a pivotal role in designing, building, and optimising data platforms that underpin critical national services. working at the heart of our healthcare team, you’ll use your expertise in aws, databricks, and python to deliver high-impact solutions that improve outcomes, enhance decision-making, and drive innovation across the sector. you’ll collaborate with experts who share your passion for problem-solving, ownership, and technical excellence-empowered to shape the data foundations of tomorrow. cgi was recognised in the sunday times best places to work list 2025 and has been named a uk ‘best employer’ by the financial times. we offer a competitive salary, excellent pension, private healthcare, plus a share scheme (3.5% + 3.5% matching) which makes you a cgi partner not just an employee. we are committed to inclusivity, building a genuinely diverse community of tech talent and inspiring everyone to pursue careers in our sector, including our armed forces, and are proud to hold a gold award in recognition of our support of the armed forces corporate covenant. join us and you’ll be part of an open, friendly community of experts. we’ll train and support you in taking your career wherever you want it to go. due to the secure nature of the programme, you will need to hold uk security clearance or be eligible to go through this clearance. this is a hybrid position based in leeds. your future duties and responsibilities in this role, you will design, build, and maintain data solutions that power some of the uk’s most critical healthcare systems. you’ll be part of a collaborative engineering team, transforming how data is captured, processed, and used to drive better patient and operational outcomes. your work will combine technical innovation with practical delivery—enabling data accessibility, quality, and security at scale. you’ll take ownership of complex data challenges, partner with architects and analysts to shape technical direction, and continuously refine processes to deliver efficient, sustainable data pipelines. working within cgi’s supportive environment, you’ll be encouraged to explore new technologies, share knowledge, and contribute to a culture of excellence and innovation. key responsibilities include: • design & build: develop and optimise data pipelines using databricks, apache spark, and python. • develop & deliver: create scalable data solutions on aws leveraging s3, glue, lambda, and related services. • integrate & automate: implement etl processes and data lake/lakehouse architectures that ensure accuracy and reliability. • collaborate & advise: partner with technical and business stakeholders to translate requirements into effective data solutions. • secure & govern: ensure compliance with data governance, nhs standards, and security frameworks. • innovate & improve: drive continuous improvement across data engineering practices and technologies. required qualifications to be successful in this role to excel in this role, you’ll bring strong data engineering expertise and hands-on experience in cloud-based data solutions, ideally within regulated or complex environments such as healthcare. you’ll be confident in both the technical and consultative aspects of data delivery. you should have: • proven experience as a data engineer working with large, complex datasets. • hands-on expertise with databricks, apache spark, and sql. • strong proficiency in python (pyspark preferred). • experience with aws cloud services including s3, glue, lambda, iam. • familiarity with etl design, data modelling, and data lake/lakehouse concepts. • understanding of data governance and compliance frameworks. • experience in the healthcare sector or knowledge of nhs data standards (advantageous). #li-mw2 together, as owners, let’s turn meaningful insights into action. life at cgi is rooted in ownership, teamwork, respect and belonging. here, you’ll reach your full potential because… you are invited to be an owner from day 1 as we work together to bring our dream to life. that’s why we call ourselves cgi partners rather than employees. we benefit from our collective success and actively shape our company’s strategy and direction. your work creates value. you’ll develop innovative solutions and build relationships with teammates and clients while accessing global capabilities to scale your ideas, embrace new opportunities, and benefit from expansive industry and technology expertise. you’ll shape your career by joining a company built to grow and last. you’ll be supported by leaders who care about your health and well-being and provide you with opportunities to deepen your skills and broaden your horizons. come join our team—one of the largest it and business consulting services firms in the world.",united kingdom,Data Engineer,"['aws', 'cloud', 'data lake', 'data pipeline', 'databricks', 'etl', 'excel', 'pyspark', 'python', 'r', 'scala', 'spark', 'sql']","['aws', 'cloud', 'data lake', 'data pipeline', 'databricks', 'etl', 'excel', 'pyspark', 'python', 'r', 'scala', 'spark', 'sql']",
"data engineer | manchester, uk",aj bell,"data engineer company description job description purpose of the role this is an exciting opportunity to join a dynamic and experienced data engineering team at aj bell, contributing significantly to the development of our state-of-the-art data platform using cutting-edge technology. as a data engineer, you will play a pivotal role in designing, building, maintaining, and evolving our data infrastructure, ensuring it meets the growing needs of our business. you'll engage in end-to-end development, collaborate closely with key stakeholders and internal customers, and empower the organisation by enabling informed, data-driven decision-making. what does the job involve? the key responsibilities of the role are as follows: • collaborating with stakeholders to identify and refine data requirements, ensuring data is accessibility and alignment with business needs. • developing data warehousing solutions. • automating extract, load and transform (elt) pipelines that follow modern ci/cd practices. • data integration design - ensure development is scalable, efficient and future-proof. • data modelling - producing clear data models where necessary. • maintaining and continuously enhancing the data platform. • provisioning data from various sources. • create automated tests to ensure quality and integrity of data. • ensure data is compliant with aj bell's data governance and data classification policies. • maintain data dictionary. • maintain business level data model. • recommending and introducing new technology where needed. core: • cloud data platforms (e.g. snowflake, bigquery, redshift) • data transformation technology such as dbt • visual studio code • python • ci automation systems such as jenkins • a git-based source control system such as bitbucket • data warehouse/kimball methodology • data replication technology such as fivetran hvr. • excellent problem-solving skills. • good communication skills and comfortable working with both technical and non-technical teams other: • good knowledge of it products and systems • good analytical skills • excellent communication skills verbal and written • able to communicate with people at all levels confidently and effectively • able to prioritise work effectively • customer focussed • flexible approach to work - team player • adaptable to changing environment • self-motivated • embraces continuous learning • previous experience working in an e-commerce and/or financial services business • ability to use docker and container orchestration tools • aws cloud infrastructure including aws cdk • ms sql • no sql database such as mongo • ai tools such as copilot, snowflake cortex about us aj bell is one of the fastest-growing investment platform businesses in the uk offering an award-winning range of solutions that caters for everyone, from professional financial advisers to diy investors with little to no experience. we have over 644,000 customers using our award-winning platform propositions to manage assets totalling more than £103.3 billion. our customers trust us with their investments, and by continuously striving to make investing easier, we aim to help even more people take control of their financial futures. having listed on the main market of the london stock exchange in december 2018, aj bell is now a ftse 250 company. headquartered in manchester with offices in central london and bristol, we now have over 1,500 employees and have been named one of the uk's 'best 100 companies to work for' for six consecutive years and in 2024 named a great place to work®. at aj bell you can expect a friendly working environment with a strong sense of teamwork, we have a great sense of pride in what we do, and this is reflected in our guiding principles. our perks and benefits • competitive starting salary • starting holiday entitlement of 25 days, increasing up to 31 days with length of service and a holiday buy and sell scheme • a choice of pension schemes with matched contributions up to 6% • discretionary bonus scheme • annual free share awards scheme • buy as you earn (baye) scheme • health cash plan - provided by simplyhealth • private healthcare scheme and dental plan • free gym membership • employee assistance programme • bike loan scheme • sick pay+ pledge • enhanced maternity, paternity, and shared parental leave • loans for travel season tickets • death in service scheme • paid time off for volunteer work • charitable giving opportunities through salary sacrifice • calendar of social events, including monthly payday drinks, annual christmas party, summer party and much more • ongoing technical training • professional qualification support • talent development programmes • peer recognition scheme, with rewards including restaurant and shopping vouchers or time off • monthly leadership breakfasts and lunches hybrid working at aj bell, our people are the heart of our culture. we believe in building strong connections by working together. that's why we offer a hybrid working model, where you'll spend a minimum of 50% of your working time per month in the office. for new team members, an initial period will be full-time in the office to help you immerse yourself in our business and build valuable relationships with your colleagues. aj bell is committed to providing an environment of mutual respect where equal employment opportunities are available to all applicants and all employees are empowered to bring their whole self to work.",manchester,Data Engineer,"['aws', 'bigquery', 'classification', 'cloud', 'data warehouse', 'dbt', 'elt', 'excel', 'python', 'r', 'redshift', 'scala', 'snowflake', 'sql']","['aws', 'bigquery', 'classification', 'cloud', 'data warehouse', 'dbt', 'elt', 'excel', 'python', 'r', 'redshift', 'scala', 'snowflake', 'sql']",
senior data engineer,aubay,"aubay uk is seeking a senior data engineer to join a leading energy client's team, supporting strategic data transformation initiatives across global trading functions. this role focuses on designing and building robust data foundations and end-to-end solutions to maximise value from data, working closely with it and business stakeholders. the senior data engineer will play a critical role in realising the vision and data strategy through cutting-edge cloud technologies and will be responsible for mentoring junior team members and promoting a data-driven culture across the organisation. required skills and experience: • strong proficiency with cloud platforms including aws, azure, and sap. • expertise in elt development and automation, with a deep understanding of scalable pipeline design. • mastery of data modelling techniques, including conceptual, logical, and physical models. • skilled in data ingestion, integration, and manipulation using modern tools (e.g., data factory, databricks, sql db, synapse, glue, airflow, kinesis, redshift). • hands-on experience with devops tooling including github actions, azure devops, sonarqube, and pytest. • proven ability to coordinate incident and change management processes. • adept at communicating with both technical and non-technical stakeholders. • demonstrated experience in documenting solution architecture and contributing to strategic technical documentation. desired skills and experience: • experience in project management or running agile/scrum teams. • exposure to business planning and consolidation tools such as bpc. • familiarity with external partner ecosystems and cross-functional delivery models. • experience in writing and maintaining technical documentation using tools such as mkdocs. • domain knowledge in trading and supply, specifically within crude and renewables portfolios. • understanding of commodity data models and business applications in energy and power trading. • experience working in enterprise environments involving global-scale data transformation projects. role responsibilities: • design and build scalable, end-to-end data engineering solutions using modern cloud technologies (azure, aws, sap). • act as a subject matter expert in data engineering, providing guidance and mentoring to junior engineers. • collaborate with business and it stakeholders to translate strategic goals into data architecture and solutions. • lead efforts to modernise data platforms, including the redesign of legacy data warehouses and migration to cloud-based infrastructure. • manage data ingestion, integration, and transformation workflows using tools such as azure data factory, databricks, synapse, glue, and kinesis. • ensure high standards of data quality, performance tuning, and governance. • coordinate incident and problem management, ensuring timely resolution and minimal disruption. • present technical reports and findings to stakeholders, serving as the go-to expert on data architecture and engineering best practices. • support community development initiatives such as centres of excellence (coe) and communities of practice (cop). • contribute to continuous improvement and efficiency across pilot programmes and enterprise-scale rollouts.",peterhead,Data Engineer,"['airflow', 'aws', 'azure', 'cloud', 'data warehouse', 'databricks', 'elt', 'excel', 'r', 'redshift', 'scala', 'sql']","['airflow', 'aws', 'azure', 'cloud', 'data warehouse', 'databricks', 'elt', 'excel', 'r', 'redshift', 'scala', 'sql']","£77,000–£87,999 a year"
data engineer (backshift- uk),dayshape,"about us we’re an award-winning enterprise software scale-up with high ambitions for growth. we recently won the scotlandis digital tech scale-up business of the year award and have previously been recognised as scotland’s fastest-growing tech company in the deloitte technology fast 50 for three consecutive years. dayshape is an advanced resource management solution, incorporating ai and intelligent automation to help professional services firms optimise their workforce like never before. it’s built to handle large, complex, and ever-changing requirements with ease. our customers include accountancy firms from the big four and global top 10, and dayshape is used in more than 25 countries across four continents. our target customers are global firms, international networks, and large national or regional organisations, if they’re big enough to have the challenges that dayshape can solve. as a company, we live our values every day and we're committed to making sure our friendly and inclusive environment grows with us. about the role during 2023 we grew and gained many new customers. we are adapting our processes as we scale, and this includes growing our new, specialised team of data engineers for developing customer integrations. this is a highly collaborative role where you will have the opportunity to work directly with clients on requirements gathering and the implementation of new integrations. as the demand for integration work increases, you will be heavily involved in setting the standards for our integrations going forward. what you’ll do • work with our software implementation consultants (sics) to define and verify specification documents for etl process. • work with customer it to test customer data source endpoints to ensure they meet specification. • implement, test and deploy azure data factory (adf) pipeline definitions within version control to customer environments. • work with our site reliability engineering team to ensure your solutions are observable, reliable and performant. • work with our engineering teams to ensure end-to-end capability for integrated data. • support cutover to production systems (can be outside normal working hours). • identify improvements to existing azure data factory processes to ensure they are more maintainable across a growing set of customers. about you • you must have at least two years of experience in azure data factory and be comfortable building transparent, easy-to-support pipelines. • experience building and maintaining data integrations with a variety of external systems. • good understanding of the etl process. • comfortable being in a client-facing role. • excellent communication skills: you can clearly explain technical matters to any audience. • confident working with complex referential data. • knowledge of rest apis, sql databases and other data sources. • a team player, with experience collaborating with other departments. • you demonstrate good attention to detail and enjoy breaking complex problems down into simple steps. bonus points if you have • previous experience directly leading calls with clients • experience in other azure data technologies such as azure databricks • integrated with a variety of downstream data sources, including but not limited to: cloud services, custom rest apis, database (on-prem) what you’ll get • salary £38,000-£45,000 (dependent on experience) • 15% uplift on base salary for hours scheduled between 7:00 pm and 7:00 am. • at least £1,000 per year to spend on professional and personal development • 33 days' holiday per year (including bank holidays), increasing by 1 day each year to a maximum of 40 days • paid four week sabbatical in your fifth anniversary year on top of your holiday entitlement • private healthcare and rewards through vitality • income protection and death in service cover • enhanced family leave policies • matched 5% auto-enrolment workplace pension scheme • access to wellbeing offerings, such as our employee assistance programme and a dedicated counselling service • innovation week twice a year - a chance to experiment and work off-project • volunteering time – up to 20 hours a year to participate in volunteer work. • weekly all hands meeting for inspiration and over-communication • time out of the working week for team socials each month, with a mix of in-person and virtual options: past events include hiking, family bbqs, online games, d&d, and at-home cocktail classes! • genuinely nice, smart people to work with, who are excited about growing our company working details this is a full-time role (37.5 hours per week), typically working from 2:00pm-10:30pm from monday to friday to cover our global clients. we’re ideally looking for someone in/around edinburgh, though we’re open to the possibility of this being a remote role (as long as you're in the uk). we don't mandate required office time, but we find that most of the team in edinburgh enjoy working from home 2-3 days a week, and come into our office to connect with each other, make use of space, and for meetings. join the team! equality of opportunity is more than just a responsibility: we believe it’s a huge advantage to welcome a variety of experiences and perspectives into the team. diversity is a great asset and, as such, we strongly encourage applications from any background. this is your opportunity to really influence how we get things done, and take our customer integrations to the next level. we're doing well, but there's lots more to do in order to maintain the high bar and pace that we've set. everyone here is growing personally as the company grows, so if that sounds like something you’d like to be part of, we’d love to see your application. • please note the successful candidate for this role will be subject to background checks and will have an opportunity to declare anything to us beforehand*",edinburgh,Data Engineer,"['azure', 'cloud', 'databricks', 'etl', 'excel', 'r', 'sql']","['azure', 'cloud', 'databricks', 'etl', 'excel', 'r', 'sql']",£38k–£45k a year
senior data engineer,robert walters,"about my claim group my claim group is a fast-growing uk-based company helping customers navigate complex financial and insurance claims. our success is built on trust, innovation, and data-driven decision-making – and now we’re looking to expand our analytics team to support smarter, faster business outcomes. about my claim group my claim group is a fast-growing uk-based company helping customers navigate complex financial and insurance claims. our success is built on trust, innovation, and data-driven decision-making – and now we’re looking to expand our analytics team to support smarter, faster business outcomes. we are seeking a senior data engineer to join their high-performing data team and help shape the future of our data ecosystem. you will play a key role in designing, building, and scaling robust data platforms that support analytics, reporting, and advanced insight across the organisation. key responsibilities of the role: • design, build, and maintain reliable etl processes and end-to-end data pipelines. • develop and manage data warehousing solutions to support analytics and reporting needs. • work with both structured and unstructured data, ensuring high-quality data management practices. • design scalable, high-performance data models that enable efficient data use across the business. • build and transform datasets for downstream analytics, bi, and data science use cases. • leverage cloud technologies (aws preferred) to deliver scalable, resilient data solutions. • work with snowflake (ideal), or similar cloud data platforms. • implement or contribute to real-time data processing frameworks (a strong bonus). • integrate data solutions with ai & ml workflows where beneficial. • apply broad engineering knowledge to support system scaling, optimisation, and performance tuning. • collaborate with cross-functional teams to understand data needs and propose effective solutions. • communicate technical concepts clearly, challenge constructively, and contribute to solution design. • work both independently and collaboratively, demonstrating proactive problem-solving and critical thinking. key experience needed: • 5+ years of experience in data engineering or a similar role. • proven experience building etl pipelines and large-scale data systems. • strong understanding of data warehousing, modelling, and transformation. • experience with cloud platforms (aws preferred). • exposure to snowflake, or willingness to learn. • familiarity with real-time data processing (kafka, kinesis, or similar). • strong communication skills with the ability to challenge, propose, and influence. • a proactive mindset, excellent critical thinking, and a collaborative approach to solving complex problems. • bonus: experience working with ai & ml integration in data pipelines. what’s on offer • salary up to £75,000, depending on experience • hybrid working – 1/2 days in the heald green office • autonomy/ flexible working hours/ hybrid working • company equipment • casual dress code • employee pension • long service gifts to celebrate the milestones • team building activities (games/break room (regular tournaments with prizes!) • social events such as summer bbqs, plus more! robert walters operations limited is an employment business and employment agency and welcomes applications from all candidates",heald green,Data Engineer,"['aws', 'cloud', 'data pipeline', 'etl', 'excel', 'kafka', 'r', 'scala', 'snowflake']","['aws', 'cloud', 'data pipeline', 'etl', 'excel', 'kafka', 'r', 'scala', 'snowflake']",£65k–£75k a year
lead data engineer,jla resourcing ltd,"lead data engineer - £70-77k + bonus + benefits - basingstoke 3 days a week if you would like to know a bit more about this opportunity, or are considering applying, then please read the following job information. the opportunity: we are looking for a lead data engineer to join a basingstoke based organisation who are investing heavily in their digital transformation programme. the role: you'll play a proactive role in the delivery of next-generation xbpsjku data platforms, will manage / mentor the existing person and drive the design, d...",weston,Data Engineer,['r'],['r'],
lead data engineer,thenottingham,"about the role contract type: permanent hours: full-time, 35 hours location: head office, nottingham (hybrid working, minimum 2 days per week) salary: up to £90,000 depending on experience. application process: please apply via the application button which will direct you to our careers site. if you require any adjustments to assist you in applying, please contact careers@thenottingham.com. we’re on an exciting journey to build a brand-new greenfield data platform using microsoft fabric in azure, unlocking next generation insights, innovation and personalised experiences for our members. as our lead data engineer, you’ll take a hands-on role in designing, building and running this capability from the ground up. partnering with our head of data, you’ll set the technical foundations, create engineering standards and develop reusable frameworks that power enterprise grade integration, analytics, ai and regulatory reporting. this is a unique chance to influence the entire data ecosystem, from architecture and ingestion through to consumption and innovation, in a forward-thinking organisation that champions curiosity, inclusivity and positive change. here’s a taste of what you will be doing as a lead data engineer nottingham building society - • build a cutting-edge data platform: design, develop and operate a scalable, secure and high-performing data platform using microsoft fabric technologies such as onelake, lakehouse/warehouse, data factory, synapse, power bi and fabric pipelines. • create robust data integration frameworks: develop and maintain ingestion and transformation processes connecting multiple internal and external sources including sopra banking services, salesforce, caci and experian. • select the right tools for every challenge: choose the most suitable fabric workloads for each use case, from data engineering and spark to real-time analytics, data science and power bi. • define models and structures for insight: implement data models, lakehouse structures and orchestration patterns that enable self-service bi, single customer view, regulatory reporting and ai/ml capabilities. • champion data security and governance: establish role-based security patterns, ensure data quality, lineage and observability using microsoft purview and other governance tools. • embed modern engineering practices: lead the adoption of dataops and devops principles with azure devops, git integration, infrastructure-as-code and automated ci/cd pipelines. • mentor and inspire engineering excellence: guide and support a growing team of data engineers, promoting best practices, code reviews and continuous improvement. • drive innovation and strategic thinking: act as the technical authority for microsoft fabric, stay ahead of emerging features and help shape the society’s long-term data strategy. about you - • extensive technical expertise: strong knowledge of microsoft fabric components including onelake, lakehouse/warehouse, delta lake, direct lake, data factory, spark (pyspark/scala) and power bi (dax and semantic modelling). • advanced programming and data engineering skills: proficient in python, sql and t-sql with experience in pyspark; familiarity with kql for real-time analytics and robust elt design using medallion architectures. • proven experience in modern data platforms: track record of building and operating azure-based data platforms, ideally in greenfield or transformational contexts, with integration across on-premise, cloud and third-party sources. • strong cloud and governance knowledge: hands-on experience with azure services (adls gen2, event hubs, key vault, purview) and governance tools for data quality, lineage and compliance; awareness of aws/gcp is a plus. • expertise in engineering best practices: skilled in ci/cd, infrastructure-as-code and automation using azure devops; experienced in setting coding standards, reviewing designs and embedding reusable frameworks. • leadership and mentoring ability: proven capability to lead or mentor small technical teams, fostering collaboration, code quality and continuous improvement across multi-workstream deliveries. • inclusive and purpose-led approach: collaborative, curious and quality-driven, with strong communication skills to simplify complex topics and align decisions with member-first values and responsible innovation. • qualifications and continuous learning: degree or equivalent experience in a technical discipline; microsoft certifications (dp-203, dp-600, pl-300) or willingness to achieve; commitment to ongoing professional development and learning. reward & benefits - • competitive package: fair salary benchmarked against market data, annual discretionary bonus, and 29 days holiday plus bank holidays. • health & wellbeing: access to medicash healthcare, mental health first aiders, and a suite of wellbeing resources to support you inside and outside of work. • work-life balance: 35-hour working week for full-time roles, with flexibility to help you perform at your best. • career growth: ongoing personal and professional development, we’ll support your ambitions and help you grow your potential. • inclusive culture: be part of a friendly, values-led team that genuinely cares about doing the right thing for colleagues and customers. • giving back: use two paid volunteering days each year to support causes close to your heart, through our samuel fox foundation. • sustainability focus: join a business committed to reducing its carbon footprint and making a positive impact on the environment. • free access to octopus money: financial coaching & tools that help you plan, manage, and make the most of your money. embracing diversity together - we proudly embrace and celebrate diversity as a fundamental cornerstone of our values. we believe that a diverse and inclusive workplace is not just essential for our success but is also a reflection of the vibrant communities we serve. our commitment to diversity extends beyond our internal culture to the way we approach advertising and engage with our customers. our commitment means actively working to eliminate barriers and biases that may hinder equal opportunities within our organisation. we strive to ensure that all individuals, regardless of background, have an equal chance to thrive and advance in their careers. we acknowledge that diversity is not just a goal to be achieved but a continuous journey toward creating an environment that embraces differences and promotes equal opportunities for all. we are committed to fostering an inclusive culture that encourages collaboration, creativity, and a sense of belonging for every member of our community. about us we are a mutual, which means we don’t have shareholders. instead, we’re owned by our members and use our money to do good, investing in our community, responsible causes, and – well, you. so, we’re always striving to do the right thing for our team, communities and members. although our history spans over 170 years, our purpose of helping our members save, plan for and protect their financial futures is enduring. at the nottingham building society, we are dedicated to overcoming obstacles and turning challenges into opportunities. at the heart of our mission is our unwavering commitment to breaking down barriers and building better futures by helping our customers achieve the significant milestone of owning their own home.",nottingham,Data Engineer,"['aws', 'azure', 'cloud', 'elt', 'excel', 'gcp', 'power bi', 'pyspark', 'python', 'r', 'scala', 'spark', 'sql']","['aws', 'azure', 'cloud', 'elt', 'excel', 'gcp', 'power bi', 'pyspark', 'python', 'r', 'scala', 'spark', 'sql']",£90k a year
azure data engineer,nigel frank,"a small and highly successful organisation in the sustainability space are seeking a data engineer with some power platform experience to join their team. they have an office space in london, though this role is remote and is therefore open to candidates across the uk. they are on a mission to help businesses to reduce their carbon emissions, through the use of their intelligent sustainability platform - and your role will focus on the ongoing development and expansion of this, spanning both front-end and back-end develoent! the platform is powered by microsoft's cloud ecosystem - including azure, power apps, power pages and power bi, and allows for real-time engagement and ai-guided action plans for carbon reduction. responsibilities include: • use azure data factory to ingest, transform, and expose data across the platform • help centralise data into a data lake, ensuring it's clean, structured, and accessible • build data pipelines for llms and integrate tools like gpt to drive intelligent outputs • develop user-facing applications using power apps and power pages • design and manage apis to connect front-end applications and integrate external services (e.g. procurement systems, ai tools) • collaborate directly with users to turn complex needs into streamlined solutions it's a broad role with lots to get involved in! we're not expecting you to have experience with everything mentioned above - the core skills are azure data factory, and having a self-starter mentality with an enthusiasm to learn new things! benefits include: • salary up to £70,000 depending on experience • 25 days annual leave plus bank holidays, plus your birthday off • pension with 3% employer and 5% employee contributions • generous maternity and paternity policy • allocated training budget for everyone • regular company get-togethers with expenses paid if you're excited by the prospect of working for a mission-driven organisation who are already making a big impact, apply today! please note: this is a role for uk residents only. this role does not offer sponsorship. you must have the right to work in the uk with no restrictions. some of our roles may be subject to successful background checks including a dbs and credit check. tenth revolution group / nigel frank are the go-to recruiter for data and ai roles in the uk, offering more opportunities across the country than any other. we're the proud sponsor and supporter of sqlbits, and the london power bi user group. to find out more and speak confidentially about your job search or hiring needs, please contact me directly at v.simpson@tenthrevolution.com",peterborough,Data Engineer,"['azure', 'cloud', 'data lake', 'data pipeline', 'power bi', 'r', 'sql']","['azure', 'cloud', 'data lake', 'data pipeline', 'power bi', 'r', 'sql']",
senior data engineer,datatech analytics,"senior data engineer birmingham - hybrid (1 day per week in the office) £60,000 - £65,000 (dependent on experience) job ref: j13034 an ambitious organisation investing heavily in its data strategy is seeking a senior data engineer to design and implement strategic data solutions that enhance decision-making at every level. this role involves much more than dashboard development. you will build scalable azure-based data architectures, partner with senior leaders, and contribute to the ongoing growth of a data-driven environment. key responsibilities: ·design, build, and deliver end-to-end data solutions and power bi products within the azure cloud environment ·develop, maintain, and optimise etl and orchestration pipelines in azure data factory ·work closely with technical teams and business stakeholders to design and deliver high value bi products ·translate complex business needs into clear, efficient technical solutions using agile ways of working ·build and maintain scalable data models and azure based data architecture ·leverage the data warehouse to enable self-service reporting and insight generation ·ensure all data solutions meet governance, security, and regulatory requirements including gdpr ·mentor junior colleagues and promote quality and best practice across the team ·identify opportunities to improve data quality, reliability, automation, and reuse throughout the data lifecycle experience and skills required: ·strong hands on experience with azure data factory and azure sql ·proven expertise in power bi, including creating intuitive and user centred visualisations ·awareness or experience of microsoft fabric technologies ·solid understanding of data warehouse design principles, including kimball or inmon methodologies ·experience working with senior stakeholders and translating their needs into effective technical solutions ·strong analytical capability and attention to detail ·confident communication skills, both written and verbal ·experience working in agile environments, ideally with azure devops ·a relevant degree or equivalent professional experience ·experience mentoring or supporting junior data professionals additional information ·you must already have the right to work in the united kingdom, as sponsorship is not available now or in the future this is a great opportunity to take a leading role in modern data engineering within the azure ecosystem and to influence the organisation's data capabilities as they continue to grow. apply today to find out more. alternatively, you can refer a friend or colleague by taking part in our fantastic referral schemes! if you have a friend or colleague who would be interested in this role, please refer them to us. for each relevant candidate that you introduce to us (there is no limit) and we place, you will be entitled to our general gift/voucher scheme. datatech is one of the uk's leading recruitment agencies in the field of analytics and host of the critically acclaimed event, women in data. for more information, visit our website: www.datatech.org.uk",west midlands,Data Engineer,"['azure', 'cloud', 'dashboard', 'data warehouse', 'etl', 'power bi', 'r', 'scala', 'sql']","['azure', 'cloud', 'dashboard', 'data warehouse', 'etl', 'power bi', 'r', 'scala', 'sql']",
ai data engineer,lenovo,"lenovo is seeking a talented and motivated data engineer/scientist to join our growing team. this role is critical to the success of our machine learning initiatives, focusing on the creation, quality control, and governance of the datasets that power our models. you will bridge the gap between raw data and model readiness, working closely with model developers to understand their needs and deliver high-quality, reliable data. this is a hands-on role requiring strong technical skills in data engineering, data analysis, and machine learning fundamentals. if you are passionate about making smarter technology for all, come help us realize our hybrid ai vision!, • data creation & annotation: design, build, and implement processes for creating task-specific training datasets. this may include data labeling, annotation, and data augmentation techniques. • data pipeline development: leverage tools and technologies to accelerate dataset creation and improvement. this includes scripting, automation, and potentially working with data labeling platforms. • data quality & evaluation: perform thorough data analysis to assess data quality, identify anomalies, and ensure data integrity. utilize machine learning tools and techniques to evaluate dataset performance and identify areas for improvement. • big data technologies: utilize database systems (sql and nosql) and big data tools (e.g., spark, hadoop, cloud-based data warehouses like snowflake/redshift/bigquery) to process, transform, and store large datasets. • data governance & lineage: implement and maintain data governance best practices, including data source tracking, data lineage documentation, and license management. ensure compliance with data privacy regulations. • collaboration with model developers: work closely with machine learning engineers and data scientists to understand their data requirements, provide clean and well-documented datasets, and iterate on data solutions based on model performance feedback. • documentation: create and maintain clear and concise documentation for data pipelines, data quality checks, and data governance procedures. • stay current: keep up-to-date with the latest advancements in data engineering, machine learning, and data governance. education: bachelor's or master's degree in computer science, computer engineering, electrical engineering, statistics, mathematics, or a related field. • experience: 8+ years of experience in a data engineering or data science role. • programming skills: proficiency in python and sql. experience with other languages (e.g., java, scala) is a plus. • database skills: strong experience with relational databases (e.g., postgresql, mysql) and nosql databases (e.g., mongodb, cassandra). • big data tools: experience with big data technologies such as spark, hadoop, or cloud-based data warehousing solutions (snowflake, redshift, bigquery). • data manipulation: proficiency in data manipulation and cleaning techniques using tools like pandas, numpy, and other data processing libraries. • ml fundamentals: solid understanding of machine learning concepts and techniques, including data preprocessing, feature engineering, and model evaluation. • data governance: understanding of data governance principles and practices, including data lineage, data quality, and data security. • communication skills: excellent written and verbal communication skills, with the ability to explain complex technical concepts to both technical and non-technical audiences. • problem solving: strong analytical and problem-solving skills. bonus points: • experience with data labeling platforms (e.g., labelbox, scale ai, amazon sagemaker ground truth). • experience with mlops practices and tools (e.g., kubeflow, mlflow). • experience with cloud platforms (e.g., aws, azure, gcp). • experience with data visualization tools (e.g., tableau, power bi). • experience with building and maintaining data pipelines using orchestration tools (e.g. airflow, prefect) why work at lenovo we are lenovo. we do what we say. we own what we do. we wow our customers. lenovo is a us$69 billion revenue global technology powerhouse, ranked #196 in the fortune global 500, and serving millions of customers every day in 180 markets. focused on a bold vision to deliver smarter technology for all, lenovo has built on its success as the world's largest pc company with a full-stack portfolio of ai-enabled, ai-ready, and ai-optimized devices (pcs, workstations, smartphones, tablets), infrastructure (server, storage, edge, high performance computing and software defined infrastructure), software, solutions, and services. lenovo's continued investment in world-changing innovation is building a more equitable, trustworthy, and smarter future for everyone, everywhere. lenovo is listed on the hong kong stock exchange under lenovo group limited (hkse: 992) (adr: lnvgy). this transformation together with lenovo's world-changing innovation is building a more inclusive, trustworthy, and smarter future for everyone, everywhere. to find out more visit www.lenovo.com, and read about the latest news via our storyhub. description and requirements this role is open for the edinburgh, scotland location only. candidates must be based there, as the position requires working from the office at least three days per week (3:2 hybrid policy). the lenovo ai technology center (latc)-lenovo's global ai center of excellence-is driving our transformation into an ai-first organization. we are assembling a world-class team of researchers, engineers, and innovators to position lenovo and its customers at the forefront of the generational shift toward ai. lenovo is one of the world's leading computing companies, delivering products across the entire technology spectrum, spanning wearables, smartphones (motorola), laptops (thinkpad, yoga), pcs, workstations, servers, and services/solutions. this unmatched breadth gives us a unique canvas for ai innovation, including the ability to rapidly deploy cutting-edge foundation models and to enable flexible, hybrid-cloud, and agentic computing across our full product portfolio. to this end, we are building the next wave of ai core technologies and platforms that leverage and evolve with the fast-moving ai ecosystem, including novel model and agentic orchestration & collaboration across mobile, edge, and cloud resources. this space is evolving fast and so are we. if you're ready to shape ai at a truly global scale, with products that touch every corner of life and work, there's no better time to join us.",edinburgh,Data Engineer,"['airflow', 'aws', 'azure', 'bigquery', 'cloud', 'data analysis', 'data pipeline', 'data warehouse', 'excel', 'feature engineering', 'gcp', 'hadoop', 'java', 'machine learning', 'numpy', 'pandas', 'power bi', 'python', 'r', 'redshift', 'scala', 'snowflake', 'spark', 'sql', 'statistics', 'tableau']","['airflow', 'aws', 'azure', 'bigquery', 'cloud', 'data analysis', 'data pipeline', 'data warehouse', 'excel', 'feature engineering', 'gcp', 'hadoop', 'java', 'machine learning', 'numpy', 'pandas', 'power bi', 'python', 'r', 'redshift', 'scala', 'snowflake', 'spark', 'sql', 'statistics', 'tableau']",
senior data engineer - contract role,la fosse,"senior data engineer contract role outside ir35 | £500-525/day | milton keynes or manchester | hybrid | initial 7 months we have a new senior data engineer contract opportunity with a global leading client with offices in both manchester and milton keynes. you’ll join a high-impact team focused on rebuilding and optimising data pipelines into a databricks lakehouse environment, enabling clean, scalable, and high-quality data delivery for analytics and reporting. this is outside ir35, requires 1 day per week on-site, and offers an initial 7-month contract with an immediate start and strong potential for extension on a greenfield project. key responsibilities • design, build, and optimise data pipelines using elt/etl best practices. • migrate and transform data into a databricks lakehouse architecture. • ensure data quality, reliability, and scalability for analytics and reporting. • collaborate with stakeholders to deliver robust solutions aligned with business needs. • support production environments and troubleshoot performance issues. ideal candidate • 7+ years in data engineering with strong experience in cloud-based data platforms. • proven ability to design and optimise pipelines for large-scale data processing. • hands-on experience with databricks and azure . • strong stakeholder communication and problem-solving skills. tech stack required: • databricks • dbt • python • pyspark • sql • azure bonus: experience in ecommerce environments. if you are interested please apply below!",greater manchester,Data Engineer,"['azure', 'cloud', 'data pipeline', 'databricks', 'dbt', 'elt', 'etl', 'pyspark', 'python', 'r', 'scala', 'spark', 'sql']","['azure', 'cloud', 'data pipeline', 'databricks', 'dbt', 'elt', 'etl', 'pyspark', 'python', 'r', 'scala', 'spark', 'sql']",
why this senior data engineer role is like star wars.,mongoose gray,"job description there are 2 versions of this ad. below is the anaemic version. bonus points if you can find the more analeptic version. and provide proof of your quest. good luck! • the senior data engineer will work for a progressive, fast-paced data engineering business. • the senior data engineer will earn up to £80,000 per year. • the senior data engineer will be 80% data engineer, 20% coder. • the senior data engineer will be familiar with object-orientated, function-scripting languages e.g. python (bonus marks). • the senior data engineer will have excellent database interrogation and query writing skills i.e. heavy-lifting sql. • the senior data engineer will have first-class knowledge of cloud infrastructure e.g. aws, azure, gcp et al. • the senior data engineer will work closely with our product, data and engineering teams. • the senior data engineer will build high-performance, scalable, extensible data warehouse infrastructure. • the senior data engineer will maintain data pipelines and orchestrate the delivery of data in batch and realtime mode. • the senior data engineer will be heavily involved in data collection, analysis, enrichment, abstraction and visualisation. • the senior data engineer will research, innovate and champion new ways of doing things. • the senior data engineer will be familiar with reporting and data modelling tools. • the senior data engineer will live and breathe “extract. transform. load."" still with us? great. then please send us a cv and covering letter convincing us that you actually enjoyed reading this ad. we look forward to hearing from you.",newcastle upon tyne,Data Engineer,"['aws', 'azure', 'cloud', 'data pipeline', 'data warehouse', 'excel', 'gcp', 'python', 'r', 'scala', 'sql']","['aws', 'azure', 'cloud', 'data pipeline', 'data warehouse', 'excel', 'gcp', 'python', 'r', 'scala', 'sql']",£48k–£64k a year
ai and data engineer,bcn group,"job title: ai & data developer location: hybrid (strong preference for reading office) hours: monday to friday, 37.5 hours per week salary: competitive + bcn benefits about bcn: at bcn we unite people and technology to enable organisations to fly. we believe people and organisations can achieve anything using technology to it’s full potential. our role is to help them understand what is possible, implement in the right way and utilise their technology to achieve their ambitions. which is why we put people front and centre – building client relationships for life and fostering a culture where our people thrive. we are a leading managed it services provider and technology consultant, specialising in delivering transformative technology solutions with industry-leading client experience across business, public sector and not for profit organisations. from cloud computing, cybersecurity, and data management to power app development, we are dedicated to pioneering technology with microsoft innovation. guided by our 3 values of building relationships, customer success and passion and dedication, we are on a mission to make bcn the most trusted tech partner in the uk today. the kind of company clients want to work with, and people want to work for. we are delighted you are on this journey with us! focus of the role: we are looking for a dp-100–certified azure data scientist who is passionate about applied machine learning and delivering real improvements to our clients in smart, effective ways. the successful candidate will be eager to deepen their skills and broaden into wider azure analytics capabilities, building a long career here at bcn group within a growing and exciting innovation technology team. our people have three things in common: a curiosity for learning, a drive to deliver projects brilliantly, and a belief that together we make a difference. using your expertise in azure machine learning, python (pandas, scikit-learn) and azure sdks, you will help shape our data & productivity capability—designing, training, and deploying models with mlflow/automl, and integrating them into real business processes. you’ll collaborate across disciplines (data engineering, security, product) and work with services such as azure storage, azure devops/github actions, and fabric/power bi to ensure solutions are observable, governed, and production-ready. this is an exciting opportunity to develop new capabilities and establish industry-leading machine-learning practices that drive measurable outcomes for our clients. responsibilities: • work directly with business stakeholders to conceive, design and deliver end-to-end machine learning solutions, aligned to operational goals and governance standards. • prepare and transform data for modelling using python and azure-native tools, ensuring quality and consistency across the ml and ai lifecycles. • evaluate, and deploy models using automl, mlflow, and custom pipelines, with a focus on performance, scalability, and maintainability. • monitor model performance, detect drift, and implement retraining strategies to maintain relevance and accuracy. • apply responsible ai principles including fairness, explainability, and privacy to support ethical and transparent model development. • collaborate with cross-functional teams to integrate ml and ai solutions into business processes, improving engagement, productivity, and decision-making. • support our clients maintain and optimise azure ml environments to deliver reproducible experimentation and efficient deployment. person, skills & experience: • certified in dp-100 azure data scientist associate (mandatory). • confident working independently or collaboratively to support solution design, delivery, and governance across the ml and ai lifecycle. • experienced in designing, training, registering, and deploying models using azure machine learning (aml), automl, and mlflow. • proficient in python, pandas, and scikit-learn for data science and feature engineering, with a strong focus on rigorous validation and experiment tracking. • skilled in deploying models to real-time rest endpoints and orchestrating batch inference pipelines. • capable of implementing mlops practices including ci/cd pipelines, model registry, environment management, and promotion across dev/test/prod stages. • solid understanding of cloud and data engineering principles including storage, compute, and pipeline orchestration. • strong ability to collaborate with subject matter experts to map current-state business processes and translate them into machine learning opportunities with measurable operational impact. • clear communicator with excellent problem-solving skills, able to engage effectively with both technical and non-technical stakeholders. • exposure to kubernetes, azure ai search, or azure ai foundry to support scalable and efficient solution delivery. • understanding of generative ai techniques and language model optimisation to enhance solution capability and innovation. • familiarity with ethical ai frameworks and compliance standards to ensure responsible and transparent model development. • 2+ years of experience in applied machine learning or data science roles, contributing to the delivery of impactful solutions. • proven track record of deploying machine learning models into production environments, supporting business outcomes and operational efficiency. why bcn? • the opportunity to shape your own future with industry leading training and development, with access to our bcn academy. • competitive salary with the ability to progress. • 23-days holiday allowance, increasing with length of service, plus bank holidays, an extra day off on your birthday and the option to buy more! • company pension scheme. • 2 paid leave days per year to volunteer and support your local community – if it matters to you it matters to us. • health cash plan with free access to a confidential employee assistance programme (eap) supporting bereavement, financial, health and wellbeing, and much more • life assurance • cycle to work scheme, electric vehicle scheme, home and tech scheme, and retail discounts. • balancing work, life, and fitness can be challenging, so we offer a free on-site gym at our manchester and leeds locations to make it easier to stay active. • long service recognition to celebrate all the milestones • beer (or soft drinks) and pizza friday’s, dress down every day, social events such as summer bbq, christmas party and lots more!",manchester,Data Engineer,"['azure', 'cloud', 'excel', 'experimentation', 'feature engineering', 'machine learning', 'pandas', 'power bi', 'python', 'r', 'scala', 'scikit-learn']","['azure', 'cloud', 'excel', 'experimentation', 'feature engineering', 'machine learning', 'pandas', 'power bi', 'python', 'r', 'scala', 'scikit-learn']",
technology consulting - data engineer,tenth revolution group,"databricks data engineer - manchester £100,000 | 2 days in office my client, a leading technology consultancy, is seeking an experienced databricks data engineer to join their team on a high-profile engagement with a major insurance client. this is a hands-on technical role focused on designing, building, and optimising cloud-based data integration and analytics solutions in the insurance domain. this is an opportunity to work on complex, enterprise-scale data transformation programmes using cutting-edge azure and databricks technologies. highly competitive salary of up to £100,000 (doe) • performance-relate bonus 12.5% • hybrid working: 2 days per week in manchester office • 25 days annual leave plus bank holidays • professional development and certification support • comprehensive benefits package including pension scheme and pmi design, build, and deploy scalable data pipelines using azure databricks, azure data factory, and azure sql database optimise databricks environments for performance, cost efficiency, and security develop and maintain etl/elt processes and data quality frameworks translate business requirements into robust data models ensuring consistency and accuracy produce technical documentation including data flow diagrams, architecture designs, and test plans stay current with emerging technologies in the azure ecosystem and data engineering best practices proven experience in the insurance domain, with strong understanding of policy, claims, and regulatory data extensive experience in data engineering, ideally in enterprise-scale environments deep expertise in azure databricks, azure data factory, azure synapse, and azure data lake hands-on experience with spark (pyspark/sparksql) and distributed data processing strong data modelling skills (dimensional, ods, data vault) and data warehousing concepts proficiency in sql, version control (git), and ci/cd pipelines familiarity with agile delivery and devops practices (azure devops, jira) i have limited slots for 1st stage interviews so if you’re interested, get in touch asap with a copy of your most up-to-date cv and email me at m.please note: this is a permanent role for uk residents only. some of our roles may be subject to successful background checks including a dbs and credit check. trg are the go-to recruiter for power bi and azure data platform roles in the uk, offering more opportunities across the country than any other. we're the proud sponsor and supporter of sqlbits, power platform world tour, the london power bi user group, newcastle power bi user group and newcastle data platform and cloud user group.",manchester,Data Engineer,"['azure', 'cloud', 'data lake', 'data pipeline', 'databricks', 'elt', 'etl', 'power bi', 'pyspark', 'r', 'scala', 'spark', 'sql']","['azure', 'cloud', 'data lake', 'data pipeline', 'databricks', 'elt', 'etl', 'power bi', 'pyspark', 'r', 'scala', 'spark', 'sql']",
data engineer - edinburgh 🏆,harvey nash,"data engineer - edinburgh 💰 salary: £33,000 - 65,000 per year at harvey nash we are looking for a data engineer! 🛠️ our tech stack: ai, aws, ci/cd, cloud, data warehouse, devops, docker, gitlab, grafana, support, kanban, openshift, postgresql, product manager, python, rest, sql, security, tdd, ux ui design, data 📝 rquirements: - significant commercial experience with the following technologies: - python - postgresql - rest apis - modern devops and ci/cd practices and tooling, including docker, gitlab ci, aws codepipeline, aws cdk, and aws cloudformation - significant and demonstrable commercial experience in: - sql, data transformation, and analysis - delivering high-quality software collaboratively in high-performing, cross-functional development teams - implementing data etls, data streaming systems, and data integration solutions - working in agile delivery models, such as scrum and/or kanban frameworks - desirable qualifications include: - data warehousing - hybrid on-premises/cloud solutions - aws glue, step functions, lambda functions, s3, rds, and data migration service - using testing tools for unit testing, including system test automation frameworks - openshift - postgis for postgresql - designing and implementing solutions using service and event-based architectures - monitoring, alerting, and intelligence tools and processes, including grafana - human-centered, research-driven, inclusive design practices - developing within digital first or gds quality standards - tdd 👩‍💻👨‍💻 your responsibilities are: - contribute to the development and delivery of solutions to integrate new datasets into the data warehouse - support the development of new data products in collaboration with the senior data & ai product manager and other stakeholders - assist in migrating the data warehouse from on-prem to aws cloud - ensure technical resiliency of all data integration solutions and services - enhance and support existing data product outputs for both internal and external customers - collaborate with technical colleagues across the organization to design robust data integration solutions - demonstrate excellent, sustainable, and collaborative software development practices - actively participate in all team events, leading when specialist knowledge is required and supporting the team to improve their processes - troubleshoot and fix development and production problems across multiple environments and operating platforms - engage with wider communities of practice to share knowledge, techniques, and experiences - ensure high quality of developed solutions through unit tests and code quality tools - comply with non-functional software requirements such as accessibility, security, performance, and maintainability - support and deliver disaster recovery assurance of digital services. view this job and over 500 other transparent jobs with salaries (💰💰💰) & tech stacks (🛠️) on devitjobs.uk category: data developer / engineer location address: waterloo place, edinburgh, united kingdom salary: £33,000 - 65,000 per year benefits & perks that we offer: harvey nash - more about us and the role: this role is a 6-month contract position, offered outside ir35, with a day rate of £470. we're focused on collaboratively developing internal casework-profiling solutions using data extraction and processing techniques applied to semi-structured and unstructured documents. our goal is to enhance our data analytics capabilities by creating a data warehouse that supports robust data governance and management. our small, multi-functional team fosters a collaborative and agile culture, aimed at developing high-quality and resilient digital products. this role does require the successful candidate to hold, or be willing to apply for, a valid basic disclosure scotland. we look forward to your application. are you looking for data jobs in edinburgh?",edinburgh,Data Engineer,"['aws', 'cloud', 'data analytics', 'data warehouse', 'etl', 'excel', 'python', 'r', 'sas', 'sql']","['aws', 'cloud', 'data analytics', 'data warehouse', 'etl', 'excel', 'python', 'r', 'sas', 'sql']",£33k–£65k a year
"data engineer ii - databricks and python | glasgow, uk",jpmorgan chase & co.,"data engineer ii - databricks and python job description join our innovative digital intelligence team at j.p. morgan, where we leverage cutting-edge technology to drive data-driven decision-making and enhance business performance. we are seeking a talented and motivated databricks data engineer to join our team and contribute to our mission of transforming data into actionable insights. as a data engineer ii at jpmorgan chase within the digital intelligence team, you will play a crucial role in designing, developing, and maintaining scalable data processing solutions using databricks, python, and aws. you will collaborate with cross-functional teams to deliver high-quality data solutions that support our business objectives. job responsibilities: • execute software solutions, design, development, and technical troubleshooting with the ability to think beyond routine or conventional approaches to build solutions or break down technical problems. • create secure and high-quality production code and maintain algorithms that run synchronously with appropriate systems. • produce architecture and design artifacts for complex applications while being accountable for ensuring design constraints are met by software code development. • gather, analyze, synthesize, and develop visualizations and reporting from large, diverse data sets in service of continuous improvement of software applications and systems. • proactively identify hidden problems and patterns in data and use these insights to drive improvements to coding hygiene and system architecture. • contribute to software engineering communities of practice and events that explore new and emerging technologies. • provide guidance to the immediate team of software engineers on daily tasks and activities. • set the overall guidance and expectations for team output, practices, and collaboration. • anticipate dependencies with other teams to deliver products and applications in line with business requirements. • manage stakeholder relationships and the team's work in accordance with compliance standards, service level agreements, and business requirements. required qualifications, capabilities, and skills: • formal training or certification on software engineering concepts and applied experience. • hands-on experience in data mapping, data architecture, and data modeling on databricks. • extensive experience in aws, design, implementation, and maintenance of data pipelines using python, pyspark on databricks. • proficient in python and pyspark, able to write and execute complex queries to perform curation and build views required by end users (single and multi-dimensional). • strong understanding of front-end and back-end technologies, with a focus on creating seamless user experiences. • extensive experience in databricks data engineering, data warehousing concepts, etl processes (job runs, data ingestion and delta live tables, spark streaming). • experienced in standing up and maintaining ec2/ecs instances, s3, glue, and lambda services. • experience in building notebooks with complex code structures and debugging failed jobs. • proven experience in performance and tuning to ensure jobs are running at optimal levels and no performance bottleneck. • proven ability to deliver high-quality features into production systems in a rapid-paced, iterative development environment. preferred qualifications, capabilities, and skills: • experience with machine learning and data science workflows. • familiarity with data visualization tools and techniques. • knowledge of data governance and security best practices. • experience in carrying out data analysis to support business insights. about us j.p. morgan is a global leader in financial services, providing strategic advice and products to the world's most prominent corporations, governments, wealthy individuals and institutional investors. our first-class business in a first-class way approach to serving clients drives everything we do. we strive to build trusted, long-term partnerships to help our clients achieve their business objectives. we recognize that our people are our strength and the diverse talents they bring to our global workforce are directly linked to our success. we are an equal opportunity employer and place a high value on diversity and inclusion at our company. we do not discriminate on the basis of any protected attribute, including race, religion, color, national origin, gender, sexual orientation, gender identity, gender expression, age, marital or veteran status, pregnancy or disability, or any other basis protected under applicable law. we also make reasonable accommodations for applicants' and employees' religious practices and beliefs, as well as mental health or physical disability needs. visit our faqs for more information about requesting an accommodation. about the team j.p. morgan asset & wealth management delivers industry-leading investment management and private banking solutions. asset management provides individuals, advisors and institutions with strategies and expertise that span the full spectrum of asset classes through our global network of investment professionals. wealth management helps individuals, families and foundations take a more intentional approach to their wealth or finances to better define, focus and realize their goals.",glasgow,Data Engineer,"['aws', 'data analysis', 'data pipeline', 'databricks', 'elt', 'etl', 'machine learning', 'pyspark', 'python', 'r', 'scala', 'spark']","['aws', 'data analysis', 'data pipeline', 'databricks', 'elt', 'etl', 'machine learning', 'pyspark', 'python', 'r', 'scala', 'spark']",
data engineer - data infrastructure,starling,"starling is the uk’s first and leading digital bank on a mission to fix banking! our vision is fast technology, fair service, and honest values. all at the tap of a phone, all the time. starling is the uk’s first and leading digital bank on a mission to fix banking! we built a new kind of bank because we knew technology had the power to help people save, spend and manage their money in a new and transformative way. we’re a fully licensed uk bank with the culture and spirit of a fast-moving, disruptive tech company. we’re a bank, but better: fairer, easier to use and designed to demystify money for everyone. we employ more than 3,000 people across our london, southampton, cardiff and manchester offices. our technologists are at the very heart of starling and enjoy working in a fast-paced environment that is all about building things, creating new stuff, and disruptive technology that keeps us on the cutting edge of fintech. we operate a flat structure to empower you to make decisions regardless of what your primary responsibilities may be, innovation and collaboration will be at the core of everything you do. help is never far away in our open culture, you will find support in your team and from across the business, we are in this together! the way to thrive and shine within starling is to be a self-driven individual and be able to take full ownership of everything around you: from building things, designing, discovering, to sharing knowledge with your colleagues and making sure all processes are efficient and productive to deliver the best possible results for our customers. our purpose is underpinned by five starling values: listen, keep it simple, do the right thing, own it, and aim for greatness. hybrid working we have a hybrid approach to working here at starling - our preference is that you're located within a commutable distance of one of our offices so that we're able to interact and collaborate in person. in technology, we're asking that you attend the office a minimum of 1 day per week. our data environment our data teams are aligned to divisions covering the following banking services & products, customer identity & financial crime and data & ml engineering. our data teams are excited about delivering meaningful and impactful insights to both the business and more importantly our customers. hear from the team in our latest blogs or our case studies with women in tech. we are looking for talented data professionals at all levels to join the team. we value people being engaged and caring about customers, caring about the code they write and the contribution they make to starling. people with a broad ability to apply themselves to a multitude of problems and challenges, who can work across teams do great things here at starling, to continue changing banking for good. responsibilities: • building out cloud-native warehouse and machine learning platforms • improve existing and implement new tooling in and between aws and gcp requirements: • good knowledge of programming languages such as python or java. • solid understanding and practical experience with kubernetes (operators, helm, etc) • proficiency with either aws or gcp • experience with terraform to define and manage cloud infrastructure through code • good practical experience with sql and relational databases (postgresql preferred) desirables: • experience in sql-based transformation workflows, particularly using dbt in bigquery • familiarity with streaming data ingestion technologies (kafka, debezium) • exposure to data management and linux administration interview process interviewing is a two way process and we want you to have the time and opportunity to get to know us, as much as we are getting to know you! our interviews are conversational and we want to get the best from you, so come with questions and be curious. in general you can expect the below, following a chat with one of our talent team: • stage 1 - 30 mins with one of the team • stage 2 - take-home challenge • stage 3 - 60 mins technical interview with two team members • stage 4 - 45 min final with two data executives benefits: • 25 days holiday (plus take your public holiday allowance whenever works best for you) • an extra day’s holiday for your birthday • annual leave is increased with length of service, and you can choose to buy or sell up to five extra days off • 16 hours paid volunteering time a year • salary sacrifice, company enhanced pension scheme • life insurance at 4x your salary & group income protection • private medical insurance with vitalityhealth including mental health support and cancer care. partner benefits include discounts with waitrose, mr&mrs smith and peloton • generous family-friendly policies • perkbox membership giving access to retail discounts, a wellness platform for physical and mental health, and weekly free and boosted perks • access to initiatives like cycle to work, salary sacrificed gym partnerships and electric vehicle (ev) leasing about us you may be put off applying for a role because you don't tick every box. forget that! while we can’t accommodate every flexible working request, we're always open to discussion. so, if you're excited about working with us, but aren’t sure if you're 100% there yet, get in touch anyway. we’re on a mission to radically reshape banking – and that starts with our brilliant team. whatever came before, we’re proud to bring together people of all backgrounds and experiences who love working together to solve problems. starling bank is an equal opportunity employer, and we’re proud of our ongoing efforts to foster diversity & inclusion in the workplace. individuals seeking employment at starling bank are considered without regard to race, religion, national origin, age, sex, gender, gender identity, gender expression, sexual orientation, marital status, medical condition, ancestry, physical or mental disability, military or veteran status, or any other characteristic protected by applicable law. when you provide us with this information, you are doing so at your own consent, with full knowledge that we will process this personal data in accordance with our privacy notice. by submitting your application, you agree that starling bank will collect your personal data for recruiting and related purposes. our privacy notice explains what personal information we will process, where we will process your personal information, its purposes for processing your personal information, and the rights you can exercise over our use of your personal information.",manchester,Data Engineer,"['aws', 'bigquery', 'cloud', 'dbt', 'gcp', 'java', 'kafka', 'machine learning', 'python', 'r', 'sql']","['aws', 'bigquery', 'cloud', 'dbt', 'gcp', 'java', 'kafka', 'machine learning', 'python', 'r', 'sql']",
snowflake data engineer,talenting career science,"snowflake data engineer london salary up to £72,000 plus benefits hybrid working london based consultancy seeks snowflake data engineer to join their client projects team on a permanent basis. this consultancy is renowned for its comprehensive and cutting-edge data solutions, serving a diverse clientele across various industries. your initial deployment will be on a greenfield snowflake implementation. key responsibilities: • design, develop, and maintain scalable data pipelines using snowflake. • collaborate with business stakeholders to translate business requirements into robust technical solutions. • ensure the performance, quality, and responsiveness of data processes. • conduct data modelling, performance tuning, and troubleshooting within snowflake environments. required skills and experience: • proven experience as a data engineer with specific expertise in snowflake. ideally seeking 2+ previous snowflake project experience. • strong proficiency in sql and experience with scripting languages such as python. • solid understanding of cloud computing environments, particularly azure. • experience with data warehousing, data modelling, and data architecture. • ability to handle large datasets and integrate complex business logic into designed solutions. for more details about this role or to apply, please follow the “i’m interested” link or contact ben halfpenny at talenting career science directly. talenting career science focuses on you as an individual. we will adapt our processes to best suit your personal needs. please do let us know if there are any adjustments that will assist you in your career search. talenting champions diversity. we were founded to champion neurodiversity in recruitment, recognising traditional recruitment methods often prevent many from displaying their true qualities.",manchester,Data Engineer,"['azure', 'cloud', 'data pipeline', 'python', 'r', 'scala', 'snowflake', 'sql']","['azure', 'cloud', 'data pipeline', 'python', 'r', 'scala', 'snowflake', 'sql']",
"data engineer, lead",twelve,"lead data engineer - london (hybrid, 3 days) - banking/finance about the company: we are a rapidly growing financial technology firm challenging traditional banking models. our mission is to empower businesses with innovative, data-driven lending solutions. we believe in leveraging modern technology to build a more efficient and responsive financial ecosystem. the opportunity: we are seeking a highly experienced and technically proficient lead data engineer to join our expanding data and analytics team. this is a crucial role where you will be responsible for defining the technical strategy, guiding architectural decisions, and leading a team of talented data engineers. you will be at the forefront of building a scalable and secure data infrastructure that drives our core business functions. key responsibilities: • set the technical direction and standards for the data engineering function. • lead and mentor a squad of 4-6 data engineers, fostering a culture of innovation and continuous improvement. • design, build, and maintain robust data pipelines and data models to support analytics, reporting, and machine learning initiatives. • develop and manage a reliable data platform using a modern data stack. • enhance data governance, security, and quality across the organization. • implement a ""you build it, you run it"" devops culture, taking ownership of the entire lifecycle of your team's services. skills and experience: • minimum of 7 years of professional experience in data engineering. • at least 2 years of experience in a technical leadership or management role. • deep expertise in modern data stacks and cloud environments, with hands-on experience in tools such as: • data transformation: dbt (data build tool) • data warehousing: bigquery or similar • workflow orchestration: airflow • infrastructure as code: terraform • cloud platforms: google cloud platform (gcp) or amazon web services (aws) • strong software engineering fundamentals, including proficiency in python, sql, and devops practices. • proven ability to work in a fast-paced, agile environment with a focus on delivering outcomes. what we offer: • a competitive salary and equity package. • comprehensive benefits, including generous holiday allowance and enhanced family leave policies. • support for professional development and continuous learning. • a collaborative and dynamic work environment. if you're a data engineer and this opportunity looks interesting then please apply and we will be in touch with more details.",united kingdom,Data Engineer,"['airflow', 'aws', 'bigquery', 'cloud', 'data pipeline', 'dbt', 'gcp', 'google cloud', 'machine learning', 'python', 'r', 'scala', 'sql']","['airflow', 'aws', 'bigquery', 'cloud', 'data pipeline', 'dbt', 'gcp', 'google cloud', 'machine learning', 'python', 'r', 'scala', 'sql']",
data engineer - remote,nigel frank,"a leading financial services organisation are looking for a data engineer to join their team on a fully-remote basis - as such, this role is open to candidates across the uk. joining their data engineering team, you'll work with microsoft technologies to build high-quality enterprise-level solutions that enable data-driven decision making, and empower employees to deliver first-class customer experiences. a lot of their current project work is on-premise, so you'll be using the likes of sql server, the bi stack (ssis, ssas, ssrs) and power bi. that being said, they're starting to explore azure technologies, so you also have the chance to gain hands-on skills with the likes of data factory, synapse etc. going forward. they pride themselves on being a people-first business, with a focus on personal and professional growth whilst supporting a healthy work-life balance. requirements: • hands-on experience with sql, ssis, ssrs and ssas • experience developing reporting solutions in power bi with use of dax • strong understanding of data warehousing principles • experience working in agile environments • experience working in regulated environments, ideally financial services • knowledge of azure data platform technologies would be desirable but not essential benefits: • salary up to £60,000 depending on experience • discretionary bonus • 25 days annual leave plus bank holidays • holiday purchase scheme • pension • private medical and dental insurance • health cash plan • critical illness insurance • health assessment • life assurance • travel insurance please note: this is a role for uk residents only. this role does not offer sponsorship. you must have the right to work in the uk with no restrictions. some of our roles may be subject to successful background checks including a dbs and credit check. tenth revolution group / nigel frank are the go-to recruiter for data and ai roles in the uk, offering more opportunities across the country than any other. we're the proud sponsor and supporter of sqlbits, and the london power bi user group. to find out more and speak confidentially about your job search or hiring needs, please contact me directly at v.simpson@tenthrevolution.com",peterborough,Data Engineer,"['azure', 'power bi', 'r', 'sas', 'sql', 'sql server']","['azure', 'power bi', 'r', 'sas', 'sql', 'sql server']",
lead data engineer,midnite,"why midnite? midnite is a next-generation betting platform that is built for today’s fandom. we are a collective of engineers and designers who all share a passion for building the best sportsbook & casino experience possible, allowing our fans to feel closer to the games they love through the rush of winning money. unlike the alternatives, midnite doesn't feel like a website built two decades ago. instead, it's a cutting-edge creation, designed and constructed from the ground up with the latest technologies. crafting an experience that's truly intuitive, immersive, and immediately understandable is no walk in the park, but we thrive on the challenge. we believe we're on the brink of creating something truly awesome. what will you do? we’re looking for a lead data engineer to drive the next phase of our data strategy at midnite. this is a hands-on leadership role where you’ll set the technical direction, own the design and scalability of our data infrastructure, and ensure the team delivers high-quality, impactful solutions. you’ll work across the full data lifecycle from ingestion and modelling to orchestration, monitoring, and analytics enablement, while also mentoring engineers and shaping engineering best practices. as a lead, you’ll partner with our leadership team to make sure our data function not only delivers but also drives strategic decision-making. our tech stack: python, docker, dagster, dbt, fivetran, apache iceberg, snowflake, s3, glue, ecs, and omni. we’re constantly evolving our stack and welcome input from engineering leaders on how we can improve scalability, reliability, and efficiency. leadership & collaboration: as lead data engineer, you’ll be both a technical expert and a team leader. you’ll: • set technical standards and drive adoption of best practices across the team. • mentor and coach engineers, raising the bar on quality and delivery. • collaborate closely with senior stakeholders to align data initiatives with business priorities. • champion innovation, evaluating new tools, platforms, and methodologies. responsibilities: • own the technical strategy for data engineering, ensuring our stack scales with the business. • design, maintain, and evolve robust data pipelines and architecture to support low latency batch use cases. • oversee the implementation of data models and frameworks that support analytics, and business intelligence. • drive engineering best practices across testing, monitoring, version control, and automation. • lead code reviews, enforce quality standards, and ensure technical debt is managed proactively. • manage and mentor engineers, supporting career development and creating a culture of excellence. • stay ahead of industry trends, introducing tools and methods that future-proof the data platform. essential experience: • 7+ years in data engineering, with at least 2+ years in a lead or equivalent role. • proven track record of designing and scaling data platforms in a high-growth or start-up environment. • strong expertise in python and sql, with deep experience in orchestration frameworks (dagster, airflow, prefect). • advanced knowledge of data modelling and architecture (kimball dimensional modelling, data vault etc). • hands-on experience with dbt, modern data warehouses, and aws. • demonstrated ability to mentor and develop engineers. desirable experience: • experience with snowflake. • experience with apache iceberg. • experience with infrastructure-as-code (terraform preferred). • experience embedding observability and monitoring in data systems. • previous experience building and leading data teams in a scale-up environment. what’s in it for you: • shape our future: play a key role in our team's success, where your voice matters, and you'll have a direct impact on shaping midnite's future. • connect and unwind: take part in our quarterly gatherings where our community comes together to bond and have fun. • comprehensive health coverage: look after your well-being with our outstanding zero-excess health insurance plan, which includes optical and dental coverage. • simplify life: take advantage of our nursery salary sacrifice scheme, allowing you to conveniently pay your child's nursery fees straight from your paycheck. • work-life balance: enjoy 25 paid holidays a year, plus generous paid maternity, paternity, and adoption leave, supporting you during life's most important moments. • productive home office: we provide everything you need for a comfortable and ergonomic home setup, ensuring you're as productive as possible. • flexible working: we embrace flexible working, allowing you to adjust your schedule when life's unexpected moments arise. • latest tech made easy: with our salary sacrifice schemes, you can upgrade to the latest gadgets, household items, and mobile tech without the upfront cost. • exclusive perks: enjoy a wide range of discounts on retailers, groceries, and subscriptions, making life a little more affordable. • grow with us: expand your skills through internal and external learning opportunities while benefiting from access to mentorship programs that support your development. • transparent compensation: we provide competitive pay with clear team bandings and salary grids, ensuring that salary discussions are simple and fair. • constructive feedback: we foster a transparent culture, encouraging individual feedback and review sessions to help everyone improve. • work from anywhere: whether it's a cosy cottage in the cotswolds or anywhere else, enjoy the freedom of working remotely.",greater manchester,Data Engineer,"['airflow', 'aws', 'business intelligence', 'data pipeline', 'data warehouse', 'dbt', 'excel', 'python', 'r', 'scala', 'snowflake', 'sql']","['airflow', 'aws', 'business intelligence', 'data pipeline', 'data warehouse', 'dbt', 'excel', 'python', 'r', 'scala', 'snowflake', 'sql']",
(senior) forecasting data engineer,sefe securing energy for europe gmbh,"in short shape the future of energy forecasting with sefe energy! we’re looking for a talented forecasting data engineer to join our portfolio management team in manchester. in this role, you’ll design and deliver accurate energy demand forecasts and analyse business operations to identify opportunities to improve stability, scalability, and performance. if you’re passionate about data, innovation, and making an impact in the energy market, we’d love to hear from you. what will you do working in collaboration with key stakeholders in it, risk, sales and finance you will • create energy demand forecasts and related reports (pnl attribution and aoc reporting) according to business needs • stakeholder management – engage with key stakeholders and ensure clear communication on issues that influence forecasting • design and implement improvements to current codebases to improve stability, scalability, and performance • lead design, implementation and system development in line with business requirements and corporate strategy • monitor and interpret model performance and forecast accuracy • analyze business operations to identify opportunities for improvement and propose actionable solutions that create measurable value and efficiency • provide advice and mentoring to junior developers who support their technical growth and integration into the team what will you bring • knowledge and experience in the power market (highly desirable) • proven experience as a data engineer in a similar environment and tech stack • strong programming skills in python (or similar) • expertise in data wrangling using python libraries (pandas, polars, numpy) • experience with microsoft sql server and preferably time series databases • familiarity with machine learning operations (mlops) • experience designing and developing scalable etl pipelines and knowledge of devops principles • collaborative approach with strong stakeholder engagement skills • effective problem-solving and critical thinking abilities • proactive mindset with a focus on continuous improvement • accuracy and attention to detail, even under tight deadlines about us securing energy for europe – it’s a simple statement, with a bold ambition. sefe is not just our name, but also encompasses everything that drives us. to accomplish this, we’re taking immediate action to secure gas supply – but also looking forward, to explore our role in the european energy transformation and how we can contribute to a stable and sustainable future. sefe, an international energy company, ensures the security of supply and drives the decarbonisation of its customers. sefe’s activities span the energy value chain, from origination and trading to sales, transport, and storage. through its decades-long expertise in trading and the development of its lng business, sefe has become one of the most important suppliers to industrial customers in europe, with an annual sales volume of 200 twh of gas and power. its 50,000 customers range from small businesses to municipalities and multinational organisations. by investing in clean energies and especially in the hydrogen ecosystem, sefe is contributing to the energy transition. the company employs around 2,000 people globally and is owned by the federal government of germany. our international teams work across locations in europe, asia, and north america. we’re passionate about energy and the important role it can play in shaping a better future. securing energy – now and for the future. our benefits in return we offer a competitive starting salary supported by a comprehensive range of financial, lifestyle and wellness benefits with the flexibility to follow a hybrid working model. • bonus earning potential • non-contributory pension with 10% employer contribution • 25 days holiday plus bank holidays and volunteering days • buy / sell holidays • life assurance • medical and dental insurance (family cover) • range of optional flexible benefits we are committed to supporting your career growth with opportunities to develop both your knowledge and experience through a blended approach to learning. join sefe and help us secure energy supply across europe and shape a better, more sustainable tomorrow.",manchester,Data Engineer,"['etl', 'machine learning', 'numpy', 'pandas', 'python', 'r', 'scala', 'sql', 'sql server', 'time series']","['etl', 'machine learning', 'numpy', 'pandas', 'python', 'r', 'scala', 'sql', 'sql server', 'time series']",
data engineer,tasker & partners,"the role we’re now looking for a motivated & experienced data engineer to join the jensten group, based from our oldham office (we will also consider remote working for the right person). the main purpose of the role is to play a pivotal role in designing, building, and maintaining scalable data solutions to support business intelligence, analytics, and reporting needs. this position requires expertise in azure/microsoft data stack, database management, data transformation processes, and modern development practices, with a focus on enabling robust and efficient data-driven decision-making. reporting to the bi analyst, you’ll work closely with the wider data and technology team, as well as other stakeholders to ensure data is collected, stored and processed in line with business needs. some key objectives & responsibilities will include: • data pipeline development: improve and maintain existing ssis data flows, and work towards implementation of azure data factory pipelines to replace existing ssis architecture. • data modelling & transformation: leverage t-sql and ssis to manage and transform data, ensuring high-quality outputs, as well as assisting our data analysts with azure analysis services models and implementation of new fabric models. • database management: maintain, improve, & optimise the existing sql databases hosted on our vms and in azure, and assist in implementation of onelake with medallion architecture. • reporting & visualization: collaborate with business teams and our analysts to deliver insightful and actionable dashboards and reports using power bi and ssrs. • development & source control: leverage devops practices to streamline deployments, monitor pipelines, & maintain operational efficiency, and use git source control to manage code repositories, enforce versioning, and support collaborative development workflows. • collaboration & strategy: work closely with stakeholders to understand business objectives and translate them into technical solutions, and also partner with cross-functional teams to establish data architecture best practices and maintain a cohesive data strategy. about you as our data engineer, you’ll have a strong teamwork ethic, communication, and collaborative skills with a keen eye for detail. also having strong technical skills, you’ll work closely with other it professionals, management, and various stakeholders within the group. you’ll also be or have: • experience in data engineering or a similar role. • extensive experience with t-sql, ssis, and sql server. • experience using devops and git. • proficient with analysis services (ideally in azure). • experience with azure data factory. • exposure to data lake architecture and/or onelake. • knowledge of the component parts of the fabric stack. • strong analytical and problem solving mentality. • clear communication skills and the ability to work independently. • detail-oriented with a commitment to delivering high-quality work. • collaborative mindset with the ability to work effectively in a team environment. • adaptable and eager to learn new tools and technologies. rewards & benefits when you join us, you can expect a supportive culture and an attractive range of rewards and benefits which include: • competitive salary with an annual pay review and bonus scheme. • 27 days annual leave (includes a day off for your birthday and another for a religious holiday of your choice) + bank holidays. • auto enrolment into our excellent pension scheme (5% employer matched contribution). • flex-benefits - a range of flexible benefits to choose from, that are most important to you. • group life assurance cover - a massive x4 of salary. • 3 months maternity, paternity & adoption leave all fully paid. • professional qualification study support relevant to your role and career. • perks at work - amazing discounts on cinema tickets, meals out, luxury items etc. • holiday purchase scheme - up to 5 days annually. about us: launched in 2018, jensten is one of the uk's largest independent broking groups. growing organically and through strategic acquisition, we now place over £650m gwp into the market via our retail insurance broking, lloyd's and london market broking, and specialist underwriting arms. we fill the void in the market between a consolidator and a provincial broker. we have the scale, ambition and expertise to stand out, but the people, culture and entrepreneurial dna to maintain our client focus. which is why we are not just one of the leading independent broking groups, we are one of the most exciting too. and that is why you should join us! our goal is to have people in our group that enjoy being part of one team with the shared commitment to delivering insurance distribution excellence. a big part of how we do this is by listening and then acting on what we hear. our evp work is key to maintaining and enhancing our culture and making jensten group a fantastic place to work, learn and grow. #j-18808-ljbffr",oldham,Data Engineer,"['azure', 'business intelligence', 'dashboard', 'data lake', 'data pipeline', 'excel', 'power bi', 'r', 'scala', 'sql', 'sql server']","['azure', 'business intelligence', 'dashboard', 'data lake', 'data pipeline', 'excel', 'power bi', 'r', 'scala', 'sql', 'sql server']",£36k–£60k a year
junior data engineer - 32482,environment agency,"job description the environment agency are fully committed to having an inclusive workforce to reflect the communities we serve. we don't just talk about diversity; we seek it, embrace it, and live it, for the benefit of our staff, our communities, and our environment. the environment agency is undergoing modernisation of systems and it infrastructure and moving towards microsoft azure cloud computing and microsoft analytics tooling and technology such as synapse analytics, fabric and powerbi. this represents a new and exciting opportunity for the water quality digital services team to become a pioneering group delivering new ways to do reporting and analytics. to prepare for this challenge we are recruiting a junior analytics engineer to support the development and deployment of data products and analytics tools to enable the water quality user community to efficiently leverage data from various sources and, in turn, better inform business insights and drive effective policy changes. the team this is an exciting opportunity to join the digital services team within national evidence and business (e&b). we focus on building innovative products that improve origination efficiency and enhance our ability to transform data into meaningful insights for effective decision-making. we’re a dispersed team and work with teams across the country. we promote a positive, inclusive, and supportive culture where everyone feels valued. we use evidence, expertise, engagement, and innovation to enhance delivery. experience/skills required applicant must have: • extensive experience managing and analysing large datasets from multiple sources to support varied reporting and analytics needs. • experience in organising data in star schema data models that effectively represent business entities and relationships and support efficient querying and analysis from relational data warehouses. • experience building and maintaining reliable etl pipelines to integrate data from diverse sources. • strong python expertise, including development of clean, reusable functions and object-oriented code for production-quality solutions. • solid knowledge of sql server, database management, and data warehousing best practices. • proficiency with analytical tools and programming languages to transform raw data into actionable insights. • solve complex problems and formulate efficient and innovative solutions using sound data management and data analysis practices. • ability to work effectively with people from diverse backgrounds while promoting inclusion and collaboration. contact and additional information you’ll have an inclusive incident management objective in your development plan. we’ll help you find a role to suit your needs. appropriate training will be given. you’ll have an ea office base location, as a national role the working location is flexible / hybrid. we use smart tools to stay connected and reduce travel some travel and overnights may be required. please read the candidate / additional information pack for information. any queries, contact andrew.chiverton@environment-agency.gov.uk applications are “blind” assessed purely using your answers to the competency questions. interviews will be held via ms teams within four weeks of the closing date. if you consent to being held on a reserve list, we’ll hold your details for 6 months and may offer you an alternative post. competence 1 focuses on efficiency, innovation and quality description if a large number of applications are received, an initial sift using this lead capability may be conducted. successful candidates will then proceed to a full sift or directly to assessment/interview. describe a time when you implemented a new innovative approach to satisfy a specific data reporting requirement. describe the approach and what you did to ensure the process was resilient, repeatable and quality assured. competence 2 takes decisions and solves problems description give an example of when you have had to solve a complex data analysis problem that involved diverse data sources. what data did you use? what was the problem and what decisions you took to solve it? competence 3 data and information management description provide an example of a complex data management task you have undertaken. explain how you approached this and provide details on the data, tools and techniques used. if you are applying from the civil service please note that the environment agency is not a part of hm civil service and you would not be a crown servant in the event of being appointed. therefore, you will not be eligible for continuous service. for applicants who currently work in local government or other bodies listed in the redundancy payments (continuity of employment in local government etc) (modification) order 1999, you may be eligible for continuous service for the purpose of calculating any future redundancy payment. if you are unsure of your status then you should contact your own hr team. we are fully committed to having a diverse and inclusive workforce to reflect the communities we serve. we welcome flexible working patterns for all our vacancies, including job share, so please include clearly any information regarding your preferred working arrangements on your application. we also have a guaranteed interview policy to support those with a disability who are seeking employment. we have committed to guaranteeing an interview to anyone with a disability whose application meets the minimum criteria for the post. the environment agency, as a non-departmental public body, is committed to providing value for money and utilises central government frameworks and contracts for all external recruitment needs. for this reason, we are unable to engage with the market directly through post, email or phone calls . should you wish to become a support supplier on one of these frameworks or contracts please visit https://www.gov.uk/government/publications/become-a-crown-commercial-service-supplier/becoming-a-supplier-through-the-crown-commercial-service-what-you-need-to-know for more information. artificial intelligence can be a useful tool to support your application, however, all examples and statements provided must be truthful, factually accurate and taken directly from your own experience. where plagiarism has been identified (presenting the ideas and experiences of others, or generated by artificial intelligence, as your own) applications may be withdrawn and internal candidates may be subject to disciplinary action. although the environment agency is a non-departmental public body sponsored by defra, we subscribe to and align with the candidate guidance on the use of artificial intelligence found on the civil service careers website. please review for more information on appropriate and inappropriate use.",tewkesbury,Data Engineer,"['azure', 'cloud', 'data analysis', 'data warehouse', 'etl', 'python', 'r', 'sql', 'sql server']","['azure', 'cloud', 'data analysis', 'data warehouse', 'etl', 'python', 'r', 'sql', 'sql server']",
senior data engineer ( sc cleared/ sc eligible ),scrumconnect consulting,"job information date opened 01/10/2025 job type permanent work experience 5+ years industry public sector and government salary 60k city newcastle upon tyne province tyne and wear country united kingdom postal code ne1 about scrumconnect scrumconnect is a leading force in technology consultancy, proudly contributing to over 20% of the uk’s most significant citizen-facing public services. our award-winning team has made a substantial impact, delivering more than 64 services in the past two years alone. this work has not only reached over 50 million citizens but also achieved considerable savings for the taxpayer, amounting to over £25 million. at scrumconnect, we foster a community of talented consultants who thrive on collaboration, sharing knowledge, and continuous learning to address and solve complex challenges. our mission is to combine advanced software engineering, human-focused design, and data-driven insights to deliver unparalleled service to our clients. role overview we are seeking an experienced senior data engineer to develop, maintain, and optimise data products and a strategic data platform. you will be part of multi-functional agile delivery teams, ensuring operational stability, ongoing support, and enhancement of scalable data solutions. the role requires strong technical expertise in azure data tools, sql, python, and modern engineering practices, with a focus on migration, data governance, and advanced analytics capabilities. key responsibilities • design, build, and maintain data solutions using azure data factory and azure synapse. • manage the end-to-end data development lifecycle within agile delivery teams. • integrate and automate workflows into azure devops pipelines. • create and maintain dimension data models and semantic models for power bi integration. • develop advanced dashboards, visualisations, and reporting solutions using power bi. • implement data governance, quality checks, and profiling to ensure accuracy and compliance. • lead the migration of legacy data capabilities to modern azure-based platforms. • apply dbt with sql databases for data transformation, modelling, and workflow optimisation. • collaborate with stakeholders to translate business requirements into robust technical solutions. • coach and mentor team members, fostering best practices in data engineering. essential skills & experience • active sc clearance (mandatory at application stage). • proven expertise in: • azure data factory & azure synapse • azure devops & microsoft azure ecosystem • power bi (including semantic models) • python (incl. pyspark) and advanced sql • dbt with sql dbs (data transformation & modelling) • dimension data modelling • terraform for infrastructure-as-code deployments • strong experience with both structured and unstructured data. • delivery track record in agile environments. • business analysis skills to capture and translate service needs into technical solutions. • proven success in legacy migration projects within complex organisations. • excellent communication and collaboration skills across technical and non-technical teams. desirable • azure certifications (data engineer, data scientist, or related). knowledge of gdpr compliance, data security, and governance best practices.",newcastle upon tyne,Data Engineer,"['azure', 'dashboard', 'dbt', 'excel', 'power bi', 'pyspark', 'python', 'r', 'scala', 'spark', 'sql']","['azure', 'dashboard', 'dbt', 'excel', 'power bi', 'pyspark', 'python', 'r', 'scala', 'spark', 'sql']",
python data engineer | outside ir35 | london | £700pd | 6 months,linkedin,"python data engineer | outside ir35 | london | £700pd | 6 months our client is a leading global cross asset management house, recently they have had a wider transformation initiative with their japanese office and are launching several projects to streamline data access and automation. one of the key projects involves the development of a scraper tool that extracts the data from various sources some structured and some unstructured sources such as api's, pdf, excel, html, csv etc.in order to build this they are looking for a seasoned python engineer who has a good understanding of data engineering as well to help build this from scratch. you will be joining a small team with clearly defined scopes of what needs to be done, however you will need to be confident within problem solving and being comfortable independently owning parts of the project. in addition to this there will be involvement with the stakeholders across the business so being able to confidently hold technical conversations in meetings is essential.requirements:professional experience as a python developer in a software engineering contextstrong knowledge of web scraping techniques and tools (e.g. beautifulsoup, requests, selenium, or scrapy)understanding of website structures, and how to extract dataknowledge of modern cloud-based data architectures, including data lakehouse on databricks. experience with databricks and azure is highly desirablesource code management e.g. azure devops, gitagile delivery methodologies such as scrum or kanbanknowledge and work management tools (e.g., jira, confluence)experience building tools or reports in data-focused trading or reporting environments (nice to have)contract role:6 months rolling contractoutside ir35up to £700pdhybrid working 2 days a week in office (liverpool street)3 days a week from home start time 8am for at least 2-3 days for interaction with offshore stake holders (this can be flexible on days based on personal commitments etc)if you are interested in the role then please apply with your latest version of your cv!python data engineer | outside ir35 | london | £700pd | 6 months",united kingdom,Data Engineer,"['azure', 'cloud', 'data lake', 'databricks', 'excel', 'python', 'r']","['azure', 'cloud', 'data lake', 'databricks', 'excel', 'python', 'r']",
lead data engineer,ge vernova,"job description summary we are seeking a lead data engineer with solid experience typically gained over a minimum of 5 years in large multinational companies within the energy sector or related industrial domains such as smart infrastructure or industrial automation, with a strong track record of building robust data infrastructures for ai/ml initiatives. in this position, you will be responsible for designing and optimizing data pipelines and platforms that power ai solutions at the edge and in the cloud. you will collaborate closely with r&d, grid automation, and business units to deliver impactful, sustainable solutions across complex energy and industrial systems. job description key responsibilities • design and maintain database structures, schemas, and data models. • apply appropriate storage technologies (relational, nosql, data lakes, etc.) to ensure secure and efficient data management. • build and manage scalable, reliable data pipelines for data cleaning, transformation, feature extraction, and processing of both structured and unstructured data. • integrate data from internal and external apis, ensuring seamless and automated data flows. • identify and onboard new datasets that enhance our ai/ml capabilities and support product development. • automate data integration processes and standardize data transformations based on business-specific needs. • monitor and optimize pipeline performance to ensure scalability and efficiency. • implement data quality checks and adhere to data governance best practices. • collaborate closely with data scientists and ml engineers to ensure delivery of high-quality, relevant data. • work cross-functionally with product management, r&d, and engineering to translate business needs into technical data solutions. must-have qualifications • experience typically gained over +5 years in large multinational companies within the energy sector or related industrial domains such as smart infrastructure or industrial automation. • bachelor’s, master’s, or phd in computer science, electrical/computer engineering, or a related field with a focus on data engineering or electric power systems. • hands-on experience building and managing production-grade data pipelines. • proficiency in python, sql, and one additional language (e.g., scala, java). • strong knowledge of relational databases (e.g., postgresql) and nosql databases (e.g., mongodb, cassandra). • experience working with cloud platforms like aws, azure, or gcp for deploying data systems. • solid understanding and hands-on experience with etl/elt processes and workflow automation. • experience with data architectures supporting genai models. • strong communication and collaboration skills; able to work cross-functionally in fast-paced environments. nice-to-have skills • familiarity with big data technologies like apache spark, kafka, or hadoop. • experience with data visualization tools (e.g., tableau, power bi) for reporting and dashboard creation. • knowledge of graph databases, and cloud-based data warehousing solutions (e.g., snowflake, redshift). • data storytelling and the ability to translate insights into actionable business recommendations. at ge vernova - grid automation, you will have the opportunity to work on cutting-edge projects that shape the future of energy. we offer a collaborative environment where your expertise will be valued, and your contributions will make a tangible impact. join us and be part of a team that is driving innovation and excellence in control systems. about gev grid solutions at gev grid solutions we are electrifying the world with advanced grid technologies. as leaders in the energy space our goal is to accelerate the transition for a more energy efficient grid to full fill the needs of tomorrow. with a focus on growth and sustainability ge grid solutions plays a pivotable role in integrating renewables onto the grid to drive to carbon neutral. in grid solutions we help enable the transition for a greener more reliable grid. ge grid solutions has the most advanced and comprehensive product and solutions portfolio within the energy sector. why we come to work at gev, our engineers are always up for the challenge - and we’re always driven to find the best solution. our projects are unique and interesting, and you’ll need to bring a solution-focused, positive approach to each one to do your best. surrounded by committed, loyal colleagues, if you can dare to bring your ingenuity and desire to make an impact, you’ll be exposed to game-changing, diverse projects that truly allow you to play your part in the energy transition. what we offer a key role in a dynamic, international working environment with a large degree of flexibility of work agreements competitive benefits, and great development opportunities - including private health insurance. additional information relocation assistance provided: no",stafford,Data Engineer,"['aws', 'azure', 'cloud', 'dashboard', 'data lake', 'data pipeline', 'elt', 'etl', 'excel', 'gcp', 'hadoop', 'java', 'kafka', 'power bi', 'python', 'r', 'recommendation', 'redshift', 'scala', 'snowflake', 'spark', 'sql', 'tableau']","['aws', 'azure', 'cloud', 'dashboard', 'data lake', 'data pipeline', 'elt', 'etl', 'excel', 'gcp', 'hadoop', 'java', 'kafka', 'power bi', 'python', 'r', 'recommendation', 'redshift', 'scala', 'snowflake', 'spark', 'sql', 'tableau']",
data engineering manager,fanduel,"our roster has an opening with your name on it we are seeking a data engineering manager to lead a team of data engineers in building high-quality, scalable data products and infrastructure. in this hybrid role, you'll balance people management with technical delivery—mentoring engineers, guiding solution design, and collaborating across teams to deliver data systems that support analytics, data science, and operational needs. this role is ideal for an experienced data engineer or tech lead who is ready to take the next step into engineering management and enjoys blending hands-on support with strategic execution. if you're excited by this challenge and want to work within a dynamic company, then we'd love to hear from you. the game plan everyone on our team has a part to play team management & growth • lead a team of data engineers through coaching, mentorship, and technical guidance • support individual career development and performance feedback, creating growth opportunities for your team • foster a collaborative, inclusive, and high-performance team culture technical oversight & delivery • guide the design and implementation of scalable data pipelines, platforms, and data products • review architecture and code, provide technical direction, and help resolve complex engineering challenges by being hands-on when needed • ensure delivery of high-quality, reliable solutions aligned with business goals and engineering best practices cross-functional collaboration • partner with product managers, data scientists, analysts, and business stakeholders to understand requirements and prioritize work • translate business needs into actionable engineering detailed plans and ensure timely delivery of key projects • communicate clearly across technical and non-technical teams to align on priorities and progress operational excellence • promote operational stability and reliability of data pipelines and systems through monitoring, alerting, and incident response • advocate for high standards in data quality, governance, and compliance by collaborating with platform and data governance teams • drive continuous improvement in development workflows and team productivity the stats what we're looking for in our next teammate • 6+ years of experience in data engineering or software engineering, with at least 1–2 years in a team leadership or mentorship role. • strong technical background in building and maintaining data pipelines and platforms using tools like spark, dbt, airflow, kafka, or databricks. • proficiency in one or more programming languages (e.g., python, scala, or java) and sql. • experience with cloud data platforms (e.g., aws, gcp, or azure). • experience working with behavioural analytics tools and data (e.g., snowplow, amplitude, or google analytics). • strong communication and collaboration skills with a passion for working across teams. preferred skills • prior experience in a people management or tech lead role within a data engineering or analytics team • familiarity with machine learning workflows, data observability, and streaming architectures • experience working in a product-driven, customer-focused organization • understanding of data privacy, security, and compliance frameworks about fanduel fanduel group is the premier mobile gaming company in the united states and canada. fanduel group consists of a portfolio of leading brands across mobile wagering including: america's #1 sportsbook, fanduel sportsbook; its leading igaming platform, fanduel casino; the industry's unquestioned leader in horse racing and advance-deposit wagering, fanduel racing; and its daily fantasy sports product. in addition, fanduel group operates fanduel tv, its broadly distributed linear cable television network and fanduel tv+, its leading direct-to-consumer ott platform. fanduel group has a presence across all 50 states, canada, and puerto rico. the company is based in new york with us offices in los angeles, atlanta, and jersey city, as well as global offices in canada and scotland. the company's affiliates have offices worldwide, including in ireland, portugal, romania, and australia. fanduel group is a subsidiary of flutter entertainment, the world's largest sports betting and gaming operator with a portfolio of globally recognised brands and traded on the new york stock exchange (nyse: flut). player contract we treat our team right from our many opportunities for professional development to our generous insurance and paid leave policies, we're committed to making sure our employees get as much out of fanduel as we ask them to give. competitive compensation is just the beginning. as part of our team, you can expect: • an exciting and fun environment committed to driving real growth • opportunities to build really cool products that fans love • mentorship and professional development resources to help you refine your game • flexible vacation allowance to let you refuel • hall of fame benefit programs and platforms fanduel group is an equal opportunities employer and we believe, as one of our principal states, ""we are one team!."" we are committed to equal employment opportunity regardless of race, color, ethnicity, ancestry, religion, creed, sex, national origin, sexual orientation, age, citizenship status, marital status, disability, gender identity, gender expression, and veteran status. we believe fanduel is strongest and best able to compete if all employees feel valued, respected, and included. we want our team to include diverse individuals because diversity of thought, diversity of perspectives, and diversity of experiences leads to better performance. having a diverse and inclusive workforce is a core value that we believe makes our company stronger and more competitive as one team",edinburgh,Data Engineer,"['airflow', 'aws', 'azure', 'cloud', 'data pipeline', 'databricks', 'dbt', 'excel', 'gcp', 'java', 'kafka', 'machine learning', 'python', 'r', 'scala', 'spark', 'sql']","['airflow', 'aws', 'azure', 'cloud', 'data pipeline', 'databricks', 'dbt', 'excel', 'gcp', 'java', 'kafka', 'machine learning', 'python', 'r', 'scala', 'spark', 'sql']",
data engineer,viridien,"viridien ( www.viridiengroup.com ) is an advanced technology, digital and earth data company that pushes the boundaries of science for a more prosperous and sustainable future. with our ingenuity, drive and deep curiosity we discover new insights, innovations, and solutions that efficiently and responsibly resolve complex natural resource, digital, energy transition and infrastructure challenges. job summary the data engineer plays a n important role in the development of our software solution , used by our clients to help them with their complex data transformation challenges . our system combines the latest ml based techniques with l ogic-based transformation , overseen by domain experts , to provide innovative solution s to our clients. this role supports the development of th e data system focusing on orchestration, resilience and scaling . additionally, we aim to provide a framework on which our data transformation modules can be developed by a growing team of junior engineers and technical smes. the role may also support the implementation of the system s, including deployment and integration with clients ’ own data stores, processes and workflows. team description data h ub is a dynamic team of scientists and developers who love solving complex problems. we provide leading edge technology solutions and services to solve our clients’ data transformation and analytics challenges across a range of industries including geothermal, environmental, hydrocarbon and mineral exploration. y ou will be working in an open and collaborative environment with opportunities to learn, grow , and develop. we have an informal team culture and believe work should be fun and rewarding. you will be based in one of our hub locations (crawley or llandudno) and you will be working alongside our teams of data engineers, machine learning engineers, software engineers and subject matter experts. we offer a hybrid working and remote working can be considered. key responsibilities • contribute to the development of our data platform infrastructure. this includes our orchestration systems, data processing logic and the interactions between system components. • help develop a flexible framework for data transformations by creat ing a modular system where new transformation logic can be easily developed and integrated into our product offering. • build robust data pipelines with a focus on dynamic, end-to-end, metadata driven solutions that consider a wide range of implications, such as downstream application/ui data access patterns, maintainability, monitoring, access control etc. • influence our choice of architecture and technology. you will be expected to communicate design ideas and solutions clearly through architectural diagrams and documentation to both technical and non-technical stakeholders . • awareness of best practices in software and data engineering, writing secure, performant, and maintainable code (python, sql). you will have a keen eye for minimising technical debt and optimising performance where it matters. • partner with data analysts, data scientists, and other end-users to understand their requirements and ensure the platform and its data are accessible, reliable, and meet project delivery needs. • share your work and best practices; collaborate with others ; ensure w hat we build and how we build it aligns to our ambition for growth . qualifications and experience required: • previous experience of designing, building and maintaining data transformation s in a system or product setting . • ability to write secure and performant code in python and sql, and ability to optimise queries and data pipelines. • s ignificant experience using o rchestrators and etl tools , especially airflow • s ignificant rdbms experience (postgresql , oracle ) . experience with other database types such as nosql database ( e.g. neo4j , elastic ) or vector also beneficial • data architecture experience relating to data modelling, data warehousing and schema design (3nf, dimensional modelling, medallion architecture). • experience using docker, vcs (git, gitlab) and k nowledge of ci/cd • e nthusiastic attitude towards learning and the flexibility to adapt to new challenges or changes in direction . preferred : • knowledge of devops and dataops best practices . • kubernetes deployment experience. • microsoft azure and cloud native data technologies, e.g. azure data factory , databricks. • restful api / graphql . • infrastructure as code • previous experience building web applications together with wide-ranging knowledge of web frameworks, http, networking, security etc. benefits package • highly attractive bonus scheme • initial 22 days annual leave with future increases, complemented by a flexible buying and selling holiday program • company contributory pension plan • flexible private medical & dental care tailored to suit individual or family needs • employee assistance program to support our staff we care about our staff and environment we recognise the importance of work life balance for our employees, which is supported through our flexible working and relaxed dress code policies. we recognise and actively support the wellbeing of our staff through many different initiatives; • social club events, spontaneous reward events throughout the year • discounts schemes, including gym membership and a cycle purchase scheme • discounts on nationwide restaurants, cinema tickets and days out through our benefits platform • tech, travel and fashion discounts all available through our benefits platform we encourage and actively support a strong sense of community, through volunteering and various company initiatives, as well as a strong company commitment to protecting our environment through sustainable solutions, energy saving and waste reduction enterprises. our hiring process at viridien, we are committed to delivering a respectful, inclusive, and transparent recruitment experience. due to the high volume of applications we receive, we may not be able to provide individual feedback to every applicant. only candidates whose qualifications closely match the role criteria will be contacted for an interview. we do, however, aim to share personalized feedback with those who progress to the first round of interviews and beyond. we are also dedicated to ensuring that our hiring process accessible to all. if you require any reasonable adjustments to fully participate in the application or interview stages, please don’t hesitate to contact your recruiter directly. we see things differently. diversity fuels our innovation, we value the unique ways in which we differ, and we are committed to equal employment opportunities for all professionals.",llandudno,Data Engineer,"['airflow', 'azure', 'cloud', 'data pipeline', 'databricks', 'etl', 'machine learning', 'python', 'r', 'sql']","['airflow', 'azure', 'cloud', 'data pipeline', 'databricks', 'etl', 'machine learning', 'python', 'r', 'sql']",
data engineer technology – data & reporting · nottingham ·,ensek ltd,"we are a technology business operating in the global energy sector. ensek has become the go-to option for top energy suppliers across the globe. why? because our technology offers a significant step change away from the legacy systems that have historically dominated the market. it’s also much more cost-effective to adopt the ensek solution, with no loss in customer service or standards. but the biggest reason why ensek is the best choice in energy supplier software is because of the people who work here and their enthusiasm, energy, and support for their colleagues. all our clients comment on our great people. our people are our superpower. this is where you come in. role summary reporting to the engineering manager, data engineers are responsible for working together to deliver clean, maintainable code. as part of a squad, you will contribute to the design and implementation of etl solutions for our data warehouse. you will demonstrate your knowledge and practical experience, follow coding standards, and participate in daily scrums, sprint reviews, retrospectives, and refinements. the level of technical skill, competency, and experience varies according to the role level – please refer to the competency framework for details. key responsibilities • working with the product owner to define pbis and subtasks for business-driven work or technical debt, ensuring non-functional requirements are captured appropriately. • collaborating with data architects to understand technical context and review deliverables for alignment and confidence in functionality. • writing new software and modifying existing software to meet new business or technical requirements or fix defects, either autonomously or with squad members. • ensuring data solutions are designed and maintained for optimal performance, scalability, and reliability. • creating, optimizing, and maintaining logical and physical data models, including data warehouses and data lakes. • designing and managing data integration processes for seamless data flow between systems. • identifying and troubleshooting functional or non-functional issues, raising clearly defined defects as appropriate. • working with the engineering manager and product manager to adopt tools and methodologies supporting high-quality software delivery. • aligning with data scientists, data engineers, data analysts, and other stakeholders to understand and address data needs across ensek. • learning and developing expertise within the data engineering domain. • proactively suggesting improvements to data engineering processes at ensek. • promoting a positive 'can do' attitude, building collaborative relationships, and sharing skills and knowledge. • adding value to our clients, colleagues, and stakeholders by practicing and promoting ensek values. skills • experience with sql queries, performance tuning, and investigative processes. • experience with etl processes and tools. • enthusiastic about technology and data engineering best practices. • experience with aws, data warehousing, and data lake solutions (databricks) is advantageous. • strong problem-solving and analytical skills with attention to detail. • excellent verbal and written communication skills. • team player with experience working in remote, multi-disciplinary teams. • detail-oriented with adherence to policies and governance applicable to the role. • a degree in computer science or equivalent work experience is advantageous but not essential. #j-18808-ljbffr",nottingham,Data Engineer,"['aws', 'data lake', 'data warehouse', 'databricks', 'etl', 'excel', 'r', 'scala', 'sql']","['aws', 'data lake', 'data warehouse', 'databricks', 'etl', 'excel', 'r', 'scala', 'sql']",£36k–£60k a year
senior data engineer | ai-first saas scale up,women in data®,"london | hybrid (mon–wed in office) salary up to £80,000 doe we’re partnering with a fast-growing ai-first saas company building a modern data and ai platform used by global clients. as they scale, they’re looking for a senior data engineer who thinks in systems, not projects . someone who understands what it takes to run data and ml pipelines in production, at scale, day after day. sql is the latin of data: essential, universal, the foundation. but python is your superpower: the place where you design algorithms, engineer clean solutions, automate intelligently, and write code that doesn’t just work today… it works next month, next quarter, and under load. this role is for someone who builds with reliability, repeatability, and production readiness at the forefront, not someone who sees delivery as “done” when the notebook runs once. the opportunity • build and optimise robust etl/elt pipelines across azure, aws, gcp, snowflake or databricks • lead ci/cd automation, environment management and reliable deployments • support production-grade ml pipelines that power real decisions • create monitoring, alerting and data-quality controls for high-trust systems • influence engineering culture with clean, scalable, maintainable code all about you • 3+ years in data engineering or cloud platform development • strong sql — but exceptional python, with a real software engineering mindset • an instinct for scaling systems, not just completing projects • experience deploying and maintaining production ml models • understanding of orchestration, workflow tools and modern data architectures • a proactive, improvement driven approach to platform engineering the why ? • shape a next-generation ai and data platform • work in a high-ownership, high impact engineering environment • help create a culture where production quality and pythonic excellence matter however you see your career developing, whether you want to move into leadership, drive architectural direction, or stay hands-on and push the limits of modern data and ai engineering, this company will support and champion you. they’re committed to helping talented engineers grow in the direction that excites them most. still thinking about it… then this is not the role for you…. excited ? apply now !! no sponsorship unfortunately we are proud supporters of women in data®. connect, engage and belong to the largest free female data community in the uk – visit: www.womenindata.co.uk to join our community. stay connected! follow us on linkedin for updates on career opportunities and more.",united kingdom,Data Engineer,"['aws', 'azure', 'cloud', 'databricks', 'elt', 'etl', 'excel', 'gcp', 'python', 'r', 'scala', 'snowflake', 'sql']","['aws', 'azure', 'cloud', 'databricks', 'elt', 'etl', 'excel', 'gcp', 'python', 'r', 'scala', 'snowflake', 'sql']",
junior data engineer,the citation group,"job title: junior data engineer location: remote – 1 day per fortnight in wilmslow office the role following recent investment, the citation group are looking to build out their data capabilities to be best in class. as such, exciting opportunity for a recent graduate who is passionate about data and eager to build a career in data engineering as part of our team. you’ll work alongside experienced engineers and analysts on a variety of projects, gaining hands-on experience with a modern data stack and contributing to both project-based and business-as-usual (bau) work. responsibilities • support the development and maintenance of data pipelines and etl/elt processes • assist in ingestion, transformation, and modelling of data for analytics and ai use cases • help monitor and improve data quality and reliability across ai-enabled systems • participate in the development and testing of new data solutions and improvements • collaborate with analysts, engineers, and stakeholders to understand data requirements • contribute to documentation and best practices across the data engineering function • take ownership tasks and projects with guidance and support from senior team members • support integration of ai technologies (e.g., generative ai, llms, machine learning apis) into internal data workflows the person we’re looking for someone who’s curious, motivated, and passionate about working with data — whatever path you’ve taken to get here. whether you’ve recently graduated, completed a coding bootcamp, are self-taught, or are making a career change into tech, we want to hear from you. you might be a good fit if you: • have a strong interest in data engineering, analytics, or ai • enjoy solving problems and working with data and technology • are eager to learn, take feedback on board, and grow your skills in a supportive environment • can communicate clearly with both technical and non-technical people • are comfortable working in a fast-moving, collaborative team juggling multiple priorities • pay attention to detail and take pride in delivering quality work we value potential, attitude, and willingness to learn over having a “perfect” cv. if you’re excited by the opportunity and meet most of the criteria, we encourage you to apply — even if you don’t tick every box. skills and experience we’d love you to have… but if not? we’ll help you get there: • exposure to sql and database concepts • familiarity with python or another programming language • interest or experience in ai/ml concepts, even from coursework or personal projects • understanding of data pipelines, etl/elt, or data warehousing • awareness of cloud platforms (e.g., aws, azure, or gcp) • understanding of modern data tools (e.g., dbt, airflow, snowflake, bigquery) • version control tools (e.g., git) what you’ll gain • hands-on experience with a modern ai-enabled data stack in a real-world, production environment • the chance to contribute to real-world ai and machine learning projects • exposure to tools and technologies used in deploying and supporting ai applications at scale • exposure to a variety of projects, stakeholders, and business domains • training and mentoring from experienced data engineers about us we are citation. we are far from your average service provider. our colleagues bring their brilliant selves to work every day and we create an environment where they can shine. we are a nice bunch. we don’t do office politics or “that’s not my job”. we listen, support and take ownership. we have been proudly delivering valuable hr and health and safety services to sme’s across the uk for over 20 years. passionate about service, we’re on a mission to revolutionise our colleague’s and client’s experience by employing brilliant people who are experts at what they do and smile whilst they are doing it. working for citation you will have access to 25 days holiday, plus your birthday off work, gym membership discount, healthcare, childcare vouchers, the opportunity to purchase extra leave, pension contributions and more. it’s a great place to work because of the people we employ. fun and professional, we want likeminded individuals who love to love their job and want the company to succeed. so, if our culture sounds like a good fit for you and you want to be part of our success story, then send us your details.",edinburgh,Data Engineer,"['airflow', 'aws', 'azure', 'bigquery', 'cloud', 'data pipeline', 'dbt', 'elt', 'etl', 'gcp', 'machine learning', 'python', 'r', 'snowflake', 'sql']","['airflow', 'aws', 'azure', 'bigquery', 'cloud', 'data pipeline', 'dbt', 'elt', 'etl', 'gcp', 'machine learning', 'python', 'r', 'snowflake', 'sql']",£28.8k–£43.2k a year
